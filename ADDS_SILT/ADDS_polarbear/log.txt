GUIDE learning . . .
w1 ( -2.559032517211812 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8144596229892833) - present_state_Q ( -0.9320463457009742)) * f1( 0.2757337699852224)
w2 ( -2.363228450399798 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8144596229892833) - present_state_Q (-0.9320463457009742)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.50795518336437 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0998376672194934) - present_state_Q ( -1.1845883039540814)) * f1( 0.32438198061607665)
w2 ( -2.3396093823413158 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0998376672194934) - present_state_Q (-1.1845883039540814)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.531149966178694 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6433300883412951) - present_state_Q ( -0.6433300883412951)) * f1( 0.16322825576093564)
w2 ( -2.3538194115462443 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6433300883412951) - present_state_Q (-0.6433300883412951)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.4578527427221077 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.9190570759887366) - present_state_Q ( -1.8950651955894051)) * f1( 0.6092062123784122)
w2 ( -2.335772019226386 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.9190570759887366) - present_state_Q (-1.8950651955894051)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3560287418627017 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.174301675147536) - present_state_Q ( -2.2910902761088554)) * f1( 0.6470520559256915)
w2 ( -2.2885622159685632 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -2.174301675147536) - present_state_Q (-2.2910902761088554)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.1840661530797396 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.013054402194328) - present_state_Q ( -2.013054402194328)) * f1( 0.6115858531772347)
w2 ( -2.218268491919191 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -2.013054402194328) - present_state_Q (-2.013054402194328)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.0416118063630395 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8377832565799546) - present_state_Q ( -1.8377832565799546)) * f1( 0.5367523814931795)
w2 ( -2.138648343991532 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.8377832565799546) - present_state_Q (-1.8377832565799546)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.0425805793066174 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5959032360922671) - present_state_Q ( -1.6397757019578691)) * f1( 0.4889182143487824)
w2 ( -2.1392427826410727 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.5959032360922671) - present_state_Q (-1.6397757019578691)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.06995242355234 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4889339954390628) - present_state_Q ( -1.4889339954390628)) * f1( 0.4147504236696119)
w2 ( -2.159041564764218 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4889339954390628) - present_state_Q (-1.4889339954390628)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.1030711550617043 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0428294700358671) - present_state_Q ( -1.150781548274078)) * f1( 0.34733804851773914)
w2 ( -2.178111592738808 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0428294700358671) - present_state_Q (-1.150781548274078)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.138343067047124 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7955157709982813) - present_state_Q ( -0.7955157709982813)) * f1( 0.27469570410586064)
w2 ( -2.190951950799824 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7955157709982813) - present_state_Q (-0.7955157709982813)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.0960854057529783 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0190672135342234) - present_state_Q ( -1.0190672135342234)) * f1( 0.22041796430969068)
w2 ( -2.143022938495304 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.0190672135342234) - present_state_Q (-1.0190672135342234)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.0654231809184536 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.740687442216891) - present_state_Q ( -0.8897341761079244)) * f1( 0.16887596302734548)
w2 ( -2.097631302698148 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.740687442216891) - present_state_Q (-0.8897341761079244)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.0476063465047933 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0010314305450188) - present_state_Q ( -1.0421806234997129)) * f1( 0.4030251528695142)
w2 ( -2.093210527893696 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0010314305450188) - present_state_Q (-1.0421806234997129)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.082382710004627 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7448794872889509) - present_state_Q ( -0.7448794872889509)) * f1( 0.26155341597459125)
w2 ( -2.1065066125080953 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7448794872889509) - present_state_Q (-0.7448794872889509)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.109723190361001 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46540395605767076) - present_state_Q ( -0.46540395605767076)) * f1( 0.1729166419324841)
w2 ( -2.114412294705836 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.46540395605767076) - present_state_Q (-0.46540395605767076)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.1118751039330625 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31863521605268474) - present_state_Q ( -0.31863521605268474)) * f1( 0.10092063370690847)
w2 ( -2.115478436233599 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.31863521605268474) - present_state_Q (-0.31863521605268474)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.0451845555782717 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8026275859010128) - present_state_Q ( -1.8444415043468625)) * f1( 0.5728548867419805)
w2 ( -2.080553073860896 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.8026275859010128) - present_state_Q (-1.8444415043468625)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -1.9559961086759143 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.833555556500424) - present_state_Q ( -1.833555556500424)) * f1( 0.540470529974529)
w2 ( -2.022796073831133 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.833555556500424) - present_state_Q (-1.833555556500424)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.8608274395945257 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.64702595770293) - present_state_Q ( -1.64702595770293)) * f1( 0.4800865030849724)
w2 ( -1.9534147561634905 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.64702595770293) - present_state_Q (-1.64702595770293)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.8894915756899084 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4995400488561028) - present_state_Q ( -1.4807670342973096)) * f1( 0.42834271071033314)
w2 ( -1.976836300134081 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4995400488561028) - present_state_Q (-1.4807670342973096)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.9133108419777125 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6170189842339266) - present_state_Q ( -1.6170189842339266)) * f1( 0.43730518559131115)
w2 ( -1.9986236167016596 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6170189842339266) - present_state_Q (-1.6170189842339266)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.9382869541171952 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6336995705349397) - present_state_Q ( -1.6719535332895874)) * f1( 0.5082474034584152)
w2 ( -2.0158231915333964 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6336995705349397) - present_state_Q (-1.6719535332895874)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.9600841561631783 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7654414401784653) - present_state_Q ( -1.7852392700252375)) * f1( 0.5570388588207288)
w2 ( -2.0295188621231377 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.7654414401784653) - present_state_Q (-1.7852392700252375)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.9769179024017585 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.873840561826412) - present_state_Q ( -1.9130403311970068)) * f1( 0.6136005567272093)
w2 ( -2.039120892497635 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.873840561826412) - present_state_Q (-1.9130403311970068)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.9831815972086295 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.00215396651434) - present_state_Q ( -2.1041100111392215)) * f1( 0.6517527372152455)
w2 ( -2.0429651079181235 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -2.00215396651434) - present_state_Q (-2.1041100111392215)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.9930109158288363 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0459542438006357) - present_state_Q ( -2.0459542438006357)) * f1( 0.6195943943625253)
w2 ( -2.049310755141301 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -2.0459542438006357) - present_state_Q (-2.0459542438006357)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -2.0016310607345535 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0245930806267607) - present_state_Q ( -2.0644349131557416)) * f1( 0.6245377791027208)
w2 ( -2.054831730937578 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -2.0245930806267607) - present_state_Q (-2.0644349131557416)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -2.0041165675964607 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.1369666427751652) - present_state_Q ( -2.176981623042726)) * f1( 0.6769723738052017)
w2 ( -2.0563003325869698 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -2.1369666427751652) - present_state_Q (-2.176981623042726)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -2.0076786713795447 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.135209280930333) - present_state_Q ( -2.155488562941456)) * f1( 0.6138133046585429)
w2 ( -2.058911789018791 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -2.135209280930333) - present_state_Q (-2.155488562941456)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -2.0280101941767317 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5592330178962623) - present_state_Q ( -1.7651241967981415)) * f1( 0.5202551013424123)
w2 ( -2.072589757693493 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5592330178962623) - present_state_Q (-1.7651241967981415)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.0455013886735203 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5810746386685697) - present_state_Q ( -1.788333614437919)) * f1( 0.47302410713470183)
w2 ( -2.0873807116706504 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5810746386685697) - present_state_Q (-1.788333614437919)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -2.0651343240495024 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.690388439260527) - present_state_Q ( -1.7102136498348566)) * f1( 0.4278957570076212)
w2 ( -2.105733719434298 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.690388439260527) - present_state_Q (-1.7102136498348566)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -2.091697054921359 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1380072689471479) - present_state_Q ( -1.3485806408905776)) * f1( 0.34712537422486067)
w2 ( -2.1286903220144224 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1380072689471479) - present_state_Q (-1.3485806408905776)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.1217609827891017 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8043015642938126) - present_state_Q ( -1.0171705964952549)) * f1( 0.2827524811496215)
w2 ( -2.149955513213105 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8043015642938126) - present_state_Q (-1.0171705964952549)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1487896205613803 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9188970570883216) - present_state_Q ( -0.9188970570883216)) * f1( 0.23042461352221813)
w2 ( -2.1734153661855156 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9188970570883216) - present_state_Q (-0.9188970570883216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.169119091805927 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9737582309940895) - present_state_Q ( -1.0824289993033653)) * f1( 0.20030085092055946)
w2 ( -2.203863770899397 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9737582309940895) - present_state_Q (-1.0824289993033653)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.1807274708182613 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7439993909852723) - present_state_Q ( -0.7439993909852723)) * f1( 0.13979252589259072)
w2 ( -2.220471781861662 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.7439993909852723) - present_state_Q (-0.7439993909852723)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.138891473629185 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.376417875932729) - present_state_Q ( -1.4073457888914092)) * f1( 0.5435335806819045)
w2 ( -2.2127747418486807 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.376417875932729) - present_state_Q (-1.4073457888914092)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.1809878769608773 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2442119766024855) - present_state_Q ( -1.2442119766024855)) * f1( 0.47825451409273384)
w2 ( -2.2215768340592583 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2442119766024855) - present_state_Q (-1.2442119766024855)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.19510309341241 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3200330567494614) - present_state_Q ( -1.3200330567494614)) * f1( 0.45245392790336625)
w2 ( -2.2262563877931405 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.3200330567494614) - present_state_Q (-1.3200330567494614)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.230927063158162 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1452043078071394) - present_state_Q ( -1.1452043078071394)) * f1( 0.36957983981381504)
w2 ( -2.240796129637744 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1452043078071394) - present_state_Q (-1.1452043078071394)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.2639451558514 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0129174213516938) - present_state_Q ( -1.0129174213516938)) * f1( 0.30337074352755317)
w2 ( -2.2571217444494964 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0129174213516938) - present_state_Q (-1.0129174213516938)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.29361595610639 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.905334343740859) - present_state_Q ( -0.905334343740859)) * f1( 0.2503444399298141)
w2 ( -2.2748997308089947 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.905334343740859) - present_state_Q (-0.905334343740859)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.318812582510389 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6310440957946821) - present_state_Q ( -0.6310440957946821)) * f1( 0.17594668437816868)
w2 ( -2.2892203339468424 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6310440957946821) - present_state_Q (-0.6310440957946821)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.2891885718462053 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1749832593839313) - present_state_Q ( -1.3205062109587458)) * f1( 0.4213894508925201)
w2 ( -2.2786752156715373 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1749832593839313) - present_state_Q (-1.3205062109587458)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.271935543568536 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0702090987557211) - present_state_Q ( -1.0738691349590745)) * f1( 0.3695639685592305)
w2 ( -2.2740067334207024 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0702090987557211) - present_state_Q (-1.0738691349590745)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.2880605744953373 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.03843685840627) - present_state_Q ( -1.1521371950773052)) * f1( 0.35698027937460886)
w2 ( -2.280782330782152 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.03843685840627) - present_state_Q (-1.1521371950773052)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.2251456473584112 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1343310725754736) - present_state_Q ( -1.1592946485477489)) * f1( 0.4069894065959884)
w2 ( -2.26532371536925 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1343310725754736) - present_state_Q (-1.1592946485477489)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.1290463309490786 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2379685554283502) - present_state_Q ( -1.2379685554283502)) * f1( 0.4545483056770485)
w2 ( -2.244181998370395 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2379685554283502) - present_state_Q (-1.2379685554283502)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.0303955158675095 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3032246712249578) - present_state_Q ( -1.3032246712249578)) * f1( 0.4540048553281188)
w2 ( -2.211588465308858 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3032246712249578) - present_state_Q (-1.3032246712249578)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9454350920935255 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1702209318232617) - present_state_Q ( -1.171417320612211)) * f1( 0.41355442535890347)
w2 ( -2.18077253689741 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1702209318232617) - present_state_Q (-1.171417320612211)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8580062803072237 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.098933758955428) - present_state_Q ( -1.2361073064231223)) * f1( 0.411194802794882)
w2 ( -2.138248258286858 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.098933758955428) - present_state_Q (-1.2361073064231223)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.8914362200787396 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2145733526397522) - present_state_Q ( -1.2362013692572864)) * f1( 0.37763021154565474)
w2 ( -2.160379657437025 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2145733526397522) - present_state_Q (-1.2362013692572864)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.8437824927252038 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.108135934858673) - present_state_Q ( -1.1324456458995233)) * f1( 0.3131751022065168)
w2 ( -2.122338856126684 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.108135934858673) - present_state_Q (-1.1324456458995233)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.8708809420713122 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9760814262964544) - present_state_Q ( -0.9760814262964544)) * f1( 0.24162107733560093)
w2 ( -2.1503770240350137 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9760814262964544) - present_state_Q (-0.9760814262964544)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.8926288667003388 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.748871218386657) - present_state_Q ( -0.877358931731365)) * f1( 0.18160678645133196)
w2 ( -2.180315228787696 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.748871218386657) - present_state_Q (-0.877358931731365)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.903537726503795 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42225927703327604) - present_state_Q ( -0.5312750384726609)) * f1( 0.10790692129226727)
w2 ( -2.195479492126156 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.42225927703327604) - present_state_Q (-0.5312750384726609)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.268459585080396 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.503485452942099) - present_state_Q ( -1.5405402426945363)) * f1( 0.5656421822068388)
w2 ( -2.280318416972839 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.503485452942099) - present_state_Q (-1.5405402426945363)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.157452472119993 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3602017636457329) - present_state_Q ( -1.3602017636457329)) * f1( 0.49909195182259497)
w2 ( -2.2580766011000275 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3602017636457329) - present_state_Q (-1.3602017636457329)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.059396697031689 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3096950690366544) - present_state_Q ( -1.3096950690366544)) * f1( 0.45006024068633393)
w2 ( -2.225395717668033 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3096950690366544) - present_state_Q (-1.3096950690366544)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.078266721654968 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.111322427376038) - present_state_Q ( -1.111322427376038)) * f1( 0.3775440986413649)
w2 ( -2.232892864898456 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.111322427376038) - present_state_Q (-1.111322427376038)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.1205244283766875 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0706883635278122) - present_state_Q ( -1.0706883635278122)) * f1( 0.40774317762407547)
w2 ( -2.243256669626706 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0706883635278122) - present_state_Q (-1.0706883635278122)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.14225206120826 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.035794677751166) - present_state_Q ( -1.035794677751166)) * f1( 0.3826737385947939)
w2 ( -2.248934517526945 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.035794677751166) - present_state_Q (-1.035794677751166)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.173606442176014 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0672961140328856) - present_state_Q ( -1.157118207292549)) * f1( 0.3301811755000555)
w2 ( -2.26792674560916 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0672961140328856) - present_state_Q (-1.157118207292549)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1703580501559725 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2295226679920979) - present_state_Q ( -1.2295226679920979)) * f1( 0.3048118411567336)
w2 ( -2.2652624855793375 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -1.2295226679920979) - present_state_Q (-1.2295226679920979)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.1825924070734644 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1712181992891801) - present_state_Q ( -1.1942812210009923)) * f1( 0.2893373282629699)
w2 ( -2.275833500552536 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1712181992891801) - present_state_Q (-1.1942812210009923)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.1539830497447725 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0116587147669132) - present_state_Q ( -1.0116587147669132)) * f1( 0.20283234661407773)
w2 ( -2.2405711794702805 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0116587147669132) - present_state_Q (-1.0116587147669132)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.127857484026388 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8749676474890875) - present_state_Q ( -0.8749676474890875)) * f1( 0.1461593918572485)
w2 ( -2.195884407401776 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.8749676474890875) - present_state_Q (-0.8749676474890875)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.097066349911889 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6195242871284032) - present_state_Q ( -0.6195242871284032)) * f1( 0.2911493329694821)
w2 ( -2.195884407401776 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6195242871284032) - present_state_Q (-0.6195242871284032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0617020482487423 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.310736542324638) - present_state_Q ( -1.310736542324638)) * f1( 0.520321210452071)
w2 ( -2.1890877785208542 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.310736542324638) - present_state_Q (-1.310736542324638)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.100800028285152 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8924382974766889) - present_state_Q ( -0.8924382974766889)) * f1( 0.32668615729257056)
w2 ( -2.201055833843564 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8924382974766889) - present_state_Q (-0.8924382974766889)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.0421843607575862 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.573421406754611) - present_state_Q ( -1.615427691061003)) * f1( 0.6117999327302048)
w2 ( -2.186684550587781 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.573421406754611) - present_state_Q (-1.615427691061003)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.007656985902012 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.288129057889005) - present_state_Q ( -1.288129057889005)) * f1( 0.5236846503091869)
w2 ( -2.18009138906678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.288129057889005) - present_state_Q (-1.288129057889005)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.9514808199683549 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2948216065349998) - present_state_Q ( -1.2948216065349998)) * f1( 0.48205839193200645)
w2 ( -2.162611297378558 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.2948216065349998) - present_state_Q (-1.2948216065349998)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9883949079262977 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3088281827457824) - present_state_Q ( -1.3088281827457824)) * f1( 0.44904664924366555)
w2 ( -2.179052390089134 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3088281827457824) - present_state_Q (-1.3088281827457824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.024311358928637 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3946260896045337) - present_state_Q ( -1.3946260896045337)) * f1( 0.48220582730553163)
w2 ( -2.1939491204762525 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3946260896045337) - present_state_Q (-1.3946260896045337)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.0578382543653504 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.506022896944845) - present_state_Q ( -1.5268312044355203)) * f1( 0.5374871684344614)
w2 ( -2.206424542181432 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.506022896944845) - present_state_Q (-1.5268312044355203)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.087577066220195 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.618334719582223) - present_state_Q ( -1.6594630782965778)) * f1( 0.5919698340120443)
w2 ( -2.216471950054665 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.618334719582223) - present_state_Q (-1.6594630782965778)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1122721798763213 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.751544480503939) - present_state_Q ( -1.7932753316378076)) * f1( 0.6466735832038885)
w2 ( -2.2241095323829168 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.751544480503939) - present_state_Q (-1.7932753316378076)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1294070700187846 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.929403189648313) - present_state_Q ( -1.929403189648313)) * f1( 0.6501888438605475)
w2 ( -2.2306979606158297 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.929403189648313) - present_state_Q (-1.929403189648313)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.1423095547072473 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.964263687594603) - present_state_Q ( -2.0068365111892703)) * f1( 0.680547200879975)
w2 ( -2.2354377070550844 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.964263687594603) - present_state_Q (-2.0068365111892703)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.1575834905954863 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.934572625789274) - present_state_Q ( -1.934572625789274)) * f1( 0.58999004644101)
w2 ( -2.243204246158774 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.934572625789274) - present_state_Q (-1.934572625789274)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.1716733318967303 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.9105672036713361) - present_state_Q ( -1.953692192561351)) * f1( 0.5935950679527313)
w2 ( -2.250325181992947 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.9105672036713361) - present_state_Q (-1.953692192561351)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.185708616377 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.924272361886791) - present_state_Q ( -1.924272361886791)) * f1( 0.523402176328475)
w2 ( -2.259710602593513 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.924272361886791) - present_state_Q (-1.924272361886791)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.203168145669847 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6687419684213085) - present_state_Q ( -1.7817274985509841)) * f1( 0.4533215361916075)
w2 ( -2.273190737033703 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6687419684213085) - present_state_Q (-1.7817274985509841)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.2345314675301085 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.171509953471312) - present_state_Q ( -1.2851694903229973)) * f1( 0.37697138302793654)
w2 ( -2.2898303671341855 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.171509953471312) - present_state_Q (-1.2851694903229973)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.270607365607161 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9132668829991465) - present_state_Q ( -0.9132668829991465)) * f1( 0.3062314656244633)
w2 ( -2.3016109651871934 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9132668829991465) - present_state_Q (-0.9132668829991465)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3006005872014046 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.798539289618774) - present_state_Q ( -0.9457301611243009)) * f1( 0.26446162618945396)
w2 ( -2.318622821704757 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.798539289618774) - present_state_Q (-0.9457301611243009)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.318635078942367 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3257685450388035) - present_state_Q ( -1.3257685450388035)) * f1( 0.22352882995118475)
w2 ( -2.346861112536035 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3257685450388035) - present_state_Q (-1.3257685450388035)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.283196238982247 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2140594941850147) - present_state_Q ( -1.2140594941850147)) * f1( 0.16934881575953356)
w2 ( -2.273618238469207 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2140594941850147) - present_state_Q (-1.2140594941850147)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.2942983989634413 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40768492094877123) - present_state_Q ( -0.635046744795692)) * f1( 0.07897836113387738)
w2 ( -2.3017326734151906 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.40768492094877123) - present_state_Q (-0.635046744795692)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.265274785666407 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6641338111738248) - present_state_Q ( -0.7792204448445843)) * f1( 0.23930940184202867)
w2 ( -2.2896046027779184 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6641338111738248) - present_state_Q (-0.7792204448445843)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.247411538677807 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4981778674375306) - present_state_Q ( -1.4981778674375306)) * f1( 0.5526764089051769)
w2 ( -2.293249072608253 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.4981778674375306) - present_state_Q (-1.4981778674375306)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.327340262474661 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0876876267134896) - present_state_Q ( -1.0876876267134896)) * f1( 0.3235968460060113)
w2 ( -2.3170488904545583 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0876876267134896) - present_state_Q (-1.0876876267134896)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3441360900831296 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9809699947011774) - present_state_Q ( -0.9809699947011774)) * f1( 0.2721616049642376)
w2 ( -2.3263057955260926 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.9809699947011774) - present_state_Q (-0.9809699947011774)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.371593635112482 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8829265628012275) - present_state_Q ( -0.8829265628012275)) * f1( 0.22779423760050427)
w2 ( -2.344386286928276 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8829265628012275) - present_state_Q (-0.8829265628012275)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3990964828014234 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8996107262120452) - present_state_Q ( -0.8996107262120452)) * f1( 0.23104834448032033)
w2 ( -2.3622415421244134 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8996107262120452) - present_state_Q (-0.8996107262120452)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.2405084978372862 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5005573530986878) - present_state_Q ( -1.5252210981859036)) * f1( 0.614625571439286)
w2 ( -2.2973568466008105 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.5005573530986878) - present_state_Q (-1.5252210981859036)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.1737710389766165 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3200685779012102) - present_state_Q ( -1.346274981749876)) * f1( 0.5496105641234952)
w2 ( -2.2912855059810115 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.3200685779012102) - present_state_Q (-1.346274981749876)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.1809822613099468 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3892383245240871) - present_state_Q ( -1.5038025998231377)) * f1( 0.5336853574386339)
w2 ( -2.2933123244704507 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.3892383245240871) - present_state_Q (-1.5038025998231377)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.0741282021368663 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3824394053133968) - present_state_Q ( -1.3824394053133968)) * f1( 0.4761352602744771)
w2 ( -2.2596493924987198 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3824394053133968) - present_state_Q (-1.3824394053133968)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9838191607669524 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2285133789263392) - present_state_Q ( -1.2285133789263392)) * f1( 0.4288866855650762)
w2 ( -2.228064461883214 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2285133789263392) - present_state_Q (-1.2285133789263392)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.877333704133183 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2829354205014107) - present_state_Q ( -1.3047945248809942)) * f1( 0.4892506710255183)
w2 ( -2.1954169471407514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2829354205014107) - present_state_Q (-1.3047945248809942)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8246076478705608 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8558222711441597) - present_state_Q ( -0.9655931185011971)) * f1( 0.28045612131389874)
w2 ( -2.1578167293130157 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.8558222711441597) - present_state_Q (-0.9655931185011971)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.8540297023750851 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7210079356933734) - present_state_Q ( -0.7210079356933734)) * f1( 0.21776485852185157)
w2 ( -2.178083122181155 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7210079356933734) - present_state_Q (-0.7210079356933734)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.879270899163213 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6234927333278508) - present_state_Q ( -0.6605556897253831)) * f1( 0.18006357771428555)
w2 ( -2.199110025935266 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6234927333278508) - present_state_Q (-0.6605556897253831)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9020290536172644 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45835012280728005) - present_state_Q ( -0.4958390613664009)) * f1( 0.1468271864879818)
w2 ( -2.2146099854444095 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.45835012280728005) - present_state_Q (-0.4958390613664009)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3324667859403854 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.059463436806716) - present_state_Q ( -1.105334474443705)) * f1( 0.38145047196022186)
w2 ( -2.31173879210756 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.059463436806716) - present_state_Q (-1.105334474443705)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.368186804772634 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9710855765531607) - present_state_Q ( -0.9710855765531607)) * f1( 0.3172228225509728)
w2 ( -2.3229990219185814 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9710855765531607) - present_state_Q (-0.9710855765531607)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3997673550016128 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0484042665516142) - present_state_Q ( -1.073869035307902)) * f1( 0.30631839538932026)
w2 ( -2.3384635927887905 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0484042665516142) - present_state_Q (-1.073869035307902)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.423702345922141 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1146789208669097) - present_state_Q ( -1.2561987991703805)) * f1( 0.2798533364384113)
w2 ( -2.359845320111698 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1146789208669097) - present_state_Q (-1.2561987991703805)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.446817772802166 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8593440098819136) - present_state_Q ( -0.9773362758874985)) * f1( 0.20851042732843617)
w2 ( -2.382017282613712 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8593440098819136) - present_state_Q (-0.9773362758874985)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.4650387344615097 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7185287523122549) - present_state_Q ( -0.8376296164429404)) * f1( 0.14763100216756703)
w2 ( -2.4067017477894774 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7185287523122549) - present_state_Q (-0.8376296164429404)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.4947960033064036 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3286275133944199) - present_state_Q ( -1.4735207167313227)) * f1( 0.4513176361124934)
w2 ( -2.416591878308599 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3286275133944199) - present_state_Q (-1.4735207167313227)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.5216420215011595 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4612885679440006) - present_state_Q ( -1.4612885679440006)) * f1( 0.3920040720708856)
w2 ( -2.4302886840856073 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4612885679440006) - present_state_Q (-1.4612885679440006)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.548043903699518 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4344339306035367) - present_state_Q ( -1.4595637114384918)) * f1( 0.38606033938228557)
w2 ( -2.4439662777180446 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4344339306035367) - present_state_Q (-1.4595637114384918)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.5677669327253554 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5996848559832615) - present_state_Q ( -1.6973789422094145)) * f1( 0.42636132415244965)
w2 ( -2.4555310163027673 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5996848559832615) - present_state_Q (-1.6973789422094145)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.5862428500393646 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7638230764693965) - present_state_Q ( -1.7638230764693965)) * f1( 0.4478367205909885)
w2 ( -2.465844997082206 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.7638230764693965) - present_state_Q (-1.7638230764693965)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.6077255680238567 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5845355953957254) - present_state_Q ( -1.5845355953957254)) * f1( 0.37431687674281594)
w2 ( -2.480192946185802 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5845355953957254) - present_state_Q (-1.5845355953957254)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.633206098059899 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2776684943240562) - present_state_Q ( -1.2776684943240562)) * f1( 0.2997362585508634)
w2 ( -2.4971949132879687 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2776684943240562) - present_state_Q (-1.2776684943240562)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.6541273320656855 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.150120428128436) - present_state_Q ( -1.30141070174286)) * f1( 0.25714355360172997)
w2 ( -2.517534946814718 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.150120428128436) - present_state_Q (-1.30141070174286)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.6756175662333366 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7153642331331831) - present_state_Q ( -0.8412409804739192)) * f1( 0.17467539437562962)
w2 ( -2.5359893784573093 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7153642331331831) - present_state_Q (-0.8412409804739192)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.6743369841970166 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6582845270340851) - present_state_Q ( -0.6801389194652925)) * f1( 0.1120266649761396)
w2 ( -2.534274721455881 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6582845270340851) - present_state_Q (-0.6801389194652925)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.6597656762994375 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38406054948205814) - present_state_Q ( -0.4106838466101369)) * f1( 0.10618336889305904)
w2 ( -2.5274133324975714 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.38406054948205814) - present_state_Q (-0.4106838466101369)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.651349918150673 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3023314726243151) - present_state_Q ( -0.3023314726243151)) * f1( 0.06615650678079762)
w2 ( -2.521052840870762 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3023314726243151) - present_state_Q (-0.3023314726243151)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.6840886722985027 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1904212128014982) - present_state_Q ( -1.2434041204158777)) * f1( 0.3738845746246261)
w2 ( -2.5298092208794047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1904212128014982) - present_state_Q (-1.2434041204158777)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.71544127474085 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0520142418315408) - present_state_Q ( -1.0520142418315408)) * f1( 0.2976925941345123)
w2 ( -2.540341092702921 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0520142418315408) - present_state_Q (-1.0520142418315408)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.7386791258642744 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0847418875744796) - present_state_Q ( -1.2117589422096258)) * f1( 0.25914415097641863)
w2 ( -2.5582753976338775 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0847418875744796) - present_state_Q (-1.2117589422096258)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.7577256097682237 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0632143135582344) - present_state_Q ( -1.245592459712468)) * f1( 0.22128317427940716)
w2 ( -2.5797936219249613 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0632143135582344) - present_state_Q (-1.245592459712468)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.6641889675896664 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0219756505770263) - present_state_Q ( -2.047042597281879)) * f1( 0.6955198549818146)
w2 ( -2.5730693967638403 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -2.0219756505770263) - present_state_Q (-2.047042597281879)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.6797517432355153 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.9797077248117583) - present_state_Q ( -1.9535329779599961)) * f1( 0.6366763239839607)
w2 ( -2.575513774709052 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.9797077248117583) - present_state_Q (-1.9535329779599961)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.6107697536023604 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8939560165280795) - present_state_Q ( -1.9058922926463184)) * f1( 0.5670544781902995)
w2 ( -2.557266324344149 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.8939560165280795) - present_state_Q (-1.9058922926463184)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.488849225011519 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5275842596579516) - present_state_Q ( -1.6554475758751592)) * f1( 0.4871580979014398)
w2 ( -2.5197259870955087 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.5275842596579516) - present_state_Q (-1.6554475758751592)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3938999908975083 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1980711143149332) - present_state_Q ( -1.3240574136697087)) * f1( 0.4307552278323313)
w2 ( -2.4976834840731263 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1980711143149332) - present_state_Q (-1.3240574136697087)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.425939191300542 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3172261315325313) - present_state_Q ( -1.3227390613074883)) * f1( 0.39604266773946045)
w2 ( -2.509818237350813 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3172261315325313) - present_state_Q (-1.3227390613074883)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.4566876650876845 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4142675906320392) - present_state_Q ( -1.4401303199762243)) * f1( 0.43845187389193263)
w2 ( -2.5203376839371177 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4142675906320392) - present_state_Q (-1.4401303199762243)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.4792373191683117 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5625528371387363) - present_state_Q ( -1.688569721335592)) * f1( 0.48215416285158536)
w2 ( -2.5296913951846833 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5625528371387363) - present_state_Q (-1.688569721335592)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.5033776548839013 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6991487413647315) - present_state_Q ( -1.7251746730510606)) * f1( 0.5427963484450917)
w2 ( -2.5363624982009645 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6991487413647315) - present_state_Q (-1.7251746730510606)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.5156016944449826 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8499272117759085) - present_state_Q ( -1.9767453366859566)) * f1( 0.5869960667656088)
w2 ( -2.540527445890797 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.8499272117759085) - present_state_Q (-1.9767453366859566)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.342951023413354 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0397034891273345) - present_state_Q ( -2.0397034891273345)) * f1( 0.6088396280425831)
w2 ( -2.4838127830865053 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -2.0397034891273345) - present_state_Q (-2.0397034891273345)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.3638939411464035 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.77636366007237) - present_state_Q ( -1.801582800175925)) * f1( 0.5569131537618239)
w2 ( -2.4913338544031314 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.77636366007237) - present_state_Q (-1.801582800175925)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.364839287669548 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6450141311191269) - present_state_Q ( -1.6450141311191269)) * f1( 0.485109479862015)
w2 ( -2.491723600042987 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.6450141311191269) - present_state_Q (-1.6450141311191269)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.3924149138612285 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4957879917118189) - present_state_Q ( -1.4957879917118189)) * f1( 0.42178057380218875)
w2 ( -2.5047994161921743 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4957879917118189) - present_state_Q (-1.4957879917118189)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.4201421049651777 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4218493278208828) - present_state_Q ( -1.4218493278208828)) * f1( 0.3849204580890118)
w2 ( -2.5192061282913984 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4218493278208828) - present_state_Q (-1.4218493278208828)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.4473910819861207 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2816899583612011) - present_state_Q ( -1.303913370318117)) * f1( 0.33058891170828547)
w2 ( -2.5356912408017585 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2816899583612011) - present_state_Q (-1.303913370318117)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.4729074157811586 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1609485057601612) - present_state_Q ( -1.1609485057601612)) * f1( 0.26714580371406177)
w2 ( -2.5547941676980757 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1609485057601612) - present_state_Q (-1.1609485057601612)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.494574662883092 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.878673145535364) - present_state_Q ( -1.0064128539202677)) * f1( 0.20035283861371142)
w2 ( -2.576423256910741 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.878673145535364) - present_state_Q (-1.0064128539202677)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.5142338799820982 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8933740663779559) - present_state_Q ( -0.9431499642925062)) * f1( 0.17151834309735786)
w2 ( -2.5993470057576467 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8933740663779559) - present_state_Q (-0.9431499642925062)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.5345150040545836 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0190659720945403) - present_state_Q ( -1.044084087071825)) * f1( 0.36357665214122853)
w2 ( -2.602136118308335 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0190659720945403) - present_state_Q (-1.044084087071825)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.546565617457059 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2890780099337016) - present_state_Q ( -1.2890780099337016)) * f1( 0.3546073275359058)
w2 ( -2.60723356517423 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2890780099337016) - present_state_Q (-1.2890780099337016)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.576084792272482 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2517384573087946) - present_state_Q ( -1.2517384573087946)) * f1( 0.33796632477591076)
w2 ( -2.620335096000561 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2517384573087946) - present_state_Q (-1.2517384573087946)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.6032846742661078 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0669687964407646) - present_state_Q ( -1.0669687964407646)) * f1( 0.26160572589157133)
w2 ( -2.635931017248611 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0669687964407646) - present_state_Q (-1.0669687964407646)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.605351868718728 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9557250400326995) - present_state_Q ( -1.0076814787181563)) * f1( 0.23519971987061916)
w2 ( -2.6372493826278873 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.9557250400326995) - present_state_Q (-1.0076814787181563)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.5961469785470075 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9693637988958796) - present_state_Q ( -0.9962170219120692)) * f1( 0.23053685059948023)
w2 ( -2.63126017299755 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.9693637988958796) - present_state_Q (-0.9962170219120692)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.592300069254717 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8178142133585453) - present_state_Q ( -0.8178142133585453)) * f1( 0.16298198480492979)
w2 ( -2.6277196811172097 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8178142133585453) - present_state_Q (-0.8178142133585453)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.600532607640733 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2512531933672135) - present_state_Q ( -1.2857289826449525)) * f1( 0.24256414981558405)
w2 ( -2.636204589534504 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2512531933672135) - present_state_Q (-1.2857289826449525)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.6048946361173884 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7980999860252083) - present_state_Q ( -0.7980999860252083)) * f1( 0.15484108771100702)
w2 ( -2.640430239723164 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.7980999860252083) - present_state_Q (-0.7980999860252083)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.591609064965431 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5671091960195402) - present_state_Q ( -0.5953216694010207)) * f1( 0.17896681439295767)
w2 ( -2.6275266273682143 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.5671091960195402) - present_state_Q (-0.5953216694010207)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.5685206751304954 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5352126552300536) - present_state_Q ( -0.5352126552300536)) * f1( 0.1558245529084185)
w2 ( -2.620118170419679 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5352126552300536) - present_state_Q (-0.5352126552300536)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.5548177500674902 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3913003792564711) - present_state_Q ( -0.3913003792564711)) * f1( 0.10134022795913943)
w2 ( -2.613357318713025 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3913003792564711) - present_state_Q (-0.3913003792564711)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.4988385101168964 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1694775198863598) - present_state_Q ( -1.1776798018074037)) * f1( 0.35867296988675385)
w2 ( -2.5977499982148373 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1694775198863598) - present_state_Q (-1.1776798018074037)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.474764776240854 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2045243208746346) - present_state_Q ( -1.2364087526425749)) * f1( 0.3908350815256981)
w2 ( -2.591590435009286 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2045243208746346) - present_state_Q (-1.2364087526425749)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.5010287372859588 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.308323609239179) - present_state_Q ( -1.333481070959229)) * f1( 0.3293900865986938)
w2 ( -2.60753746080858 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.308323609239179) - present_state_Q (-1.333481070959229)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.522400606010137 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4055520269944564) - present_state_Q ( -1.5359289000348855)) * f1( 0.3534723618538182)
w2 ( -2.6226531183751938 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4055520269944564) - present_state_Q (-1.5359289000348855)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.54171410868908 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5472514330811356) - present_state_Q ( -1.6783840889998953)) * f1( 0.4054553455820042)
w2 ( -2.634561644732899 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5472514330811356) - present_state_Q (-1.6783840889998953)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.5529217897327436 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8134648864011393) - present_state_Q ( -1.9317697418743345)) * f1( 0.44906751886550916)
w2 ( -2.642048947135873 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.8134648864011393) - present_state_Q (-1.9317697418743345)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.566736278972639 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8525060096488926) - present_state_Q ( -1.8525060096488926)) * f1( 0.4151679576596379)
w2 ( -2.6520312848753527 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.8525060096488926) - present_state_Q (-1.8525060096488926)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.582595874536519 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7138800209392595) - present_state_Q ( -1.7406323311315717)) * f1( 0.36818077237261804)
w2 ( -2.6649539550042234 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.7138800209392595) - present_state_Q (-1.7406323311315717)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.5997601157801005 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4252546326761477) - present_state_Q ( -1.558502330426359)) * f1( 0.2938965989254154)
w2 ( -2.682474648989461 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4252546326761477) - present_state_Q (-1.558502330426359)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.6136831349597855 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4312167096173252) - present_state_Q ( -1.5653404420667982)) * f1( 0.2409738925979726)
w2 ( -2.7026969920007837 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4312167096173252) - present_state_Q (-1.5653404420667982)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.630798466816193 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5477679426119213) - present_state_Q ( -1.5477679426119213)) * f1( 0.2819618167001047)
w2 ( -2.720907257550262 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5477679426119213) - present_state_Q (-1.5477679426119213)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.6282124277421435 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7338888227181573) - present_state_Q ( -1.7578469711874662)) * f1( 0.3061919950180489)
w2 ( -2.717951224438214 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.7338888227181573) - present_state_Q (-1.7578469711874662)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.6417447760765835 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6019555520013964) - present_state_Q ( -1.6542713382991234)) * f1( 0.26747777399016215)
w2 ( -2.7356585720297497 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6019555520013964) - present_state_Q (-1.6542713382991234)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.6548764053731246 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4696687025055857) - present_state_Q ( -1.4696687025055857)) * f1( 0.19388254570748317)
w2 ( -2.7593640079008237 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4696687025055857) - present_state_Q (-1.4696687025055857)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.6659098013311984 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.204191377610473) - present_state_Q ( -1.342159578005514)) * f1( 0.14177013079722925)
w2 ( -2.7866030924922676 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.204191377610473) - present_state_Q (-1.342159578005514)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.676269687871536 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2630194207095276) - present_state_Q ( -1.316264918699699)) * f1( 0.1278939880700964)
w2 ( -2.8149543883102615 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2630194207095276) - present_state_Q (-1.316264918699699)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.6782372712740923 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6079455698313709) - present_state_Q ( -0.8894410086623972)) * f1( 0.01679751946264858)
w2 ( -2.8500949947598837 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6079455698313709) - present_state_Q (-0.8894410086623972)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.5133404717431973 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0508217320924702) - present_state_Q ( -1.2155716129085061)) * f1( 0.3735044822715811)
w2 ( -2.6022524243160325 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.0508217320924702) - present_state_Q (-1.2155716129085061)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.5719221529441407 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.863307434109001) - present_state_Q ( -0.8800539683977859)) * f1( 0.24217705412064675)
w2 ( -2.620420086463156 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.863307434109001) - present_state_Q (-0.8800539683977859)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.5849456934496127 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.847261485059863) - present_state_Q ( -0.847261485059863)) * f1( 0.1765988412870342)
w2 ( -2.631482056414848 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.847261485059863) - present_state_Q (-0.847261485059863)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.602382482234556 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7503951568987546) - present_state_Q ( -0.914766706569976)) * f1( 0.15028180138229227)
w2 ( -2.654687512597246 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7503951568987546) - present_state_Q (-0.914766706569976)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.536297905923427 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8674136743139689) - present_state_Q ( -0.8674136743139689)) * f1( 0.2372294236748025)
w2 ( -2.6055505956441993 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.8674136743139689) - present_state_Q (-0.8674136743139689)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.521168033303941 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.131342751133424) - present_state_Q ( -1.131342751133424)) * f1( 0.2919649777959288)
w2 ( -2.597777468503898 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.131342751133424) - present_state_Q (-1.131342751133424)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.509057562676586 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1202856080778827) - present_state_Q ( -1.1202856080778827)) * f1( 0.23827452452261105)
w2 ( -2.5876123275584964 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1202856080778827) - present_state_Q (-1.1202856080778827)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.499282369884069 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0063173419134372) - present_state_Q ( -1.0560599986136154)) * f1( 0.21463737664408972)
w2 ( -2.578503762270051 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0063173419134372) - present_state_Q (-1.0560599986136154)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.4659366202170916 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0693156993383164) - present_state_Q ( -1.0693156993383164)) * f1( 0.16992468073565578)
w2 ( -2.529444159034939 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.0693156993383164) - present_state_Q (-1.0693156993383164)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.446778772624783 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8941307287150417) - present_state_Q ( -0.8941307287150417)) * f1( 0.1061542647974715)
w2 ( -2.4843262176388503 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.8941307287150417) - present_state_Q (-0.8941307287150417)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.4708113996670007 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6306436881805879) - present_state_Q ( -0.6306436881805879)) * f1( 0.25774446600420053)
w2 ( -2.4843262176388503 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6306436881805879) - present_state_Q (-0.6306436881805879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4156611129156613 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3321045930116102) - present_state_Q ( -1.3595724159040796)) * f1( 0.44970643825342016)
w2 ( -2.4720625980728212 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.3321045930116102) - present_state_Q (-1.3595724159040796)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.4274903777084953 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1983590278511544) - present_state_Q ( -1.3182186946076282)) * f1( 0.3921946252441007)
w2 ( -2.4765868561954836 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1983590278511544) - present_state_Q (-1.3182186946076282)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.4264442976928073 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1189829937203704) - present_state_Q ( -1.14034446954872)) * f1( 0.36774019461689894)
w2 ( -2.476302394493717 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -1.1189829937203704) - present_state_Q (-1.14034446954872)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3736186960733665 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2547448890813269) - present_state_Q ( -1.2731806250397075)) * f1( 0.3206008672363311)
w2 ( -2.4433482717710855 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2547448890813269) - present_state_Q (-1.2731806250397075)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.322303869816598 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1005789394362853) - present_state_Q ( -1.1005789394362853)) * f1( 0.25779594932174166)
w2 ( -2.403537850861232 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1005789394362853) - present_state_Q (-1.1005789394362853)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.278691068451942 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0160001823813003) - present_state_Q ( -1.106146713012234)) * f1( 0.21756939600536795)
w2 ( -2.3534241834918794 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.0160001823813003) - present_state_Q (-1.106146713012234)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.299062987073966 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9865444786270197) - present_state_Q ( -1.0178716711253526)) * f1( 0.18849225820864768)
w2 ( -2.3804437529103133 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9865444786270197) - present_state_Q (-1.0178716711253526)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.3128261829298027 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8516298144294594) - present_state_Q ( -0.8516298144294594)) * f1( 0.11157540165019769)
w2 ( -2.4112820820856506 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8516298144294594) - present_state_Q (-0.8516298144294594)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.2848574498536918 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1425654995764345) - present_state_Q ( -1.3552411115612606)) * f1( 0.3774536545752287)
w2 ( -2.3964623908535785 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1425654995764345) - present_state_Q (-1.3552411115612606)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.3263815704350326 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39435492817246004) - present_state_Q ( -0.39435492817246004)) * f1( 0.11837933437840514)
w2 ( -2.4170074849088747 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.39435492817246004) - present_state_Q (-0.39435492817246004)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.2510668475926447 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6911157969505823) - present_state_Q ( -1.848348765913236)) * f1( 0.6386732348894343)
w2 ( -2.399318927115602 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.6911157969505823) - present_state_Q (-1.848348765913236)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.1780270141220446 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8422024487047797) - present_state_Q ( -1.868360742143736)) * f1( 0.616817291856754)
w2 ( -2.375636117170137 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.8422024487047797) - present_state_Q (-1.868360742143736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1106856554578752 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8259409448152761) - present_state_Q ( -1.8501423340192868)) * f1( 0.5767758143409144)
w2 ( -2.3464474111816926 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.8259409448152761) - present_state_Q (-1.8501423340192868)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.054546939373886 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7795591155517663) - present_state_Q ( -1.7795591155517663)) * f1( 0.5096092302593115)
w2 ( -2.313399315061795 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.7795591155517663) - present_state_Q (-1.7795591155517663)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.0789715133523883 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6202295501236867) - present_state_Q ( -1.6202295501236867)) * f1( 0.4508097322358604)
w2 ( -2.3296531172084554 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6202295501236867) - present_state_Q (-1.6202295501236867)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.103020160115089 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2858302334472755) - present_state_Q ( -1.518795545168121)) * f1( 0.39437751058141135)
w2 ( -2.3479467415537534 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2858302334472755) - present_state_Q (-1.518795545168121)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.134087877250385 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.047097026787628) - present_state_Q ( -1.1644943638653158)) * f1( 0.33043193248158687)
w2 ( -2.3667510483300225 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.047097026787628) - present_state_Q (-1.1644943638653158)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1550113132227184 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1912664838798857) - present_state_Q ( -1.1912664838798857)) * f1( 0.22550203977585156)
w2 ( -2.3945868532652654 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1912664838798857) - present_state_Q (-1.1912664838798857)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.137496777629041 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0757258236162444) - present_state_Q ( -1.0990577283477307)) * f1( 0.176649500646407)
w2 ( -2.3648422988856823 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.0757258236162444) - present_state_Q (-1.0990577283477307)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.139067293585256 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8063831978533273) - present_state_Q ( -0.9246253127976116)) * f1( 0.1006657064393711)
w2 ( -2.369522689095314 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.8063831978533273) - present_state_Q (-0.9246253127976116)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.2604968208610603 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5567800624937058) - present_state_Q ( -1.7039084675298786)) * f1( 0.6285330564949234)
w2 ( -2.40652518029607 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.5567800624937058) - present_state_Q (-1.7039084675298786)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.287850318550425 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6792860375315088) - present_state_Q ( -1.709360392301879)) * f1( 0.5964979038297642)
w2 ( -2.413403703467839 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6792860375315088) - present_state_Q (-1.709360392301879)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3190407474012775 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5551297262369979) - present_state_Q ( -1.559714259023493)) * f1( 0.5235061462684231)
w2 ( -2.422340684171842 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5551297262369979) - present_state_Q (-1.559714259023493)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.352169718436524 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3938426223536053) - present_state_Q ( -1.3938426223536053)) * f1( 0.4443611096021492)
w2 ( -2.4335238087700684 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3938426223536053) - present_state_Q (-1.3938426223536053)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3434565353782926 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3420942240446971) - present_state_Q ( -1.3436380612242094)) * f1( 0.4160454418906371)
w2 ( -2.430382379187772 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -1.3420942240446971) - present_state_Q (-1.3436380612242094)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.26797945623882 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2109270817556965) - present_state_Q ( -1.2109270817556965)) * f1( 0.3611629710644091)
w2 ( -2.3990348635840704 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2109270817556965) - present_state_Q (-1.2109270817556965)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.2849203443880546 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0075584322337123) - present_state_Q ( -1.0075584322337123)) * f1( 0.2855860095709341)
w2 ( -2.407932824748915 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0075584322337123) - present_state_Q (-1.0075584322337123)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3008294279588477 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0975342868589426) - present_state_Q ( -1.2183846611043865)) * f1( 0.4198296294235427)
w2 ( -2.4109211725846897 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0975342868589426) - present_state_Q (-1.2183846611043865)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.2914941298402955 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2100621256953945) - present_state_Q ( -1.3519962707384552)) * f1( 0.47726286021082204)
w2 ( -2.4096975843271857 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2100621256953945) - present_state_Q (-1.3519962707384552)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.234177746455595 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0368939084612963) - present_state_Q ( -1.0368939084612963)) * f1( 0.39991768571923225)
w2 ( -2.40253156173911 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0368939084612963) - present_state_Q (-1.0368939084612963)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.2753451082143616 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9130424423253446) - present_state_Q ( -0.8784857637531097)) * f1( 0.3394354754760453)
w2 ( -2.4085956541415072 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9130424423253446) - present_state_Q (-0.8784857637531097)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.312466610606983 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0242776253316195) - present_state_Q ( -1.0242776253316195)) * f1( 0.34430735675621404)
w2 ( -2.419377155513523 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0242776253316195) - present_state_Q (-1.0242776253316195)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.29987013842315 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9847251872916176) - present_state_Q ( -0.9885962499306835)) * f1( 0.32288402822964274)
w2 ( -2.4154759182015075 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.9847251872916176) - present_state_Q (-0.9885962499306835)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3302327370182394 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9721948605227457) - present_state_Q ( -0.9970312812843249)) * f1( 0.2759764053414216)
w2 ( -2.431978741273027 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9721948605227457) - present_state_Q (-0.9970312812843249)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3547194777597267 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8154828886004978) - present_state_Q ( -0.8154828886004978)) * f1( 0.19340818204547267)
w2 ( -2.4509697222769202 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8154828886004978) - present_state_Q (-0.8154828886004978)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3736219573266197 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7583284777004489) - present_state_Q ( -0.8546883854198417)) * f1( 0.15479314814656248)
w2 ( -2.475392611523924 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7583284777004489) - present_state_Q (-0.8546883854198417)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.3872053926060715 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5909141144903067) - present_state_Q ( -0.5909141144903067)) * f1( 0.09251903913505105)
w2 ( -2.497415270978305 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5909141144903067) - present_state_Q (-0.5909141144903067)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3641184555929184 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1754213399518845) - present_state_Q ( -1.221945008240472)) * f1( 0.4213600521114338)
w2 ( -2.425963456166422 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1754213399518845) - present_state_Q (-1.221945008240472)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3033720381043143 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1119958094789506) - present_state_Q ( -1.137646715051979)) * f1( 0.437072040858884)
w2 ( -2.414375249238354 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1119958094789506) - present_state_Q (-1.137646715051979)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.27142196184913 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6008177258643745) - present_state_Q ( -1.6008177258643745)) * f1( 0.5842192848524554)
w2 ( -2.4076001253760952 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.6008177258643745) - present_state_Q (-1.6008177258643745)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.3474847422135636 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6659476749482993) - present_state_Q ( -0.6659476749482993)) * f1( 0.23431121860242493)
w2 ( -2.4215107203716073 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6659476749482993) - present_state_Q (-0.6659476749482993)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6537678180199458 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1293321698111203) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8253838097475062 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1293321698111203) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6751380255862772 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1270254391501509)) * f3(0.24479759805975718)
w4 ( 0.8218919115041069 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1270254391501509)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6751380255862772 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8218919115041069 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7049076489253862 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.15592468866129394)) * f3(0.352689184711424)
w4 ( 0.8134511583907198 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.15592468866129394)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7049076489253862 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.058464994942340254) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8134511583907198 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.058464994942340254) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7049076489253862 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8134511583907198 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7049076489253862 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8134511583907198 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6598100428360082 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2998372907707169)) * f3(0.5638347525147556)
w4 ( 0.8230492058799684 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2998372907707169)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6179173386397141 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2858795273479356)) * f3(0.533067763422558)
w4 ( 0.8293362420987519 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2858795273479356)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6179173386397141 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8293362420987519 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6545069498623889 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.22562046506504846)) * f3(0.4725023011584152)
w4 ( 0.8231412058192723 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.22562046506504846)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7326056418556802 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.22157181794901515)) * f3(0.4391444803985475)
w4 ( 0.8089137803628644 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.22157181794901515)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7326056418556802 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8089137803628644 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7326056418556802 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8089137803628644 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7326056418556802 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2344015868098887) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8089137803628644 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2344015868098887) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6827782605562785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.24473174168041062)) * f3(0.4003061835005013)
w4 ( 0.816382170812947 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.24473174168041062)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6827782605562785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1554936541744535) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.816382170812947 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1554936541744535) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6827782605562785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.17708269312439495) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.816382170812947 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.17708269312439495) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6827782605562785 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.816382170812947 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6827782605562785 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.816382170812947 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.26900386985809754)) * f3(0.46572484579078666)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.26900386985809754)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7633950511337572 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8059961940320955 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.0588531315545016)) * f3(0.09821003597520993)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.0588531315545016)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.0939688385872938) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.0939688385872938) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.06766231939953879) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.06766231939953879) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.09258671581492908) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.09258671581492908) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10404944552584325) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10404944552584325) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3472106902405523 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1068771234465138) - present_state_Q ( -1.1231135997666324)) * f1( 0.22054921608799508)
w2 ( -2.421200073186058 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -1.1068771234465138) - present_state_Q (-1.1231135997666324)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -2.3286384030248803 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7371477552707386) - present_state_Q ( -0.7376967354327714)) * f1( 0.15955820498435125)
w2 ( -2.4037403437874723 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.7371477552707386) - present_state_Q (-0.7376967354327714)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.3056512891035132 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7855388504085135) - present_state_Q ( -0.792974170521141)) * f1( 0.1340809725365981)
w2 ( -2.3694519380778662 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.7855388504085135) - present_state_Q (-0.792974170521141)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2807850172166115 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46415088665750126) - present_state_Q ( -0.5101595015697638)) * f1( 0.1698812420234487)
w2 ( -2.3621332160133464 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.46415088665750126) - present_state_Q (-0.5101595015697638)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.260045587953491 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4539312839551315) - present_state_Q ( -0.4539312839551315)) * f1( 0.14724080552067662)
w2 ( -2.355090525235548 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.4539312839551315) - present_state_Q (-0.4539312839551315)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.241582017913597 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4001129258729528) - present_state_Q ( -0.5178674521347302)) * f1( 0.12493482481778415)
w2 ( -2.3403119636400738 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.4001129258729528) - present_state_Q (-0.5178674521347302)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.228003247151112 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45058948253535747) - present_state_Q ( -0.45058948253535747)) * f1( 0.09660957504152204)
w2 ( -2.3262566582972557 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.45058948253535747) - present_state_Q (-0.45058948253535747)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7677275561157956 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8051139002952046 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2263866769745912 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6330077431881411) - present_state_Q ( -0.6330077431881411)) * f1( 0.2319094062964056)
w2 ( -2.325908123452909 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6330077431881411) - present_state_Q (-0.6330077431881411)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7834278297012527 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.10028982518464086) - present_state_Q (-0.10028982518464086)) * f3(0.17257994732764945)
w4 ( 0.8014749436658692 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.10028982518464086) - present_state_Q (-0.10028982518464086)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7834278297012527 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8014749436658692 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7834278297012527 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8014749436658692 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7834278297012527 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8014749436658692 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7669330022579344 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1628845531809685)) * f3(0.24883408979987573)
w4 ( 0.8041264818785931 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1628845531809685)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7669330022579344 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.15834579923981906) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8041264818785931 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.15834579923981906) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7669330022579344 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8041264818785931 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7520072099953835 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.07438080100365879)) * f3(0.13892459962619927)
w4 ( 0.8084240050826078 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.07438080100365879)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7520072099953835 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8084240050826078 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7443347547065754 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.024007747256285336)) * f3(0.07492575431548794)
w4 ( 0.8125200360716329 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.024007747256285336)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7443347547065754 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01625040072143266) - present_state_Q (0.01625040072143266)) * f3(0.0)
w4 ( 0.8144907853503344 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01625040072143266) - present_state_Q (0.01625040072143266)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -2.2023975678119174 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8158867240143796) - present_state_Q ( -0.9581723853182447)) * f1( 0.2736659238494274)
w2 ( -2.3127593677591567 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.8158867240143796) - present_state_Q (-0.9581723853182447)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7443347547065754 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8144907853503344 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7443347547065754 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.024861795306116236) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8144907853503344 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.024861795306116236) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01628981570700669) - present_state_Q (-0.3237502547577982)) * f3(0.5443778229226519)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01628981570700669) - present_state_Q (-0.3237502547577982)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.11097669092440283) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.11097669092440283) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8354973952614315 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09795861252476783) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7977445777136194 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09795861252476783) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8884921573958635 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21293670815421034) - present_state_Q (-0.21293670815421034)) * f3(0.29305476312842527)
w4 ( 0.7905111498629745 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21293670815421034) - present_state_Q (-0.21293670815421034)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8884921573958635 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.201343094177764) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7905111498629745 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.201343094177764) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8884921573958635 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7905111498629745 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8884921573958635 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4129563431079871) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7905111498629745 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4129563431079871) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8968513767133713 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2893659438740669)) * f3(0.39685981798261677)
w4 ( 0.788826077413967 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2893659438740669)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8968513767133713 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.788826077413967 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8968513767133713 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.34865916152656906) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.788826077413967 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.34865916152656906) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9462898777122979 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.038273133379640834) - present_state_Q (-0.30752869522528115)) * f3(0.4132622093714545)
w4 ( 0.7792556884690656 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.038273133379640834) - present_state_Q (-0.30752869522528115)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9064307754784414 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.3604364426659251)) * f3(0.46324284116046266)
w4 ( 0.7878600528957248 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.3604364426659251)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9064307754784414 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7878600528957248 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9064307754784414 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7878600528957248 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9064307754784414 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7878600528957248 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9064307754784414 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7878600528957248 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9804255732584879 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.36226434093144966)) * f3(0.4518116056783572)
w4 ( 0.7780336389413135 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.36226434093144966)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9748862282812193 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.5840339168668347)) * f3(0.6591796721848154)
w4 ( 0.7787059102762481 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.5840339168668347)) * f4(0.08)
============================================================================
GUIDE learning . . .
w1 ( -2.164936751214315 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3177883725896404) - present_state_Q ( -1.361654666991481)) * f1( 0.5132491729631707)
w2 ( -2.3054606094618317 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.3177883725896404) - present_state_Q (-1.361654666991481)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.093485889758516 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2061547764613034) - present_state_Q ( -1.2061547764613034)) * f1( 0.45064074734187975)
w2 ( -2.28960521647368 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2061547764613034) - present_state_Q (-1.2061547764613034)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.0177046847891122 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2956529087260868) - present_state_Q ( -1.2956529087260868)) * f1( 0.4548452563799571)
w2 ( -2.264613902205878 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2956529087260868) - present_state_Q (-1.2956529087260868)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9974442803941344 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1301676425077494) - present_state_Q ( -1.1301676425077494)) * f1( 0.3917696990724324)
w2 ( -2.2568566390320237 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1301676425077494) - present_state_Q (-1.1301676425077494)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.931998694495258 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.020069830232602) - present_state_Q ( -1.020069830232602)) * f1( 0.34120668149166944)
w2 ( -2.2280856963238835 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.020069830232602) - present_state_Q (-1.020069830232602)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8805855836327345 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8866617330290429) - present_state_Q ( -0.8866617330290429)) * f1( 0.2859468177460595)
w2 ( -2.2011157629279916 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.8866617330290429) - present_state_Q (-0.8866617330290429)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8878673776839832 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7383117626603568) - present_state_Q ( -0.7383117626603568)) * f1( 0.21703048336292352)
w2 ( -2.206148554132077 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.7383117626603568) - present_state_Q (-0.7383117626603568)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9099279532732967 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5120997303899928) - present_state_Q ( -0.6224071580965966)) * f1( 0.15439902104478115)
w2 ( -2.227580596356213 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5120997303899928) - present_state_Q (-0.6224071580965966)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.925804266734576 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4085206750915183) - present_state_Q ( -0.4085206750915183)) * f1( 0.09726158263589522)
w2 ( -2.2439039102803893 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4085206750915183) - present_state_Q (-0.4085206750915183)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0495662511348196 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.3469918797769338)) * f3(0.4517825529104034)
w4 ( 0.7588698128335714 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.3469918797769338)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0495662511348196 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7588698128335714 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0325598651341197 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.14122728365078954)) * f3(0.14901839663608843)
w4 ( 0.7611522674008729 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.14122728365078954)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0325598651341197 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7611522674008729 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0325598651341197 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.21353972790350817) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7611522674008729 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.21353972790350817) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0202816952214728 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.14952024511108325)) * f3(0.18903444511644116)
w4 ( 0.7650493888715394 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.14952024511108325)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0202816952214728 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7650493888715394 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0025439736130468 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1430282001593839)) * f3(0.15518183721060105)
w4 ( 0.7673354452718582 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1430282001593839)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0091783846941416 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.18304617000341247) - present_state_Q (-0.18304617000341247)) * f3(0.19788945336119848)
w4 ( 0.7666649283778643 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.18304617000341247) - present_state_Q (-0.18304617000341247)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9893250143492749 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1577265054609426)) * f3(0.1714858410101107)
w4 ( 0.7689803813887861 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1577265054609426)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9893250143492749 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.13284925774352155) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7689803813887861 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.13284925774352155) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9893250143492749 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7689803813887861 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9893250143492749 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7689803813887861 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9730900514876236 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.12712188316851097)) * f3(0.1440391061879867)
w4 ( 0.7712346251551231 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.12712188316851097)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9588802779303501 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10923256770584545)) * f3(0.12810454697217033)
w4 ( 0.7734530902905348 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10923256770584545)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9465965426921219 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.0923584674381299)) * f3(0.11245150382764763)
w4 ( 0.775637807225411 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.0923584674381299)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9465965426921219 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09052778976938328) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.775637807225411 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09052778976938328) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9327655534802282 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1031670615079052)) * f3(0.12537529168962297)
w4 ( 0.7778441413484268 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1031670615079052)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9574460891280961 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10599029529229495)) * f3(0.13030839064088565)
w4 ( 0.7740561219390114 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10599029529229495)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.09396326626848556)) * f3(0.11430866964732392)
w4 ( 0.7702440484715484 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.09396326626848556)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08086291901361205) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7702440484715484 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08086291901361205) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.06519826659076285) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7702440484715484 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.06519826659076285) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05053916159734454) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7702440484715484 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05053916159734454) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7702440484715484 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.015404880969430967) - present_state_Q (0.015404880969430967)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.015404880969430967) - present_state_Q (0.015404880969430967)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -1.8810152322914797 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3206330360918919) - present_state_Q ( -1.3365538701452875)) * f1( 0.6357648571977202)
w2 ( -2.240381457447709 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.3206330360918919) - present_state_Q (-1.3365538701452875)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2581340375019512) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2581340375019512) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.11041216876656412) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.11041216876656412) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.893763120287177 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4190217457031828) - present_state_Q ( -1.4456053970120075)) * f1( 0.6494191170260253)
w2 ( -2.242344425223292 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.4190217457031828) - present_state_Q (-1.4456053970120075)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.9033436615469574 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4922662837561111) - present_state_Q ( -1.4922662837561111)) * f1( 0.6103797289057622)
w2 ( -2.244698830392584 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.4922662837561111) - present_state_Q (-1.4922662837561111)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9441666881552127 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4390187329250848) - present_state_Q ( -1.4390187329250848)) * f1( 0.5791460210975683)
w2 ( -2.2552720774980957 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4390187329250848) - present_state_Q (-1.4390187329250848)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9849802148727547 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.321432801983092) - present_state_Q ( -1.340067792442551)) * f1( 0.5152731948968873)
w2 ( -2.267153209814432 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.321432801983092) - present_state_Q (-1.340067792442551)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.024217826274416 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1389408904453047) - present_state_Q ( -1.1389408904453047)) * f1( 0.4024563585004551)
w2 ( -2.2817775077934206 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1389408904453047) - present_state_Q (-1.1389408904453047)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.060622712289328 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.027755301188745) - present_state_Q ( -1.027755301188745)) * f1( 0.3386437299988498)
w2 ( -2.2979028112273725 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.027755301188745) - present_state_Q (-1.027755301188745)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.022731682348259598) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.022731682348259598) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9792337414612742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.049819156271096475) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7712163196858034 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.049819156271096475) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.13288973221873526)) * f3(0.1829621507246325)
w4 ( 0.7600136580791158 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.13288973221873526)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09207103526879132) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7600136580791158 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09207103526879132) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.030400546323164634) - present_state_Q (0.030400546323164634)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.030400546323164634) - present_state_Q (0.030400546323164634)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.015038084322247047) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.015038084322247047) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0133947924846047 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7519042161123524 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0169029698363814 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.191744539068336) - present_state_Q ( -1.1877608547921823)) * f1( 0.4091362422048767)
w2 ( -2.2818740152140924 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.191744539068336) - present_state_Q (-1.1877608547921823)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.0306967297515137 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2984742678952472) - present_state_Q ( -1.30011840331544)) * f1( 0.4183362377323828)
w2 ( -2.288468595683574 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2984742678952472) - present_state_Q (-1.30011840331544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.06434974334886 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2018736888033106) - present_state_Q ( -1.2018736888033106)) * f1( 0.36646534106432394)
w2 ( -2.3068348692851144 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2018736888033106) - present_state_Q (-1.2018736888033106)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.065102547922098 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9545794090674461) - present_state_Q ( -1.0699211525317018)) * f1( 0.2947921885016737)
w2 ( -2.3073456050526153 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.9545794090674461) - present_state_Q (-1.0699211525317018)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.015140875625154 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0962009900578604) - present_state_Q ( -1.0962009900578604)) * f1( 0.2514957861619464)
w2 ( -2.2576810827763136 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.0962009900578604) - present_state_Q (-1.0962009900578604)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.9765633918634438 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9335556838642652) - present_state_Q ( -0.977103266791695)) * f1( 0.20479113946293734)
w2 ( -2.210587390316182 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.9335556838642652) - present_state_Q (-0.977103266791695)) * f2(0.25)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2304486807862441)) * f3(0.2570812987818822)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2304486807862441)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.012499112933051358) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.012499112933051358) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.42876316550132765) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.42876316550132765) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.4664385492710882) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.4664385492710882) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.37798651004669437) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.37798651004669437) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9946163229296007 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7548260108354974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9577721503735885 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.3537506400047469)) * f3(0.43155660247361327)
w4 ( 0.7633635172355449 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.3537506400047469)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9652919190922974 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.31680156748936983)) * f3(0.41047123687985393)
w4 ( 0.7615315329104385 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.31680156748936983)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9652919190922974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7615315329104385 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9652919190922974 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7615315329104385 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715177181242103 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.35577890950413976)) * f3(0.4316843681119965)
w4 ( 0.7603777641864716 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.35577890950413976)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715177181242103 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7603777641864716 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715177181242103 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7603777641864716 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715177181242103 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7603777641864716 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0103800764315403 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.03521910132817619) - present_state_Q (-0.25734859087758805)) * f3(0.3118535576620754)
w4 ( 0.7529007242709402 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.03521910132817619) - present_state_Q (-0.25734859087758805)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0382073424820002 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10304288213559645)) * f3(0.14669422829013545)
w4 ( 0.7415189815637538 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10304288213559645)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.071052958960007 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.15349277690661406) - present_state_Q (-0.15349277690661406)) * f3(0.17641325453478926)
w4 ( 0.7340715555606177 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.15349277690661406) - present_state_Q (-0.15349277690661406)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.071052958960007 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1454846804343204) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7340715555606177 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1454846804343204) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.071052958960007 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7340715555606177 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.1040827701181501 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.16324093454169994)) * f3(0.17982658574711657)
w4 ( 0.7267245192987845 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.16324093454169994)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.110708295583463 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.25512220119534496)) * f3(0.27056456312718724)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.25512220119534496)) * f4(0.06)
============================================================================
GUIDE learning . . .
w1 ( -1.9392721322142097 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9383083840783324) - present_state_Q ( -0.9607574719530803)) * f1( 0.430154735202144)
w2 ( -2.2062527571484556 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.9383083840783324) - present_state_Q (-0.9607574719530803)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.101564396428261 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0802182532898864) - present_state_Q (-0.0802182532898864)) * f3(0.08528194010673763)
w4 ( 0.7273996453618784 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0802182532898864) - present_state_Q (-0.0802182532898864)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.101564396428261 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7273996453618784 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.101564396428261 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16843770550105633) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7273996453618784 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16843770550105633) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9632841826182308 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8704337103823246) - present_state_Q ( -0.8704337103823246)) * f1( 0.3350785193440309)
w2 ( -2.2134188537550146 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8704337103823246) - present_state_Q (-0.8704337103823246)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.999178202956178 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7550072404552368) - present_state_Q ( -0.7550072404552368)) * f1( 0.2718227752275988)
w2 ( -2.2266237885909175 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7550072404552368) - present_state_Q (-0.7550072404552368)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.101564396428261 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7273996453618784 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.101564396428261 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.8445454939521831) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7273996453618784 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.8445454939521831) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.8417864098504261)) * f3(0.8698269081817833)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.8417864098504261)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.984852203996208 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7488682279194853 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0869129753696514 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.6275140222921535)) * f3(0.7436197748547672)
w4 ( 0.7296534242315754 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.6275140222921535)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0869129753696514 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7296534242315754 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0869129753696514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7296534242315754 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.000898260578879 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.6860797481199262)) * f3(0.7252017828237582)
w4 ( 0.7462585407052544 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.6860797481199262)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.000898260578879 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7462585407052544 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.000898260578879 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7462585407052544 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9218215370731169 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.09531432575948406) - present_state_Q (-0.6310768041813078)) * f3(0.7050693222742574)
w4 ( 0.757473994421308 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.09531432575948406) - present_state_Q (-0.6310768041813078)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9218215370731169 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.757473994421308 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.016695439153286 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6832204800287855) - present_state_Q ( -0.6832204800287855)) * f1( 0.28974082636077797)
w2 ( -2.2175128981560523 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6832204800287855) - present_state_Q (-0.6832204800287855)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.1009591112849066 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.16660624433751944) - present_state_Q (0.16660624433751944)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.16660624433751944) - present_state_Q (0.16660624433751944)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.1136352459892205 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10219513646989294) - present_state_Q (0.16514386669273598)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10219513646989294) - present_state_Q (0.16514386669273598)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.995956904193486 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1670709955708791) - present_state_Q ( -1.1852290347818641)) * f1( 0.20820149248197328)
w2 ( -2.2431891225833147 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1670709955708791) - present_state_Q (-1.1852290347818641)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.011702016134189 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8076440521149018) - present_state_Q ( -0.8076440521149018)) * f1( 0.12367339743180347)
w2 ( -2.2750171314107295 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8076440521149018) - present_state_Q (-0.8076440521149018)) * f2(0.25)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0784591112849065 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16660624433751944) - present_state_Q (0.16660624433751944)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16660624433751944) - present_state_Q (0.16660624433751944)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0462752315845545 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16176886669273596) - present_state_Q (0.16176886669273596)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16176886669273596) - present_state_Q (0.16176886669273596)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0141565242405957 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15694128473768318) - present_state_Q (0.15694128473768318)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15694128473768318) - present_state_Q (0.15694128473768318)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9821028572790085 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15212347863608935) - present_state_Q (0.15212347863608935)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15212347863608935) - present_state_Q (0.15212347863608935)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0387532008777627 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45611704560298894) - present_state_Q ( -0.45611704560298894)) * f1( 0.1701873270924908)
w2 ( -2.282964604705516 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.45611704560298894) - present_state_Q (-0.45611704560298894)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.04033096960652 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4476188192443745) - present_state_Q ( -0.4485777608647998)) * f1( 0.1640362994822223)
w2 ( -2.2834455253108144 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4476188192443745) - present_state_Q (-0.4485777608647998)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.0356496871970307 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38782033692518136) - present_state_Q ( -0.38782033692518136)) * f1( 0.13411944666625042)
w2 ( -2.281700333794651 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.38782033692518136) - present_state_Q (-0.38782033692518136)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9651140989930185 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.14731542859185126) - present_state_Q (0.14731542859185126)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.14731542859185126) - present_state_Q (0.14731542859185126)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9406597429425576 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.14476711484895277) - present_state_Q (0.14476711484895277)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.14476711484895277) - present_state_Q (0.14476711484895277)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0592605230432786 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4085333551461798) - present_state_Q ( -0.4085333551461798)) * f1( 0.14464587905686524)
w2 ( -2.289861933696493 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4085333551461798) - present_state_Q (-0.4085333551461798)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.081720473190687 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.395873514982717) - present_state_Q ( -0.395873514982717)) * f1( 0.13664148617876393)
w2 ( -2.298080502879071 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.395873514982717) - present_state_Q (-0.395873514982717)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.099283471967819 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32953908410490484) - present_state_Q ( -0.32953908410490484)) * f1( 0.10310464912322097)
w2 ( -2.306597577000599 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.32953908410490484) - present_state_Q (-0.32953908410490484)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.089630569998865 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3609889986502605) - present_state_Q ( -0.3609889986502605)) * f1( 0.11702046106710662)
w2 ( -2.3024731265066727 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3609889986502605) - present_state_Q (-0.3609889986502605)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9462549069630989 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.14109896144138365) - present_state_Q (0.14109896144138365)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.14109896144138365) - present_state_Q (0.14109896144138365)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0798497338869346 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2785405782041166) - present_state_Q ( -0.2785405782041166)) * f1( 0.07820373812720004)
w2 ( -2.2962196939047543 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2785405782041166) - present_state_Q (-0.2785405782041166)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9593387407764986 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14193823604446482) - present_state_Q (0.14193823604446482)) * f3(-0.15)
w4 ( 0.7252552525059566 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14193823604446482) - present_state_Q (0.14193823604446482)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0707913339322035 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16415031425104187) - present_state_Q ( -0.16415031425104187)) * f1( 0.07892412205388943)
w2 ( -2.2962196939047543 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16415031425104187) - present_state_Q (-0.16415031425104187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0646914269592176 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8392174332529878) - present_state_Q ( -0.8392174332529878)) * f1( 0.23893497672106517)
w2 ( -2.292390258555839 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8392174332529878) - present_state_Q (-0.8392174332529878)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9789720699444425 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.1439008111164748) - present_state_Q (-0.17406912335547364)) * f3(0.2419265830629278)
w4 ( 0.7187629261416936 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.1439008111164748) - present_state_Q (-0.17406912335547364)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.975199542615674 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17585186668927544) - present_state_Q (-0.17585186668927544)) * f3(0.2383652281253069)
w4 ( 0.7200290595818564 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17585186668927544) - present_state_Q (-0.17585186668927544)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0058132340686796 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1462799313923511) - present_state_Q (-0.11654824758719362)) * f3(0.16381261903961172)
w4 ( 0.7088161170262149 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1462799313923511) - present_state_Q (-0.11654824758719362)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0130979370846986 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15087198511030195) - present_state_Q (-0.2699151214098824)) * f3(0.3388270521495578)
w4 ( 0.7066661402254241 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15087198511030195) - present_state_Q (-0.2699151214098824)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0259339220570058 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07697022049856012) - present_state_Q (0.15196469056270479)) * f3(-0.15)
w4 ( 0.7066661402254241 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07697022049856012) - present_state_Q (0.15196469056270479)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0388564058648404 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.15389008830855086) - present_state_Q (0.15389008830855086)) * f3(-0.15)
w4 ( 0.7066661402254241 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.15389008830855086) - present_state_Q (0.15389008830855086)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.022945844284905 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7451586822692624) - present_state_Q ( -0.7451586822692624)) * f1( 0.24987736650483464)
w2 ( -2.275683830415416 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.7451586822692624) - present_state_Q (-0.7451586822692624)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.9957037855568531 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5879568753303202) - present_state_Q ( -0.5879568753303202)) * f1( 0.17815034115071576)
w2 ( -2.260392218537443 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5879568753303202) - present_state_Q (-0.5879568753303202)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0378222258130065 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.15582846087972604) - present_state_Q (-0.08192064600509055)) * f3(0.10606595001200131)
w4 ( 0.7070561541937963 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.15582846087972604) - present_state_Q (-0.08192064600509055)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0357206358057351 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.15567333387195098) - present_state_Q (0.15567333387195098)) * f3(-0.15)
w4 ( 0.7070561541937963 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.15567333387195098) - present_state_Q (0.15567333387195098)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0411233015182286 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15535809537086026) - present_state_Q (0.15535809537086026)) * f3(-0.15)
w4 ( 0.7070561541937963 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15535809537086026) - present_state_Q (0.15535809537086026)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0535513240545862 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1529666901509125) - present_state_Q (0.1561684952277343)) * f3(-0.15)
w4 ( 0.7070561541937963 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1529666901509125) - present_state_Q (0.1561684952277343)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0698880263765924 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.17217382169206386) - present_state_Q (-0.04646469049871492)) * f3(0.08436993786715347)
w4 ( 0.6954382466298038 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.17217382169206386) - present_state_Q (-0.04646469049871492)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0602215031231799 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.16048320395648885) - present_state_Q (0.16048320395648885)) * f3(-0.15)
w4 ( 0.6954382466298038 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.16048320395648885) - present_state_Q (0.16048320395648885)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0353081625157905 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.018561483574839985) - present_state_Q (0.15903322546847698)) * f3(-0.15)
w4 ( 0.6954382466298038 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.018561483574839985) - present_state_Q (0.15903322546847698)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0451848747276118 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15529622437736856) - present_state_Q (-0.024349879127696202)) * f3(0.05038829102450228)
w4 ( 0.6875977646360656 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15529622437736856) - present_state_Q (-0.024349879127696202)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0479176319734338 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15677773120914176) - present_state_Q (-0.03623961490021233)) * f3(0.060987799409427265)
w4 ( 0.6858054341881501 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15677773120914176) - present_state_Q (-0.03623961490021233)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0529859614870272 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15718764479601507) - present_state_Q (-0.11729059954761119)) * f3(0.1381051453849439)
w4 ( 0.684337471644259 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15718764479601507) - present_state_Q (-0.11729059954761119)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0429891849494772 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.085038749469404) - present_state_Q (0.15794789422305408)) * f3(-0.15)
w4 ( 0.684337471644259 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.085038749469404) - present_state_Q (0.15794789422305408)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0462721303810678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15644837774242157) - present_state_Q (-0.0517826263064215)) * f3(0.07589352441466227)
w4 ( 0.6826071815005816 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15644837774242157) - present_state_Q (-0.0517826263064215)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0530817482504096 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15694081955716016) - present_state_Q (-0.02139812183011217)) * f3(0.04654851035016798)
w4 ( 0.676755550315725 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15694081955716016) - present_state_Q (-0.02139812183011217)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.04322270138739 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.006924713695902095) - present_state_Q (0.15796226223756143)) * f3(-0.15)
w4 ( 0.676755550315725 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.006924713695902095) - present_state_Q (0.15796226223756143)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0585736362074343 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9093595646336344) - present_state_Q ( -0.9093595646336344)) * f1( 0.3836932127920571)
w2 ( -2.294627575863903 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.9093595646336344) - present_state_Q (-0.9093595646336344)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0675302972104084 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05497651309449966) - present_state_Q (-0.6302597103253036)) * f3(0.7930521550297989)
w4 ( 0.7006988850153821 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05497651309449966) - present_state_Q (-0.6302597103253036)) * f4(0.18)
============================================================================
GUIDE learning . . .
w1 ( -2.0895801227939104 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5842577532068268) - present_state_Q ( -0.698989132000022)) * f1( 0.2280833515767028)
w2 ( -2.3082219422971098 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5842577532068268) - present_state_Q (-0.698989132000022)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.1118816378474137 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16012954458156126) - present_state_Q (-0.1945128600890625)) * f3(0.24784565766609987)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16012954458156126) - present_state_Q (-0.1945128600890625)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0867304286687551 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.09965032900121112) - present_state_Q (0.16678224567711206)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.09965032900121112) - present_state_Q (0.16678224567711206)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.099529799550701 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.16300956430031327) - present_state_Q (0.16300956430031327)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.16300956430031327) - present_state_Q (0.16300956430031327)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.1123032517066107 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.16492946993260513) - present_state_Q (0.16492946993260513)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.16492946993260513) - present_state_Q (0.16492946993260513)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0797930618773532 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.005005008611664051) - present_state_Q (0.1668454877559916)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.005005008611664051) - present_state_Q (0.1668454877559916)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0476064809270516 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.161968959281603) - present_state_Q (0.161968959281603)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.161968959281603) - present_state_Q (0.161968959281603)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0154850778031743 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15714097213905773) - present_state_Q (0.15714097213905773)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15714097213905773) - present_state_Q (0.15714097213905773)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9834287205206229 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15232276167047615) - present_state_Q (0.15232276167047615)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.15232276167047615) - present_state_Q (0.15232276167047615)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9658755623981575 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.22696233419597184) - present_state_Q (0.14751430807809343)) * f3(-0.15)
w4 ( 0.6828041431608544 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.22696233419597184) - present_state_Q (0.14751430807809343)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9458854938240335 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1448813343597236) - present_state_Q (-0.2113750796923379)) * f3(0.27539718520754414)
w4 ( 0.6886110488658809 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1448813343597236) - present_state_Q (-0.2113750796923379)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.928849517925847 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.14188282407360503) - present_state_Q (-0.1777831823086999)) * f3(0.24619477488391678)
w4 ( 0.6941468205836093 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.14188282407360503) - present_state_Q (-0.1777831823086999)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9269685976520472 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.13932742768887704) - present_state_Q (0.13932742768887704)) * f3(-0.15)
w4 ( 0.6941468205836093 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.13932742768887704) - present_state_Q (0.13932742768887704)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9055156586210831 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.13904528964780707) - present_state_Q (0.10880944435462132)) * f3(-0.10240531144570807)
w4 ( 0.6899570107528297 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.13904528964780707) - present_state_Q (0.10880944435462132)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8961819894123754 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.13582734879316247) - present_state_Q (0.13582734879316247)) * f3(-0.15)
w4 ( 0.6899570107528297 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.13582734879316247) - present_state_Q (0.13582734879316247)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9268907614339372 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1344272984118563) - present_state_Q (-0.16719276226516894)) * f3(0.23275426796638798)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1344272984118563) - present_state_Q (-0.16719276226516894)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9025138076420335 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.13903361421509058) - present_state_Q (0.13903361421509058)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.13903361421509058) - present_state_Q (0.13903361421509058)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8781862171815583 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.135377071146305) - present_state_Q (0.135377071146305)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.135377071146305) - present_state_Q (0.135377071146305)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8687513754829272 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.027384860018289735) - present_state_Q (0.13172793257723375)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.027384860018289735) - present_state_Q (0.13172793257723375)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8369921539475743 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.13031270632243908) - present_state_Q (0.13031270632243908)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.13031270632243908) - present_state_Q (0.13031270632243908)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8052972448358304 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.12554882309213614) - present_state_Q (0.12554882309213614)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.12554882309213614) - present_state_Q (0.12554882309213614)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7959282809019002 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.03803008869975283) - present_state_Q (0.12079458672537456)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.03803008869975283) - present_state_Q (0.12079458672537456)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7865483716773255 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05938039503028601) - present_state_Q (0.11938924213528501)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05938039503028601) - present_state_Q (0.11938924213528501)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7771212092027503 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10495242553411026) - present_state_Q (0.11798225575159882)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10495242553411026) - present_state_Q (0.11798225575159882)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.767717783101031 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10326892067541658) - present_state_Q (0.11656818138041254)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10326892067541658) - present_state_Q (0.11656818138041254)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7586631545902515 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11515766746515464) - present_state_Q (0.11515766746515464)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11515766746515464) - present_state_Q (0.11515766746515464)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7271268617022062 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.11379947318853771) - present_state_Q (0.11379947318853771)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.11379947318853771) - present_state_Q (0.11379947318853771)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6956544298072592 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.10906902925533092) - present_state_Q (0.10906902925533092)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.10906902925533092) - present_state_Q (0.10906902925533092)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6642457295868995 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.10434816447108888) - present_state_Q (0.10434816447108888)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.10434816447108888) - present_state_Q (0.10434816447108888)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6402279969881038 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.015453138150101853) - present_state_Q (0.09963685943803492)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.015453138150101853) - present_state_Q (0.09963685943803492)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.108618390309196 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6146842999324367) - present_state_Q ( -1.730095397047292)) * f1( 0.44134116092673964)
w2 ( -2.323319998450218 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.6146842999324367) - present_state_Q (-1.730095397047292)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.1161039867102023 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4261355793698012) - present_state_Q ( -1.4261355793698012)) * f1( 0.34579020233614616)
w2 ( -2.3298143378072336 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.4261355793698012) - present_state_Q (-1.4261355793698012)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -2.1363456477374028 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2970087258569243) - present_state_Q ( -1.4134994427472858)) * f1( 0.28262525295107727)
w2 ( -2.3548813878515777 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2970087258569243) - present_state_Q (-1.4134994427472858)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -2.160499250499297 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9068523664850004) - present_state_Q ( -0.9068523664850004)) * f1( 0.20402882341456305)
w2 ( -2.378558045254848 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9068523664850004) - present_state_Q (-0.9068523664850004)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1789788995913946 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6689366280933781) - present_state_Q ( -0.7878645303561205)) * f1( 0.14448184660698746)
w2 ( -2.404138627903912 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6689366280933781) - present_state_Q (-0.7878645303561205)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.608931535294203 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09603419954821557) - present_state_Q (0.09603419954821557)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09603419954821557) - present_state_Q (0.09603419954821557)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5776984489352323 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09133973029413044) - present_state_Q (0.09133973029413044)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09133973029413044) - present_state_Q (0.09133973029413044)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5915286095761384 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08665476734028484) - present_state_Q (0.08665476734028484)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08665476734028484) - present_state_Q (0.08665476734028484)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6051964519457067 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0008121725902670712) - present_state_Q (0.08872929143642076)) * f3(-0.15)
w4 ( 0.6820408237054678 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0008121725902670712) - present_state_Q (0.08872929143642076)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.597953413558997 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.090779467791856) - present_state_Q (-0.015501399417361991)) * f3(0.0706928010368092)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.090779467791856) - present_state_Q (-0.015501399417361991)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6117425578965401 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08969301203384955) - present_state_Q (0.08969301203384955)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08969301203384955) - present_state_Q (0.08969301203384955)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6255037792167996 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09176138368448102) - present_state_Q (0.09176138368448102)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09176138368448102) - present_state_Q (0.09176138368448102)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6392371340638855 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09382556688251993) - present_state_Q (0.09382556688251993)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09382556688251993) - present_state_Q (0.09382556688251993)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6529426788674061 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09588557010958282) - present_state_Q (0.09588557010958282)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09588557010958282) - present_state_Q (0.09588557010958282)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6666204699426996 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09794140183011091) - present_state_Q (0.09794140183011091)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09794140183011091) - present_state_Q (0.09794140183011091)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6799931752924867 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0849323952279195) - present_state_Q (0.09999307049140495)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0849323952279195) - present_state_Q (0.09999307049140495)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6861161891125194 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.101998976293873) - present_state_Q (0.101998976293873)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.101998976293873) - present_state_Q (0.101998976293873)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6843792142067535 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1288089868417965) - present_state_Q (0.10291742836687791)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1288089868417965) - present_state_Q (0.10291742836687791)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6829933462979848 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.10265688213101302) - present_state_Q (0.10265688213101302)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.10265688213101302) - present_state_Q (0.10265688213101302)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6966102847717314 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10244900194469772) - present_state_Q (0.10244900194469772)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10244900194469772) - present_state_Q (0.10244900194469772)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7101996489450686 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10449154271575971) - present_state_Q (0.10449154271575971)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10449154271575971) - present_state_Q (0.10449154271575971)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7237614946559549 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10652994734176029) - present_state_Q (0.10652994734176029)) * f3(-0.15)
w4 ( 0.686139141090254 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10652994734176029) - present_state_Q (0.10652994734176029)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.707692143810291 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10856422419839322) - present_state_Q (-0.0792451976075817)) * f3(0.14741149403355322)
w4 ( 0.6904995475703637 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10856422419839322) - present_state_Q (-0.0792451976075817)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6912040491738188 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10615382157154364) - present_state_Q (-0.07942633442713194)) * f3(0.15126113418978704)
w4 ( 0.6948597144367008 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10615382157154364) - present_state_Q (-0.07942633442713194)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6710421169758656 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10368060737607282) - present_state_Q (-0.09794620346012549)) * f3(0.18191530010260867)
w4 ( 0.6992929714934918 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10368060737607282) - present_state_Q (-0.09794620346012549)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6514805344915123 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10065631754637984) - present_state_Q (-0.09122190893257427)) * f3(0.17762465987898762)
w4 ( 0.7036981216562407 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10065631754637984) - present_state_Q (-0.09122190893257427)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6283707527043358 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09772208017372684) - present_state_Q (-0.1067014140929174)) * f3(0.20698905311793916)
w4 ( 0.7081640161446818 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09772208017372684) - present_state_Q (-0.1067014140929174)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6091290445274296 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09425561290565038) - present_state_Q (-0.08241262075711905)) * f3(0.1762322337987807)
w4 ( 0.7125313688728726 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09425561290565038) - present_state_Q (-0.08241262075711905)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6484440578957494 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09136935667911443) - present_state_Q (-0.09801676488357963)) * f3(0.20770314726438455)
w4 ( 0.7049599836750786 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09136935667911443) - present_state_Q (-0.09801676488357963)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6314760035550815 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09726660868436242) - present_state_Q (-0.0733868365281405)) * f3(0.1566599842163648)
w4 ( 0.7092924376646649 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09726660868436242) - present_state_Q (-0.0733868365281405)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6145549256112749 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09472140053326222) - present_state_Q (-0.07056265807455511)) * f3(0.15667159959232244)
w4 ( 0.7136125768571764 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09472140053326222) - present_state_Q (-0.07056265807455511)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.603303334956179 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09218323884169123) - present_state_Q (-0.037515468508775456)) * f3(0.1074923799810979)
w4 ( 0.7177995120267482 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09218323884169123) - present_state_Q (-0.037515468508775456)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6088615578981236 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09049550024342685) - present_state_Q (-0.04678444141191347)) * f3(0.12513841299827572)
w4 ( 0.7160228479924932 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09049550024342685) - present_state_Q (-0.04678444141191347)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6222191207037187 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09132923368471853) - present_state_Q (-0.012467601327385832)) * f3(0.06751701550841538)
w4 ( 0.7081092500912766 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09132923368471853) - present_state_Q (-0.012467601327385832)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6396769991194348 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0933328681055578) - present_state_Q (-0.026993519930736617)) * f3(0.0889041948306316)
w4 ( 0.7002545573182417 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0933328681055578) - present_state_Q (-0.026993519930736617)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -2.120722165052428 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3563170795844295) - present_state_Q ( -1.3804057993397505)) * f1( 0.4680105003978677)
w2 ( -2.3854670165331924 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.3563170795844295) - present_state_Q (-1.3804057993397505)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.0410982537074505 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2044971629643109) - present_state_Q ( -1.1783615078882639)) * f1( 0.3869160557333071)
w2 ( -2.354598339659315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2044971629643109) - present_state_Q (-1.1783615078882639)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9744003257339968 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.331975335957442) - present_state_Q ( -1.2142454189744762)) * f1( 0.4218589999092683)
w2 ( -2.3308826213786342 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.331975335957442) - present_state_Q (-1.2142454189744762)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8788641637631698 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3920828069685283) - present_state_Q ( -1.5865498406255578)) * f1( 0.39036709683306375)
w2 ( -2.2452256667811294 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3920828069685283) - present_state_Q (-1.5865498406255578)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.792342032859222 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3514901010708753) - present_state_Q ( -1.4793107126813032)) * f1( 0.3690962564951667)
w2 ( -2.163180007191032 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3514901010708753) - present_state_Q (-1.4793107126813032)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.821961984934437 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8419523744494152) - present_state_Q ( -1.0582703751685183)) * f1( 0.2887146336379006)
w2 ( -2.1888281287479425 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8419523744494152) - present_state_Q (-1.0582703751685183)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.785938426810847 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8900722572596058) - present_state_Q ( -1.0093149975530222)) * f1( 0.25363205664396204)
w2 ( -2.153320434452266 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8900722572596058) - present_state_Q (-1.0093149975530222)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.7932822798524222 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7840717213260779) - present_state_Q ( -0.8079594890283761)) * f1( 0.2715443133874064)
w2 ( -2.1573771496988297 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.7840717213260779) - present_state_Q (-0.8079594890283761)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.7789133001355952 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7926691113843102) - present_state_Q ( -0.7926691113843102)) * f1( 0.20141485002253443)
w2 ( -2.143109105693912 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.7926691113843102) - present_state_Q (-0.7926691113843102)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7563187519341235 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6782118339273526) - present_state_Q ( -0.6782118339273526)) * f1( 0.14030476514484744)
w2 ( -2.11090129268322 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6782118339273526) - present_state_Q (-0.6782118339273526)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6470343911323116 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10815928680467932) - present_state_Q (0.10815928680467932)) * f3(-0.08150820225050029)
w4 ( 0.707475810453248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10815928680467932) - present_state_Q (0.10815928680467932)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6607241464902687 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09705515866984675) - present_state_Q (0.09705515866984675)) * f3(-0.15)
w4 ( 0.707475810453248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09705515866984675) - present_state_Q (0.09705515866984675)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6742316480703883 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.003912726851551512) - present_state_Q (0.0991086219735403)) * f3(-0.15)
w4 ( 0.707475810453248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.003912726851551512) - present_state_Q (0.0991086219735403)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6878663289830458 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10113474721055823) - present_state_Q (0.10113474721055823)) * f3(-0.15)
w4 ( 0.707475810453248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10113474721055823) - present_state_Q (0.10113474721055823)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7014733996668551 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10317994934745686) - present_state_Q (0.10317994934745686)) * f3(-0.15)
w4 ( 0.707475810453248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10317994934745686) - present_state_Q (0.10317994934745686)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7146476433940532 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16496074903429744) - present_state_Q (0.10522100995002825)) * f3(-0.15)
w4 ( 0.707475810453248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16496074903429744) - present_state_Q (0.10522100995002825)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6780512590867412 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10719714650910797) - present_state_Q (-0.16571384970290867)) * f3(0.31107905636314653)
w4 ( 0.7168872789680786 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10719714650910797) - present_state_Q (-0.16571384970290867)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6912750026936396 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16709404010434167) - present_state_Q (0.10170768886301118)) * f3(-0.15)
w4 ( 0.7168872789680786 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16709404010434167) - present_state_Q (0.10170768886301118)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.704875170813185 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10369125040404593) - present_state_Q (0.10369125040404593)) * f3(-0.15)
w4 ( 0.7168872789680786 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10369125040404593) - present_state_Q (0.10369125040404593)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.666014043957014 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10573127562197775) - present_state_Q (-0.17390837731004602)) * f3(0.3280855521704617)
w4 ( 0.7263631310070565 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10573127562197775) - present_state_Q (-0.17390837731004602)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6792750367549689 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16031706876123322) - present_state_Q (0.0999021065935521)) * f3(-0.15)
w4 ( 0.7263631310070565 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16031706876123322) - present_state_Q (0.0999021065935521)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6549994117433697 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.16483745260034283) - present_state_Q (0.10189125551324534)) * f3(-0.15)
w4 ( 0.7263631310070565 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.16483745260034283) - present_state_Q (0.10189125551324534)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6232800594419833 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.163735749975919) - present_state_Q (0.09824991176150545)) * f3(-0.15)
w4 ( 0.7263631310070565 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.163735749975919) - present_state_Q (0.09824991176150545)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6896426568768225 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16532624035791105) - present_state_Q (-0.16532624035791105)) * f3(0.3584829763983065)
w4 ( 0.7115534799376335 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16532624035791105) - present_state_Q (-0.16532624035791105)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7479099586392378 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08487419444992693) - present_state_Q (-0.17900907587659465)) * f3(0.3214741467369108)
w4 ( 0.700678458909563 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08487419444992693) - present_state_Q (-0.17900907587659465)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7388954409729933 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11218649379588568) - present_state_Q (0.11218649379588568)) * f3(-0.15)
w4 ( 0.700678458909563 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11218649379588568) - present_state_Q (0.11218649379588568)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7519855791971861 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16489802241199528) - present_state_Q (0.11083431614594899)) * f3(-0.15)
w4 ( 0.700678458909563 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16489802241199528) - present_state_Q (0.11083431614594899)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7650445534390306 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16603880330792037) - present_state_Q (0.11279783687957791)) * f3(-0.15)
w4 ( 0.700678458909563 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16603880330792037) - present_state_Q (0.11279783687957791)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7784953382183165 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11475668301585458) - present_state_Q (0.11475668301585458)) * f3(-0.15)
w4 ( 0.700678458909563 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11475668301585458) - present_state_Q (0.11475668301585458)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7915616756757733 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.12136535436799911) - present_state_Q (0.11677430073274747)) * f3(-0.15)
w4 ( 0.700678458909563 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.12136535436799911) - present_state_Q (0.11677430073274747)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7692222507272206 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11873425135136599) - present_state_Q (-0.11489509849067422)) * f3(0.198260995760398)
w4 ( 0.7074390700513179 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11873425135136599) - present_state_Q (-0.11489509849067422)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7823196765591265 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.11454940263860275) - present_state_Q (0.11538333760908309)) * f3(-0.15)
w4 ( 0.7074390700513179 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.11454940263860275) - present_state_Q (0.11538333760908309)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7953881635539236 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1141958219632502) - present_state_Q (0.11734795148386896)) * f3(-0.15)
w4 ( 0.7074390700513179 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1141958219632502) - present_state_Q (0.11734795148386896)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.808777502522727 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11930822453308854) - present_state_Q (0.11930822453308854)) * f3(-0.15)
w4 ( 0.7074390700513179 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11930822453308854) - present_state_Q (0.11930822453308854)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8142996612360249 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10539460401727607) - present_state_Q (0.12131662537840904)) * f3(-0.15)
w4 ( 0.7074390700513179 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10539460401727607) - present_state_Q (0.12131662537840904)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7151810911008085 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2692735110823588) - present_state_Q ( -1.2940736521173613)) * f1( 0.6166212833840199)
w2 ( -2.1042298296731285 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2692735110823588) - present_state_Q (-1.2940736521173613)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.805150704422022 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12214494918540374) - present_state_Q (0.12214494918540374)) * f3(-0.15)
w4 ( 0.7074390700513179 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12214494918540374) - present_state_Q (0.12214494918540374)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8108769225524299 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12077260566330329) - present_state_Q (-0.08657793993643055)) * f3(0.142675777475655)
w4 ( 0.7058336908533289 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12077260566330329) - present_state_Q (-0.08657793993643055)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -1.7675379006216256 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3348156567056444) - present_state_Q ( -1.3348156567056444)) * f1( 0.6555533287838969)
w2 ( -2.112216488762778 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3348156567056444) - present_state_Q (-1.3348156567056444)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.8129772967430875 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3929337368035055) - present_state_Q ( -1.3929337368035055)) * f1( 0.6088136854721105)
w2 ( -2.1234118833159306 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3929337368035055) - present_state_Q (-1.3929337368035055)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.857179044532437 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3953464292750257) - present_state_Q ( -1.3953464292750257)) * f1( 0.5939592562532965)
w2 ( -2.1345747065207177 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3953464292750257) - present_state_Q (-1.3953464292750257)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8969553854191405 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5143517759343803) - present_state_Q ( -1.5514850990171873)) * f1( 0.6629941774671871)
w2 ( -2.1435739576993615 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.5143517759343803) - present_state_Q (-1.5514850990171873)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9289346889759194 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.640927122361679) - present_state_Q ( -1.67883153705824)) * f1( 0.6590122019354445)
w2 ( -2.15327918120292 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.640927122361679) - present_state_Q (-1.67883153705824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.936539090720415 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5165806186416904) - present_state_Q ( -1.5165806186416904)) * f1( 0.5629660706540712)
w2 ( -2.1559807300673697 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.5165806186416904) - present_state_Q (-1.5165806186416904)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.9779037692301114 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1584287688535708) - present_state_Q ( -1.2662278053569394)) * f1( 0.4868637562570916)
w2 ( -2.168724956140296 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1584287688535708) - present_state_Q (-1.2662278053569394)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7790194022629201 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.022031475844592002) - present_state_Q (0.12163153838286449)) * f3(-0.15)
w4 ( 0.7058336908533289 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.022031475844592002) - present_state_Q (0.12163153838286449)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7473669202784307 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.06687444706811513) - present_state_Q (0.11685291033943801)) * f3(-0.15)
w4 ( 0.7058336908533289 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.06687444706811513) - present_state_Q (0.11685291033943801)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7158535022648669 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1121050380417646) - present_state_Q (0.1121050380417646)) * f3(-0.15)
w4 ( 0.7058336908533289 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1121050380417646) - present_state_Q (0.1121050380417646)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6835959332156951 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4312657793838768) - present_state_Q (0.10737802533973002)) * f3(-0.15)
w4 ( 0.7058336908533289 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4312657793838768) - present_state_Q (0.10737802533973002)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0094032893745717 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6599929755766225) - present_state_Q ( -0.6599929755766225)) * f1( 0.22403540903057947)
w2 ( -2.1827850193601064 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6599929755766225) - present_state_Q (-0.6599929755766225)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.9774370384679654 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6972772789507152) - present_state_Q ( -0.8064165299187205)) * f1( 0.18406435781331795)
w2 ( -2.1480512433196335 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6972772789507152) - present_state_Q (-0.8064165299187205)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.9530713348445543 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5908682704930623) - present_state_Q ( -0.7197459237620866)) * f1( 0.1467230912813005)
w2 ( -2.114838061385378 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5908682704930623) - present_state_Q (-0.7197459237620866)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6797453850455188 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.011925990761323705) - present_state_Q (-0.011925990761323705)) * f3(0.03809657622725066)
w4 ( 0.7078551576366993 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.011925990761323705) - present_state_Q (-0.011925990761323705)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6933689006408017 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10196180775682782) - present_state_Q (0.10196180775682782)) * f3(-0.15)
w4 ( 0.7078551576366993 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10196180775682782) - present_state_Q (0.10196180775682782)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6664699200474187 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.13763781379979995) - present_state_Q (-0.13763781379979995)) * f3(0.2393415971669589)
w4 ( 0.7123506537663785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.13763781379979995) - present_state_Q (-0.13763781379979995)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6801203184593226 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09997048800711279) - present_state_Q (0.09997048800711279)) * f3(-0.15)
w4 ( 0.7123506537663785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09997048800711279) - present_state_Q (0.09997048800711279)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6937430748144425 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10201804776889839) - present_state_Q (0.10201804776889839)) * f3(-0.15)
w4 ( 0.7123506537663785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10201804776889839) - present_state_Q (0.10201804776889839)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7073382450879433 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10406146122216638) - present_state_Q (0.10406146122216638)) * f3(-0.15)
w4 ( 0.7123506537663785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10406146122216638) - present_state_Q (0.10406146122216638)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7209058851416402 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1061007367631915) - present_state_Q (0.1061007367631915)) * f3(-0.15)
w4 ( 0.7123506537663785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1061007367631915) - present_state_Q (0.1061007367631915)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7344460507242284 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10813588277124603) - present_state_Q (0.10813588277124603)) * f3(-0.15)
w4 ( 0.7123506537663785 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10813588277124603) - present_state_Q (0.10813588277124603)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6624300320359994 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.05934428346343485) - present_state_Q (-0.35060382723758465)) * f3(0.5355667255283009)
w4 ( 0.7204186701597259 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.05934428346343485) - present_state_Q (-0.35060382723758465)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6354532414941518 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.13103425435236998) - present_state_Q (-0.13103425435236998)) * f3(0.24131001528939136)
w4 ( 0.7248903934753945 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.13103425435236998) - present_state_Q (-0.13103425435236998)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6100966120852539 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.11680636769467391) - present_state_Q (-0.11680636769467391)) * f3(0.22944565219442903)
w4 ( 0.7293108963990953 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.11680636769467391) - present_state_Q (-0.11680636769467391)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6198444750717367 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.12391703655421628) - present_state_Q (-0.12391703655421628)) * f3(0.2509266063401572)
w4 ( 0.7277569977306905 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.12391703655421628) - present_state_Q (-0.12391703655421628)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6301256917687091 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.14218486757215149) - present_state_Q (-0.14218486757215149)) * f3(0.2763518178677878)
w4 ( 0.7262688632539502 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.14218486757215149) - present_state_Q (-0.14218486757215149)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6138496872428775 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.09451885376530637) - present_state_Q (0.09451885376530637)) * f3(-0.15)
w4 ( 0.7262688632539502 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.09451885376530637) - present_state_Q (0.09451885376530637)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6276066416262106 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09207745308643162) - present_state_Q (0.09207745308643162)) * f3(-0.15)
w4 ( 0.7262688632539502 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09207745308643162) - present_state_Q (0.09207745308643162)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6413357381769176 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09414099624393159) - present_state_Q (0.09414099624393159)) * f3(-0.15)
w4 ( 0.7262688632539502 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09414099624393159) - present_state_Q (0.09414099624393159)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6381801165032276 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09620036072653763) - present_state_Q (-0.005413017338928119)) * f3(0.031088856299642167)
w4 ( 0.7282989293607735 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09620036072653763) - present_state_Q (-0.005413017338928119)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6114601339360305 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09572701747548414) - present_state_Q (-0.12161383913276107)) * f3(0.23621199158189318)
w4 ( 0.7328236755242947 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09572701747548414) - present_state_Q (-0.12161383913276107)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -1.913205759545335 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0418486981983561) - present_state_Q ( -1.0418486981983561)) * f1( 0.4251585066276688)
w2 ( -2.105461423101593 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -1.0418486981983561) - present_state_Q (-1.0418486981983561)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.8957060517280153 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9855118987657027) - present_state_Q ( -1.0528275108514444)) * f1( 0.38522165935845404)
w2 ( -2.0986472782869696 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.9855118987657027) - present_state_Q (-1.0528275108514444)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8396390084209502 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0502364287116133) - present_state_Q ( -1.0502364287116133)) * f1( 0.3879500918922446)
w2 ( -2.076969086499363 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0502364287116133) - present_state_Q (-1.0502364287116133)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8444880451237837 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9577220975373217) - present_state_Q ( -0.9577220975373217)) * f1( 0.351251920406418)
w2 ( -2.079039838182609 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.9577220975373217) - present_state_Q (-0.9577220975373217)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8658684702892623 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8453832258451845) - present_state_Q ( -0.8453832258451845)) * f1( 0.2892549244373054)
w2 ( -2.090127164633699 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8453832258451845) - present_state_Q (-0.8453832258451845)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9042113750490333 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9111919306058027) - present_state_Q ( -0.9293037960345444)) * f1( 0.3300257928920496)
w2 ( -2.1075543955890894 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9111919306058027) - present_state_Q (-0.9293037960345444)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9254591701602628 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0109781352382379) - present_state_Q ( -1.0490622065006803)) * f1( 0.3848989963855478)
w2 ( -2.1158349296944365 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0109781352382379) - present_state_Q (-1.0490622065006803)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9015363776441674 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1383466535951048) - present_state_Q ( -1.160295998888368)) * f1( 0.43777649118575873)
w2 ( -2.1076380096915037 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1383466535951048) - present_state_Q (-1.160295998888368)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8046297970144003 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2783857545576858) - present_state_Q ( -1.2783857545576858)) * f1( 0.45061359997801115)
w2 ( -2.0646270661094652 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2783857545576858) - present_state_Q (-1.2783857545576858)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7862475370338127 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0955314647576058) - present_state_Q ( -1.0955314647576058)) * f1( 0.3782526769008379)
w2 ( -2.054907499743828 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0955314647576058) - present_state_Q (-1.0955314647576058)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.775048127048239 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8509554223113609) - present_state_Q ( -0.9537007972985523)) * f1( 0.3038320759568461)
w2 ( -2.04753539464248 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8509554223113609) - present_state_Q (-0.9537007972985523)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.736632408258649 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6162732486087521) - present_state_Q ( -0.7186500183408762)) * f1( 0.2318358037023075)
w2 ( -2.02268005424028 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6162732486087521) - present_state_Q (-0.7186500183408762)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5866801341796298 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.056796104668197504) - present_state_Q (-0.15578484576756438)) * f3(0.37462330020677737)
w4 ( 0.7394383200866386 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.056796104668197504) - present_state_Q (-0.15578484576756438)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6004921069079161 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08800202012694447) - present_state_Q (0.08800202012694447)) * f3(-0.15)
w4 ( 0.7394383200866386 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08800202012694447) - present_state_Q (0.08800202012694447)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6142761103914276 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09007381603618742) - present_state_Q (0.09007381603618742)) * f3(-0.15)
w4 ( 0.7394383200866386 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09007381603618742) - present_state_Q (0.09007381603618742)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6278919864323115 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0013351404902786235) - present_state_Q (0.09214141655871413)) * f3(-0.15)
w4 ( 0.7394383200866386 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0013351404902786235) - present_state_Q (0.09214141655871413)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6040066793139829 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.007302185281018292) - present_state_Q (-0.09278247811663952)) * f3(0.2184273414622132)
w4 ( 0.745999396266507 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.007302185281018292) - present_state_Q (-0.09278247811663952)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5948722984977757 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.18357719183381857) - present_state_Q (0.09060100189709742)) * f3(-0.15)
w4 ( 0.745999396266507 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.18357719183381857) - present_state_Q (0.09060100189709742)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.598207250430463 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.010609942979004146) - present_state_Q (-0.010609942979004146)) * f3(0.06799765080978246)
w4 ( 0.7440375920612314 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.010609942979004146) - present_state_Q (-0.010609942979004146)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6128890574510758 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.014439522123819402) - present_state_Q (-0.014439522123819402)) * f3(0.07388915091627879)
w4 ( 0.7360895743408772 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.014439522123819402) - present_state_Q (-0.014439522123819402)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5815134296179721 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0022483641075214285) - present_state_Q (0.09193335861766137)) * f3(-0.15)
w4 ( 0.7360895743408772 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0022483641075214285) - present_state_Q (0.09193335861766137)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5503358649229958 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08722701444269582) - present_state_Q (0.08722701444269582)) * f3(-0.15)
w4 ( 0.7360895743408772 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08722701444269582) - present_state_Q (0.08722701444269582)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5192214347965267 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08255037973844936) - present_state_Q (0.08255037973844936)) * f3(-0.15)
w4 ( 0.7360895743408772 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08255037973844936) - present_state_Q (0.08255037973844936)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5256700113910637 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.07788321521947901) - present_state_Q (0.07788321521947901)) * f3(-0.15)
w4 ( 0.7360895743408772 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.07788321521947901) - present_state_Q (0.07788321521947901)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5395134847594563 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.017487262681625433) - present_state_Q (0.07885050170865955)) * f3(-0.15)
w4 ( 0.7360895743408772 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.017487262681625433) - present_state_Q (0.07885050170865955)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5405040097144682 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.08092702271391844) - present_state_Q (0.01898338896064902)) * f3(0.01938819753068782)
w4 ( 0.7340460115941202 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.08092702271391844) - present_state_Q (0.01898338896064902)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5410100063825795 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08107560145717023) - present_state_Q (0.03216418776953531)) * f3(-0.005184692907737901)
w4 ( 0.737949785083625 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08107560145717023) - present_state_Q (0.03216418776953531)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5548421914799275 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.032971741139276874) - present_state_Q (0.08115150095738692)) * f3(-0.15)
w4 ( 0.737949785083625 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.032971741139276874) - present_state_Q (0.08115150095738692)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5543250670536561 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08322632872198912) - present_state_Q (0.02659536249613035)) * f3(0.005267495789062359)
w4 ( 0.7418766941651292 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08322632872198912) - present_state_Q (0.02659536249613035)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5543429015745522 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.08314876005804842) - present_state_Q (0.01509584017535025)) * f3(0.0263008629011607)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.08314876005804842) - present_state_Q (0.01509584017535025)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5457203571988638 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.08315143523618283) - present_state_Q (0.08315143523618283)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.08315143523618283) - present_state_Q (0.08315143523618283)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5369734239772932 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.012708278582139604) - present_state_Q (0.08185805357982957)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.012708278582139604) - present_state_Q (0.08185805357982957)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5283860527937392 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.08054601359659398) - present_state_Q (0.08054601359659398)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.08054601359659398) - present_state_Q (0.08054601359659398)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5346601273641532 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.024704540533354594) - present_state_Q (0.07925790791906087)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.024704540533354594) - present_state_Q (0.07925790791906087)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5035774406062409 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08019901910462299) - present_state_Q (0.08019901910462299)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08019901910462299) - present_state_Q (0.08019901910462299)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7025004178964784 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.150353241711451) - present_state_Q ( -1.1697316607640198)) * f1( 0.6153274884023999)
w2 ( -2.0199065725573155 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.150353241711451) - present_state_Q (-1.1697316607640198)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.7608447154797795 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.015934538398935) - present_state_Q ( -1.015934538398935)) * f1( 0.5374090955592956)
w2 ( -2.02533486713452 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.015934538398935) - present_state_Q (-1.015934538398935)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.7855003677197898 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0406149977125703) - present_state_Q ( -1.1418817410692963)) * f1( 0.5334645616946971)
w2 ( -2.02995666472154 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0406149977125703) - present_state_Q (-1.1418817410692963)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.8420711181025549 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1366658476150753) - present_state_Q ( -1.153786267778724)) * f1( 0.5893521242375849)
w2 ( -2.034756066306454 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1366658476150753) - present_state_Q (-1.153786267778724)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.7149367044219384 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.266242340119108) - present_state_Q ( -1.2873348938254316)) * f1( 0.5883916622672132)
w2 ( -2.0131489597083188 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.266242340119108) - present_state_Q (-1.2873348938254316)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.6294051332616173 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1521088413630123) - present_state_Q ( -1.1543334281328712)) * f1( 0.5557164469713054)
w2 ( -1.997757734268353 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1521088413630123) - present_state_Q (-1.1543334281328712)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.627555368638848 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1277022920459843) - present_state_Q ( -1.1446682088954947)) * f1( 0.5799002446845412)
w2 ( -1.997438754471444 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -1.1277022920459843) - present_state_Q (-1.1446682088954947)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.6839783996307824 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2130229363853557) - present_state_Q ( -1.2296651019603175)) * f1( 0.6328025739453115)
w2 ( -2.006355126388226 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2130229363853557) - present_state_Q (-1.2296651019603175)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.7309511617919708 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3380171776775518) - present_state_Q ( -1.438334933996963)) * f1( 0.6754134526239194)
w2 ( -2.016787128144788 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3380171776775518) - present_state_Q (-1.438334933996963)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.783739463757976 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.285317053512383) - present_state_Q ( -1.285317053512383)) * f1( 0.6260363461532128)
w2 ( -2.0252192746631765 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.285317053512383) - present_state_Q (-1.285317053512383)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.8351709189448357 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2338306962748349) - present_state_Q ( -1.2338306962748349)) * f1( 0.5781723114628857)
w2 ( -2.034114798396703 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2338306962748349) - present_state_Q (-1.2338306962748349)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.7895744180968787 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3467066420218106) - present_state_Q ( -1.442566950434287)) * f1( 0.5643855730617537)
w2 ( -2.017956872672061 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.3467066420218106) - present_state_Q (-1.442566950434287)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.698910431858053 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1131863852808173) - present_state_Q ( -1.1131863852808173)) * f1( 0.4528969827596921)
w2 ( -1.98792885647077 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1131863852808173) - present_state_Q (-1.1131863852808173)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.7433650374286707 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9664104040550363) - present_state_Q ( -0.9664104040550363)) * f1( 0.3933233106665931)
w2 ( -2.004882316016027 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9664104040550363) - present_state_Q (-0.9664104040550363)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.7502314191867476 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8824767025781576) - present_state_Q ( -0.8824767025781576)) * f1( 0.3336905023825542)
w2 ( -2.0079688805312217 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.8824767025781576) - present_state_Q (-0.8824767025781576)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.7583842053314496 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7799335141254702) - present_state_Q ( -0.7799335141254702)) * f1( 0.2735285041724566)
w2 ( -2.0124397780905277 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.7799335141254702) - present_state_Q (-0.7799335141254702)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.7447465821765504 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6698763683164292) - present_state_Q ( -0.6880077475999944)) * f1( 0.21960034656568625)
w2 ( -2.0031244764290026 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.6698763683164292) - present_state_Q (-0.6880077475999944)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.7387329692340747 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4559814751462603) - present_state_Q ( -0.4559814751462603)) * f1( 0.14653648278503348)
w2 ( -1.9990206431526862 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.4559814751462603) - present_state_Q (-0.4559814751462603)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.7169977407514716 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5267472330899554) - present_state_Q ( -0.5519242648159148)) * f1( 0.14497405455769966)
w2 ( -1.9765319000300825 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5267472330899554) - present_state_Q (-0.5519242648159148)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5175576962890133 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07553661609093613) - present_state_Q (0.07553661609093613)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07553661609093613) - present_state_Q (0.07553661609093613)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5315096419540281 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07763365444335199) - present_state_Q (0.07763365444335199)) * f3(-0.15)
w4 ( 0.7418495703084511 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07763365444335199) - present_state_Q (0.07763365444335199)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5283829320115364 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02306102916769564) - present_state_Q (0.02306102916769564)) * f3(-0.015472979438890292)
w4 ( 0.7378080604559493 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02306102916769564) - present_state_Q (0.02306102916769564)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5513331743501181 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.014597112358391864) - present_state_Q (-0.03215681012359262)) * f3(0.11671295343901106)
w4 ( 0.729942526541387 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.014597112358391864) - present_state_Q (-0.03215681012359262)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -1.6993339024930874 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.24546347790477) - present_state_Q ( -1.24546347790477)) * f1( 0.6007002216011688)
w2 ( -2.0164708829391373 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.24546347790477) - present_state_Q (-1.24546347790477)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.22167014027213106 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07553661609093613) - present_state_Q (-0.6985507276057481)) * f3(1.6523449685505178)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07553661609093613) - present_state_Q (-0.6985507276057481)) * f4(0.18)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.23622125823808 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03325052104081966) - present_state_Q (0.03325052104081966)) * f3(-0.15)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03325052104081966) - present_state_Q (0.03325052104081966)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20574291019014787 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.035433188735711996) - present_state_Q (0.035433188735711996)) * f3(-0.15)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.035433188735711996) - present_state_Q (0.035433188735711996)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17532628079701282 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03086143652852218) - present_state_Q (0.03086143652852218)) * f3(-0.15)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03086143652852218) - present_state_Q (0.03086143652852218)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14497124507839887 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026298942119551923) - present_state_Q (0.026298942119551923)) * f3(-0.15)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026298942119551923) - present_state_Q (0.026298942119551923)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.11467767830711512 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02174568676175983) - present_state_Q (0.02174568676175983)) * f3(-0.15)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02174568676175983) - present_state_Q (0.02174568676175983)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.08444545600854321 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.017201651746067265) - present_state_Q (0.017201651746067265)) * f3(-0.15)
w4 ( 0.7725594493143183 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.017201651746067265) - present_state_Q (0.017201651746067265)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7479179393192532 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.890466042345128) - present_state_Q ( -0.890466042345128)) * f1( 0.4053464437098855)
w2 ( -2.0284566885580313 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.890466042345128) - present_state_Q (-0.890466042345128)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.11609395900281269 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03358001879574654) - present_state_Q (0.01763422619139426)) * f3(0.15712096788057078)
w4 ( 0.764502344417071 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03358001879574654) - present_state_Q (0.01763422619139426)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.09898274625420683 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025111603282463202) - present_state_Q (0.025111603282463202)) * f3(-0.08460006427969116)
w4 ( 0.7604571435311626 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025111603282463202) - present_state_Q (0.025111603282463202)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.07863487292318752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025166813506376815) - present_state_Q (0.025166813506376815)) * f3(-0.10060006427969113)
w4 ( 0.7564118432668511 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025166813506376815) - present_state_Q (0.025166813506376815)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.02378787592623864 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.049844070835786194) - present_state_Q (0.030189131025599975)) * f3(1.1551714973308063)
w4 ( 0.7640085676837788 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.049844070835786194) - present_state_Q (0.030189131025599975)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.020571068991958298 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05670112778014859) - present_state_Q (0.06759378550604636)) * f3(1.0125848432494233)
w4 ( 0.7692654836110424 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05670112778014859) - present_state_Q (0.06759378550604636)) * f4(0.12)
============================================================================
GUIDE learning . . .
w1 ( -1.799713363972026 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.323854921852574) - present_state_Q ( -1.3271131140474284)) * f1( 0.6432037911513651)
w2 ( -2.0365094123394094 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.323854921852574) - present_state_Q (-1.3271131140474284)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.8388756750713346 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4048331434438455) - present_state_Q ( -1.4887649383101191)) * f1( 0.6009084988152852)
w2 ( -2.0495437798600946 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.4048331434438455) - present_state_Q (-1.4887649383101191)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.8837203483648604 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2833094165742207) - present_state_Q ( -1.2833094165742207)) * f1( 0.5306926742382133)
w2 ( -2.062219102736343 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2833094165742207) - present_state_Q (-1.2833094165742207)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.9031572841964155 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2198818492181132) - present_state_Q ( -1.2198818492181132)) * f1( 0.483378004913549)
w2 ( -2.0682506977718984 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2198818492181132) - present_state_Q (-1.2198818492181132)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8138876192796947 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1380534628246406) - present_state_Q ( -1.1461779061557138)) * f1( 0.43923868428083934)
w2 ( -2.0377651093737996 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1380534628246406) - present_state_Q (-1.1461779061557138)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8368397809299086 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0084509953126246) - present_state_Q ( -1.0084509953126246)) * f1( 0.3874475030518347)
w2 ( -2.0466510209370794 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0084509953126246) - present_state_Q (-1.0084509953126246)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8751664243344133 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8974585520044235) - present_state_Q ( -0.8974585520044235)) * f1( 0.32145476431533837)
w2 ( -2.0645353304850196 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8974585520044235) - present_state_Q (-0.8974585520044235)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.8849853552883333 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6634849893356455) - present_state_Q ( -0.6634849893356455)) * f1( 0.24372847676673068)
w2 ( -2.068563965580999 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.6634849893356455) - present_state_Q (-0.6634849893356455)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.913842030860784 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6106901145356171) - present_state_Q ( -0.7141183128146671)) * f1( 0.21423705857690628)
w2 ( -2.088768226060582 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6106901145356171) - present_state_Q (-0.7141183128146671)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14041517196129263 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1054818070556486) - present_state_Q (0.09283053497757414)) * f3(0.7731239743878713)
w4 ( 0.7484426600683223 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1054818070556486) - present_state_Q (0.09283053497757414)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1767131063605653 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.004505364789553892) - present_state_Q (0.004505364789553892)) * f3(0.1811224617535616)
w4 ( 0.7404264407550799 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.004505364789553892) - present_state_Q (0.004505364789553892)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20913927523605658 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0009790293449340563) - present_state_Q (0.0009790293449340563)) * f3(0.1620594469480726)
w4 ( 0.7324229162494381 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0009790293449340563) - present_state_Q (0.0009790293449340563)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2446086592142893 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.00792605370943952) - present_state_Q (-0.00792605370943952)) * f3(0.1779817316350709)
w4 ( 0.7244514500427921 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.00792605370943952) - present_state_Q (-0.00792605370943952)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.37794279108628004 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03669129888214339) - present_state_Q (-0.09949766549321697)) * f3(0.702930186730903)
w4 ( 0.7054831179966065 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03669129888214339) - present_state_Q (-0.09949766549321697)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.34717745693433033 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.056691418662942) - present_state_Q (0.056691418662942)) * f3(-0.15)
w4 ( 0.7054831179966065 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.056691418662942) - present_state_Q (0.056691418662942)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3389744225840383 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05207661854014955) - present_state_Q (0.05207661854014955)) * f3(-0.15)
w4 ( 0.7054831179966065 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05207661854014955) - present_state_Q (0.05207661854014955)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.36205798415571067 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.050846163387605746) - present_state_Q (-0.17107479997762343)) * f3(0.7128063230711191)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.050846163387605746) - present_state_Q (-0.17107479997762343)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.37632481673779533 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0543086976233566) - present_state_Q (0.0543086976233566)) * f3(-0.15)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0543086976233566) - present_state_Q (0.0543086976233566)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.881111681331425 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8682944996490968) - present_state_Q ( -0.8871417985799341)) * f1( 0.4089696927206006)
w2 ( -2.084766664317507 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.8682944996490968) - present_state_Q (-0.8871417985799341)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3755627589839013 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0564487225106693) - present_state_Q (0.0564487225106693)) * f3(-0.15)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0564487225106693) - present_state_Q (0.0564487225106693)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3898022443969589 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05633441384758519) - present_state_Q (0.05633441384758519)) * f3(-0.15)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05633441384758519) - present_state_Q (0.05633441384758519)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.40401289485205505 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05847033665954383) - present_state_Q (0.05847033665954383)) * f3(-0.15)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05847033665954383) - present_state_Q (0.05847033665954383)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.770519401944041 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1041430393069767) - present_state_Q ( -1.2027534661727433)) * f1( 0.5285580912650848)
w2 ( -2.0638432726950864 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1041430393069767) - present_state_Q (-1.2027534661727433)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.827009204676287 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0491243648916853) - present_state_Q ( -1.064579585501692)) * f1( 0.5429973943303466)
w2 ( -2.0690449369500237 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0491243648916853) - present_state_Q (-1.064579585501692)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.37319476873997964 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.060601934227808255) - present_state_Q (0.060601934227808255)) * f3(-0.15)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.060601934227808255) - present_state_Q (0.060601934227808255)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3424390493332812 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05597921531099694) - present_state_Q (0.05597921531099694)) * f3(-0.15)
w4 ( 0.7022447121597702 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05597921531099694) - present_state_Q (0.05597921531099694)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8822916618019143 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.155286173742505) - present_state_Q ( -1.155286173742505)) * f1( 0.5757135345584479)
w2 ( -2.0738461491681823 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.155286173742505) - present_state_Q (-1.155286173742505)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.9351843582128965 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.037156669993436) - present_state_Q ( -1.037156669993436)) * f1( 0.4959190870778353)
w2 ( -2.079178944153212 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.037156669993436) - present_state_Q (-1.037156669993436)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.34849647998727407 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05136585739999217) - present_state_Q (0.0036657573055509218)) * f3(0.03030944326545807)
w4 ( 0.6982476538166391 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05136585739999217) - present_state_Q (0.0036657573055509218)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3177225975474323 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.006823093419740356) - present_state_Q (0.052274471998091106)) * f3(-0.15)
w4 ( 0.6982476538166391 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.006823093419740356) - present_state_Q (0.052274471998091106)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.35795112365782805 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04765838963211485) - present_state_Q (-0.037352443103824406)) * f3(0.20546964477949692)
w4 ( 0.6904161269449073 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04765838963211485) - present_state_Q (-0.037352443103824406)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.33895221595880104 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05369266854867421) - present_state_Q (0.04711646357642693)) * f3(-0.09305220415893606)
w4 ( 0.6863326325514642 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05369266854867421) - present_state_Q (0.04711646357642693)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -1.9756933496098772 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6623575910748504) - present_state_Q ( -0.6623575910748504)) * f1( 0.28855061870325344)
w2 ( -2.086198334993375 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6623575910748504) - present_state_Q (-0.6623575910748504)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.30826583772148447 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.050842832393820155) - present_state_Q (0.050842832393820155)) * f3(-0.15)
w4 ( 0.6863326325514642 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.050842832393820155) - present_state_Q (0.050842832393820155)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.27764159940009847 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04623987565822267) - present_state_Q (0.04623987565822267)) * f3(-0.15)
w4 ( 0.6863326325514642 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04623987565822267) - present_state_Q (0.04623987565822267)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.29207937516131327 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.041646239910014766) - present_state_Q (0.041646239910014766)) * f3(-0.15)
w4 ( 0.6863326325514642 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.041646239910014766) - present_state_Q (0.041646239910014766)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3064879144266116 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04381190627419699) - present_state_Q (0.04381190627419699)) * f3(-0.15)
w4 ( 0.6863326325514642 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04381190627419699) - present_state_Q (0.04381190627419699)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3208672763998977 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04597318716399174) - present_state_Q (0.04597318716399174)) * f3(-0.15)
w4 ( 0.6863326325514642 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04597318716399174) - present_state_Q (0.04597318716399174)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.325376474323652 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.048130091459984654) - present_state_Q (-0.0018761549755937704)) * f3(0.09140682903762039)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.048130091459984654) - present_state_Q (-0.0018761549755937704)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3246335252561083 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.0072346668770177334) - present_state_Q (0.0488064711485478)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.0072346668770177334) - present_state_Q (0.0488064711485478)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.32387854458212817 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.016370161435925585) - present_state_Q (0.04869502878841624)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.016370161435925585) - present_state_Q (0.04869502878841624)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3232226905293494 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.04858178168731923) - present_state_Q (0.04858178168731923)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.04858178168731923) - present_state_Q (0.04858178168731923)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2925681645810274 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0484834035794024) - present_state_Q (0.0484834035794024)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0484834035794024) - present_state_Q (0.0484834035794024)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2617981572996681 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.07448594070132358) - present_state_Q (0.04388522468715411)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.07448594070132358) - present_state_Q (0.04388522468715411)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.25364265696078386 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04430298997332218) - present_state_Q (0.039269723594950214)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04430298997332218) - present_state_Q (0.039269723594950214)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.23062903058043827 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03804639854411758) - present_state_Q (0.03804639854411758)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03804639854411758) - present_state_Q (0.03804639854411758)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.22259954539043658 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.007046580797136923) - present_state_Q (0.03459435458706574)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.007046580797136923) - present_state_Q (0.03459435458706574)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.19214878131102095 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03338993180856548) - present_state_Q (0.03338993180856548)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03338993180856548) - present_state_Q (0.03338993180856548)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.19925968002886613 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.028822317196653142) - present_state_Q (0.028822317196653142)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.028822317196653142) - present_state_Q (0.028822317196653142)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17632017842496536 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.005888450776116945) - present_state_Q (0.029888952004329918)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.005888450776116945) - present_state_Q (0.029888952004329918)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.16846313006365482 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.026448026763744803) - present_state_Q (0.026448026763744803)) * f3(-0.15)
w4 ( 0.6843593892079505 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.026448026763744803) - present_state_Q (0.026448026763744803)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20227712105216933 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025269469509548222) - present_state_Q (0.012723989264146127)) * f3(0.16821232086583798)
w4 ( 0.6722982069540713 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025269469509548222) - present_state_Q (0.012723989264146127)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1718675098820387 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.030341568157825398) - present_state_Q (0.030341568157825398)) * f3(-0.15)
w4 ( 0.6722982069540713 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.030341568157825398) - present_state_Q (0.030341568157825398)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.911818021340981 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6431487984003774) - present_state_Q ( -1.6709286583453804)) * f1( 0.634556466768621)
w2 ( -2.066066059423268 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.6431487984003774) - present_state_Q (-1.6709286583453804)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.949413583952541 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3043752872092127) - present_state_Q ( -1.4076785901803759)) * f1( 0.5201673837126966)
w2 ( -2.0805212381940787 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3043752872092127) - present_state_Q (-1.4076785901803759)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.9913688658978537 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2087993627337141) - present_state_Q ( -1.2087993627337141)) * f1( 0.45999534649104673)
w2 ( -2.0942024467971736 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.2087993627337141) - present_state_Q (-1.2087993627337141)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -2.0304792672796155 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9738871956717563) - present_state_Q ( -1.078597318011615)) * f1( 0.3838901793050589)
w2 ( -2.109484317820507 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9738871956717563) - present_state_Q (-1.078597318011615)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14151947817452756 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025780126482305803) - present_state_Q (0.025780126482305803)) * f3(-0.15)
w4 ( 0.6722982069540713 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.025780126482305803) - present_state_Q (0.025780126482305803)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.11117860693587223 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.014968275175099341) - present_state_Q (0.021227921726179134)) * f3(-0.15)
w4 ( 0.6722982069540713 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.014968275175099341) - present_state_Q (0.021227921726179134)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.08093806011009391 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.00640335988493132) - present_state_Q (0.016676791040380834)) * f3(-0.15)
w4 ( 0.6722982069540713 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.00640335988493132) - present_state_Q (0.016676791040380834)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0552612062729816 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5767978685841515) - present_state_Q ( -0.682272084475177)) * f1( 0.18017885860625243)
w2 ( -2.1301154333562553 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5767978685841515) - present_state_Q (-0.682272084475177)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.05082957389926246 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.049082949610841295) - present_state_Q (0.012140709016514087)) * f3(-0.15)
w4 ( 0.6722982069540713 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.049082949610841295) - present_state_Q (0.012140709016514087)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.09112371509570905 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04374086650724689) - present_state_Q (0.04374086650724689)) * f3(0.19758162972175825)
w4 ( 0.6559832727152191 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04374086650724689) - present_state_Q (0.04374086650724689)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.0912931080724834 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.013905573224955697) - present_state_Q (0.013905573224955697)) * f3(0.13535178708087872)
w4 ( 0.6559332126516092 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.013905573224955697) - present_state_Q (0.013905573224955697)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18558909996978118 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.008891435913957281) - present_state_Q (0.00849225732697724)) * f3(0.6254696016357282)
w4 ( 0.6408571815142534 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.008891435913957281) - present_state_Q (0.00849225732697724)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.35191787198482877 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.027838364995467175) - present_state_Q (-0.13726460172373908)) * f3(1.223049237119495)
w4 ( 0.6218178596493794 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.027838364995467175) - present_state_Q (-0.13726460172373908)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.48324945930970187 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.052787680797724314) - present_state_Q (-0.2996993664194894)) * f3(1.098988990212681)
w4 ( 0.6050875535323691 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.052787680797724314) - present_state_Q (-0.2996993664194894)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5988499802546907 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.07248741889645528) - present_state_Q (-0.45210901354669164)) * f3(1.1108574685381878)
w4 ( 0.5905185621084779 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.07248741889645528) - present_state_Q (-0.45210901354669164)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4853149305614496 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0898274970382036) - present_state_Q (-0.5558411046596128)) * f3(1.0662331542254373)
w4 ( 0.6054260960695659 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0898274970382036) - present_state_Q (-0.5558411046596128)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.33109283035031856 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07279723958421744) - present_state_Q (-0.44331760882188365)) * f3(1.0631627173582303)
w4 ( 0.6228332640629295 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07279723958421744) - present_state_Q (-0.44331760882188365)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20038951981551892 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.049663924552547783) - present_state_Q (-0.26580155990523796)) * f3(1.0285379820290084)
w4 ( 0.6380824794912554 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.049663924552547783) - present_state_Q (-0.26580155990523796)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1034661066058356 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.030058427972327835) - present_state_Q (-0.09958311172453947)) * f3(0.8790530034986798)
w4 ( 0.6513135469455167 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.030058427972327835) - present_state_Q (-0.09958311172453947)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.020876458215361465 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01551991599087534) - present_state_Q (-0.0066034079230482545)) * f3(0.8192154545779501)
w4 ( 0.6634114117397824 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01551991599087534) - present_state_Q (-0.0066034079230482545)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.046604488965011376 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.051567865778091485) - present_state_Q (0.051567865778091485)) * f3(0.7076523825778159)
w4 ( 0.6729473009477795 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.051567865778091485) - present_state_Q (0.051567865778091485)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.10563506774639556 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09745044920498006) - present_state_Q (0.09745044920498006)) * f3(0.6470561051069929)
w4 ( 0.6820702469049347 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09745044920498006) - present_state_Q (0.09745044920498006)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0800829282041828 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11306243350504663) - present_state_Q (0.11306243350504663)) * f3(0.42462611868854233)
w4 ( 0.6760526850033892 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11306243350504663) - present_state_Q (0.11306243350504663)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.11736745512731434 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08646229000245953) - present_state_Q (0.08646229000245953)) * f3(0.40430683453077393)
w4 ( 0.6834301565153715 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08646229000245953) - present_state_Q (0.08646229000245953)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07929222034543101 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06960346774146331) - present_state_Q (0.06960346774146331)) * f3(0.24365918405165823)
w4 ( 0.6740542977895676 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06960346774146331) - present_state_Q (0.06960346774146331)) * f4(0.06)
============================================================================
GUIDE learning . . .
w1 ( -2.0615269755711325 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9635894532167533) - present_state_Q ( -0.9422760186803152)) * f1( 0.4066491618980594)
w2 ( -2.130885847989462 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.9635894532167533) - present_state_Q (-0.9422760186803152)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.065972298360972 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9779909588235387) - present_state_Q ( -0.9779909588235387)) * f1( 0.3710368009192222)
w2 ( -2.13208392936005 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.9779909588235387) - present_state_Q (-0.9779909588235387)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.1035997506946056 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8365504473336016) - present_state_Q ( -0.8365504473336016)) * f1( 0.30171849588308697)
w2 ( -2.1445549753340476 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8365504473336016) - present_state_Q (-0.8365504473336016)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.135369268731055 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6018674317245192) - present_state_Q ( -0.7090951804912216)) * f1( 0.235139637563889)
w2 ( -2.15806589096086 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6018674317245192) - present_state_Q (-0.7090951804912216)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -2.1659502100024057 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5127244256511581) - present_state_Q ( -0.5400001654266466)) * f1( 0.202352294381092)
w2 ( -2.165622252346552 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5127244256511581) - present_state_Q (-0.5400001654266466)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -2.192686847949596 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5575921631592916) - present_state_Q ( -0.6198670763868991)) * f1( 0.18620227246673227)
w2 ( -2.1799811737458423 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5575921631592916) - present_state_Q (-0.6198670763868991)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07397230920596345 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.07031000830333708) - present_state_Q (0.05793828086881352)) * f3(0.050622079040814776)
w4 ( 0.6656470395492597 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.07031000830333708) - present_state_Q (0.05793828086881352)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.06679538319581331 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.024873571275640226) - present_state_Q (0.023502543773365484)) * f3(0.137748883220734)
w4 ( 0.6646050091759681 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.024873571275640226) - present_state_Q (0.023502543773365484)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05158217469203165 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04194592772738741) - present_state_Q (-0.010019307479371997)) * f3(-0.15)
w4 ( 0.6646050091759681 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04194592772738741) - present_state_Q (-0.010019307479371997)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1203295053423847 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6615258816605372) - present_state_Q ( -1.6776048729937)) * f1( 0.7153806827332536)
w2 ( -2.174923912321704 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.6615258816605372) - present_state_Q (-1.6776048729937)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.9768211583692266 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5082458988560838) - present_state_Q ( -1.5082458988560838)) * f1( 0.608751377732437)
w2 ( -2.151349699231999 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.5082458988560838) - present_state_Q (-1.5082458988560838)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.819757357232816 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5861913089292143) - present_state_Q ( -1.5963663054372617)) * f1( 0.6442989772545572)
w2 ( -2.114783491613834 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.5861913089292143) - present_state_Q (-1.5963663054372617)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.6814657494141967 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.401098616454679) - present_state_Q ( -1.420694224261247)) * f1( 0.6063867230063877)
w2 ( -2.0805747261745973 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.401098616454679) - present_state_Q (-1.420694224261247)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.5607275998074306 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3373906384880898) - present_state_Q ( -1.3373906384880898)) * f1( 0.547900362272697)
w2 ( -2.0365016946818115 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.3373906384880898) - present_state_Q (-1.3373906384880898)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4547557841799297 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.201830389438248) - present_state_Q ( -1.201830389438248)) * f1( 0.5090766964074438)
w2 ( -1.9948687476719231 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.201830389438248) - present_state_Q (-1.201830389438248)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.360080843912014 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1696784008554346) - present_state_Q ( -1.1696784008554346)) * f1( 0.46121914154524973)
w2 ( -1.9435509836526759 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1696784008554346) - present_state_Q (-1.1696784008554346)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.3389919669821755 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0749241472565056) - present_state_Q ( -1.0856916565584427)) * f1( 0.44100607205087294)
w2 ( -1.9315960026068562 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0749241472565056) - present_state_Q (-1.0856916565584427)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.3798482858364256 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9672506956502531) - present_state_Q ( -0.9672506956502531)) * f1( 0.36172860401109985)
w2 ( -1.9598328619547254 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9672506956502531) - present_state_Q (-0.9672506956502531)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.4169173339277785 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6945466896909432) - present_state_Q ( -0.7925383327886795)) * f1( 0.2903013066794651)
w2 ( -1.9853711886783336 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6945466896909432) - present_state_Q (-0.7925383327886795)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4472164056029742 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6168665023258271) - present_state_Q ( -0.7161350617597437)) * f1( 0.2251795615624389)
w2 ( -2.0122822204477906 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6168665023258271) - present_state_Q (-0.7161350617597437)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4743440718177303 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5884556943278936) - present_state_Q ( -0.6890698053502831)) * f1( 0.19804457726645874)
w2 ( -2.0396777357294407 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5884556943278936) - present_state_Q (-0.6890698053502831)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.002032282868089069 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08313646546658789) - present_state_Q (0.07981099023543473)) * f3(0.258819822885449)
w4 ( 0.6438900357390803 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08313646546658789) - present_state_Q (0.07981099023543473)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.011361061126148705 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06384733180388447) - present_state_Q (0.05116319504381729)) * f3(0.17123985089554217)
w4 ( 0.6395318080441729 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06384733180388447) - present_state_Q (0.05116319504381729)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.008810916508006805 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.059826303116461345) - present_state_Q (0.025285846157886614)) * f3(0.026003395334291947)
w4 ( 0.643454595180788 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.059826303116461345) - present_state_Q (0.025285846157886614)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.012835820653573169 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02378917917076564) - present_state_Q (0.02378917917076564)) * f3(0.22120339407310768)
w4 ( 0.6473689541357732 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02378917917076564) - present_state_Q (0.02378917917076564)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05259243465071384 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03114501317121388) - present_state_Q (0.03114501317121388)) * f3(0.4090315023466311)
w4 ( 0.6512568320883568 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03114501317121388) - present_state_Q (0.03114501317121388)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.037377976228663554 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06408362939078752) - present_state_Q (-0.007888865197607075)) * f3(-0.15)
w4 ( 0.6512568320883568 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06408362939078752) - present_state_Q (-0.007888865197607075)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.014406035256981857 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.03437901921444035) - present_state_Q (0.03437901921444035)) * f3(0.22282495659888407)
w4 ( 0.6471330676191849 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.03437901921444035) - present_state_Q (0.03437901921444035)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.027427565908670792 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.044621866907841375) - present_state_Q (0.044621866907841375)) * f3(0.4021844141942028)
w4 ( 0.6408921095378826 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.044621866907841375) - present_state_Q (0.044621866907841375)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.06616510056792613 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.02809075203907551) - present_state_Q (0.02809075203907551)) * f3(0.37782333903430404)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.02809075203907551) - present_state_Q (0.02809075203907551)) * f4(0.06)
============================================================================
GUIDE learning . . .
w1 ( -1.4107995362939088 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.682390928805871) - present_state_Q ( -0.682390928805871)) * f1( 0.39367136417743426)
w2 ( -2.0316069765498144 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.682390928805871) - present_state_Q (-0.682390928805871)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.058541786952043304 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01703857359667214) - present_state_Q (0.009924765085188919)) * f3(-0.15)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01703857359667214) - present_state_Q (0.009924765085188919)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.07344085758655552 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.020526436769544094) - present_state_Q (0.008781268042806495)) * f3(-0.15)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.020526436769544094) - present_state_Q (0.008781268042806495)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.08830205984630321 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01762945954495703) - present_state_Q (0.011016128637983328)) * f3(-0.15)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01762945954495703) - present_state_Q (0.011016128637983328)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.10312324817511445 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.013245308976945482) - present_state_Q (0.013245308976945482)) * f3(-0.15)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.013245308976945482) - present_state_Q (0.013245308976945482)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1179004400507672 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.006146122697841899) - present_state_Q (0.015468487226267168)) * f3(-0.15)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.006146122697841899) - present_state_Q (0.015468487226267168)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.13264059976950301 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0036238059000235495) - present_state_Q (0.01768506600761508)) * f3(-0.15)
w4 ( 0.6347404194768715 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0036238059000235495) - present_state_Q (0.01768506600761508)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.10047240302802277 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01989608996542545) - present_state_Q (-0.0165037247662235)) * f3(0.31584101412462523)
w4 ( 0.6388143928119226 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01989608996542545) - present_state_Q (-0.0165037247662235)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.11525136341110147 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.003348859927831613) - present_state_Q (0.015070860454203415)) * f3(-0.15)
w4 ( 0.6388143928119226 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.003348859927831613) - present_state_Q (0.015070860454203415)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.12999158461697607 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0003088176336010426) - present_state_Q (0.017287704511665218)) * f3(-0.15)
w4 ( 0.6388143928119226 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0003088176336010426) - present_state_Q (0.017287704511665218)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14468984882167302 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0061698199432441495) - present_state_Q (0.01949873769254641)) * f3(-0.15)
w4 ( 0.6388143928119226 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0061698199432441495) - present_state_Q (0.01949873769254641)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.15934171259873756 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.015056042057796114) - present_state_Q (0.021703477323250953)) * f3(-0.15)
w4 ( 0.6388143928119226 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.015056042057796114) - present_state_Q (0.021703477323250953)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.15569450629537615 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0030009050378691678) - present_state_Q (0.0069224873405434725)) * f3(0.03673740177775244)
w4 ( 0.6407999476562339 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0030009050378691678) - present_state_Q (0.0069224873405434725)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17030926518327566 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.02328564862393915) - present_state_Q (0.023354175944306423)) * f3(-0.15)
w4 ( 0.6407999476562339 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.02328564862393915) - present_state_Q (0.023354175944306423)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3972265313954464 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.497375516110825) - present_state_Q ( -0.5140193146165039)) * f1( 0.29234413194695763)
w2 ( -2.0292855677347874 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.497375516110825) - present_state_Q (-0.5140193146165039)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18693570833732545 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03588006083975814) - present_state_Q (0.022306945573216524)) * f3(0.32052893943724314)
w4 ( 0.6345753203823631 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03588006083975814) - present_state_Q (0.022306945573216524)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1962361804184411 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.036971240260255334) - present_state_Q (0.03045184115762637)) * f3(0.17656172367588102)
w4 ( 0.629307773211047 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.036971240260255334) - present_state_Q (0.03045184115762637)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1763273238049711 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.031844375656621675) - present_state_Q (0.031844375656621675)) * f3(-0.09813796900926106)
w4 ( 0.6252504533348651 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.031844375656621675) - present_state_Q (0.031844375656621675)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14597026097426602 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026449098570745663) - present_state_Q (0.026449098570745663)) * f3(-0.15)
w4 ( 0.6252504533348651 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026449098570745663) - present_state_Q (0.026449098570745663)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.26419706481161676 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0218955391461399) - present_state_Q (-0.016458930056682197)) * f3(0.7981009406484443)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0218955391461399) - present_state_Q (-0.016458930056682197)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2334024382436501 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1334554480936861) - present_state_Q (0.039629559721742515)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1334554480936861) - present_state_Q (0.039629559721742515)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2329297983062067 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.03501036573654751) - present_state_Q (0.03501036573654751)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.03501036573654751) - present_state_Q (0.03501036573654751)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.22495811546463662 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.034939469745931005) - present_state_Q (0.034939469745931005)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.034939469745931005) - present_state_Q (0.034939469745931005)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.21700257528082073 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03374371731969549) - present_state_Q (0.03374371731969549)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03374371731969549) - present_state_Q (0.03374371731969549)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18656314506587707 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03255038629212311) - present_state_Q (0.03255038629212311)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03255038629212311) - present_state_Q (0.03255038629212311)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.15605183866634093 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.06102621542527173) - present_state_Q (0.02798447175988156)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.06102621542527173) - present_state_Q (0.02798447175988156)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1257358336930416 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02340777579995114) - present_state_Q (0.02340777579995114)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02340777579995114) - present_state_Q (0.02340777579995114)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.09548121862981318 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.018860375053956237) - present_state_Q (0.018860375053956237)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.018860375053956237) - present_state_Q (0.018860375053956237)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.06528967416086782 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.015525515314485622) - present_state_Q (0.014322182794471977)) * f3(-0.15)
w4 ( 0.6015488290784058 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.015525515314485622) - present_state_Q (0.014322182794471977)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.09964140295170708 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04255183253410984) - present_state_Q (0.04255183253410984)) * f3(0.6381560954678408)
w4 ( 0.594012675988476 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04255183253410984) - present_state_Q (0.04255183253410984)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.10097776644668983 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.025787012406183425) - present_state_Q (0.025787012406183425)) * f3(0.5758124688389914)
w4 ( 0.593687759632158 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.025787012406183425) - present_state_Q (0.025787012406183425)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.10190530510379568 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02204247881453491) - present_state_Q (0.020755818617164766)) * f3(0.4999785033406154)
w4 ( 0.5934651407833295 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02204247881453491) - present_state_Q (0.020755818617164766)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.12381378133837305 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02470109939323596) - present_state_Q (0.015859227779036922)) * f3(0.42674212353323543)
w4 ( 0.5883312496049323 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02470109939323596) - present_state_Q (0.015859227779036922)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.23799890446057648 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.018572067200755958) - present_state_Q (0.012035850284045899)) * f3(0.5680347042179175)
w4 ( 0.5601887485950368 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.018572067200755958) - present_state_Q (0.012035850284045899)) * f4(0.14)
============================================================================
GUIDE learning . . .
w1 ( -1.4179772323396724 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5019850686525746) - present_state_Q ( -0.49025487033484483)) * f1( 0.133022119891326)
w2 ( -2.0526847222827436 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5019850686525746) - present_state_Q (-0.49025487033484483)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20751882789087206 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03694731022124207) - present_state_Q (0.03569983566908647)) * f3(-0.15)
w4 ( 0.5601887485950368 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03694731022124207) - present_state_Q (0.03569983566908647)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17709860226439303 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.031127824183630807) - present_state_Q (0.031127824183630807)) * f3(-0.15)
w4 ( 0.5601887485950368 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.031127824183630807) - present_state_Q (0.031127824183630807)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.22351940002474951 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026564790339658955) - present_state_Q (-0.007708096205617784)) * f3(0.23331308431013836)
w4 ( 0.5482509360464742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026564790339658955) - present_state_Q (-0.007708096205617784)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.26288224526854337 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.008219830860907683) - present_state_Q (-0.015438469581012443)) * f3(0.2652948444661379)
w4 ( 0.536381019667811 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.008219830860907683) - present_state_Q (-0.015438469581012443)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2698499087218746 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.039432336790281505) - present_state_Q (0.039432336790281505)) * f3(-0.15)
w4 ( 0.536381019667811 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.039432336790281505) - present_state_Q (0.039432336790281505)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.26826204224432687 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.040477486308281185) - present_state_Q (0.01273401852539227)) * f3(0.03231878899877209)
w4 ( 0.5383462745882328 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.040477486308281185) - present_state_Q (0.01273401852539227)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2752188116087821 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04023930633664903) - present_state_Q (0.04023930633664903)) * f3(-0.15)
w4 ( 0.5383462745882328 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04023930633664903) - present_state_Q (0.04023930633664903)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2673519153446345 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03532277970486835) - present_state_Q (-4.025326387611938e-05)) * f3(0.0783889157913834)
w4 ( 0.5423605647131703 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03532277970486835) - present_state_Q (-4.025326387611938e-05)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.25514453899062617 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04010278730169517) - present_state_Q (3.423039455557192e-05)) * f3(0.12159031457220132)
w4 ( 0.548384421003184 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04010278730169517) - present_state_Q (3.423039455557192e-05)) * f4(0.06)
============================================================================
GUIDE learning . . .
w1 ( -1.461979062633079 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3753762738090794) - present_state_Q ( -0.3753762738090794)) * f1( 0.2647265874570538)
w2 ( -2.0526847222827436 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3753762738090794) - present_state_Q (-0.3753762738090794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4338495771852322 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0096106868683992) - present_state_Q ( -1.1122449229825364)) * f1( 0.5501735525483342)
w2 ( -2.045015464468308 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0096106868683992) - present_state_Q (-1.1122449229825364)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.345964425733472 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9005289191176707) - present_state_Q ( -0.9005289191176707)) * f1( 0.4854256567395308)
w2 ( -2.026910704196249 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.9005289191176707) - present_state_Q (-0.9005289191176707)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.3364645695858226 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7952966033353874) - present_state_Q ( -0.7952966033353874)) * f1( 0.4402832062911523)
w2 ( -2.0247530347662304 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.7952966033353874) - present_state_Q (-0.7952966033353874)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.22462787129917014 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03827168084859393) - present_state_Q (0.03827168084859393)) * f3(-0.15)
w4 ( 0.548384421003184 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03827168084859393) - present_state_Q (0.03827168084859393)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3822466802385893 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6769767611921108) - present_state_Q ( -0.7782144129304224)) * f1( 0.3550423022905413)
w2 ( -2.044095283714062 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6769767611921108) - present_state_Q (-0.7782144129304224)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4231553935991668 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49047056262037403) - present_state_Q ( -0.5926753268060772)) * f1( 0.2808947230516426)
w2 ( -2.0586590010086216 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.49047056262037403) - present_state_Q (-0.5926753268060772)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2084977870608628 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03369418069487552) - present_state_Q (0.028856063514882103)) * f3(-0.07963559905259411)
w4 ( 0.5443334477122932 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03369418069487552) - present_state_Q (0.028856063514882103)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -1.4593136978310706 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6793946315827417) - present_state_Q ( -0.6793946315827417)) * f1( 0.260404298152017)
w2 ( -2.0794871734822546 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6793946315827417) - present_state_Q (-0.6793946315827417)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.186284121782917 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.033698249567614504) - present_state_Q (0.033698249567614504)) * f3(-0.10940922172334465)
w4 ( 0.5402727908630715 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.033698249567614504) - present_state_Q (0.033698249567614504)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.15590689643630662 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02794261826743755) - present_state_Q (0.02794261826743755)) * f3(-0.15)
w4 ( 0.5402727908630715 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02794261826743755) - present_state_Q (0.02794261826743755)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1255911849710231 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02338603446544599) - present_state_Q (0.02338603446544599)) * f3(-0.15)
w4 ( 0.5402727908630715 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02338603446544599) - present_state_Q (0.02338603446544599)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.12576309863480054 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02098385008519802) - present_state_Q (0.02098385008519802)) * f3(0.09102961620454508)
w4 ( 0.5401594780726113 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02098385008519802) - present_state_Q (0.02098385008519802)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.12155714408535546 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.010817225911970737) - present_state_Q (0.010817225911970737)) * f3(0.08578949889159454)
w4 ( 0.5421205360593282 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.010817225911970737) - present_state_Q (0.010817225911970737)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1168153719148327 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0050521199724807035) - present_state_Q (0.0050521199724807035)) * f3(0.04763431053167892)
w4 ( 0.5441114422433777 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0050521199724807035) - present_state_Q (0.0050521199724807035)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.11346657251072924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.006945708636113243) - present_state_Q (0.006945708636113243)) * f3(0.033698648938294995)
w4 ( 0.5460989399678327 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.006945708636113243) - present_state_Q (0.006945708636113243)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.02376490953847747 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.017019985876609384) - present_state_Q (-0.03285021125121053)) * f3(0.8670578644476776)
w4 ( 0.5585135664858991 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.017019985876609384) - present_state_Q (-0.03285021125121053)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.038768240248311256 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03786783753024141) - present_state_Q (0.0035647364307716204)) * f3(-0.15)
w4 ( 0.5585135664858991 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03786783753024141) - present_state_Q (0.0035647364307716204)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.0016392477440262745 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01886790405556616) - present_state_Q (0.01886790405556616)) * f3(0.377703755440012)
w4 ( 0.5644116798039991 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01886790405556616) - present_state_Q (0.01886790405556616)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0730566419798771 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.055152764164599) - present_state_Q (0.055152764164599)) * f3(0.7859726026746707)
w4 ( 0.5739153049265177 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.055152764164599) - present_state_Q (0.055152764164599)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1290857258110787 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11492416968717926) - present_state_Q (0.09035042009019867)) * f3(0.608256751087973)
w4 ( 0.5812844409015459 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11492416968717926) - present_state_Q (0.09035042009019867)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.19650721461378315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08743611445475544) - present_state_Q (0.16076220130841637)) * f3(0.7950821562445235)
w4 ( 0.5897642550029164 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08743611445475544) - present_state_Q (0.16076220130841637)) * f4(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.3458125274358586 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0568469267563634) - present_state_Q ( -1.0568469267563634)) * f1( 0.5817105744089343)
w2 ( -2.0599755511414473 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.0568469267563634) - present_state_Q (-1.0568469267563634)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.3531543293183101 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9520985153445665) - present_state_Q ( -0.9650361087037675)) * f1( 0.5640002140831599)
w2 ( -2.0612772885697543 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.9520985153445665) - present_state_Q (-0.9650361087037675)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.4169337380744818 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9638534511382426) - present_state_Q ( -0.9773754478569461)) * f1( 0.5699628654985042)
w2 ( -2.072467387542323 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9638534511382426) - present_state_Q (-0.9773754478569461)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.4726465425825783 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.087264990643831) - present_state_Q ( -1.0732026334542346)) * f1( 0.5380156494536189)
w2 ( -2.0880002455264752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.087264990643831) - present_state_Q (-1.0732026334542346)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.5253360618225984 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0464724447395453) - present_state_Q ( -1.0464724447395453)) * f1( 0.49792831253630965)
w2 ( -2.1038728675224916 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0464724447395453) - present_state_Q (-1.0464724447395453)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.5695623499941453 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9840656281735585) - present_state_Q ( -1.089259271549683)) * f1( 0.4382540443228121)
w2 ( -2.124055813347845 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9840656281735585) - present_state_Q (-1.089259271549683)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.614147394884886 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9120971546658125) - present_state_Q ( -0.9120971546658125)) * f1( 0.37812373791066634)
w2 ( -2.1417425017598566 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9120971546658125) - present_state_Q (-0.9120971546658125)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.653583858857372 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7287667511637052) - present_state_Q ( -0.835853876251698)) * f1( 0.31880143202437716)
w2 ( -2.1602978437428266 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7287667511637052) - present_state_Q (-0.835853876251698)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.10790069885606342 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1117058294710515) - present_state_Q (0.14073863810491083)) * f3(0.41607740848250935)
w4 ( 0.5684685744513384 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1117058294710515) - present_state_Q (0.14073863810491083)) * f4(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.6848631444042885 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8439344249139842) - present_state_Q ( -0.851627697690372)) * f1( 0.25373259825584454)
w2 ( -2.184953158638847 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8439344249139842) - present_state_Q (-0.851627697690372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7123963633753185 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.799208669290213) - present_state_Q ( -0.799208669290213)) * f1( 0.21498365535825867)
w2 ( -2.2105674025916233 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.799208669290213) - present_state_Q (-0.799208669290213)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7100909338426626 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7158179786581707) - present_state_Q ( -0.7158179786581707)) * f1( 0.1598371171498781)
w2 ( -2.207682678975776 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.7158179786581707) - present_state_Q (-0.7158179786581707)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7213030722209401 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6181702637340114) - present_state_Q ( -0.6523641238619223)) * f1( 0.12328443119280597)
w2 ( -2.2258717370260057 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6181702637340114) - present_state_Q (-0.6523641238619223)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.657358839942456 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.520088855249495) - present_state_Q ( -1.5540647082041708)) * f1( 0.7088722301970373)
w2 ( -2.2123408996858176 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.520088855249495) - present_state_Q (-1.5540647082041708)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.5870971003784968 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5208524454099965) - present_state_Q ( -1.6314694903942875)) * f1( 0.7174072879102069)
w2 ( -2.192753214768752 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.5208524454099965) - present_state_Q (-1.6314694903942875)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.6319437677405957 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3976653774949026) - present_state_Q ( -1.3976653774949026)) * f1( 0.6043201353669029)
w2 ( -2.2075952379738437 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.3976653774949026) - present_state_Q (-1.3976653774949026)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.5186954170087765 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.214233800158209) - present_state_Q ( -1.214233800158209)) * f1( 0.5411304800561633)
w2 ( -2.1762030816717077 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.214233800158209) - present_state_Q (-1.214233800158209)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4346052712125381 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2299547251511074) - present_state_Q ( -1.2299547251511074)) * f1( 0.5232873556582104)
w2 ( -2.144063896618988 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.2299547251511074) - present_state_Q (-1.2299547251511074)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.316751805567399 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2833838687129284) - present_state_Q ( -1.3905870635438782)) * f1( 0.5209571647025255)
w2 ( -2.0761964363188103 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.2833838687129284) - present_state_Q (-1.3905870635438782)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -1.24767902177956 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1216013212664757) - present_state_Q ( -1.1216013212664757)) * f1( 0.45760500167085655)
w2 ( -2.0384604065903145 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1216013212664757) - present_state_Q (-1.1216013212664757)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.1905905164578128 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0138859849590671) - present_state_Q ( -1.0138859849590671)) * f1( 0.4041671571845849)
w2 ( -2.0031479719287355 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.0138859849590671) - present_state_Q (-1.0138859849590671)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.1268863646851615 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9164280143146183) - present_state_Q ( -0.9164280143146183)) * f1( 0.34910493203744764)
w2 ( -1.9575283416066567 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.9164280143146183) - present_state_Q (-0.9164280143146183)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.1631685083148091 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7167609954703652) - present_state_Q ( -0.814637412550698)) * f1( 0.2886318774829672)
w2 ( -1.988954308781565 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7167609954703652) - present_state_Q (-0.814637412550698)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.195681331419125 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5495584935903839) - present_state_Q ( -0.5495584935903839)) * f1( 0.21597502466526383)
w2 ( -2.0115352691180948 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5495584935903839) - present_state_Q (-0.5495584935903839)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2197918499745852 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3745122446887529) - present_state_Q ( -0.3745122446887529)) * f1( 0.14498739189244358)
w2 ( -2.028164658915896 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3745122446887529) - present_state_Q (-0.3745122446887529)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.237863396680733 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5482547823885761) - present_state_Q ( -0.6657310649853942)) * f1( 0.13009588501491184)
w2 ( -2.0628920192472324 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5482547823885761) - present_state_Q (-0.6657310649853942)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.2121233193813261 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3325319296183229) - present_state_Q ( -0.3458838597812486)) * f1( 0.1960953522575914)
w2 ( -2.056328865913135 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3325319296183229) - present_state_Q (-0.3458838597812486)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.12264492340287317 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.039230127511234655) - present_state_Q (0.039230127511234655)) * f3(0.15283853309588086)
w4 ( 0.572327345992298 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.039230127511234655) - present_state_Q (0.039230127511234655)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.13050808557129634 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.027229424705431937) - present_state_Q (0.021272585650683518)) * f3(0.08011777787622122)
w4 ( 0.5742902467059376 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.027229424705431937) - present_state_Q (0.021272585650683518)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.13681279338705069 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03133967661220834) - present_state_Q (0.01985387167808959)) * f3(0.06411914409241239)
w4 ( 0.5762568068979039 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03133967661220834) - present_state_Q (0.01985387167808959)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.14156495188912066 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01813457025339158) - present_state_Q (0.01813457025339158)) * f3(0.048310058963090256)
w4 ( 0.5782241646714478 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01813457025339158) - present_state_Q (0.01813457025339158)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -1.1005059787814513 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7905159266400065) - present_state_Q ( -0.7905159266400065)) * f1( 0.6521745056794137)
w2 ( -2.056328865913135 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.7905159266400065) - present_state_Q (-0.7905159266400065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9923139053609589 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7820625927211085) - present_state_Q ( -0.7959642719163436)) * f1( 0.6298446732549179)
w2 ( -2.047740075849914 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.7820625927211085) - present_state_Q (-0.7959642719163436)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.9084441410939537 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5550144237257181) - present_state_Q ( -0.5550144237257181)) * f1( 0.5593133591369245)
w2 ( -2.047740075849914 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5550144237257181) - present_state_Q (-0.5550144237257181)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8283314375523092 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5797778886401188) - present_state_Q ( -0.5804207747425797)) * f1( 0.5262115184918612)
w2 ( -2.040127860920521 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5797778886401188) - present_state_Q (-0.5804207747425797)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7648925940541308 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37650132959578936) - present_state_Q ( -0.4691043309659853)) * f1( 0.44317759930097667)
w2 ( -2.032970589930489 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.37650132959578936) - present_state_Q (-0.4691043309659853)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7123871954840308 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3213920552951392) - present_state_Q ( -0.3134393629824101)) * f1( 0.4097821908839508)
w2 ( -2.032970589930489 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3213920552951392) - present_state_Q (-0.3134393629824101)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7012739231660099 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35173828770597165) - present_state_Q ( -0.35173828770597165)) * f1( 0.3510587497849732)
w2 ( -2.0313877676358123 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.35173828770597165) - present_state_Q (-0.35173828770597165)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7540446872906855 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43294448071876845) - present_state_Q ( -0.43294448071876845)) * f1( 0.3276974893315493)
w2 ( -2.0474912673093435 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.43294448071876845) - present_state_Q (-0.43294448071876845)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.7999512845356235 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4179158417706025) - present_state_Q ( -0.4179158417706025)) * f1( 0.28269772154430944)
w2 ( -2.0637300247334083 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4179158417706025) - present_state_Q (-0.4179158417706025)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8373802629070465 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3877201816797889) - present_state_Q ( -0.3877201816797889)) * f1( 0.22669777861750817)
w2 ( -2.08024054309829 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3877201816797889) - present_state_Q (-0.3877201816797889)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8703932583506113 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36338337207754684) - present_state_Q ( -0.374358375016255)) * f1( 0.19863654312675139)
w2 ( -2.096860342720205 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.36338337207754684) - present_state_Q (-0.374358375016255)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.12626093613660505 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.009670259489939142) - present_state_Q (-0.021234742783368097)) * f3(-0.15)
w4 ( 0.5782241646714478 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.009670259489939142) - present_state_Q (-0.021234742783368097)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9306080547071532 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4127424651507112) - present_state_Q ( -0.4301465472356372)) * f1( 0.37374316376952954)
w2 ( -2.1049159812166023 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4127424651507112) - present_state_Q (-0.4301465472356372)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.08867108921559921 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.21567366458142456) - present_state_Q (0.23807832945109486)) * f3(0.9696862724377495)
w4 ( 0.5338939454115887 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.21567366458142456) - present_state_Q (0.23807832945109486)) * f4(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.058502265643478525 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.020457585742938655) - present_state_Q (0.013300663382339881)) * f3(-0.15)
w4 ( 0.5338939454115887 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.020457585742938655) - present_state_Q (0.013300663382339881)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18782790147810532 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026686965081713483) - present_state_Q (0.026686965081713483)) * f3(0.6389548841659962)
w4 ( 0.5096057261887063 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.026686965081713483) - present_state_Q (0.026686965081713483)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.19494754997761216 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.028174185221715796) - present_state_Q (0.028174185221715796)) * f3(-0.15)
w4 ( 0.5096057261887063 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.028174185221715796) - present_state_Q (0.028174185221715796)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1720527811889075 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.029242132496641822) - present_state_Q (0.029242132496641822)) * f3(-0.15)
w4 ( 0.5096057261887063 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.029242132496641822) - present_state_Q (0.029242132496641822)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17920437430699995 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.025807917178336123) - present_state_Q (0.025807917178336123)) * f3(-0.15)
w4 ( 0.5096057261887063 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.025807917178336123) - present_state_Q (0.025807917178336123)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1656657392769968 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.007139522141206819) - present_state_Q (-0.007139522141206819)) * f3(0.2673371139603481)
w4 ( 0.5136571307481229 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.007139522141206819) - present_state_Q (-0.007139522141206819)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14283026615496086 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02484986089154952) - present_state_Q (0.02484986089154952)) * f3(-0.15)
w4 ( 0.5136571307481229 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02484986089154952) - present_state_Q (0.02484986089154952)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1362771296400016 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02142453992324413) - present_state_Q (-0.06253372046324288)) * f3(1.013222654265257)
w4 ( 0.514691949539412 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02142453992324413) - present_state_Q (-0.06253372046324288)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17957668801623597 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02044156944600024) - present_state_Q (-0.06388161865035383)) * f3(0.9975150778782569)
w4 ( 0.5086149103977413 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02044156944600024) - present_state_Q (-0.06388161865035383)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.06680634123833556 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.026936503202435395) - present_state_Q (-0.11068160328661875)) * f3(1.0128691688859839)
w4 ( 0.5242021639482374 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.026936503202435395) - present_state_Q (-0.11068160328661875)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.02369118355785904 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012255908016892497) - present_state_Q (0.012255908016892497)) * f3(0.9150687465096662)
w4 ( 0.5380477395072245 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012255908016892497) - present_state_Q (0.012255908016892497)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.10521064379875551 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09647596622683365) - present_state_Q (0.09647596622683365)) * f3(0.892706885840931)
w4 ( 0.5508321423327665 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09647596622683365) - present_state_Q (0.09647596622683365)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1359128315265954 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1702712975308609) - present_state_Q (0.1702712975308609)) * f3(0.8854122951899993)
w4 ( 0.5556867239838776 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1702712975308609) - present_state_Q (0.1702712975308609)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.16103874120786627 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.17840145248712475) - present_state_Q (0.17840145248712475)) * f3(0.7402193744282007)
w4 ( 0.5604388656825399 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.17840145248712475) - present_state_Q (0.17840145248712475)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.18119282970637357 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1579510939854463) - present_state_Q (0.1579510939854463)) * f3(0.5632087622100164)
w4 ( 0.5647329938674971 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1579510939854463) - present_state_Q (0.1579510939854463)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.23525044465026163 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.19828801038556976) - present_state_Q (0.19828801038556976)) * f3(0.6580028108028733)
w4 ( 0.5762345649366389 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.19828801038556976) - present_state_Q (0.19828801038556976)) * f4(0.14)
============================================================================
GUIDE learning . . .
w1 ( -0.8283175641070953 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6918785678514295) - present_state_Q ( -0.6918785678514295)) * f1( 0.6303757697167182)
w2 ( -2.096802527661271 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6918785678514295) - present_state_Q (-0.6918785678514295)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7419785956973032 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5758721575414079) - present_state_Q ( -0.5758721575414079)) * f1( 0.5686611651970758)
w2 ( -2.089211102952335 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5758721575414079) - present_state_Q (-0.5758721575414079)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7944734143733011 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6239828064655055) - present_state_Q ( -0.6239828064655055)) * f1( 0.5593984766908292)
w2 ( -2.098595257694145 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6239828064655055) - present_state_Q (-0.6239828064655055)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8668125766597128 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6046259372203501) - present_state_Q ( -0.6046259372203501)) * f1( 0.4968906502206073)
w2 ( -2.1131536242591618 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6046259372203501) - present_state_Q (-0.6046259372203501)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8027386086862628 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5769007442281338) - present_state_Q ( -0.5769007442281338)) * f1( 0.4217582804474427)
w2 ( -2.0979615175611084 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5769007442281338) - present_state_Q (-0.5769007442281338)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.7551241727476071 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38816104897191633) - present_state_Q ( -0.38816104897191633)) * f1( 0.3528707477486854)
w2 ( -2.0912147928407347 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.38816104897191633) - present_state_Q (-0.38816104897191633)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7047687864482135 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4681683702323983) - present_state_Q ( -0.4753083550414371)) * f1( 0.35250742243995675)
w2 ( -2.076929877660553 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.4681683702323983) - present_state_Q (-0.4753083550414371)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6599794245086044 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.427576420812317) - present_state_Q ( -0.4345027905190886)) * f1( 0.3218215776780848)
w2 ( -2.0630124261761744 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.427576420812317) - present_state_Q (-0.4345027905190886)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.626589177195751 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3714567091758674) - present_state_Q ( -0.3714567091758674)) * f1( 0.25024335672467124)
w2 ( -2.0496693157935915 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3714567091758674) - present_state_Q (-0.3714567091758674)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6025990908819845 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32154832099719627) - present_state_Q ( -0.32154832099719627)) * f1( 0.18605713864958162)
w2 ( -2.0367753809046167 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.32154832099719627) - present_state_Q (-0.32154832099719627)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.23068977754181624 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023549866496669257) - present_state_Q (0.012213567186917896)) * f3(-0.04606076484427852)
w4 ( 0.5801951306144899 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023549866496669257) - present_state_Q (0.012213567186917896)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.23416251804419308 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.019760213348072504) - present_state_Q (0.019760213348072504)) * f3(0.03535618622851306)
w4 ( 0.5821595622304634 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.019760213348072504) - present_state_Q (0.019760213348072504)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -0.5386454168962078 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29632483841429297) - present_state_Q ( -0.3027169571938112)) * f1( 0.5023521637756612)
w2 ( -2.0367753809046167 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.29632483841429297) - present_state_Q (-0.3027169571938112)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4793952937343704 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24186697023407053) - present_state_Q ( -0.34370573927930137)) * f1( 0.44902817818029656)
w2 ( -2.030177785693337 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.24186697023407053) - present_state_Q (-0.34370573927930137)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.42810634879745507 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2957038845181704) - present_state_Q ( -0.2957038845181704)) * f1( 0.40508323250479306)
w2 ( -2.0238471182130056 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2957038845181704) - present_state_Q (-0.2957038845181704)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4385825893565457 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35954867810868735) - present_state_Q ( -0.36761603726658487)) * f1( 0.6223306010394732)
w2 ( -2.024688812365727 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.35954867810868735) - present_state_Q (-0.36761603726658487)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4509795462817687 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7388696569558817) - present_state_Q ( -0.835162461902301)) * f1( 0.5192997253418757)
w2 ( -2.0318505474795256 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.7388696569558817) - present_state_Q (-0.835162461902301)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.35847377446056694 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.756199665079965) - present_state_Q ( -0.756199665079965)) * f1( 0.550439660194671)
w2 ( -1.9898360550152265 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.756199665079965) - present_state_Q (-0.756199665079965)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.26138804271123484 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7029519144230472) - present_state_Q ( -0.7983489943189097)) * f1( 0.5618212325780502)
w2 ( -1.9379944409289285 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.7029519144230472) - present_state_Q (-0.7983489943189097)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.1613278324433991 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6443392661382199) - present_state_Q ( -0.6495041827826882)) * f1( 0.6312667206921316)
w2 ( -1.8983676845247068 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6443392661382199) - present_state_Q (-0.6495041827826882)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.068122736432861 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.663648715205528) - present_state_Q ( -0.663648715205528)) * f1( 0.5835224364099966)
w2 ( -1.8504491692141576 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.663648715205528) - present_state_Q (-0.663648715205528)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.01591469326335368 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5920469417416926) - present_state_Q ( -0.5924723904191534)) * f1( 0.5480936558046889)
w2 ( -1.804451138326808 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5920469417416926) - present_state_Q (-0.5924723904191534)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.10787279114847045 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5314355046166354) - present_state_Q ( -0.5314355046166354)) * f1( 0.6220564052090934)
w2 ( -1.7601023797021589 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5314355046166354) - present_state_Q (-0.5314355046166354)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.004332187874448154 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4555522606494248) - present_state_Q ( -0.4576896591929732)) * f1( 0.6520741140447619)
w2 ( -1.807738346708318 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4555522606494248) - present_state_Q (-0.4576896591929732)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.10219687983197405 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5391885371638125) - present_state_Q ( -0.5392745580284918)) * f1( 0.7033272961163536)
w2 ( -1.8531776755789546 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5391885371638125) - present_state_Q (-0.5392745580284918)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.19539872867700442 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6220933804667249) - present_state_Q ( -0.6220933804667249)) * f1( 0.6471829463069905)
w2 ( -1.8963811543063531 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6220933804667249) - present_state_Q (-0.6220933804667249)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.2755890524528638 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6819467017966537) - present_state_Q ( -0.6819467017966537)) * f1( 0.5784702708664549)
w2 ( -1.9379685933578434 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6819467017966537) - present_state_Q (-0.6819467017966537)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.3205165213018818 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7279432545093452) - present_state_Q ( -0.7279432545093452)) * f1( 0.5317797466830004)
w2 ( -1.963314125486091 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.7279432545093452) - present_state_Q (-0.7279432545093452)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.24334378021551537 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6386795427950799) - present_state_Q ( -0.7368452490693845)) * f1( 0.46128982937607177)
w2 ( -1.9131248066423947 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6386795427950799) - present_state_Q (-0.7368452490693845)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.18974281002120524 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3839088095182539) - present_state_Q ( -0.3839088095182539)) * f1( 0.39836682259164574)
w2 ( -1.8929420377138984 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3839088095182539) - present_state_Q (-0.3839088095182539)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.14269967732447955 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3517396414626457) - present_state_Q ( -0.3517396414626457)) * f1( 0.3573170219097308)
w2 ( -1.8731935525541525 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3517396414626457) - present_state_Q (-0.3517396414626457)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.19836566082454343 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4208744240842076) - present_state_Q ( -0.4237223054423204)) * f1( 0.3439643021748431)
w2 ( -1.9055608552934746 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4208744240842076) - present_state_Q (-0.4237223054423204)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24333128646861268 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4366154710459013) - present_state_Q ( -0.4366154710459013)) * f1( 0.27980296466886806)
w2 ( -1.9377017768146485 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4366154710459013) - present_state_Q (-0.4366154710459013)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2783322131816817 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4406573311389993) - present_state_Q ( -0.4406573311389993)) * f1( 0.21829077775792358)
w2 ( -1.9697699448541466 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4406573311389993) - present_state_Q (-0.4406573311389993)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3060432937603494 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4395971607932083) - present_state_Q ( -0.44210367398904016)) * f1( 0.17299357651707037)
w2 ( -2.001807065695952 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4395971607932083) - present_state_Q (-0.44210367398904016)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1792390010119349 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.10790015455297977) - present_state_Q (0.10790015455297977)) * f3(0.26190096556344894)
w4 ( 0.565382681117682 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.10790015455297977) - present_state_Q (0.10790015455297977)) * f4(0.08)
============================================================================
GUIDE learning . . .
w1 ( -0.3008875876246632 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19159517020144884) - present_state_Q ( -0.19159517020144884)) * f1( 0.2989930470043402)
w2 ( -2.0009448874300455 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.19159517020144884) - present_state_Q (-0.19159517020144884)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.29573865981296343 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1873482771582966) - present_state_Q ( -0.19033152617851812)) * f1( 0.30005984135058217)
w2 ( -2.0000869039377323 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1873482771582966) - present_state_Q (-0.19033152617851812)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.28976366480113636 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27215141515065555) - present_state_Q ( -0.27215141515065555)) * f1( 0.24394079827949505)
w2 ( -1.9976375412013763 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.27215141515065555) - present_state_Q (-0.27215141515065555)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.16541560650650086 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.07741607103145923) - present_state_Q (0.07741607103145923)) * f3(0.24265427679717014)
w4 ( 0.5619646343341121 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.07741607103145923) - present_state_Q (0.07741607103145923)) * f4(0.06)
============================================================================
GUIDE learning . . .
w1 ( -0.30660147486550804 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2637391422164541) - present_state_Q ( -0.2637391422164541)) * f1( 0.22078471481309625)
w2 ( -2.0052638889214283 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.2637391422164541) - present_state_Q (-0.2637391422164541)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.32124423345620656 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25907210977765827) - present_state_Q ( -0.25907210977765827)) * f1( 0.190950552051965)
w2 ( -2.0129322399334293 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.25907210977765827) - present_state_Q (-0.25907210977765827)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.34388391233178345 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25603159232230777) - present_state_Q ( -0.2586998652138516)) * f1( 0.1787009236022118)
w2 ( -2.025601272873613 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.25603159232230777) - present_state_Q (-0.2586998652138516)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.13065936601466624 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.050584917940640045) - present_state_Q (0.050584917940640045)) * f3(0.16991342691822112)
w4 ( 0.5537825286295258 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.050584917940640045) - present_state_Q (0.050584917940640045)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -0.3928764213956185 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08386861596327497) - present_state_Q ( -0.08771806858706385)) * f1( 0.25508046594058864)
w2 ( -2.025601272873613 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08386861596327497) - present_state_Q (-0.08771806858706385)) * f2(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0903285341615277 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04794287089465567) - present_state_Q (0.04794287089465567)) * f3(0.19739549131578968)
w4 ( 0.545609934294305 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04794287089465567) - present_state_Q (0.04794287089465567)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -0.4018379929593556 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2132000497709008) - present_state_Q ( -0.21695764501254228)) * f1( 0.29443757647236524)
w2 ( -2.027123084673436 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2132000497709008) - present_state_Q (-0.21695764501254228)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3886804287879061 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4245689784813754) - present_state_Q ( -0.43780425452158384)) * f1( 0.33281022243731767)
w2 ( -2.0211928743233343 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.4245689784813754) - present_state_Q (-0.43780425452158384)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.025787084740695312 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11523445359320714) - present_state_Q (0.08079927155287185)) * f3(0.41128174119258987)
w4 ( 0.5330557276847566 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.11523445359320714) - present_state_Q (0.08079927155287185)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.03536317010511242 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09051133995259682) - present_state_Q (0.04800047100466768)) * f3(0.20770136848523538)
w4 ( 0.5367441329886814 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09051133995259682) - present_state_Q (0.04800047100466768)) * f4(0.08)
============================================================================
GUIDE learning . . .
w1 ( -0.3693776103770682 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3825425931531703) - present_state_Q ( -0.48360223686933695)) * f1( 0.20418744070074343)
w2 ( -2.002285914772254 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3825425931531703) - present_state_Q (-0.48360223686933695)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3716017983365103 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24633659942719796) - present_state_Q ( -0.3464508951658107)) * f1( 0.12482621213263188)
w2 ( -2.0049586562439075 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.24633659942719796) - present_state_Q (-0.3464508951658107)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.004846590752294884 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.10405744739012229) - present_state_Q (0.10405744739012229)) * f3(0.5140485442311983)
w4 ( 0.5272457057462636 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.10405744739012229) - present_state_Q (0.10405744739012229)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.04420581298396321 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06529616619347833) - present_state_Q (0.06529616619347833)) * f3(0.4181664199658426)
w4 ( 0.538540507151374 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06529616619347833) - present_state_Q (0.06529616619347833)) * f4(0.12)
============================================================================
GUIDE learning . . .
w1 ( -0.3514537657992833 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2571164696028648) - present_state_Q ( -0.26110062460523287)) * f1( 0.1630905965798379)
w2 ( -1.992604766467458 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2571164696028648) - present_state_Q (-0.26110062460523287)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.36696062603547763 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2275086265983352) - present_state_Q ( -0.33110758056717166)) * f1( 0.09166743603894707)
w2 ( -2.0179794156988478 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2275086265983352) - present_state_Q (-0.33110758056717166)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07160918492790949 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05583811100918661) - present_state_Q (0.05583811100918661)) * f3(0.28853378268835017)
w4 ( 0.5461384727521079 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05583811100918661) - present_state_Q (0.05583811100918661)) * f4(0.08)
============================================================================
GUIDE learning . . .
w1 ( -0.3508776490977684 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15278288664813308) - present_state_Q ( -0.15278288664813308)) * f1( 0.1413882367264508)
w2 ( -2.012291892708931 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.15278288664813308) - present_state_Q (-0.15278288664813308)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.08785162188953723 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05945008467384354) - present_state_Q (0.04487046716847119)) * f3(0.16900288441389497)
w4 ( 0.5519049199999014 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05945008467384354) - present_state_Q (0.04487046716847119)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.09290400442614767 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.028853178344428836) - present_state_Q (0.03149211535618425)) * f3(0.1071797919454187)
w4 ( 0.5537904928098145 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.028853178344428836) - present_state_Q (0.03149211535618425)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.09148214199775236 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02889413779256999) - present_state_Q (0.020806552828594806)) * f3(-0.014478029145309976)
w4 ( 0.5577188242536172 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02889413779256999) - present_state_Q (0.020806552828594806)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07636875994939463 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.049908584092582566) - present_state_Q (-0.0025679448145905087)) * f3(-0.15)
w4 ( 0.5597339418600649 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.049908584092582566) - present_state_Q (-0.0025679448145905087)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07772438519494912 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023204951000366993) - present_state_Q (0.012240325666167558)) * f3(0.013692075524850118)
w4 ( 0.5617141021989326 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023204951000366993) - present_state_Q (0.012240325666167558)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.08497849978974963 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04733702567390398) - present_state_Q (-0.011658657779242368)) * f3(-0.15)
w4 ( 0.5617141021989326 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04733702567390398) - present_state_Q (-0.011658657779242368)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.41586107380659654 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22272253199454217) - present_state_Q ( -0.22766903821237755)) * f1( 0.3621046934839916)
w2 ( -2.0212649087838663 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.22272253199454217) - present_state_Q (-0.22766903821237755)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.08360553991851935 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.018396656894670942) - present_state_Q (0.013514481625722621)) * f3(0.0268326645844013)
w4 ( 0.56069075256706 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.018396656894670942) - present_state_Q (0.013514481625722621)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.08281981673202456 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.012498723781269995) - present_state_Q (0.012498723781269995)) * f3(0.015368703212502973)
w4 ( 0.5596682548642538 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.012498723781269995) - present_state_Q (0.012498723781269995)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.09008434189707742 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03275349820005193) - present_state_Q (-0.012422972509803683)) * f3(-0.15)
w4 ( 0.5596682548642538 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.03275349820005193) - present_state_Q (-0.012422972509803683)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.09009860481302367 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.009826364563572065) - present_state_Q (0.0097233949616593)) * f3(-0.01631770965597147)
w4 ( 0.5596507733472431 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.009826364563572065) - present_state_Q (0.0097233949616593)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -0.3863494295968615 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26687011186921283) - present_state_Q ( -0.26687011186921283)) * f1( 0.39870734933736757)
w2 ( -2.0175639932804548 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.26687011186921283) - present_state_Q (-0.26687011186921283)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.39604423328275895 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2278909183009452) - present_state_Q ( -0.2278909183009452)) * f1( 0.3287508894977652)
w2 ( -2.0190384841481004 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2278909183009452) - present_state_Q (-0.2278909183009452)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4050550835957522 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2138306731264783) - present_state_Q ( -0.21895234793254653)) * f1( 0.29794758718502573)
w2 ( -2.020550637745001 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2138306731264783) - present_state_Q (-0.21895234793254653)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3733666211885643 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2032145366763512) - present_state_Q ( -0.30218703528126867)) * f1( 0.24720581363373517)
w2 ( -2.0077319819288646 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2032145366763512) - present_state_Q (-0.30218703528126867)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.35310732741109196 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16432232143187053) - present_state_Q ( -0.16617640062547667)) * f1( 0.17620697136664254)
w2 ( -2.001983261086453 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.16432232143187053) - present_state_Q (-0.16617640062547667)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3568913058554297 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24855728535936358) - present_state_Q ( -0.24855728535936358)) * f1( 0.13695257927745627)
w2 ( -2.0047462455182186 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.24855728535936358) - present_state_Q (-0.24855728535936358)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.11237686406334843 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01267925923062581) - present_state_Q (-0.013514790721953551)) * f3(-0.15)
w4 ( 0.5596507733472431 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01267925923062581) - present_state_Q (-0.013514790721953551)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36270995247923715 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21560554289468795) - present_state_Q ( -0.21638531865003263)) * f1( 0.04458134405957177)
w2 ( -2.017797997874613 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.21560554289468795) - present_state_Q (-0.21638531865003263)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3655590386168898 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3737370382615678) - present_state_Q ( -0.478652524828635)) * f1( 0.4851888511592935)
w2 ( -2.018678815559576 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3737370382615678) - present_state_Q (-0.478652524828635)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.36939182640422985 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4492590548473431) - present_state_Q ( -0.4492590548473431)) * f1( 0.4006390679533863)
w2 ( -2.0201138183191367 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4492590548473431) - present_state_Q (-0.4492590548473431)) * f2(0.15000000000000002)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.17684414767702528 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.187931315505786) - present_state_Q (0.187931315505786)) * f3(0.7759086092145235)
w4 ( 0.5746062860360494 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.187931315505786) - present_state_Q (0.187931315505786)) * f4(0.18)
============================================================================
GUIDE learning . . .
w1 ( -0.401756003423821 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5198513794938742) - present_state_Q ( -0.5198513794938742)) * f1( 0.3135657249310495)
w2 ( -2.040756493488247 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.5198513794938742) - present_state_Q (-0.5198513794938742)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3636380421595496 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8364991145577267) - present_state_Q ( -0.8364991145577267)) * f1( 0.3042501936377849)
w2 ( -1.9969067713796789 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8364991145577267) - present_state_Q (-0.8364991145577267)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.39520412307854974 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7837212556036481) - present_state_Q ( -0.7878639416276733)) * f1( 0.24460194295557083)
w2 ( -2.042074557817323 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7837212556036481) - present_state_Q (-0.7878639416276733)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.4230652990252774 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47508570583701215) - present_state_Q ( -0.478596349772083)) * f1( 0.17758275815019606)
w2 ( -2.073452802233555 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.47508570583701215) - present_state_Q (-0.478596349772083)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4390158139760417 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46375799439497334) - present_state_Q ( -0.671103274618329)) * f1( 0.11598075772537085)
w2 ( -2.11471097797819 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.46375799439497334) - present_state_Q (-0.671103274618329)) * f2(0.30000000000000004)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.17800459124486107 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05189689248733152) - present_state_Q (0.0652888018368288)) * f3(-0.02071853966119753)
w4 ( 0.5678850966849922 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05189689248733152) - present_state_Q (0.0652888018368288)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.15993613488085182 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03852272141432937) - present_state_Q (0.03852272141432937)) * f3(0.08880286422042519)
w4 ( 0.5597464148879007 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03852272141432937) - present_state_Q (0.03852272141432937)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1608367338257703 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012651901575293067) - present_state_Q (0.012651901575293067)) * f3(0.009109719192729387)
w4 ( 0.5617236414650651 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012651901575293067) - present_state_Q (0.012651901575293067)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1601539853534093 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.010126262661592325) - present_state_Q (0.010126262661592325)) * f3(-0.006890280232309806)
w4 ( 0.5637054141922743 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.010126262661592325) - present_state_Q (0.010126262661592325)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.15788063113067408 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.007608138810770536) - present_state_Q (0.007608138810770536)) * f3(-0.022890279408191504)
w4 ( 0.5656917195424149 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.007608138810770536) - present_state_Q (0.007608138810770536)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.15246678557493593 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.002647722993677869) - present_state_Q (0.013961557384526167)) * f3(-0.05489027586922739)
w4 ( 0.5696369324020742 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.002647722993677869) - present_state_Q (0.013961557384526167)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -0.42122045584374274 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06700004450514012) - present_state_Q ( -0.17273559340404965)) * f1( 0.15261419377661983)
w2 ( -2.1088808000334223 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06700004450514012) - present_state_Q (-0.17273559340404965)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.40821342813336586 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15357902902613743) - present_state_Q ( -0.15357902902613743)) * f1( 0.11427505088291012)
w2 ( -2.1031896944028046 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.15357902902613743) - present_state_Q (-0.15357902902613743)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.38856643597731355 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4255711688597368) - present_state_Q ( -0.42179828083642634)) * f1( 0.518060643823442)
w2 ( -2.0993972827633 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.4255711688597368) - present_state_Q (-0.42179828083642634)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.32121696984310666 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49585599187767343) - present_state_Q ( -0.49585599187767343)) * f1( 0.46567686426148996)
w2 ( -2.0777032268729516 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.49585599187767343) - present_state_Q (-0.49585599187767343)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2840778263111117 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4441218969938593) - present_state_Q ( -0.44423418523132086)) * f1( 0.4127387829638453)
w2 ( -2.064205896939973 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4441218969938593) - present_state_Q (-0.44423418523132086)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.32466308467178917 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41170936384174245) - present_state_Q ( -0.41170936384174245)) * f1( 0.3593327949114686)
w2 ( -2.081147820528109 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.41170936384174245) - present_state_Q (-0.41170936384174245)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3561330981297755 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29876864987857316) - present_state_Q ( -0.40282604090497864)) * f1( 0.27922443944437575)
w2 ( -2.098053582889352 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.29876864987857316) - present_state_Q (-0.40282604090497864)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.32720768643325426 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28544965791250726) - present_state_Q ( -0.3903523370569749)) * f1( 0.2124045757634331)
w2 ( -2.0776264723203663 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.28544965791250726) - present_state_Q (-0.3903523370569749)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3001521707415003 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37313427671060895) - present_state_Q ( -0.4770156003266272)) * f1( 0.18792439301421202)
w2 ( -2.048832428867255 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.37313427671060895) - present_state_Q (-0.4770156003266272)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3214154819633909 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5547353726033191) - present_state_Q ( -0.5547353726033191)) * f1( 0.14168568323675743)
w2 ( -2.0863508829836803 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5547353726033191) - present_state_Q (-0.5547353726033191)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.3326359119907001 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3319624367571198) - present_state_Q ( -0.43990521221708007)) * f1( 0.07042297863835355)
w2 ( -2.1182167036128527 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3319624367571198) - present_state_Q (-0.43990521221708007)) * f2(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.10383019402435914 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1262263701865051) - present_state_Q (0.1262263701865051)) * f3(0.2301121576733809)
w4 ( 0.5358192726713886 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1262263701865051) - present_state_Q (0.1262263701865051)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07699864500444373 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03514552180681155) - present_state_Q (0.03514552180681155)) * f3(0.13206900968266433)
w4 ( 0.5276927487928841 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.03514552180681155) - present_state_Q (0.03514552180681155)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05708464254837192 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04477653110836161) - present_state_Q (0.031151448244859277)) * f3(0.13044045505689447)
w4 ( 0.521586053612348 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04477653110836161) - present_state_Q (0.031151448244859277)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -0.3407374507806918 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.057483065973775845) - present_state_Q ( -0.06052886490523868)) * f1( 0.1819673183902373)
w2 ( -2.1182167036128527 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.057483065973775845) - present_state_Q (-0.06052886490523868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34728943719736566 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17029797900488258) - present_state_Q ( -0.17029797900488258)) * f1( 0.18896409442729947)
w2 ( -2.1199503627073306 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.17029797900488258) - present_state_Q (-0.17029797900488258)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05692018417830978 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.010615961418495157) - present_state_Q (0.010615961418495157)) * f3(0.0032274940863837)
w4 ( 0.5205669448817947 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.010615961418495157) - present_state_Q (0.010615961418495157)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05573436581569684 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.00973040572114779) - present_state_Q (0.00973040572114779)) * f3(-0.011962947525872245)
w4 ( 0.5225494301514966 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.00973040572114779) - present_state_Q (0.00973040572114779)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -0.3046334767247399 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12185842596088019) - present_state_Q ( -0.2278559440962467)) * f1( 0.35088434288206605)
w2 ( -2.11387201219983 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.12185842596088019) - present_state_Q (-0.2278559440962467)) * f2(0.05)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.051776761326613054 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.008228785409652078) - present_state_Q (0.008228785409652078)) * f3(-0.03987132823447321)
w4 ( 0.5245346183377592 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.008228785409652078) - present_state_Q (0.008228785409652078)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.04752341195709978 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.008271927700501988) - present_state_Q (0.008271927700501988)) * f3(-0.04285251934274148)
w4 ( 0.5265197288678982 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.008271927700501988) - present_state_Q (0.008271927700501988)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.03238366547229404 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02187920526817774) - present_state_Q (-0.007128511793564967)) * f3(-0.15)
w4 ( 0.5265197288678982 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02187920526817774) - present_state_Q (-0.007128511793564967)) * f4(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.28675710102105456 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17688961801103964) - present_state_Q ( -0.28258321862103114)) * f1( 0.2337104187186207)
w2 ( -2.1062230696316306 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.17688961801103964) - present_state_Q (-0.28258321862103114)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2620966284017456 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2676119614474943) - present_state_Q ( -0.2676119614474943)) * f1( 0.19873842454609988)
w2 ( -2.0938145619786033 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2676119614474943) - present_state_Q (-0.2676119614474943)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.24791697914395283 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23994550364050288) - present_state_Q ( -0.23994550364050288)) * f1( 0.11661366126310306)
w2 ( -2.081655052445839 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.23994550364050288) - present_state_Q (-0.23994550364050288)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2522484649137548 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11767561811534519) - present_state_Q ( -0.22175837073763713)) * f1( 0.0548282959077222)
w2 ( -2.0895551443565776 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.11767561811534519) - present_state_Q (-0.22175837073763713)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.03996024887650698 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0056728447565138574) - present_state_Q (0.0056728447565138574)) * f3(-0.15)
w4 ( 0.5255095177473365 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0056728447565138574) - present_state_Q (0.0056728447565138574)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -0.262121335319115 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33078424849221405) - present_state_Q ( -0.3341421649203228)) * f1( 0.4962831013757292)
w2 ( -2.0915445069558665 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.33078424849221405) - present_state_Q (-0.3341421649203228)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2567297052559439 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.557255548179241) - present_state_Q ( -0.6593150892811719)) * f1( 0.5204801905045703)
w2 ( -2.0889547685942853 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.557255548179241) - present_state_Q (-0.6593150892811719)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.1771608895584797 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6474089359121129) - present_state_Q ( -0.6510163196990226)) * f1( 0.5016078190954483)
w2 ( -2.04929788294159 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.6474089359121129) - present_state_Q (-0.6510163196990226)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.24206032142357245 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5906228378408551) - present_state_Q ( -0.5906228378408551)) * f1( 0.441961921170032)
w2 ( -2.086008869090171 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5906228378408551) - present_state_Q (-0.5906228378408551)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.29704201808588454 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6134193347936971) - present_state_Q ( -0.6134193347936971)) * f1( 0.3797281478458916)
w2 ( -2.1222069340573126 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6134193347936971) - present_state_Q (-0.6134193347936971)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.3406119974355056 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6279537822834412) - present_state_Q ( -0.7340641289863069)) * f1( 0.3279066355553475)
w2 ( -2.162068871534574 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6279537822834412) - present_state_Q (-0.7340641289863069)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.38251396679498256 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3044209766529575) - present_state_Q ( -0.41252442022968616)) * f1( 0.25898702971025933)
w2 ( -2.1863376366961083 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3044209766529575) - present_state_Q (-0.41252442022968616)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4147506078897168 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5241708344873716) - present_state_Q ( -0.6334877163221769)) * f1( 0.22718989289802252)
w2 ( -2.2218108708742723 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5241708344873716) - present_state_Q (-0.6334877163221769)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.4206673796793281 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6321447231077905) - present_state_Q ( -0.7432352666515042)) * f1( 0.18491113437889156)
w2 ( -2.2314102470440504 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.6321447231077905) - present_state_Q (-0.7432352666515042)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.40297013980683943 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7251345860277613) - present_state_Q ( -0.7335441321876726)) * f1( 0.1524269795374587)
w2 ( -2.1965793268365035 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.7251345860277613) - present_state_Q (-0.7335441321876726)) * f2(0.30000000000000004)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.013382459282731716 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09211278245998092) - present_state_Q (0.05112867037268479)) * f3(0.4904398710931582)
w4 ( 0.5222580133945763 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09211278245998092) - present_state_Q (0.05112867037268479)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0568325814399689 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0426737678481342) - present_state_Q (0.0373491172706403)) * f3(0.4493670662406215)
w4 ( 0.5280595229516614 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0426737678481342) - present_state_Q (0.0373491172706403)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.07639606546870775 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05643472095956277) - present_state_Q (0.05643472095956277)) * f3(0.4355098599314414)
w4 ( 0.5307547754584797 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05643472095956277) - present_state_Q (0.05643472095956277)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.10017509069084639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05088337625877419) - present_state_Q (0.05088337625877419)) * f3(0.24920248987251206)
w4 ( 0.5364800052266824 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05088337625877419) - present_state_Q (0.05088337625877419)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.12038051740113644 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.042505107433417094) - present_state_Q (0.042505107433417094)) * f3(0.21009122207136563)
w4 ( 0.5403269868399221 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.042505107433417094) - present_state_Q (0.042505107433417094)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1662978947612422 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10422333099759497) - present_state_Q (0.10422333099759497)) * f3(0.5067030227752233)
w4 ( 0.5475765788567394 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10422333099759497) - present_state_Q (0.10422333099759497)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.1507323933619283 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1275540907340732) - present_state_Q (-0.02494468421418633)) * f3(-0.15)
w4 ( 0.5475765788567394 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1275540907340732) - present_state_Q (-0.02494468421418633)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.18167321066356712 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08327080812455051) - present_state_Q (0.08327080812455051)) * f3(0.33447497428167405)
w4 ( 0.5531269164928668 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08327080812455051) - present_state_Q (0.08327080812455051)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.20234298101179807 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04797851188902344) - present_state_Q (0.06195287040497604)) * f3(0.2192276648812948)
w4 ( 0.5568982964160025 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04797851188902344) - present_state_Q (0.06195287040497604)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.2258461042207113 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0541191193851869) - present_state_Q (0.08508800948061515)) * f3(0.25537881984965927)
w4 ( 0.56242023983075 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0541191193851869) - present_state_Q (0.08508800948061515)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.24428972180798536 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06681612146410762) - present_state_Q (0.06681612146410762)) * f3(0.19623677824243513)
w4 ( 0.5661797017934792 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06681612146410762) - present_state_Q (0.06681612146410762)) * f4(0.04)
============================================================================
GUIDE learning . . .
w1 ( -0.33045161146297836 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3344466252173284) - present_state_Q ( -0.3344466252173284)) * f1( 0.5574052186178655)
w2 ( -2.1900743170230257 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3344466252173284) - present_state_Q (-0.3344466252173284)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.2929741790765536 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27515202918340675) - present_state_Q ( -0.27515202918340675)) * f1( 0.5012785763062124)
w2 ( -2.1863361328917 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.27515202918340675) - present_state_Q (-0.27515202918340675)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.2346540590111169 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24601242491350395) - present_state_Q ( -0.24887900284785436)) * f1( 0.4763634687642627)
w2 ( -2.1802147440899176 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.24601242491350395) - present_state_Q (-0.24887900284785436)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.289098812873291 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.323723673489029) - present_state_Q ( -0.323723673489029)) * f1( 0.45045970875376784)
w2 ( -2.1923012310285164 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.323723673489029) - present_state_Q (-0.323723673489029)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3385990448582769 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3332785260396975) - present_state_Q ( -0.3390559172096798)) * f1( 0.41448040867378655)
w2 ( -2.2042439503824593 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3332785260396975) - present_state_Q (-0.3390559172096798)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3619970150249211 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3336494134024603) - present_state_Q ( -0.3336494134024603)) * f1( 0.334392610031152)
w2 ( -2.2112411056618373 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.3336494134024603) - present_state_Q (-0.3336494134024603)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.39688686564648074 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3210474948021137) - present_state_Q ( -0.32582650250012785)) * f1( 0.2892355118639197)
w2 ( -2.223303888131638 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3210474948021137) - present_state_Q (-0.32582650250012785)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.42687749279927534 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31695973833290214) - present_state_Q ( -0.32061341612348726)) * f1( 0.24763486982677615)
w2 ( -2.235414713708736 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.31695973833290214) - present_state_Q (-0.32061341612348726)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4025207628611652 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3051146238849653) - present_state_Q ( -0.3051146238849653)) * f1( 0.19109265278703433)
w2 ( -2.2226686820937713 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3051146238849653) - present_state_Q (-0.3051146238849653)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3881134061350331 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1574232933907088) - present_state_Q ( -0.26855672749539733)) * f1( 0.11499992933777234)
w2 ( -2.210140538112208 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1574232933907088) - present_state_Q (-0.26855672749539733)) * f2(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.2475674291675272 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01947353353428653) - present_state_Q (0.01947353353428653)) * f3(0.033361778130079886)
w4 ( 0.5681446494331175 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01947353353428653) - present_state_Q (0.01947353353428653)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.24509615664803958 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.015404251094460458) - present_state_Q (0.015404251094460458)) * f3(0.01632427221701829)
w4 ( 0.5651169217811475 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.015404251094460458) - present_state_Q (0.015404251094460458)) * f4(0.02)
============================================================================
GUIDE learning . . .
w1 ( -0.38977035314812947 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39064757231535263) - present_state_Q ( -0.5011545992209632)) * f1( 0.437072040858884)
w2 ( -2.210709190482367 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.39064757231535263) - present_state_Q (-0.5011545992209632)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.38493094916903164 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6988937497147781) - present_state_Q ( -0.6988937497147781)) * f1( 0.3751348734279385)
w2 ( -2.207484081113784 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6988937497147781) - present_state_Q (-0.6988937497147781)) * f2(0.25)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.31231838914656807 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2889013577087724) - present_state_Q (0.32423342179924364)) * f3(0.9539713617379165)
w4 ( 0.5763914292046937 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2889013577087724) - present_state_Q (0.32423342179924364)) * f4(0.16)
============================================================================
GUIDE learning . . .
w1 ( -0.4251376118377723 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7816145150966849) - present_state_Q ( -0.7816145150966849)) * f1( 0.3101057242090758)
w2 ( -2.2463804892061736 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7816145150966849) - present_state_Q (-0.7816145150966849)) * f2(0.30000000000000004)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.09059238955962232 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.37119619243285046) - present_state_Q (0.40743031151688064)) * f3(0.9354301117979886)
w4 ( 0.5289852153592217 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.37119619243285046) - present_state_Q (0.40743031151688064)) * f4(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4509039492116298 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7516373579146752) - present_state_Q ( -0.7570175154786268)) * f1( 0.1954740451157401)
w2 ( -2.285924875815559 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7516373579146752) - present_state_Q (-0.7570175154786268)) * f2(0.30000000000000004)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.0013054350275886195 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0928551582386804) - present_state_Q (0.0928551582386804)) * f3(0.4410595293599274)
w4 ( 0.5081495189350735 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0928551582386804) - present_state_Q (0.0928551582386804)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.007911877442150553 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05065977103779578) - present_state_Q (0.06082276141649725)) * f3(0.11887290629716483)
w4 ( 0.5014804375233209 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05065977103779578) - present_state_Q (0.06082276141649725)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.022481217790789012 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04635160755141651) - present_state_Q (0.037631717439143945)) * f3(0.31430183049521676)
w4 ( 0.509216465069849 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04635160755141651) - present_state_Q (0.037631717439143945)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.0569605692029176 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05228150930104098) - present_state_Q (0.05228150930104098)) * f3(0.513503859216334)
w4 ( 0.4968400382028815 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05228150930104098) - present_state_Q (0.05228150930104098)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.0643811391265308 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01708432734100868) - present_state_Q (0.01708432734100868)) * f3(0.048968509727668065)
w4 ( 0.49077853462445387 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01708432734100868) - present_state_Q (0.01708432734100868)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.06449804104468708 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.006249044426163751) - present_state_Q (0.006249044426163751)) * f3(0.2078574119745542)
w4 ( 0.4907560380645197 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.006249044426163751) - present_state_Q (0.006249044426163751)) * f4(0.04)
============================================================================
