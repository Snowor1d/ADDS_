GUIDE learning . . .
w1 ( 0.972504822637235 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43798605243694566) - present_state_Q ( 0.3791464465744451)) * f1( 0.3291464465744451)
w2 ( 0.9958232607933463 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.43798605243694566) - present_state_Q (0.3791464465744451)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.9289898188329174 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6249358239457795) - present_state_Q ( 0.577986496111919)) * f1( 0.2871343271251595)
w2 ( 0.950358473381826 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.6249358239457795) - present_state_Q (0.577986496111919)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.8224890890331069 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7500048217895205) - present_state_Q ( 0.7500048217895205)) * f1( 0.39813292346028517)
w2 ( 0.8433582997974032 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7500048217895205) - present_state_Q (0.7500048217895205)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.741866906817708 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6604256658076229) - present_state_Q ( 0.6548077514689963)) * f1( 0.38598011302889923)
w2 ( 0.7598076924018738 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6604256658076229) - present_state_Q (0.6548077514689963)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.6852460750396663 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5935610740928239) - present_state_Q ( 0.5801316337473474)) * f1( 0.3723155113784689)
w2 ( 0.6989766713483513 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.5935610740928239) - present_state_Q (0.5801316337473474)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.6856802188577878 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5484548378301052) - present_state_Q ( 0.5435760986805271)) * f1( 0.3852417981757948)
w2 ( 0.6994274467524506 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.5484548378301052) - present_state_Q (0.5435760986805271)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.6284161905394042 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5485089196792565) - present_state_Q ( 0.5435274721681325)) * f1( 0.3846639967338129)
w2 ( 0.6398803835444423 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.5485089196792565) - present_state_Q (0.5435274721681325)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.6499757232417327 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5038461182510274) - present_state_Q ( 0.5038461182510274)) * f1( 0.3944741853650674)
w2 ( 0.6617419232874053 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5038461182510274) - present_state_Q (0.5038461182510274)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.6136632043852756 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5195671296537787) - present_state_Q ( 0.51083993937391)) * f1( 0.3786959439520553)
w2 ( 0.623386594231064 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.5195671296537787) - present_state_Q (0.51083993937391)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5554584282712485 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.48705067022776805) - present_state_Q ( 0.43837269028997927)) * f1( 0.308015946282617)
w2 ( 0.5477998893003759 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.48705067022776805) - present_state_Q (0.43837269028997927)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5032574804157051 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43556421601520695) - present_state_Q ( 0.42848432970930483)) * f1( 0.37692177007874117)
w2 ( 0.49240277297606455 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.43556421601520695) - present_state_Q (0.42848432970930483)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5288268518646985 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3933269183541599) - present_state_Q ( 0.39742604445346413)) * f1( 0.398334735327627)
w2 ( 0.5180790388713427 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3933269183541599) - present_state_Q (0.39742604445346413)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5341012840151095 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.41282273690472165) - present_state_Q ( 0.3720553414520639)) * f1( 0.3116780574253014)
w2 ( 0.524848116160879 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.41282273690472165) - present_state_Q (0.3720553414520639)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5580313538378258 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4204248194234529) - present_state_Q ( 0.4131807647381289)) * f1( 0.380529918868399)
w2 ( 0.5500025848490476 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4204248194234529) - present_state_Q (0.4131807647381289)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5817629244027253 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43889489941070225) - present_state_Q ( 0.43889489941070225)) * f1( 0.3922608720202805)
w2 ( 0.5742023684702624 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.43889489941070225) - present_state_Q (0.43889489941070225)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5265718203259887 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4571646016876235) - present_state_Q ( 0.4571646016876235)) * f1( 0.39102466788007795)
w2 ( 0.5177444428095079 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.4571646016876235) - present_state_Q (0.4571646016876235)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.47243720540078216 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.41017332493486935) - present_state_Q ( 0.41462287663564856)) * f1( 0.39410597282507687)
w2 ( 0.46280022104382146 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.41017332493486935) - present_state_Q (0.41462287663564856)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.3976539727163243 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36747728081384623) - present_state_Q ( 0.338606407031966)) * f1( 0.32488194591751196)
w2 ( 0.3707258738857982 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.36747728081384623) - present_state_Q (0.338606407031966)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.4065594292308104 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.305895042109836) - present_state_Q ( 0.305895042109836)) * f1( 0.3963362706499293)
w2 ( 0.3797136523698441 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.305895042109836) - present_state_Q (0.305895042109836)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.43484368002517715 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31171827971845256) - present_state_Q ( 0.31171827971845256)) * f1( 0.393135190771274)
w2 ( 0.40849179429997984 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.31171827971845256) - present_state_Q (0.31171827971845256)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.46162103573738356 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33426585749351523) - present_state_Q ( 0.3285999510554651)) * f1( 0.37991407239932284)
w2 ( 0.4366848596877353 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.33426585749351523) - present_state_Q (0.3285999510554651)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.3702031600958538 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3523469669424412) - present_state_Q ( 0.3564750821899535)) * f1( 0.3938320055637285)
w2 ( 0.3438352442679069 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.3523469669424412) - present_state_Q (0.3564750821899535)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.28274912853591805 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28382567111492407) - present_state_Q ( 0.28124362115304824)) * f1( 0.3881909690038192)
w2 ( 0.25372080210624465 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.28382567111492407) - present_state_Q (0.28124362115304824)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1972452387047441 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21184826876474305) - present_state_Q ( 0.21184826876474305)) * f1( 0.3903104794475998)
w2 ( 0.1660942644307139 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.21184826876474305) - present_state_Q (0.21184826876474305)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11232416114781758 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14344735660125002) - present_state_Q ( 0.14505142712980548)) * f1( 0.3985582712858108)
w2 ( 0.08086599677192667 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.14344735660125002) - present_state_Q (0.14505142712980548)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.03193119965382399 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0768006124973442) - present_state_Q ( 0.07600525841979117)) * f1( 0.3886862743053636)
w2 ( -0.001867011114875597 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0768006124973442) - present_state_Q (0.07600525841979117)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.07099776933048671 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011587063263318905) - present_state_Q ( 0.01186258939258931)) * f1( 0.3948925807749752)
w2 ( 0.03770483356247411 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.011587063263318905) - present_state_Q (0.01186258939258931)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04977291162113784 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04241536079936236) - present_state_Q ( 0.043049549252523175)) * f1( 0.3939224582866455)
w2 ( 0.01615251303557063 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.04241536079936236) - present_state_Q (0.043049549252523175)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03070519025706875 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02583331415804366) - present_state_Q ( 0.026254876712449707)) * f1( 0.39768361651994016)
w2 ( -0.0647943487762952 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.02583331415804366) - present_state_Q (0.026254876712449707)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07871289497446571 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03308732314597884) - present_state_Q ( -0.033400757631656994)) * f1( 0.24370531686955507)
w2 ( -0.14359066776361284 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03308732314597884) - present_state_Q (-0.033400757631656994)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11137473872390617 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056364144011732974) - present_state_Q ( -0.07072321078809425)) * f1( 0.16880262994976072)
w2 ( -0.22098719590813604 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.056364144011732974) - present_state_Q (-0.07072321078809425)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1319819403773334 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05610526271354006) - present_state_Q ( -0.07820398230435367)) * f1( 0.10691673595241287)
w2 ( -0.27880939222714607 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.05610526271354006) - present_state_Q (-0.07820398230435367)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.1414989467166381 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0762057306323492) - present_state_Q ( -0.0762057306323492)) * f1( 0.049274791361376115)
w2 ( -0.3270947632879182 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.0762057306323492) - present_state_Q (-0.0762057306323492)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.11215608877619296 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08689593834929316) - present_state_Q ( -0.08802302562072999)) * f1( 0.5064934341868638)
w2 ( -0.3241980961289892 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.08689593834929316) - present_state_Q (-0.08802302562072999)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.20818575502952386 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10565468809757478) - present_state_Q ( -0.12186459290402424)) * f1( 0.5084429592763307)
w2 ( -0.36197211364710385 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.10565468809757478) - present_state_Q (-0.12186459290402424)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.179547702097719 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16440603865900974) - present_state_Q ( -0.16440603865900974)) * f1( 0.44196883651592944)
w2 ( -0.34901280495124165 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.16440603865900974) - present_state_Q (-0.16440603865900974)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2557609997052057 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14292514879814444) - present_state_Q ( -0.14292514879814444)) * f1( 0.40725994793349723)
w2 ( -0.38644015227287504 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.14292514879814444) - present_state_Q (-0.14292514879814444)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21374872788806326 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1910097027454401) - present_state_Q ( -0.18849601523357948)) * f1( 0.35926500628035546)
w2 ( -0.35720527614889913 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1910097027454401) - present_state_Q (-0.18849601523357948)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.17741273422818885 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17429778488667536) - present_state_Q ( -0.17429778488667536)) * f1( 0.3140893642050761)
w2 ( -0.3224992359569589 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.17429778488667536) - present_state_Q (-0.17429778488667536)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.188202767662079 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14727814343638082) - present_state_Q ( -0.14910700177996672)) * f1( 0.29511540544511866)
w2 ( -0.33346786033386905 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.14727814343638082) - present_state_Q (-0.14910700177996672)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.23227705788743858 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.161437289167853) - present_state_Q ( -0.161437289167853)) * f1( 0.2376348584378983)
w2 ( -0.3983825857250817 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.161437289167853) - present_state_Q (-0.161437289167853)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.25401033030781844 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1373660818931074) - present_state_Q ( -0.17720434046561556)) * f1( 0.16260941052620242)
w2 ( -0.445161215095411 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.1373660818931074) - present_state_Q (-0.17720434046561556)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.27873583613260877 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1901475401214793) - present_state_Q ( -0.1901475401214793)) * f1( 0.1351957410411998)
w2 ( -0.5091715675815844 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1901475401214793) - present_state_Q (-0.1901475401214793)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.2916074424150127 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1470412691031916) - present_state_Q ( -0.19795842586135007)) * f1( 0.07084979628668983)
w2 ( -0.5727576671182983 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1470412691031916) - present_state_Q (-0.19795842586135007)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.298219062295196 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15370753415479826) - present_state_Q ( -0.1823454175107132)) * f1( 0.03606944078009639)
w2 ( -0.6277484271954413 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.15370753415479826) - present_state_Q (-0.1823454175107132)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.3079680459961842 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0778391744220685) - present_state_Q ( -0.0778391744220685)) * f1( 0.05051431517014411)
w2 ( -0.6470478746256427 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.0778391744220685) - present_state_Q (-0.0778391744220685)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.25460466719239994 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14341221975416277) - present_state_Q ( -0.1453108476549984)) * f1( 0.4718374180183578)
w2 ( -0.6470478746256427 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.14341221975416277) - present_state_Q (-0.1453108476549984)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2019097502392813 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15049646532671226) - present_state_Q ( -0.15051035142839211)) * f1( 0.46408402092574463)
w2 ( -0.6413705711011641 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.15049646532671226) - present_state_Q (-0.15051035142839211)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.15624656982186785 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1155799252062392) - present_state_Q ( -0.1155799252062392)) * f1( 0.41360754769005686)
w2 ( -0.635850461437736 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1155799252062392) - present_state_Q (-0.1155799252062392)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11847664113076566 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08654237684606661) - present_state_Q ( -0.08654237684606661)) * f1( 0.35040675668335314)
w2 ( -0.6304610207419288 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.08654237684606661) - present_state_Q (-0.08654237684606661)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.08484374107059539 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06783411296609353) - present_state_Q ( -0.06903500265961915)) * f1( 0.31661896610589957)
w2 ( -0.6251497627851137 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06783411296609353) - present_state_Q (-0.06903500265961915)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.0573389543841321 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05352115845355388) - present_state_Q ( -0.05352115845355388)) * f1( 0.2624079281908775)
w2 ( -0.6199089175720727 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.05352115845355388) - present_state_Q (-0.05352115845355388)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.03211576052222238 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043750679570561404) - present_state_Q ( -0.04489495496372782)) * f1( 0.24240953178195232)
w2 ( -0.6147063181370394 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.043750679570561404) - present_state_Q (-0.04489495496372782)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.01592921030184424 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03577160893810727) - present_state_Q ( -0.03577160893810727)) * f1( 0.15681686964163455)
w2 ( -0.6095453458968179 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.03577160893810727) - present_state_Q (-0.03577160893810727)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.03306287072831926 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036217007498009914) - present_state_Q ( -0.03631769833682697)) * f1( 0.3666491264359719)
w2 ( -0.6118818659088827 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.036217007498009914) - present_state_Q (-0.03631769833682697)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.09737399948656045 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07151845876275298) - present_state_Q ( -0.07217701446611403)) * f1( 0.3323615775992954)
w2 ( -0.6312316142229843 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.07151845876275298) - present_state_Q (-0.07217701446611403)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.15435407766558198 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09106469006970797) - present_state_Q ( -0.09206557003932958)) * f1( 0.29722932989956696)
w2 ( -0.6504020232126607 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09106469006970797) - present_state_Q (-0.09206557003932958)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.13952941620605688 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1344099239692791) - present_state_Q ( -0.1344099239692791)) * f1( 0.23873435055740497)
w2 ( -0.6410874892390754 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.1344099239692791) - present_state_Q (-0.1344099239692791)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11980228819263672 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16886766446681734) - present_state_Q ( -0.1703400881494766)) * f1( 0.3018903930584424)
w2 ( -0.6280184228050195 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.16886766446681734) - present_state_Q (-0.1703400881494766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15332602687814317 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14710837132830035) - present_state_Q ( -0.14710837132830035)) * f1( 0.1795014693936219)
w2 ( -0.6653704721211101 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.14710837132830035) - present_state_Q (-0.14710837132830035)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1958676453683548 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13452071524464823) - present_state_Q ( -0.13452071524464823)) * f1( 0.22641390462737146)
w2 ( -0.6935544424653074 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13452071524464823) - present_state_Q (-0.13452071524464823)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.19372508413460582 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13611298811914238) - present_state_Q ( -0.13781978174450343)) * f1( 0.17249717436061054)
w2 ( -0.6916913152213185 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.13611298811914238) - present_state_Q (-0.13781978174450343)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.168633626363638 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07993089771897943) - present_state_Q ( -0.07993089771897943)) * f1( 0.234075685967469)
w2 ( -0.6863316261815832 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.07993089771897943) - present_state_Q (-0.07993089771897943)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.21241687070625404 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10406271517909987) - present_state_Q ( -0.10743204472334217)) * f1( 0.23007796808875328)
w2 ( -0.7053613684495288 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.10406271517909987) - present_state_Q (-0.10743204472334217)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.24534846642879782 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1054775065942665) - present_state_Q ( -0.10729010189689633)) * f1( 0.17302752333061897)
w2 ( -0.7243939449371541 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1054775065942665) - present_state_Q (-0.10729010189689633)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.23645393747606866 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02068807757670116) - present_state_Q ( -0.05690777482355887)) * f1( 0.08432120191264784)
w2 ( -0.7191197501018247 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.02068807757670116) - present_state_Q (-0.05690777482355887)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.24634589163186738 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09755312723139176) - present_state_Q ( -0.09755312723139176)) * f1( 0.1084403689568689)
w2 ( -0.7282417719567422 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.09755312723139176) - present_state_Q (-0.09755312723139176)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.28899315057662667 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.129710139438876) - present_state_Q ( -0.16612222803671312)) * f1( 0.23091906208125682)
w2 ( -0.7559445037453498 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.129710139438876) - present_state_Q (-0.16612222803671312)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3260138019199852 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16834016284222844) - present_state_Q ( -0.1713645371511574)) * f1( 0.2006028913615494)
w2 ( -0.7836265459323458 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16834016284222844) - present_state_Q (-0.1713645371511574)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3159540083183493 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10629746475137254) - present_state_Q ( -0.18466011934460713)) * f1( 0.0856859740097571)
w2 ( -0.7601459384749564 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10629746475137254) - present_state_Q (-0.18466011934460713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3003499221867192 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1877220846830872) - present_state_Q ( -0.1877220846830872)) * f1( 0.23326241152663216)
w2 ( -0.7501116903317346 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.1877220846830872) - present_state_Q (-0.1877220846830872)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.34217319499667115 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17775302294415843) - present_state_Q ( -0.18090264114980464)) * f1( 0.2276873824443039)
w2 ( -0.7776647802489038 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.17775302294415843) - present_state_Q (-0.18090264114980464)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3752554839747983 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1745522085734572) - present_state_Q ( -0.17819552564115188)) * f1( 0.17986741657076624)
w2 ( -0.8052536756771468 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1745522085734572) - present_state_Q (-0.17819552564115188)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.39879172232876525 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16445464370297574) - present_state_Q ( -0.1685844074764772)) * f1( 0.12737017356451247)
w2 ( -0.8329715915305541 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16445464370297574) - present_state_Q (-0.1685844074764772)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.41899082202246674 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16852698469898278) - present_state_Q ( -0.16852698469898278)) * f1( 0.1092832261284278)
w2 ( -0.8606964772371178 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.16852698469898278) - present_state_Q (-0.16852698469898278)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4305204126861847 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15507037936053347) - present_state_Q ( -0.15507037936053347)) * f1( 0.06197249774977997)
w2 ( -0.8886030271157507 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.15507037936053347) - present_state_Q (-0.15507037936053347)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.44153225109450384 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2758301561018487) - present_state_Q ( -0.281553465681613)) * f1( 0.44758194336884094)
w2 ( -0.8910633226150364 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2758301561018487) - present_state_Q (-0.281553465681613)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.44877097297161594 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35769890070900845) - present_state_Q ( -0.35769890070900845)) * f1( 0.406507646363492)
w2 ( -0.8946247424022743 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.35769890070900845) - present_state_Q (-0.35769890070900845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.40978223851960976 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3473537786849502) - present_state_Q ( -0.3490592359471145)) * f1( 0.4787865916639026)
w2 ( -0.882409884531095 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3473537786849502) - present_state_Q (-0.3490592359471145)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.37268521852266345 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36042287463217076) - present_state_Q ( -0.36079929426598256)) * f1( 0.4497933293195752)
w2 ( -0.8659147443950397 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.36042287463217076) - present_state_Q (-0.36079929426598256)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.381107169145205 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32331140667449426) - present_state_Q ( -0.32343997065698854)) * f1( 0.40317408448235326)
w2 ( -0.8700925677952489 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.32331140667449426) - present_state_Q (-0.32343997065698854)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32950542958308787 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35902678371550706) - present_state_Q ( -0.3654379326556979)) * f1( 0.3881186256313351)
w2 ( -0.8368541864381452 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.35902678371550706) - present_state_Q (-0.3654379326556979)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.27768163054621103 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30148614564841514) - present_state_Q ( -0.30166855899520195)) * f1( 0.40757362292176885)
w2 ( -0.811423787549538 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.30148614564841514) - present_state_Q (-0.30166855899520195)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2197043221679264 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28507094644519004) - present_state_Q ( -0.28991394776905643)) * f1( 0.45962417466397404)
w2 ( -0.7861956504870473 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.28507094644519004) - present_state_Q (-0.28991394776905643)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16807002908750476 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2519244252273794) - present_state_Q ( -0.24986986800298888)) * f1( 0.4216154556794701)
w2 ( -0.7617021019774423 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2519244252273794) - present_state_Q (-0.24986986800298888)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12064065143855549 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2552486739025356) - present_state_Q ( -0.2552486739025356)) * f1( 0.3856913023703064)
w2 ( -0.7309590068146352 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2552486739025356) - present_state_Q (-0.2552486739025356)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.08292413479962463 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2207009401057179) - present_state_Q ( -0.2207009401057179)) * f1( 0.3146633240901674)
w2 ( -0.7009932356622566 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2207009401057179) - present_state_Q (-0.2207009401057179)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.06704210988051094 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12547385702285802) - present_state_Q ( -0.16052351880597082)) * f1( 0.245102004653192)
w2 ( -0.6880337130001829 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.12547385702285802) - present_state_Q (-0.16052351880597082)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10529020962624944 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1509361139316831) - present_state_Q ( -0.18602778870204828)) * f1( 0.2091127572952172)
w2 ( -0.7337603585674609 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1509361139316831) - present_state_Q (-0.18602778870204828)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.13002722031739472 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12385554786598044) - present_state_Q ( -0.12385554786598044)) * f1( 0.13098553160656806)
w2 ( -0.7620883086712702 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.12385554786598044) - present_state_Q (-0.12385554786598044)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09457500131387159 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041363846580489805) - present_state_Q ( -0.08091501155753306)) * f1( 0.32924333858302474)
w2 ( -0.7567044155367728 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.041363846580489805) - present_state_Q (-0.08091501155753306)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.061534172065178576 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06564457041602811) - present_state_Q ( -0.06729452034523477)) * f1( 0.311491400043737)
w2 ( -0.7514007652202547 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06564457041602811) - present_state_Q (-0.06729452034523477)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.03645769954973892 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052306883409627684) - present_state_Q ( -0.052306883409627684)) * f1( 0.23949042709155655)
w2 ( -0.7461653842449113 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.052306883409627684) - present_state_Q (-0.052306883409627684)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.015514745041936277 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04439596075163193) - present_state_Q ( -0.0817042299638775)) * f1( 0.19440863320837587)
w2 ( -0.7353927379060241 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.04439596075163193) - present_state_Q (-0.0817042299638775)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0007193137425040366 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038911356587091175) - present_state_Q ( -0.07568099348239239)) * f1( 0.138044143555109)
w2 ( -0.7246748393277873 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.038911356587091175) - present_state_Q (-0.07568099348239239)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0011024081740340637 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.511356460616468e-06) - present_state_Q ( -5.511356460616468e-06)) * f1( 0.007661964640673523)
w2 ( -0.7246748393277873 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -5.511356460616468e-06) - present_state_Q (-5.511356460616468e-06)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.44491396935109 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11344356597957178) - present_state_Q ( -0.11344356597957178)) * f1( 0.16030230528021341)
w2 ( -0.8930925310688426 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.11344356597957178) - present_state_Q (-0.11344356597957178)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4510964649372444 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22331098855447987) - present_state_Q ( -0.22715002533417072)) * f1( 0.20944756086161312)
w2 ( -0.8975202471716617 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.22331098855447987) - present_state_Q (-0.22715002533417072)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4905528633777593 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09520860595909333) - present_state_Q ( -0.14008461831767643)) * f1( 0.21106041248259072)
w2 ( -0.9068674283830529 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09520860595909333) - present_state_Q (-0.14008461831767643)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5190107845726125 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07214262488601951) - present_state_Q ( -0.07214262488601951)) * f1( 0.14706391557736104)
w2 ( -0.9068674283830529 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.07214262488601951) - present_state_Q (-0.07214262488601951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5394036128713744 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05556996317849741) - present_state_Q ( -0.10091333459765006)) * f1( 0.10706899515442123)
w2 ( -0.9163906466916539 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.05556996317849741) - present_state_Q (-0.10091333459765006)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5057480071075451 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2855994666010339) - present_state_Q ( -0.28561618659548355)) * f1( 0.4445588582256724)
w2 ( -0.912605365491977 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2855994666010339) - present_state_Q (-0.28561618659548355)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4763076501598609 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2452543466269056) - present_state_Q ( -0.25067594043775154)) * f1( 0.4054305094267047)
w2 ( -0.9089746129631017 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2452543466269056) - present_state_Q (-0.25067594043775154)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.45232807801597813 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21100323742297575) - present_state_Q ( -0.21100323742297575)) * f1( 0.3475789370992811)
w2 ( -0.9055250983946983 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.21100323742297575) - present_state_Q (-0.21100323742297575)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.43244328683394134 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1754011515329419) - present_state_Q ( -0.18087138994416782)) * f1( 0.2997716516277)
w2 ( -0.9022084420207439 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.1754011515329419) - present_state_Q (-0.18087138994416782)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4170624539085318 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14987453165473763) - present_state_Q ( -0.14987453165473763)) * f1( 0.24226092239912597)
w2 ( -0.8990340066282976 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.14987453165473763) - present_state_Q (-0.14987453165473763)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.39590990204194343 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12429523183239892) - present_state_Q ( -0.12429523183239892)) * f1( 0.19024376507021967)
w2 ( -0.8934746780850518 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.12429523183239892) - present_state_Q (-0.12429523183239892)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.382015513413677 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09533604445055952) - present_state_Q ( -0.09533604445055952)) * f1( 0.12796424207884466)
w2 ( -0.8880456658850242 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09533604445055952) - present_state_Q (-0.09533604445055952)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.35511750922834057 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.271288219453245) - present_state_Q ( -0.271288219453245)) * f1( 0.3614548747944844)
w2 ( -0.8768832749224054 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.271288219453245) - present_state_Q (-0.271288219453245)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3371725927869448 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21933891710671546) - present_state_Q ( -0.22249594039907294)) * f1( 0.25615027926494777)
w2 ( -0.8663748441920793 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.21933891710671546) - present_state_Q (-0.22249594039907294)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3028564318790651 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2219343468340642) - present_state_Q ( -0.22606659235983106)) * f1( 0.2850479777629794)
w2 ( -0.8483167468269329 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2219343468340642) - present_state_Q (-0.22606659235983106)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.373037338929352 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24724269226240947) - present_state_Q ( -0.29235509057889747)) * f1( 0.40511519089184617)
w2 ( -0.8829641303998798 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.24724269226240947) - present_state_Q (-0.29235509057889747)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4341037813627163 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3054567031089136) - present_state_Q ( -0.3089091602800229)) * f1( 0.35469997341232856)
w2 ( -0.9173968606004971 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3054567031089136) - present_state_Q (-0.3089091602800229)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.48356830145460106 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2591463207421125) - present_state_Q ( -0.2591463207421125)) * f1( 0.2799717414819927)
w2 ( -0.9438983852704785 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2591463207421125) - present_state_Q (-0.2591463207421125)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5233192084170586 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2498722718072109) - present_state_Q ( -0.2498722718072109)) * f1( 0.2239342688321465)
w2 ( -0.9705251096010812 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2498722718072109) - present_state_Q (-0.2498722718072109)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5549817271943218 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23822205629336196) - present_state_Q ( -0.23838283787643644)) * f1( 0.1773374069661784)
w2 ( -0.9973067001173747 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.23822205629336196) - present_state_Q (-0.23838283787643644)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5584599979633108 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2762555842621826) - present_state_Q ( -0.2762555842621826)) * f1( 0.1383725634120182)
w2 ( -1.0023340996006553 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2762555842621826) - present_state_Q (-0.2762555842621826)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6230756075095701 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1932069065625257) - present_state_Q ( -0.1981422117216797)) * f1( 0.3548010823412585)
w2 ( -1.0023340996006553 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1932069065625257) - present_state_Q (-0.1981422117216797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5822879879975877 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30029939374664816) - present_state_Q ( -0.30029939374664816)) * f1( 0.3210942321851521)
w2 ( -0.9896314050569355 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.30029939374664816) - present_state_Q (-0.30029939374664816)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5472530413219822 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2638229900122824) - present_state_Q ( -0.2638229900122824)) * f1( 0.28312424934871194)
w2 ( -0.977256998146825 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2638229900122824) - present_state_Q (-0.2638229900122824)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5190744274394542 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22588096865249085) - present_state_Q ( -0.22588096865249085)) * f1( 0.23417918067340046)
w2 ( -0.9652240694289526 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.22588096865249085) - present_state_Q (-0.22588096865249085)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.49157543800902986 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2103979550018904) - present_state_Q ( -0.215976607761656)) * f1( 0.23012923485369371)
w2 ( -0.9532747013063378 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2103979550018904) - present_state_Q (-0.215976607761656)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4731751401479816 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17355884302502017) - present_state_Q ( -0.17355884302502017)) * f1( 0.15914418590814405)
w2 ( -0.9417126717191127 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.17355884302502017) - present_state_Q (-0.17355884302502017)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4592334738421785 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14685553261769208) - present_state_Q ( -0.15216652236585676)) * f1( 0.12256614997949374)
w2 ( -0.9303378620280718 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.14685553261769208) - present_state_Q (-0.15216652236585676)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.46211938489161464 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12742932620914074) - present_state_Q ( -0.12742932620914074)) * f1( 0.07489771971229178)
w2 ( -0.9341909980921895 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.12742932620914074) - present_state_Q (-0.12742932620914074)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4648117134276586 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11909728730291957) - present_state_Q ( -0.12562817630232348)) * f1( 0.06969860504912348)
w2 ( -0.9380538136164692 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.11909728730291957) - present_state_Q (-0.12562817630232348)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4744142839221477 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26129004807834705) - present_state_Q ( -0.2642281900471053)) * f1( 0.3666491264359719)
w2 ( -0.9406728217640765 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.26129004807834705) - present_state_Q (-0.2642281900471053)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5363966782702222 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2560815533589642) - present_state_Q ( -0.2606761556994629)) * f1( 0.35118856908278945)
w2 ( -0.9583221417604408 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2560815533589642) - present_state_Q (-0.2606761556994629)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5446391927800144 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26477340071190186) - present_state_Q ( -0.26477340071190186)) * f1( 0.31495569115129707)
w2 ( -0.9609391811540338 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.26477340071190186) - present_state_Q (-0.26477340071190186)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5951854880129888 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24551657109321223) - present_state_Q ( -0.25134665790623234)) * f1( 0.2850561286241059)
w2 ( -0.9786712311460647 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.24551657109321223) - present_state_Q (-0.25134665790623234)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6238096030363574 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22959024951677998) - present_state_Q ( -0.22959024951677998)) * f1( 0.22131441215397865)
w2 ( -0.9916049189004137 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.22959024951677998) - present_state_Q (-0.22959024951677998)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6593650911711026 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2180443843962926) - present_state_Q ( -0.27577066890575713)) * f1( 0.20363574150251)
w2 ( -1.0177954254434218 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2180443843962926) - present_state_Q (-0.27577066890575713)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6967302453099365 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2871232472043356) - present_state_Q ( -0.2947564237077484)) * f1( 0.21549079833582524)
w2 ( -1.043804763958612 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2871232472043356) - present_state_Q (-0.2947564237077484)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7256217194245042 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27120954995547086) - present_state_Q ( -0.27120954995547086)) * f1( 0.16453833622607064)
w2 ( -1.0701434350342132 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.27120954995547086) - present_state_Q (-0.27120954995547086)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7105736772211169 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24967877114941223) - present_state_Q ( -0.24967877114941223)) * f1( 0.1228701587998103)
w2 ( -1.0517727716236962 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.24967877114941223) - present_state_Q (-0.24967877114941223)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6461065200826156 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3342331142614619) - present_state_Q ( -0.3483692011021372)) * f1( 0.4902647146521463)
w2 ( -1.0517727716236962 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3342331142614619) - present_state_Q (-0.3483692011021372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.649340275722314 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4968779961964105) - present_state_Q ( -0.5033346284272198)) * f1( 0.6976341761547298)
w2 ( -1.0520045374796583 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4968779961964105) - present_state_Q (-0.5033346284272198)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5556917825064658 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5269118905888246) - present_state_Q ( -0.5197121697182809)) * f1( 0.63835824061463)
w2 ( -1.0373343276730642 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5269118905888246) - present_state_Q (-0.5197121697182809)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4611542495688218 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5146393333261189) - present_state_Q ( -0.5146393333261189)) * f1( 0.6461120993290586)
w2 ( -1.0153866966731615 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5146393333261189) - present_state_Q (-0.5146393333261189)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.38828600171841776 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3994779672143731) - present_state_Q ( -0.3994779672143731)) * f1( 0.5359811016476639)
w2 ( -0.9949937441157675 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.3994779672143731) - present_state_Q (-0.3994779672143731)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4948119086522381 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28726592422524994) - present_state_Q ( -0.28726592422524994)) * f1( 0.6117043518651147)
w2 ( -1.0037010474567538 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.28726592422524994) - present_state_Q (-0.28726592422524994)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5004410968861257 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39314739121487213) - present_state_Q ( -0.3983873806327943)) * f1( 0.39943899426307694)
w2 ( -1.0065195946265277 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.39314739121487213) - present_state_Q (-0.3983873806327943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5060402726310406 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3744336557137692) - present_state_Q ( -0.3844353613881917)) * f1( 0.3659400548883325)
w2 ( -1.0095797547101915 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3744336557137692) - present_state_Q (-0.3844353613881917)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5436640253352003 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3544790311163056) - present_state_Q ( -0.3645137744168618)) * f1( 0.32131399864566773)
w2 ( -1.0329984372840868 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.3544790311163056) - present_state_Q (-0.3645137744168618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5091279557104496 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39661822997056684) - present_state_Q ( -0.39661822997056684)) * f1( 0.25451126835959564)
w2 ( -0.9990745271097491 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.39661822997056684) - present_state_Q (-0.39661822997056684)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.5232664682285867 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35064523759173594) - present_state_Q ( -0.3557298328019741)) * f1( 0.20812292830527448)
w2 ( -1.016057894383679 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.35064523759173594) - present_state_Q (-0.3557298328019741)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.502545260563945 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33719743283650094) - present_state_Q ( -0.33719743283650094)) * f1( 0.1589686408192376)
w2 ( -0.9834709521448578 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.33719743283650094) - present_state_Q (-0.33719743283650094)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.5029620847269013 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4018177752292577) - present_state_Q ( -0.409889193972381)) * f1( 0.625528779349247)
w2 ( -1.0050039732922593 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.4018177752292577) - present_state_Q (-0.409889193972381)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5068319180962318 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47200813206350295) - present_state_Q ( -0.4765694265482983)) * f1( 0.5478914619170049)
w2 ( -1.0064166010254203 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.47200813206350295) - present_state_Q (-0.4765694265482983)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5350827797141833 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5196363124272694) - present_state_Q ( -0.521555472636525)) * f1( 0.5326249447630773)
w2 ( -1.0196768049905753 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.5196363124272694) - present_state_Q (-0.521555472636525)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.610979693185558 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5808093794598987) - present_state_Q ( -0.5808093794598987)) * f1( 0.5137641284392828)
w2 ( -1.0639949517451581 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5808093794598987) - present_state_Q (-0.5808093794598987)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.6759596536331796 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5893530572674608) - present_state_Q ( -0.5893530572674608)) * f1( 0.44216620414234586)
w2 ( -1.1080824191989367 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5893530572674608) - present_state_Q (-0.5893530572674608)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.7346399063803012 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3548717933886236) - present_state_Q ( -0.41027591434857047)) * f1( 0.3610623062440573)
w2 ( -1.132460588173791 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3548717933886236) - present_state_Q (-0.41027591434857047)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7840397470263683 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32588454988094834) - present_state_Q ( -0.32588454988094834)) * f1( 0.28944587575057845)
w2 ( -1.1495276272248625 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.32588454988094834) - present_state_Q (-0.32588454988094834)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8268541486078532 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25325568758585865) - present_state_Q ( -0.31073206894710176)) * f1( 0.24970584331616955)
w2 ( -1.1666735622229774 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.25325568758585865) - present_state_Q (-0.31073206894710176)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8618940539043595 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3466388374895375) - present_state_Q ( -0.3466388374895375)) * f1( 0.20757929732235333)
w2 ( -1.1919939379168687 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3466388374895375) - present_state_Q (-0.3466388374895375)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.889350003804633 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32217990467319046) - present_state_Q ( -0.3817796015690339)) * f1( 0.1663554973330521)
w2 ( -1.2250027056948345 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.32217990467319046) - present_state_Q (-0.3817796015690339)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8811853928704597 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2802768865198252) - present_state_Q ( -0.2802768865198252)) * f1( 0.10853598724086176)
w2 ( -1.2137189677268168 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2802768865198252) - present_state_Q (-0.2802768865198252)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.8628690738708406 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2762755548216708) - present_state_Q ( -0.2762755548216708)) * f1( 0.24465862482473438)
w2 ( -1.2099757277301193 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2762755548216708) - present_state_Q (-0.2762755548216708)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.8363554846162707 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24758757493033307) - present_state_Q ( -0.24758757493033307)) * f1( 0.21682175686810126)
w2 ( -1.2038615836429327 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.24758757493033307) - present_state_Q (-0.24758757493033307)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.8270661772802175 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17788970792079056) - present_state_Q ( -0.17788970792079056)) * f1( 0.14072560161740846)
w2 ( -1.2005610799572892 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.17788970792079056) - present_state_Q (-0.17788970792079056)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.8240097681823151 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02472819080942917) - present_state_Q ( -0.02472819080942917)) * f1( 0.02989868463820766)
w2 ( -1.2005610799572892 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.02472819080942917) - present_state_Q (-0.02472819080942917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8216992759886922 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5950912419663676) - present_state_Q ( -0.5950912419663676)) * f1( 0.6493408314185404)
w2 ( -1.2003831693684406 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.5950912419663676) - present_state_Q (-0.5950912419663676)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.6983691624556749 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7312215957396536) - present_state_Q ( -0.7312215957396536)) * f1( 0.7438040858286218)
w2 ( -1.1838021750067838 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.7312215957396536) - present_state_Q (-0.7312215957396536)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6037482085995879 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5116460800248601) - present_state_Q ( -0.5116460800248601)) * f1( 0.6478750718080827)
w2 ( -1.176499767646672 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.5116460800248601) - present_state_Q (-0.5116460800248601)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5194215149900979 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47443025572037617) - present_state_Q ( -0.47443025572037617)) * f1( 0.5909421740285931)
w2 ( -1.1622298953451886 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.47443025572037617) - present_state_Q (-0.47443025572037617)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.465015242327717 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43364777179206465) - present_state_Q ( -0.43364777179206465)) * f1( 0.6111121181870894)
w2 ( -1.15332706539906 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.43364777179206465) - present_state_Q (-0.43364777179206465)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5553690873213943 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3669655435203202) - present_state_Q ( -0.3669655435203202)) * f1( 0.5411281482319181)
w2 ( -1.1700243755073771 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3669655435203202) - present_state_Q (-0.3669655435203202)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6365174601939013 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3903563819887772) - present_state_Q ( -0.3903563819887772)) * f1( 0.49220230415858274)
w2 ( -1.1865111680694782 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3903563819887772) - present_state_Q (-0.3903563819887772)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.7185052876391594 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43839079798679326) - present_state_Q ( -0.4450687577420005)) * f1( 0.5128180471838377)
w2 ( -1.202498871290045 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.43839079798679326) - present_state_Q (-0.4450687577420005)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.7999993858946841 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5237935891730778) - present_state_Q ( -0.5773401449190319)) * f1( 0.5524876727488817)
w2 ( -1.2246244595000193 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5237935891730778) - present_state_Q (-0.5773401449190319)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.8895398629806982 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6175705314772684) - present_state_Q ( -0.6189368143959102)) * f1( 0.6205934369445462)
w2 ( -1.2390526618875375 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6175705314772684) - present_state_Q (-0.6189368143959102)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.9697707091801485 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.710368934067364) - present_state_Q ( -0.710368934067364)) * f1( 0.5896430914593138)
w2 ( -1.2594626812776282 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.710368934067364) - present_state_Q (-0.710368934067364)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.0400138795842278 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6795539253221021) - present_state_Q ( -0.6795539253221021)) * f1( 0.5059283792405361)
w2 ( -1.2802887032857797 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6795539253221021) - present_state_Q (-0.6795539253221021)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1042337321547964 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6608681064939497) - present_state_Q ( -0.6706827017108555)) * f1( 0.4602240466342016)
w2 ( -1.3012197649198578 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6608681064939497) - present_state_Q (-0.6706827017108555)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.162666114013956 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6306711663553608) - present_state_Q ( -0.6526612642701508)) * f1( 0.4142948057196651)
w2 ( -1.3223758527053386 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6306711663553608) - present_state_Q (-0.6526612642701508)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.214179898001675 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6115182395841744) - present_state_Q ( -0.6115182395841744)) * f1( 0.3553572747140494)
w2 ( -1.3441203564709523 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6115182395841744) - present_state_Q (-0.6115182395841744)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2574529318102665 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5509219080914731) - present_state_Q ( -0.5509219080914731)) * f1( 0.28768706778601966)
w2 ( -1.3666829107117173 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5509219080914731) - present_state_Q (-0.5509219080914731)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2824378969626051 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5056797403531148) - present_state_Q ( -0.5056797403531148)) * f1( 0.23911614990908106)
w2 ( -1.3823562342169502 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.5056797403531148) - present_state_Q (-0.5056797403531148)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.3126594851517879 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.448294296344512) - present_state_Q ( -0.45043885332334)) * f1( 0.18954946572191447)
w2 ( -1.4062720928616168 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.448294296344512) - present_state_Q (-0.45043885332334)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2750967150083918 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1155388083178448) - present_state_Q ( -1.141448670658085)) * f1( 0.7088722301970373)
w2 ( -1.3983236710142224 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -1.1155388083178448) - present_state_Q (-1.141448670658085)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.343416033271429 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0048405445407722) - present_state_Q ( -1.0048405445407722)) * f1( 0.6235542641825456)
w2 ( -1.414758323662922 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0048405445407722) - present_state_Q (-1.0048405445407722)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.406663490809646 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9604135218380799) - present_state_Q ( -0.9604135218380799)) * f1( 0.5569382490297199)
w2 ( -1.431792741118108 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.9604135218380799) - present_state_Q (-0.9604135218380799)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4688543174379343 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0341866805587845) - present_state_Q ( -1.0289637652685513)) * f1( 0.5788128144508831)
w2 ( -1.4479095646599178 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.0341866805587845) - present_state_Q (-1.0289637652685513)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.5254808253842096 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1579436276206614) - present_state_Q ( -1.1579436276206614)) * f1( 0.5911830086753106)
w2 ( -1.467066579362746 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.1579436276206614) - present_state_Q (-1.1579436276206614)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4144472080090833 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1099279719537478) - present_state_Q ( -1.2368713090182903)) * f1( 0.5222952146965176)
w2 ( -1.4032902240080585 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.1099279719537478) - present_state_Q (-1.2368713090182903)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -1.3269652300275594 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0015887950975397) - present_state_Q ( -1.0015887950975397)) * f1( 0.46008520884389625)
w2 ( -1.3557544761183637 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -1.0015887950975397) - present_state_Q (-1.0015887950975397)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -1.321477484444782 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6404854802700953) - present_state_Q ( -0.7082732040760136)) * f1( 0.38049982112023584)
w2 ( -1.3535911062776287 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.6404854802700953) - present_state_Q (-0.7082732040760136)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.361831100871494 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8700931357645536) - present_state_Q ( -0.9377726910784351)) * f1( 0.3511340975107275)
w2 ( -1.3938143880650593 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.8700931357645536) - present_state_Q (-0.9377726910784351)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.3940626222721875 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.937001255917246) - present_state_Q ( -0.937001255917246)) * f1( 0.2786509284803231)
w2 ( -1.4400823428520384 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.937001255917246) - present_state_Q (-0.937001255917246)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.421167212638698 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6592781138541347) - present_state_Q ( -0.8032863481393386)) * f1( 0.21466577136496512)
w2 ( -1.484274794065651 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6592781138541347) - present_state_Q (-0.8032863481393386)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.4442477597324093 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6980844172415837) - present_state_Q ( -0.7722981569448663)) * f1( 0.17788334600860078)
w2 ( -1.5296876540329263 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6980844172415837) - present_state_Q (-0.7722981569448663)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -1.4280530039406358 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8031462067061071) - present_state_Q ( -0.8031462067061071)) * f1( 0.1324365184602228)
w2 ( -1.4807743905915065 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.8031462067061071) - present_state_Q (-0.8031462067061071)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3767079366277564 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4904937476272255) - present_state_Q ( -0.5039691978293541)) * f1( 0.35290650727856604)
w2 ( -1.4807743905915065 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.4904937476272255) - present_state_Q (-0.5039691978293541)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.368677073868915 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4787264568357899) - present_state_Q ( -0.4787264568357899)) * f1( 0.1863941446256425)
w2 ( -1.4743115834242233 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.4787264568357899) - present_state_Q (-0.4787264568357899)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.3587412584794716 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5341411008596348) - present_state_Q ( -0.5611749323897159)) * f1( 0.3538413630805237)
w2 ( -1.4782355864799877 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.5341411008596348) - present_state_Q (-0.5611749323897159)) * f2(0.05)
============================================================================
