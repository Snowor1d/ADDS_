GUIDE learning . . .
w1 ( 0.9278724583989048 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5148153000446647) - present_state_Q ( 0.4951597366792507)) * f1( 0.29515973667925066)
w2 ( 0.9511264358665044 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5148153000446647) - present_state_Q (0.4951597366792507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8655643787039169 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4883147424090228) - present_state_Q ( 0.4883147424090228)) * f1( 0.32126123858670375)
w2 ( 0.912336770503142 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.4883147424090228) - present_state_Q (0.4883147424090228)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.767920878341903 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6519159061255677) - present_state_Q ( 0.6062990676004105)) * f1( 0.38425568869584864)
w2 ( 0.8361035461935064 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6519159061255677) - present_state_Q (0.6062990676004105)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.6544783040279821 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6692047016516978) - present_state_Q ( 0.6692047016516978)) * f1( 0.43593460292044284)
w2 ( 0.7320121769340452 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6692047016516978) - present_state_Q (0.6692047016516978)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5261219308588576 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6713289453415218) - present_state_Q ( 0.6216583605524848)) * f1( 0.5024666024143812)
w2 ( 0.6298311582933118 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6713289453415218) - present_state_Q (0.6216583605524848)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.3798151233003034 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.593098318247266) - present_state_Q ( 0.5878496065444965)) * f1( 0.5786217366296676)
w2 ( 0.5160468684309222 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.593098318247266) - present_state_Q (0.5878496065444965)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.842036245797368 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7326292609584131) - present_state_Q ( 0.7419218545298557)) * f1( 0.5919218545298557)
w2 ( 0.9599701160734898 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7326292609584131) - present_state_Q (0.7419218545298557)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.6628512047548264 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7979028772229314) - present_state_Q ( 0.795562959584158)) * f1( 0.6597939617666778)
w2 ( 0.8920757992769431 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7979028772229314) - present_state_Q (0.795562959584158)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.5018174695703436 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6733971133168004) - present_state_Q ( 0.6382139244625565)) * f1( 0.626377340291464)
w2 ( 0.8278039439486712 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6733971133168004) - present_state_Q (0.6382139244625565)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.33370160610019733 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5525036519106419) - present_state_Q ( 0.5456990858794025)) * f1( 0.6750424615194665)
w2 ( 0.7655427259314627 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5525036519106419) - present_state_Q (0.5456990858794025)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.17179551426232834 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42099079016081054) - present_state_Q ( 0.41871836072024726)) * f1( 0.6812453853432237)
w2 ( 0.7061272438888585 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.42099079016081054) - present_state_Q (0.41871836072024726)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.014981186543256808 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2954258362617513) - present_state_Q ( 0.2954258362617513)) * f1( 0.692067111298307)
w2 ( 0.6494801625729691 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2954258362617513) - present_state_Q (0.2954258362617513)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.9097907484152647 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5226993667483077) - present_state_Q ( 0.4726993667483077)) * f1( 0.37269936674830767)
w2 ( 0.9757957056992652 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5226993667483077) - present_state_Q (0.4726993667483077)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.9260301216037706 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6447148059193711) - present_state_Q ( 0.6447148059193711)) * f1( 0.38687587758249614)
w2 ( 0.9883884059394422 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6447148059193711) - present_state_Q (0.6447148059193711)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.9419463379446569 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6533223785722353) - present_state_Q ( 0.6614194116088219)) * f1( 0.394050777954201)
w2 ( 1.0005057907268942 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6533223785722353) - present_state_Q (0.6614194116088219)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.9574292586065213 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6647683121975556) - present_state_Q ( 0.6521256125116114)) * f1( 0.373666589183367)
w2 ( 1.0129363272881384 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6647683121975556) - present_state_Q (0.6521256125116114)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.9527745218465862 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6769984843496799) - present_state_Q ( 0.6847199241444508)) * f1( 0.39777249601949366)
w2 ( 1.009425725016854 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6769984843496799) - present_state_Q (0.6847199241444508)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.8486487053798544 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.674474819192652) - present_state_Q ( 0.6822399507128045)) * f1( 0.3982182819838674)
w2 ( 0.9309819509530477 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.674474819192652) - present_state_Q (0.6822399507128045)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.7482324011943726 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6543185004226829) - present_state_Q ( 0.6137093655707995)) * f1( 0.394055606477596)
w2 ( 0.8545336254871918 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6543185004226829) - present_state_Q (0.6137093655707995)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.6494142288756762 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.591105368026141) - present_state_Q ( 0.591105368026141)) * f1( 0.3902779385649252)
w2 ( 0.7659138063943683 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.591105368026141) - present_state_Q (0.591105368026141)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.5559479191806754 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5234167414782294) - present_state_Q ( 0.5145861768563792)) * f1( 0.3795980033346995)
w2 ( 0.6797352487995689 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5234167414782294) - present_state_Q (0.5145861768563792)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.46424311015524955 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45314293887191504) - present_state_Q ( 0.4499292174139056)) * f1( 0.38137004028456895)
w2 ( 0.5955737264761339 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.45314293887191504) - present_state_Q (0.4499292174139056)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.374245701558799 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4206698344591885) - present_state_Q ( 0.4143496371799237)) * f1( 0.37937051242520964)
w2 ( 0.5006824203267737 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.4206698344591885) - present_state_Q (0.4143496371799237)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.28524434816852406 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.34711495856916846) - present_state_Q ( 0.3444795432327761)) * f1( 0.3853259356124088)
w2 ( 0.4082916984317393 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.34711495856916846) - present_state_Q (0.3444795432327761)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.2001781576310866 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27525177525670863) - present_state_Q ( 0.27145147037860134)) * f1( 0.3790952974185449)
w2 ( 0.31853464671762205 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.27525177525670863) - present_state_Q (0.27145147037860134)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.12969273146210442 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2046259893201111) - present_state_Q ( 0.19237791437568663)) * f1( 0.32453118990315477)
w2 ( 0.231658034099875 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2046259893201111) - present_state_Q (0.19237791437568663)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04938597602346774 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1422662149076956) - present_state_Q ( 0.14162077264262377)) * f1( 0.37748884190151344)
w2 ( 0.14656226805380082 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1422662149076956) - present_state_Q (0.14162077264262377)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0319553127052922 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07802915808474757) - present_state_Q ( 0.07802915808474757)) * f1( 0.3929101422235034)
w2 ( 0.0637532183627499 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07802915808474757) - present_state_Q (0.07802915808474757)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08460272564764855 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017153900237172438) - present_state_Q ( 0.017153900237172438)) * f1( 0.2612206359834836)
w2 ( -0.01686432204578832 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.017153900237172438) - present_state_Q (0.017153900237172438)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12476547998322049 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02224220875584143) - present_state_Q ( -0.02392145410863221)) * f1( 0.20301621678065007)
w2 ( -0.0959964327164664 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.02224220875584143) - present_state_Q (-0.02392145410863221)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.14966869549410156 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04469201220136139) - present_state_Q ( -0.0494918338371847)) * f1( 0.12738365121954323)
w2 ( -0.16442064057486971 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.04469201220136139) - present_state_Q (-0.0494918338371847)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.16143672723566516 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03360484461413192) - present_state_Q ( -0.03360484461413192)) * f1( 0.05974361237252752)
w2 ( -0.19396697517257894 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03360484461413192) - present_state_Q (-0.03360484461413192)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.16215842384957854 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04136490733095302) - present_state_Q ( -0.05106325608958197)) * f1( 0.015928917418422023)
w2 ( -0.20529380603866676 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04136490733095302) - present_state_Q (-0.05106325608958197)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.9660814976087172 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6920790686379912) - present_state_Q ( 0.6420790686379912)) * f1( 0.5920790686379912)
w2 ( 0.997135644191129 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.6920790686379912) - present_state_Q (0.6420790686379912)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.9541682091252476 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8177647113810685) - present_state_Q ( 0.7679079291715121)) * f1( 0.6400470188833718)
w2 ( 0.9943436723206279 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.8177647113810685) - present_state_Q (0.7679079291715121)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.7658430715216343 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8489733553032682) - present_state_Q ( 0.8489733553032682)) * f1( 0.6813312523115173)
w2 ( 0.9390621519251691 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.8489733553032682) - present_state_Q (0.8489733553032682)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.5875022839931938 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7165299077257787) - present_state_Q ( 0.7062298588315095)) * f1( 0.6769238342999501)
w2 ( 0.8863706145639905 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7165299077257787) - present_state_Q (0.7062298588315095)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.4826995611515626 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5814686827974325) - present_state_Q ( 0.5814686827974325)) * f1( 0.687988065573745)
w2 ( 0.8559041782736366 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.5814686827974325) - present_state_Q (0.5814686827974325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.42478759101482355 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4989541495426608) - present_state_Q ( 0.47375890055432524)) * f1( 0.626845535508145)
w2 ( 0.8374269085616355 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.4989541495426608) - present_state_Q (0.47375890055432524)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.29450851356225344 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45992015323574775) - present_state_Q ( 0.4570657130994302)) * f1( 0.681706192723972)
w2 ( 0.7992054346061184 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.45992015323574775) - present_state_Q (0.4570657130994302)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1375322913782477 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36098601362355365) - present_state_Q ( 0.3588738352696769)) * f1( 0.6758132250271317)
w2 ( 0.752749929927972 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.36098601362355365) - present_state_Q (0.3588738352696769)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18867641431136437 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2825092670649145) - present_state_Q ( 0.2825092670649145)) * f1( 0.6858155538433761)
w2 ( 0.7713934714190114 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2825092670649145) - present_state_Q (0.2825092670649145)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.8747193996852102 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6072234095781323) - present_state_Q ( 0.5944514405140335)) * f1( 0.4944514405140335)
w2 ( 0.9746627090044377 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6072234095781323) - present_state_Q (0.5944514405140335)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7289361890156757 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6173257746973532) - present_state_Q ( 0.5998779982801912)) * f1( 0.5743690234383196)
w2 ( 0.9492812547963332 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6173257746973532) - present_state_Q (0.5998779982801912)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.5589694725800772 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.585820971685759) - present_state_Q ( 0.5852726591621421)) * f1( 0.672685128096945)
w2 ( 0.9240143491763975 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.585820971685759) - present_state_Q (0.5852726591621421)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.39452942911846944 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47153689558381756) - present_state_Q ( 0.47153689558381756)) * f1( 0.6782757901181506)
w2 ( 0.8997705171161431 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.47153689558381756) - present_state_Q (0.47153689558381756)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.23657392263412694 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3583034946993171) - present_state_Q ( 0.3583034946993171)) * f1( 0.6801176875125572)
w2 ( 0.8765457856638492 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.3583034946993171) - present_state_Q (0.3583034946993171)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.08565634643018102 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24823300428384015) - present_state_Q ( 0.24823300428384015)) * f1( 0.6787663827420133)
w2 ( 0.8543116886252946 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.24823300428384015) - present_state_Q (0.24823300428384015)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.05955462110263213 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14384154603422317) - present_state_Q ( 0.14384154603422317)) * f1( 0.6819153466848408)
w2 ( 0.8330171147109866 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.14384154603422317) - present_state_Q (0.14384154603422317)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.16310377291849532 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05318831532600135) - present_state_Q ( 0.05318831532600135)) * f1( 0.5056433167999183)
w2 ( 0.8125384198730526 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05318831532600135) - present_state_Q (0.05318831532600135)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2537186725481468 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04808021417373956) - present_state_Q ( 0.007453293180086917)) * f1( 0.4524760371061267)
w2 ( 0.7925119671554255 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04808021417373956) - present_state_Q (0.007453293180086917)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3337719396121515 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018199715748382042) - present_state_Q ( 0.018143931420927628)) * f1( 0.3970258185600064)
w2 ( 0.7622671077577341 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.018199715748382042) - present_state_Q (0.018143931420927628)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3694756696877138 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004726458323005389) - present_state_Q ( -0.005421669942956786)) * f1( 0.358813075316583)
w2 ( 0.7473413431193939 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.004726458323005389) - present_state_Q (-0.005421669942956786)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.35565381524785206 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0083969790316972) - present_state_Q ( 0.0083969790316972)) * f1( 0.2806794356008996)
w2 ( 0.754727983902466 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0083969790316972) - present_state_Q (0.0083969790316972)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.37720222422354704 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03570784216154595) - present_state_Q ( 0.039208681738449686)) * f1( 0.20806894984480884)
w2 ( 0.7391934154396316 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.03570784216154595) - present_state_Q (0.039208681738449686)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3620014184966761 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05507662163540338) - present_state_Q ( 0.05081982336294351)) * f1( 0.15922278580575777)
w2 ( 0.7535137330216405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05507662163540338) - present_state_Q (0.05081982336294351)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3404968048444057 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017031907790828055) - present_state_Q ( -0.0023035268988471636)) * f1( 0.21451545831918956)
w2 ( 0.7635384714814081 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0017031907790828055) - present_state_Q (-0.0023035268988471636)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3271834424098817 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029774090539711667) - present_state_Q ( 0.029774090539711667)) * f1( 0.13679939413738212)
w2 ( 0.7732705046665507 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.029774090539711667) - present_state_Q (0.029774090539711667)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.318326420607304 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012065224463127314) - present_state_Q ( 0.009444118730231897)) * f1( 0.08930588384265116)
w2 ( 0.7782293166851311 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012065224463127314) - present_state_Q (0.009444118730231897)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3530369857861861 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0023065383673796236) - present_state_Q ( -0.036604927466876935)) * f1( 0.2372294236748025)
w2 ( 0.7709134945916492 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0023065383673796236) - present_state_Q (-0.036604927466876935)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.32838288120090675 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06689213268316829) - present_state_Q ( 0.062044263497140595)) * f1( 0.2609880526143856)
w2 ( 0.7898063935870727 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06689213268316829) - present_state_Q (0.062044263497140595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3133016940341806 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1365280563851911) - present_state_Q ( 0.1407187483881533)) * f1( 0.17276433473371394)
w2 ( 0.8116297450183318 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1365280563851911) - present_state_Q (0.1407187483881533)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.30256143909708777 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19794248272529782) - present_state_Q ( 0.16360603037433671)) * f1( 0.1254426855284049)
w2 ( 0.8330344504657867 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.19794248272529782) - present_state_Q (0.16360603037433671)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.2956813486578522 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2242560413283509) - present_state_Q ( 0.22384350898545025)) * f1( 0.0861538279037645)
w2 ( 0.8569919133202083 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2242560413283509) - present_state_Q (0.22384350898545025)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.28168518013964816 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08378940124383233) - present_state_Q ( 0.08378940124383233)) * f1( 0.15137710226691461)
w2 ( 0.8708607564034165 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08378940124383233) - present_state_Q (0.08378940124383233)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.26860818282096544 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0905258041128699) - present_state_Q ( 0.0905258041128699)) * f1( 0.14236925537850803)
w2 ( 0.8846386580478928 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0905258041128699) - present_state_Q (0.0905258041128699)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.25171101862993484 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08361513534656417) - present_state_Q ( 0.08361513534656417)) * f1( 0.18272214511548715)
w2 ( 0.8985098537207141 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08361513534656417) - present_state_Q (0.08361513534656417)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23093816716642682 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03582155299709126) - present_state_Q ( 0.03582155299709126)) * f1( 0.21464865808840158)
w2 ( 0.9081874597437403 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03582155299709126) - present_state_Q (0.03582155299709126)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.21409901954188024 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050094673884263294) - present_state_Q ( 0.050094673884263294)) * f1( 0.17634188661748026)
w2 ( 0.9177366076787818 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.050094673884263294) - present_state_Q (0.050094673884263294)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.19748318962889166 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05436904390459628) - present_state_Q ( 0.05436904390459628)) * f1( 0.17470709087467415)
w2 ( 0.9272472862836405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05436904390459628) - present_state_Q (0.05436904390459628)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.18030671542846488 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012069212984812443) - present_state_Q ( 0.012069212984812443)) * f1( 0.17365098970607531)
w2 ( 0.9321929748252088 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012069212984812443) - present_state_Q (0.012069212984812443)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.1650879050672553 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018699397259612595) - present_state_Q ( 0.018699397259612595)) * f1( 0.1547931890130903)
w2 ( 0.9371088275375405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.018699397259612595) - present_state_Q (0.018699397259612595)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.13155234770233779 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03646893604316679) - present_state_Q ( 0.03646893604316679)) * f1( 0.3467361626962764)
w2 ( 0.946780607113152 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03646893604316679) - present_state_Q (0.03646893604316679)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.10020816307174728 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05077260250038842) - present_state_Q ( 0.006064280588476335)) * f1( 0.3137515254427328)
w2 ( 0.9517756720114599 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05077260250038842) - present_state_Q (0.006064280588476335)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.07561759080902164 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06792415479724122) - present_state_Q ( 0.06890391034492273)) * f1( 0.2621907841720618)
w2 ( 0.9611545570628078 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06792415479724122) - present_state_Q (0.06890391034492273)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.05502131410056246 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08020430218239014) - present_state_Q ( 0.07934489626556003)) * f1( 0.22178119219741038)
w2 ( 0.9704413124023347 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08020430218239014) - present_state_Q (0.07934489626556003)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0401280420243792 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08748440193392619) - present_state_Q ( 0.04006269267089531)) * f1( 0.15374719938095685)
w2 ( 0.9752847411399471 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08748440193392619) - present_state_Q (0.04006269267089531)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.029734614229937166 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09300670306791439) - present_state_Q ( 0.09297693961430939)) * f1( 0.11342528242270379)
w2 ( 0.9844479784468719 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09300670306791439) - present_state_Q (0.09297693961430939)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.024341688936104264 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09668839053669422) - present_state_Q ( 0.09668839053669422)) * f1( 0.059069449982122275)
w2 ( 0.9935777829320417 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09668839053669422) - present_state_Q (0.09668839053669422)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.02211672369520397 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0491122489518458) - present_state_Q ( 0.0491122489518458)) * f1( 0.02327858992215736)
w2 ( 0.9983567778117584 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0491122489518458) - present_state_Q (0.0491122489518458)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11346439512721133 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09007064114807727) - present_state_Q ( 0.09012787418087526)) * f1( 0.4389349767210652)
w2 ( 0.9775455697110976 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09007064114807727) - present_state_Q (0.09012787418087526)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.21568692521089142 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28926165578352475) - present_state_Q ( 0.2908635912696099)) * f1( 0.45192465946506216)
w2 ( 0.8983777598119036 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.28926165578352475) - present_state_Q (0.2908635912696099)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.3088292923274112 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2740705245195208) - present_state_Q ( 0.2697595207353185)) * f1( 0.41537790527563645)
w2 ( 0.808683661080569 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2740705245195208) - present_state_Q (0.2697595207353185)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.38531113626504726 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21059956495229676) - present_state_Q ( 0.17329360504284913)) * f1( 0.35536032061039435)
w2 ( 0.7333554833814023 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.21059956495229676) - present_state_Q (0.17329360504284913)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.4577959189847589 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17089305978900823) - present_state_Q ( 0.16320377633204977)) * f1( 0.337748911910482)
w2 ( 0.6475109045672763 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.17089305978900823) - present_state_Q (0.16320377633204977)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.435482101068207 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13644208822716225) - present_state_Q ( 0.11318478454402843)) * f1( 0.24780481290899217)
w2 ( 0.6790269844170305 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.13644208822716225) - present_state_Q (0.11318478454402843)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.41636213446286774 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17350714366859665) - present_state_Q ( 0.17299794463407164)) * f1( 0.2264452405525971)
w2 ( 0.7128010952063419 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.17350714366859665) - present_state_Q (0.17299794463407164)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4038890643852816 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22034110200595444) - present_state_Q ( 0.22034110200595444)) * f1( 0.15558411948327527)
w2 ( 0.7448688155341275 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.22034110200595444) - present_state_Q (0.22034110200595444)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3939350605235648 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24214267308834547) - present_state_Q ( 0.24626891782232968)) * f1( 0.1279524823727923)
w2 ( 0.7759866295135878 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.24214267308834547) - present_state_Q (0.24626891782232968)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.38053653709667923 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013355265268259336) - present_state_Q ( -0.013355265268259336)) * f1( 0.1323938942490215)
w2 ( 0.7810467282072949 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.013355265268259336) - present_state_Q (-0.013355265268259336)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.37209515156889295 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006733913717047499) - present_state_Q ( 0.006733913717047499)) * f1( 0.08492856675443604)
w2 ( 0.7860164255955682 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.006733913717047499) - present_state_Q (0.006733913717047499)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3624199909036276 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010585022834878138) - present_state_Q ( 0.00322196236671847)) * f1( 0.09696137872514038)
w2 ( 0.791005608295152 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.010585022834878138) - present_state_Q (0.00322196236671847)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.40204751738678507 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017670843849593623) - present_state_Q ( -0.017670843849593623)) * f1( 0.2670145331603456)
w2 ( 0.7761646458897983 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.017670843849593623) - present_state_Q (-0.017670843849593623)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.39092294915997194 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05256220429585326) - present_state_Q ( 0.055735692469236)) * f1( 0.2474763116445351)
w2 ( 0.7851550564490053 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05256220429585326) - present_state_Q (0.055735692469236)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37336576046006614 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07912318136144493) - present_state_Q ( 0.08283696871960174)) * f1( 0.18979198517157903)
w2 ( 0.8036565634373362 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07912318136144493) - present_state_Q (0.08283696871960174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3609008798008049 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1091232401391489) - present_state_Q ( 0.1091232401391489)) * f1( 0.1382239027079671)
w2 ( 0.8216923451148315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1091232401391489) - present_state_Q (0.1091232401391489)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.34894357141615806 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11614686447632255) - present_state_Q ( 0.11614686447632255)) * f1( 0.13353141331559668)
w2 ( 0.8396017015542576 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11614686447632255) - present_state_Q (0.11614686447632255)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.34086019176386056 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13578694018858545) - present_state_Q ( 0.13578694018858545)) * f1( 0.09208766905163314)
w2 ( 0.857157536630863 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.13578694018858545) - present_state_Q (0.13578694018858545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.33231358597510235 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1795427518994555) - present_state_Q ( 0.1795427518994555)) * f1( 0.1019380763663122)
w2 ( 0.8781178247131253 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1795427518994555) - present_state_Q (0.1795427518994555)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.31379308701457265 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024857259028484846) - present_state_Q ( 0.024857259028484846)) * f1( 0.18944312270020877)
w2 ( 0.8878941093818689 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.024857259028484846) - present_state_Q (0.024857259028484846)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2802390159858719 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023620291181862166) - present_state_Q ( -0.014731416703166383)) * f1( 0.3299015559146009)
w2 ( 0.8980650438400823 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023620291181862166) - present_state_Q (-0.014731416703166383)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.25227812458060744 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052468852365730775) - present_state_Q ( 0.052468852365730775)) * f1( 0.2934670032327965)
w2 ( 0.912356714333145 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.052468852365730775) - present_state_Q (0.052468852365730775)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23281214316616466 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08207765271816655) - present_state_Q ( 0.08373315909963341)) * f1( 0.2105626404930937)
w2 ( 0.9262238334257278 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08207765271816655) - present_state_Q (0.08373315909963341)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.21855539462725013 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09959263393965173) - present_state_Q ( 0.10236286804610373)) * f1( 0.15708247203262898)
w2 ( 0.9398377793559457 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09959263393965173) - present_state_Q (0.10236286804610373)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2071141008953357 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1131353506930687) - present_state_Q ( 0.1131353506930687)) * f1( 0.1273833403096973)
w2 ( 0.9533104521215893 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1131353506930687) - present_state_Q (0.1131353506930687)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.18537861070792205 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002544912331481193) - present_state_Q ( 0.002544912331481193)) * f1( 0.21785387899494008)
w2 ( 0.9582990000160976 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.002544912331481193) - present_state_Q (0.002544912331481193)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.1643503717079205 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012345399595624161) - present_state_Q ( 0.008642174323583482)) * f1( 0.21185171000714106)
w2 ( 0.9632619618442775 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012345399595624161) - present_state_Q (0.008642174323583482)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.14723333011143838 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02125395087970922) - present_state_Q ( 0.019532797440082572)) * f1( 0.17420283480108206)
w2 ( 0.9681749248325169 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02125395087970922) - present_state_Q (0.019532797440082572)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.1344040847901155 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029013368408632334) - present_state_Q ( 0.029013368408632334)) * f1( 0.13173224988060436)
w2 ( 0.973044364674678 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.029013368408632334) - present_state_Q (0.029013368408632334)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.12321950383877106 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03315729175021384) - present_state_Q ( 0.03315729175021384)) * f1( 0.11528612770747879)
w2 ( 0.9778951568618021 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03315729175021384) - present_state_Q (0.03315729175021384)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.112909647481441 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03576844701507835) - present_state_Q ( 0.03576844701507835)) * f1( 0.10652786627989619)
w2 ( 0.9827341988502342 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03576844701507835) - present_state_Q (0.03576844701507835)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11045665371918022 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002762796867830775) - present_state_Q ( -0.002762796867830775)) * f1( 0.024469094797987898)
w2 ( 0.9827341988502342 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.002762796867830775) - present_state_Q (-0.002762796867830775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09946393693481229 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08389083579814295) - present_state_Q ( 0.08512209180654566)) * f1( 0.11906324911773153)
w2 ( 0.9919668687679668 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08389083579814295) - present_state_Q (0.08512209180654566)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.08862136254312158 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3320560699787723) - present_state_Q ( 0.3318126419880967)) * f1( 0.15458630086972008)
w2 ( 1.0165156225433092 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3320560699787723) - present_state_Q (0.3318126419880967)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.08036751660142324 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3451694612552108) - present_state_Q ( 0.3451694612552108)) * f1( 0.11973418519473045)
w2 ( 1.04064278451377 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3451694612552108) - present_state_Q (0.3451694612552108)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.05242078393733893 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028970749039497708) - present_state_Q ( 0.028970749039497708)) * f1( 0.2869491451448233)
w2 ( 1.0455124161430922 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.028970749039497708) - present_state_Q (0.028970749039497708)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.03515709317179636 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04286274319098415) - present_state_Q ( 0.04286274319098415)) * f1( 0.17956384680210305)
w2 ( 1.0503195337987328 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04286274319098415) - present_state_Q (0.04286274319098415)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.017947378875427484 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.046203031930703416) - present_state_Q ( 0.046203031930703416)) * f1( 0.17956389990448865)
w2 ( 1.0551116201550446 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.046203031930703416) - present_state_Q (0.046203031930703416)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.0031008538639416706 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10278653218891594) - present_state_Q ( 0.10257566205616943)) * f1( 0.16356148603705883)
w2 ( 1.0641886500666717 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10278653218891594) - present_state_Q (0.10257566205616943)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.007460901938847908 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10605680097622032) - present_state_Q ( 0.10605680097622032)) * f1( 0.1167626874188195)
w2 ( 1.0732341388578857 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10605680097622032) - present_state_Q (0.10605680097622032)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.02520594969524165 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10879105400146631) - present_state_Q ( 0.10879105400146631)) * f1( 0.1967108169638228)
w2 ( 1.0822550193718725 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10879105400146631) - present_state_Q (0.10879105400146631)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.006545307154754796 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12136218080334354) - present_state_Q ( 0.12136218080334354)) * f1( 0.5211737317969897)
w2 ( 1.0761627597446424 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12136218080334354) - present_state_Q (0.12136218080334354)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.09725837242484703 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1047812284899677) - present_state_Q ( 0.1047812284899677)) * f1( 0.43314200807780917)
w2 ( 1.0552197286882325 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1047812284899677) - present_state_Q (0.1047812284899677)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0996298320978842 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06762729584235366) - present_state_Q ( 0.06762729584235366)) * f1( 0.3896289448576919)
w2 ( 1.0546110830256514 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.06762729584235366) - present_state_Q (0.06762729584235366)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0675570522878623 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07271364813197223) - present_state_Q ( 0.07132037134429964)) * f1( 0.3426758455712639)
w2 ( 1.0639705929603405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07271364813197223) - present_state_Q (0.07132037134429964)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.04342756188277719 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08868197932619344) - present_state_Q ( 0.08868197932619344)) * f1( 0.26222399246130823)
w2 ( 1.0731724551464048 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08868197932619344) - present_state_Q (0.08868197932619344)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.024609829417663126 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09835155223782054) - present_state_Q ( 0.09835155223782054)) * f1( 0.20645168386428875)
w2 ( 1.0822872911762644 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09835155223782054) - present_state_Q (0.09835155223782054)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.011740446916466507 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10456775371149733) - present_state_Q ( 0.10473193523464785)) * f1( 0.14208931820018397)
w2 ( 1.0913445395776296 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10456775371149733) - present_state_Q (0.10473193523464785)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.0014893705014063108 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052936284873477235) - present_state_Q ( 0.052936284873477235)) * f1( 0.1389165265179789)
w2 ( 1.096106326295699 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.052936284873477235) - present_state_Q (0.052936284873477235)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.0237469883295433 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05515412933986352) - present_state_Q ( 0.05515412933986352)) * f1( 0.23420164744045593)
w2 ( 1.1008581327136695 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05515412933986352) - present_state_Q (0.05515412933986352)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.05356617154826859 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06254645276904702) - present_state_Q ( 0.06254645276904702)) * f1( 0.31597885294904876)
w2 ( 1.1055766736762087 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06254645276904702) - present_state_Q (0.06254645276904702)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.0889166395757322 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07559713047031064) - present_state_Q ( 0.07559713047031064)) * f1( 0.37931209566080965)
w2 ( 1.1102364865890924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07559713047031064) - present_state_Q (0.07559713047031064)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.10643823935570756 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09293019297141084) - present_state_Q ( 0.09293019297141084)) * f1( 0.4208252675820728)
w2 ( 1.112318300720721 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09293019297141084) - present_state_Q (0.09293019297141084)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.1251568109675934 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10471899704932566) - present_state_Q ( 0.10471899704932566)) * f1( 0.4613293334286682)
w2 ( 1.1143470652339988 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.10471899704932566) - present_state_Q (0.10471899704932566)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.0933251591921474 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12113309386011042) - present_state_Q ( 0.12113309386011042)) * f1( 0.522670241377023)
w2 ( 1.1113019663116284 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12113309386011042) - present_state_Q (0.12113309386011042)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.003955323460094293 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10779048600731966) - present_state_Q ( 0.10779048600731966)) * f1( 0.5596067356736167)
w2 ( 1.1033169091245953 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.10779048600731966) - present_state_Q (0.10779048600731966)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11486268034857006 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057456439427759236) - present_state_Q ( 0.057456439427759236)) * f1( 0.5791167257594809)
w2 ( 1.0930583551471704 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.057456439427759236) - present_state_Q (0.057456439427759236)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.18834393769838736 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012691293160100169) - present_state_Q ( 0.012691293160100169)) * f1( 0.36531991478797793)
w2 ( 1.08300124432795 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.012691293160100169) - present_state_Q (0.012691293160100169)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.24919432345387876 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032375520259167664) - present_state_Q ( -0.0032375520259167664)) * f1( 0.3046958396623011)
w2 ( 1.0730158133120666 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.0032375520259167664) - present_state_Q (-0.0032375520259167664)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.15033058663019058 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18125107897917525) - present_state_Q ( 0.17904387441336495)) * f1( 0.603310647709962)
w2 ( 0.966211229687348 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.18125107897917525) - present_state_Q (0.17904387441336495)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.1353060026315654 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.256490051733661) - present_state_Q ( 0.2547864521007204)) * f1( 0.5546940257406334)
w2 ( 0.9756914190448905 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.256490051733661) - present_state_Q (0.2547864521007204)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.09654014723227458 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2732650254031948) - present_state_Q ( 0.22715575503534466)) * f1( 0.48446978998129137)
w2 ( 0.9996965414700398 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2732650254031948) - present_state_Q (0.22715575503534466)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.06456827815312619 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3072316584902006) - present_state_Q ( 0.3072316584902006)) * f1( 0.44191077233048676)
w2 ( 1.0250187442275984 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3072316584902006) - present_state_Q (0.3072316584902006)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.03839837663222413 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3842032368479849) - present_state_Q ( 0.38417990536671454)) * f1( 0.4000043529591069)
w2 ( 1.0511883609603219 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3842032368479849) - present_state_Q (0.38417990536671454)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.014956212115648026 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.40705475057002044) - present_state_Q ( 0.4062869031554299)) * f1( 0.36950627795008945)
w2 ( 1.0765651038363848 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.40705475057002044) - present_state_Q (0.4062869031554299)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0031984079670310493 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42607986735683706) - present_state_Q ( 0.42622094650314135)) * f1( 0.29453279997304843)
w2 ( 1.1012205854456865 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.42607986735683706) - present_state_Q (0.42622094650314135)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0272738678691454 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44176648523467227) - present_state_Q ( 0.44176648523467227)) * f1( 0.3996522862542178)
w2 ( 1.1253169919772383 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.44176648523467227) - present_state_Q (0.44176648523467227)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0565356808093786 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.46205435255565935) - present_state_Q ( 0.40633348676222014)) * f1( 0.4573073254599393)
w2 ( 1.1477125101745054 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.46205435255565935) - present_state_Q (0.40633348676222014)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.08790712567135239 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4866418425549949) - present_state_Q ( 0.4303855721054042)) * f1( 0.5073998072305623)
w2 ( 1.1693522615997587 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4866418425549949) - present_state_Q (0.4303855721054042)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.11784198798751205 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5814067666438669) - present_state_Q ( 0.5814067666438669)) * f1( 0.6279155245086551)
w2 ( 1.190805287550682 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5814067666438669) - present_state_Q (0.5814067666438669)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.05432830761426208 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6153089463859922) - present_state_Q ( 0.6153089463859922)) * f1( 0.67417877400884)
w2 ( 1.0758852752220494 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6153089463859922) - present_state_Q (0.6153089463859922)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.026423484219906687 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4583433509605738) - present_state_Q ( 0.4583433509605738)) * f1( 0.4749830065123205)
w2 ( 1.102322369508146 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4583433509605738) - present_state_Q (0.4583433509605738)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.0043281793519649135 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.48538581940990216) - present_state_Q ( 0.4856725285148704)) * f1( 0.392549963414021)
w2 ( 1.1276513419123215 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.48538581940990216) - present_state_Q (0.4856725285148704)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.01718325943637685 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5058537334613544) - present_state_Q ( 0.4495115528959419)) * f1( 0.3578834754810995)
w2 ( 1.1516942947303292 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5058537334613544) - present_state_Q (0.4495115528959419)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04115032574878218 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5260841484480011) - present_state_Q ( 0.5260841484480011)) * f1( 0.4551939548089752)
w2 ( 1.1753878867181853 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5260841484480011) - present_state_Q (0.5260841484480011)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.06417182440395922 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6086603783187166) - present_state_Q ( 0.6086424798092502)) * f1( 0.5090734051060966)
w2 ( 1.1979990646193164 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6086603783187166) - present_state_Q (0.6086424798092502)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( 0.09196687185828359 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6342847980199973) - present_state_Q ( 0.5756679080462898)) * f1( 0.5698502311139089)
w2 ( 1.2199482903483234 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6342847980199973) - present_state_Q (0.5756679080462898)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.08601877317409606 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6662922102567991) - present_state_Q ( 0.6653729222702521)) * f1( 0.602377529828971)
w2 ( 1.2150111052860948 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6662922102567991) - present_state_Q (0.6653729222702521)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.084143131336924 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6634562319781447) - present_state_Q ( 0.6044250533013369)) * f1( 0.6704356943788774)
w2 ( 1.1007975309314362 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6634562319781447) - present_state_Q (0.6044250533013369)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.22666789344336236 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5014764015396871) - present_state_Q ( 0.5014764015396871)) * f1( 0.5814183896976355)
w2 ( 0.9782310928621503 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5014764015396871) - present_state_Q (0.5014764015396871)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.34693563292849 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4165218392867717) - present_state_Q ( 0.3721407834392545)) * f1( 0.5160623377878137)
w2 ( 0.8617066628866215 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.4165218392867717) - present_state_Q (0.3721407834392545)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.3841909933046 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3076173522260123) - present_state_Q ( 0.30754449818241525)) * f1( 0.47961105926390557)
w2 ( 0.8189836109238317 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.3076173522260123) - present_state_Q (0.30754449818241525)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.3522372561979783 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28545862306868475) - present_state_Q ( 0.2852746530922976)) * f1( 0.42990683226366055)
w2 ( 0.8598635274306331 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.28545862306868475) - present_state_Q (0.2852746530922976)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.32544452309230704 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3370601947631088) - present_state_Q ( 0.3011103753904808)) * f1( 0.3657233471419913)
w2 ( 0.8964933096349246 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3370601947631088) - present_state_Q (0.3011103753904808)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.3041826996074795 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38698475546291555) - present_state_Q ( 0.3869090567709938)) * f1( 0.32620694464138694)
w2 ( 0.932341727667566 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.38698475546291555) - present_state_Q (0.3869090567709938)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.2868715199059174 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43097246234000514) - present_state_Q ( 0.4272795104752696)) * f1( 0.2811088199698165)
w2 ( 0.9662117031342962 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.43097246234000514) - present_state_Q (0.4272795104752696)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.27205004621597684 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4617355398458266) - present_state_Q ( 0.45900368362902005)) * f1( 0.25242224504750926)
w2 ( 0.9985060460038521 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4617355398458266) - present_state_Q (0.45900368362902005)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.2609171817177372 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4973287365207921) - present_state_Q ( 0.49461846758355604)) * f1( 0.20055081216655363)
w2 ( 1.0290373383376208 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4973287365207921) - present_state_Q (0.49461846758355604)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.25354977351856106 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5265954282499442) - present_state_Q ( 0.5292447247856976)) * f1( 0.14075658436217608)
w2 ( 1.0578251533297822 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5265954282499442) - present_state_Q (0.5292447247856976)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.24684248405280682 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.299288501872338) - present_state_Q ( 0.2942316272230893)) * f1( 0.09116915568513895)
w2 ( 1.0798960700187066 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.299288501872338) - present_state_Q (0.2942316272230893)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.2216978430517103 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0583430652987688) - present_state_Q ( -0.0583430652987688)) * f1( 0.4550994097745013)
w2 ( 1.0826586138125511 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.0583430652987688) - present_state_Q (-0.0583430652987688)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.18004248817255555 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035367254362729784) - present_state_Q ( -0.035367254362729784)) * f1( 0.4037034543113787)
w2 ( 1.0878177664571833 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.035367254362729784) - present_state_Q (-0.035367254362729784)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.14462291853531697 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0458383504874446) - present_state_Q ( -0.00855253783541457)) * f1( 0.3496031786560724)
w2 ( 1.0928834483216041 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0458383504874446) - present_state_Q (-0.00855253783541457)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11553132503542224 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06490009853476925) - present_state_Q ( 0.06461864457481115)) * f1( 0.3088701342065705)
w2 ( 1.1023021619743907 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06490009853476925) - present_state_Q (0.06461864457481115)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.09254809054278834 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13511651911063297) - present_state_Q ( 0.13511651911063297)) * f1( 0.2616502942059863)
w2 ( 1.115478088966397 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.13511651911063297) - present_state_Q (0.13511651911063297)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07583695133887913 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2041485653327174) - present_state_Q ( 0.2041485653327174)) * f1( 0.204726562692313)
w2 ( 1.1318034147904081 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2041485653327174) - present_state_Q (0.2041485653327174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06390162166367075 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21431933521026997) - present_state_Q ( 0.21513480867550042)) * f1( 0.14802644468681445)
w2 ( 1.1479293572873186 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.21431933521026997) - present_state_Q (0.21513480867550042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05680525181752139 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28007114868680655) - present_state_Q ( 0.2239461308148095)) * f1( 0.08825661220833352)
w2 ( 1.164010576968396 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.28007114868680655) - present_state_Q (0.2239461308148095)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0386081550243214 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04740296945113372) - present_state_Q ( 0.04740296945113372)) * f1( 0.19008030158851635)
w2 ( 1.1687972636058659 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04740296945113372) - present_state_Q (0.04740296945113372)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.02429737709952009 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05263995984300844) - present_state_Q ( 0.05263995984300844)) * f1( 0.15022482513425417)
w2 ( 1.1735603837865725 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05263995984300844) - present_state_Q (0.05263995984300844)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.005990740103305737 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05400275723125524) - present_state_Q ( 0.05400275723125524)) * f1( 0.19241838075459286)
w2 ( 1.1783173713790318 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05400275723125524) - present_state_Q (0.05400275723125524)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.007547667704028522 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.058060100353465136) - present_state_Q ( 0.058060100353465136)) * f1( 0.1428484963008549)
w2 ( 1.1830561009274412 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.058060100353465136) - present_state_Q (0.058060100353465136)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.029549098466033555 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.060909711318907156) - present_state_Q ( 0.060909711318907156)) * f1( 0.23277472477986189)
w2 ( 1.1877820072265062 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.060909711318907156) - present_state_Q (0.060909711318907156)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.06089406362644912 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06952861369028022) - present_state_Q ( 0.06926677127620282)) * f1( 0.3342799417800109)
w2 ( 1.1924704376769704 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06952861369028022) - present_state_Q (0.06926677127620282)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.09424583124360103 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14375931669259576) - present_state_Q ( 0.1425418978400166)) * f1( 0.38254720879231213)
w2 ( 1.2011887780152628 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14375931669259576) - present_state_Q (0.1425418978400166)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.12277944278389796 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16251602861091416) - present_state_Q ( 0.15120595325496303)) * f1( 0.3298509339164796)
w2 ( 1.209839234511324 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.16251602861091416) - present_state_Q (0.15120595325496303)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.1656606846635463 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.184088393824553) - present_state_Q ( 0.184088393824553)) * f1( 0.5139660919009849)
w2 ( 1.2181824389669031 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.184088393824553) - present_state_Q (0.184088393824553)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.21326467649796452 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21847106887847567) - present_state_Q ( 0.2201909688866086)) * f1( 0.5938205868804142)
w2 ( 1.2261990003469156 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.21847106887847567) - present_state_Q (0.2201909688866086)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.2629762249926974 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2592146657083734) - present_state_Q ( 0.26126707797649523)) * f1( 0.6501178733325159)
w2 ( 1.233845544232859 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2592146657083734) - present_state_Q (0.26126707797649523)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.3124598115290836 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3021098099853737) - present_state_Q ( 0.3021098099853737)) * f1( 0.6796251469768828)
w2 ( 1.2411265559429907 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3021098099853737) - present_state_Q (0.3021098099853737)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.35985648647416335 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3365587491633768) - present_state_Q ( 0.3365587491633768)) * f1( 0.6799149385946018)
w2 ( 1.2480975272005204 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3365587491633768) - present_state_Q (0.3365587491633768)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.405311411801603 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3700685739095104) - present_state_Q ( 0.3700685739095104)) * f1( 0.6815462007993213)
w2 ( 1.2547669100353347 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3700685739095104) - present_state_Q (0.3700685739095104)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.44874170906174526 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4008492176753828) - present_state_Q ( 0.4008492176753828)) * f1( 0.6794097542129954)
w2 ( 1.2611592670762561 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4008492176753828) - present_state_Q (0.4008492176753828)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.4902938093619145 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4305329054183778) - present_state_Q ( 0.4305329054183778)) * f1( 0.6783790598543749)
w2 ( 1.2672844709274906 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4305329054183778) - present_state_Q (0.4305329054183778)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.530168141783263 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4607156451897517) - present_state_Q ( 0.4607156451897517)) * f1( 0.6811980729099257)
w2 ( 1.2731380301207829 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4607156451897517) - present_state_Q (0.4607156451897517)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.5683455444595696 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.48855841464394906) - present_state_Q ( 0.48855841464394906)) * f1( 0.6813774407809484)
w2 ( 1.2787410043889873 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.48855841464394906) - present_state_Q (0.48855841464394906)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.6048850868692236 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5148821216029891) - present_state_Q ( 0.5148821216029891)) * f1( 0.6809378993761442)
w2 ( 1.2841070652945603 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5148821216029891) - present_state_Q (0.5148821216029891)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.6397548340167662 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.540267417970296) - present_state_Q ( 0.5340403990599025)) * f1( 0.6705896728747485)
w2 ( 1.2893069287219316 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.540267417970296) - present_state_Q (0.5340403990599025)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.6731897395801931 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5648231733319724) - present_state_Q ( 0.5582597430038013)) * f1( 0.6710837141097031)
w2 ( 1.2942891544652255 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5648231733319724) - present_state_Q (0.5582597430038013)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7052244104297342 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5824297049209678) - present_state_Q ( 0.5893896023217917)) * f1( 0.6832556407679425)
w2 ( 1.2989776881469286 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5824297049209678) - present_state_Q (0.5893896023217917)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7017327457589309 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6042546945439483) - present_state_Q ( 0.6115496760053107)) * f1( 0.6829767944321714)
w2 ( 1.2984664460814195 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6042546945439483) - present_state_Q (0.6115496760053107)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7325864885356955 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6081332480428017) - present_state_Q ( 0.6081332480428017)) * f1( 0.6815794279592697)
w2 ( 1.3029932468490342 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6081332480428017) - present_state_Q (0.6081332480428017)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7620657843309995 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6239623011166842) - present_state_Q ( 0.6315588887790706)) * f1( 0.6842326086249448)
w2 ( 1.3073016202623602 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6239623011166842) - present_state_Q (0.6315588887790706)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7903418650188768 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6502784885713029) - present_state_Q ( 0.6502784885713029)) * f1( 0.681763093459401)
w2 ( 1.3114491138652185 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6502784885713029) - present_state_Q (0.6502784885713029)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8174176060461656 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6692502829791713) - present_state_Q ( 0.6692502829791713)) * f1( 0.6808514079913977)
w2 ( 1.315425861318406 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6692502829791713) - present_state_Q (0.6692502829791713)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8433349291479248 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6883840327971562) - present_state_Q ( 0.6883840327971562)) * f1( 0.6812202753482981)
w2 ( 1.3192304050232315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6883840327971562) - present_state_Q (0.6883840327971562)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8681411831270929 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7067115138463846) - present_state_Q ( 0.7067115138463846)) * f1( 0.6815660699893066)
w2 ( 1.322870001398614 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.7067115138463846) - present_state_Q (0.7067115138463846)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8918811347018852 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7243064388014477) - present_state_Q ( 0.7243064388014477)) * f1( 0.681939124842689)
w2 ( 1.3263512434494011 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.7243064388014477) - present_state_Q (0.7243064388014477)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8466525804119723 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.739075699624707) - present_state_Q ( 0.739075699624707)) * f1( 0.679956724819022)
w2 ( 1.3196995621527787 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.739075699624707) - present_state_Q (0.739075699624707)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7372126444302207 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7092376546546788) - present_state_Q ( 0.7005514039648569)) * f1( 0.6715640640614516)
w2 ( 1.3034032857677849 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.7092376546546788) - present_state_Q (0.7005514039648569)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.561657394085316 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6261836518616716) - present_state_Q ( 0.6337119359764827)) * f1( 0.6828038167857955)
w2 ( 1.2776923500598818 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6261836518616716) - present_state_Q (0.6337119359764827)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.3950511049157659 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5085191326171681) - present_state_Q ( 0.5085191326171681)) * f1( 0.6779041843315318)
w2 ( 1.2531156778663273 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5085191326171681) - present_state_Q (0.5085191326171681)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.23431702823189673 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39080742154915615) - present_state_Q ( 0.3948543398649905)) * f1( 0.6822984892950257)
w2 ( 1.2295579418892266 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.39080742154915615) - present_state_Q (0.3948543398649905)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.08071172131097373 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2826129570675525) - present_state_Q ( 0.2826129570675525)) * f1( 0.6813724298373307)
w2 ( 1.2070144252756188 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2826129570675525) - present_state_Q (0.2826129570675525)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.06629069903602977 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17567922957091148) - present_state_Q ( 0.17567922957091148)) * f1( 0.6811623658913928)
w2 ( 1.1854333122094804 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.17567922957091148) - present_state_Q (0.17567922957091148)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.18782727540997435 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07965357680766046) - present_state_Q ( 0.07965357680766046)) * f1( 0.5866547642249264)
w2 ( 1.1647164300182116 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07965357680766046) - present_state_Q (0.07965357680766046)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.29015064102858934 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01946535342374843) - present_state_Q ( 0.021296880374812158)) * f1( 0.5067142800174743)
w2 ( 1.1445229265678871 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01946535342374843) - present_state_Q (0.021296880374812158)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.37887549657559866 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017994750164149337) - present_state_Q ( -0.01512922017557708)) * f1( 0.44660081526270956)
w2 ( 1.1246562240194788 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.017994750164149337) - present_state_Q (-0.01512922017557708)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4563643452307167 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040550057941609666) - present_state_Q ( -0.03676849170948328)) * f1( 0.3938869508856027)
w2 ( 1.104983358878632 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.040550057941609666) - present_state_Q (-0.03676849170948328)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5234619153438603 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05054546715875077) - present_state_Q ( -0.045789551265547734)) * f1( 0.34246296580071134)
w2 ( 1.0853907089241286 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.05054546715875077) - present_state_Q (-0.045789551265547734)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.58047510859739 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04888620924162097) - present_state_Q ( -0.04362969668607651)) * f1( 0.29069692200726815)
w2 ( 1.0657781196817477 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.04888620924162097) - present_state_Q (-0.04362969668607651)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6310272409699117 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03546094090550986) - present_state_Q ( -0.04310401125799751)) * f1( 0.25786088155933257)
w2 ( 1.0461736988534223 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03546094090550986) - present_state_Q (-0.04310401125799751)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6665940183144874 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008006428478300368) - present_state_Q ( -0.008006428478300368)) * f1( 0.1784769199353989)
w2 ( 1.026245756709727 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.008006428478300368) - present_state_Q (-0.008006428478300368)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.25442637235346977 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020379686713496027) - present_state_Q ( 0.017690808187515138)) * f1( 0.14707352931051249)
w2 ( 1.0773178058211257 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.020379686713496027) - present_state_Q (0.017690808187515138)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3092927553212446 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039142648802489646) - present_state_Q ( 0.039142648802489646)) * f1( 0.26958342071683256)
w2 ( 1.0569655219819032 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.039142648802489646) - present_state_Q (0.039142648802489646)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3371099934464778 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15954752914970313) - present_state_Q ( 0.15902988586395284)) * f1( 0.16929985468958414)
w2 ( 1.0241040193229236 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15954752914970313) - present_state_Q (0.15902988586395284)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3054652155088096 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08904526049992646) - present_state_Q ( 0.09253554690992172)) * f1( 0.2813489090396091)
w2 ( 1.0486416047058076 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08904526049992646) - present_state_Q (0.09253554690992172)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4158444555903989 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1528935083298602) - present_state_Q ( 0.10046142809456976)) * f1( 0.5293531468469893)
w2 ( 0.9965123027742679 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1528935083298602) - present_state_Q (0.10046142809456976)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.5158785415048754 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.154895187509271) - present_state_Q ( 0.1542836333195946)) * f1( 0.46771255462637407)
w2 ( 0.9216545087643646 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.154895187509271) - present_state_Q (0.1542836333195946)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.5982693757861013 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12103812348220302) - present_state_Q ( 0.12103812348220302)) * f1( 0.3906752042785248)
w2 ( 0.8478418078746752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.12103812348220302) - present_state_Q (0.12103812348220302)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6691978193768098 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07662075204266916) - present_state_Q ( 0.04619109200934979)) * f1( 0.3479393376596241)
w2 ( 0.7866859373705227 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07662075204266916) - present_state_Q (0.04619109200934979)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.686190357382889 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06839729781383574) - present_state_Q ( 0.0753031025352853)) * f1( 0.2989205430027881)
w2 ( 0.7667897193241362 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06839729781383574) - present_state_Q (0.0753031025352853)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6625533736517799 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09160766456021308) - present_state_Q ( 0.09160766456021308)) * f1( 0.25760889132488785)
w2 ( 0.7989040778904895 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09160766456021308) - present_state_Q (0.09160766456021308)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6450898731890207 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14635832642908786) - present_state_Q ( 0.14635832642908786)) * f1( 0.2011280994587771)
w2 ( 0.8292937906079731 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14635832642908786) - present_state_Q (0.14635832642908786)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6324018828959925 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19821270597507204) - present_state_Q ( 0.1914462063495418)) * f1( 0.15316721664656646)
w2 ( 0.8582869178566519 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.19821270597507204) - present_state_Q (0.1914462063495418)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6077119734485709 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10029766013986369) - present_state_Q ( -0.10029766013986369)) * f1( 0.2264572733036115)
w2 ( 0.8637382573272813 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10029766013986369) - present_state_Q (-0.10029766013986369)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5484498487762718 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2442227734200389) - present_state_Q ( -0.2505395090723667)) * f1( 0.48333163533363904)
w2 ( 0.8698688434859331 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2442227734200389) - present_state_Q (-0.2505395090723667)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.49865219677935607 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18977864886652504) - present_state_Q ( -0.18977864886652504)) * f1( 0.4253298484105882)
w2 ( 0.8757228474058324 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.18977864886652504) - present_state_Q (-0.18977864886652504)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4581409116779815 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1412409105457028) - present_state_Q ( -0.13624016489886484)) * f1( 0.361025798004886)
w2 ( 0.8813334277750539 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1412409105457028) - present_state_Q (-0.13624016489886484)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4240519051371493 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10406134167857364) - present_state_Q ( -0.09935241875275011)) * f1( 0.3130458042182213)
w2 ( 0.8867781591979783 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10406134167857364) - present_state_Q (-0.09935241875275011)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.39425637660226526 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07354648781671562) - present_state_Q ( -0.07410367193096291)) * f1( 0.279311514595258)
w2 ( 0.8921119043137248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.07354648781671562) - present_state_Q (-0.07410367193096291)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3703510442174115 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042776048089180146) - present_state_Q ( -0.04587848471672902)) * f1( 0.22950568539237007)
w2 ( 0.8973199087132638 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.042776048089180146) - present_state_Q (-0.04587848471672902)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3325833053656997 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08506072487058083) - present_state_Q ( -0.08506072487058083)) * f1( 0.3508204508530334)
w2 ( 0.9027026819751814 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.08506072487058083) - present_state_Q (-0.08506072487058083)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.2993213903437489 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052428672453885924) - present_state_Q ( -0.05976850455541538)) * f1( 0.31542063886467553)
w2 ( 0.9079753101617316 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.052428672453885924) - present_state_Q (-0.05976850455541538)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.27163386679325524 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0306481671897441) - present_state_Q ( -0.034917613622527655)) * f1( 0.26832823086374386)
w2 ( 0.9131345741462493 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0306481671897441) - present_state_Q (-0.034917613622527655)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.24829768774192634 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011328964617565537) - present_state_Q ( -0.016757072587502288)) * f1( 0.22977179551148852)
w2 ( 0.9182126950268781 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.011328964617565537) - present_state_Q (-0.016757072587502288)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.23426607843794275 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010730718104932778) - present_state_Q ( 0.010730718104932778)) * f1( 0.14168443116142165)
w2 ( 0.9231644067954059 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.010730718104932778) - present_state_Q (0.010730718104932778)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.22362706989785025 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023287402124422155) - present_state_Q ( 0.020766466044129166)) * f1( 0.10838852327639668)
w2 ( 0.9280722181662475 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023287402124422155) - present_state_Q (0.020766466044129166)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.21535093107832817 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03182713554540041) - present_state_Q ( 0.027435896719092596)) * f1( 0.08481850698076926)
w2 ( 0.9329509522504248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03182713554540041) - present_state_Q (0.027435896719092596)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.2737576926783171 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08813708928347272) - present_state_Q ( -0.09028887510154646)) * f1( 0.6358756938193163)
w2 ( 0.9283583280812908 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.08813708928347272) - present_state_Q (-0.09028887510154646)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3532343602553513 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05738547397775347) - present_state_Q ( -0.05738547397775347)) * f1( 0.5487382119427864)
w2 ( 0.9138747973470907 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05738547397775347) - present_state_Q (-0.05738547397775347)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4476120740746383 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08165859136516454) - present_state_Q ( -0.08165859136516454)) * f1( 0.48989025579159257)
w2 ( 0.8946097246693772 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08165859136516454) - present_state_Q (-0.08165859136516454)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5429036799038789 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08403579480996123) - present_state_Q ( -0.08790547093065537)) * f1( 0.4961817218407469)
w2 ( 0.865802253041122 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08403579480996123) - present_state_Q (-0.08790547093065537)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6221503813262014 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09898147217932657) - present_state_Q ( -0.09478078039128007)) * f1( 0.41379553438875727)
w2 ( 0.8370754925387223 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09898147217932657) - present_state_Q (-0.09478078039128007)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6927799852713785 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061537384914196164) - present_state_Q ( -0.05816201508821048)) * f1( 0.36257650942060904)
w2 ( 0.7981156580706581 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.061537384914196164) - present_state_Q (-0.05816201508821048)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7197314700538839 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03276449597792544) - present_state_Q ( -0.03276449597792544)) * f1( 0.2777037900664729)
w2 ( 0.7787054189982607 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.03276449597792544) - present_state_Q (-0.03276449597792544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6978367750892431 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016134332275249685) - present_state_Q ( -0.0016134332275249685)) * f1( 0.21862947998563478)
w2 ( 0.7987344607963562 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0016134332275249685) - present_state_Q (-0.0016134332275249685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6815425018219838 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07745566178820591) - present_state_Q ( 0.07745566178820591)) * f1( 0.1751526399497246)
w2 ( 0.8219917084061216 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07745566178820591) - present_state_Q (0.07745566178820591)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.6721082985508091 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1602018712396986) - present_state_Q ( 0.13270603861029773)) * f1( 0.10680462083675836)
w2 ( 0.8440745621189634 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1602018712396986) - present_state_Q (0.13270603861029773)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.6667150431983851 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12785300280745707) - present_state_Q ( 0.12785300280745707)) * f1( 0.060945400770466805)
w2 ( 0.8617732080684292 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.12785300280745707) - present_state_Q (0.12785300280745707)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6537226189118733 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2427315398810318) - present_state_Q ( -0.2868300747344595)) * f1( 0.49484219458313855)
w2 ( 0.8630859926721609 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.2427315398810318) - present_state_Q (-0.2868300747344595)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.6186179373238029 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09302984524868044) - present_state_Q ( -0.08406156043535468)) * f1( 0.3266285319782755)
w2 ( 0.8792073713108182 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09302984524868044) - present_state_Q (-0.08406156043535468)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5938981489005355 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020102732204440116) - present_state_Q ( 0.020102732204440116)) * f1( 0.25175270980900327)
w2 ( 0.8988455221311382 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.020102732204440116) - present_state_Q (0.020102732204440116)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.575651009241716 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0646992402558521) - present_state_Q ( 0.0646992402558521)) * f1( 0.19375353229068096)
w2 ( 0.9176809358065329 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0646992402558521) - present_state_Q (0.0646992402558521)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5633024036672694 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10066815268334615) - present_state_Q ( 0.0626239999225881)) * f1( 0.13033615722696928)
w2 ( 0.9318925780367191 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10066815268334615) - present_state_Q (0.0626239999225881)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5475791096093646 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011485758602710922) - present_state_Q ( 0.004336323691783231)) * f1( 0.15773576241363282)
w2 ( 0.941860700558404 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.011485758602710922) - present_state_Q (0.004336323691783231)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5326210604020005 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022420081672877262) - present_state_Q ( -0.03241511025253829)) * f1( 0.14519937646485181)
w2 ( 0.9470115660688302 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.022420081672877262) - present_state_Q (-0.03241511025253829)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5154729207382858 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10604045117771325) - present_state_Q ( -0.10604045117771325)) * f1( 0.2879927980417851)
w2 ( 0.9499887480991299 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10604045117771325) - present_state_Q (-0.10604045117771325)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4905131613611678 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06764006757191757) - present_state_Q ( -0.07315162634283084)) * f1( 0.2340589755422747)
w2 ( 0.9553206861970581 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06764006757191757) - present_state_Q (-0.07315162634283084)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.480759497807247 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04129903098059154) - present_state_Q ( -0.04129903098059154)) * f1( 0.18157528137122766)
w2 ( 0.9580065318364708 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04129903098059154) - present_state_Q (-0.04129903098059154)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4628464515084261 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03554851109701422) - present_state_Q ( -0.03554851109701422)) * f1( 0.1735770963848857)
w2 ( 0.9631665001364074 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.03554851109701422) - present_state_Q (-0.03554851109701422)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4476048915257026 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02107362332653811) - present_state_Q ( -0.02107362332653811)) * f1( 0.14957865207290696)
w2 ( 0.9682613314413768 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.02107362332653811) - present_state_Q (-0.02107362332653811)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.434573444641677 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009425674863260826) - present_state_Q ( -0.009425674863260826)) * f1( 0.1292182961588757)
w2 ( 0.9733037469782615 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.009425674863260826) - present_state_Q (-0.009425674863260826)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7311661221515354 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14350794603001946) - present_state_Q ( -0.14350794603001946)) * f1( 0.3445029014719833)
w2 ( 0.8430647795826993 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.14350794603001946) - present_state_Q (-0.14350794603001946)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.7405577822808141 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03310818026878459) - present_state_Q ( 0.019300780372514814)) * f1( 0.434819286381918)
w2 ( 0.8344251810888739 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03310818026878459) - present_state_Q (0.019300780372514814)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.749920128248383 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06433390274199852) - present_state_Q ( 0.056411865710881026)) * f1( 0.37452608474445326)
w2 ( 0.8244260420714066 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06433390274199852) - present_state_Q (0.056411865710881026)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7653435901211482 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09545014564350174) - present_state_Q ( 0.0880532918810433)) * f1( 0.3223238260214555)
w2 ( 0.8052857109787388 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09545014564350174) - present_state_Q (0.0880532918810433)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7598065096796386 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25224862673354154) - present_state_Q ( -0.2667791628879594)) * f1( 0.5293963100061295)
w2 ( 0.8441887510705752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.25224862673354154) - present_state_Q (-0.2667791628879594)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.850015609305907 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2095507809110536) - present_state_Q ( -0.2095507809110536)) * f1( 0.4980064349339554)
w2 ( 0.8079606651269742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2095507809110536) - present_state_Q (-0.2095507809110536)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9293879592304964 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21116841260731636) - present_state_Q ( -0.21116841260731636)) * f1( 0.43853376520590537)
w2 ( 0.7717616965539059 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21116841260731636) - present_state_Q (-0.21116841260731636)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9306298265810475 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18436072405284815) - present_state_Q ( -0.18436072405284815)) * f1( 0.36444744092022985)
w2 ( 0.7710801895868572 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18436072405284815) - present_state_Q (-0.18436072405284815)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9130806505805972 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15069467784435372) - present_state_Q ( -0.15069467784435372)) * f1( 0.32763909672002206)
w2 ( 0.7817926937880556 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.15069467784435372) - present_state_Q (-0.15069467784435372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8988806772253093 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.031978756061100844) - present_state_Q ( 0.016507180346092443)) * f1( 0.3672178702344934)
w2 ( 0.7991937750747564 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.031978756061100844) - present_state_Q (0.016507180346092443)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8891998111764904 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08821550784302962) - present_state_Q ( 0.08821550784302962)) * f1( 0.30195519585362884)
w2 ( 0.8136210470071137 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08821550784302962) - present_state_Q (0.08821550784302962)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8829300518829314 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14385405816145347) - present_state_Q ( 0.15291202225108333)) * f1( 0.23978575593714105)
w2 ( 0.8253873492675414 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14385405816145347) - present_state_Q (0.15291202225108333)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8782213286822897 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1908038775174527) - present_state_Q ( 0.1899659433751122)) * f1( 0.20551839118886533)
w2 ( 0.8356974992644899 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1908038775174527) - present_state_Q (0.1899659433751122)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8759473531987024 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24714747707982773) - present_state_Q ( 0.2569938281085472)) * f1( 0.13558090958588948)
w2 ( 0.8432449406464645 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24714747707982773) - present_state_Q (0.2569938281085472)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8726801269539092 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.036150906135923325) - present_state_Q ( 0.011363250996392765)) * f1( 0.08329409615979849)
w2 ( 0.8471674590426365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.036150906135923325) - present_state_Q (0.011363250996392765)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8659947634546886 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11288328062147046) - present_state_Q ( -0.11567001722320348)) * f1( 0.13254572168034787)
w2 ( 0.8471674590426365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11288328062147046) - present_state_Q (-0.11567001722320348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8586713027837775 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12397173490810326) - present_state_Q ( -0.12397173490810326)) * f1( 0.14315529393451099)
w2 ( 0.8471674590426365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12397173490810326) - present_state_Q (-0.12397173490810326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7584562799570501 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30856456342218297) - present_state_Q ( -0.3129584537891965)) * f1( 0.5340319193707977)
w2 ( 0.8531837180556641 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.30856456342218297) - present_state_Q (-0.3129584537891965)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.8428191783701647 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2809018509793072) - present_state_Q ( -0.2809018509793072)) * f1( 0.482849483170753)
w2 ( 0.8357118347144779 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2809018509793072) - present_state_Q (-0.2809018509793072)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.9194936379750113 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.287570601663348) - present_state_Q ( -0.287570601663348)) * f1( 0.4403575460308177)
w2 ( 0.818299970129448 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.287570601663348) - present_state_Q (-0.287570601663348)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.945739174246136 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22183488025774267) - present_state_Q ( -0.22183488025774267)) * f1( 0.3747496029836852)
w2 ( 0.8077947410129275 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.22183488025774267) - present_state_Q (-0.22183488025774267)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9532978594795556 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18448466600746044) - present_state_Q ( -0.18485236340389677)) * f1( 0.3235792519642328)
w2 ( 0.8042907994649747 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18448466600746044) - present_state_Q (-0.18485236340389677)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9504076267492969 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12475287596354054) - present_state_Q ( -0.12475287596354054)) * f1( 0.2574184903942392)
w2 ( 0.8059749632904826 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.12475287596354054) - present_state_Q (-0.12475287596354054)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9451672766288657 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06907592572818107) - present_state_Q ( -0.06907592572818107)) * f1( 0.1998849387094252)
w2 ( 0.809907488287813 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06907592572818107) - present_state_Q (-0.06907592572818107)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9398312943758044 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017152163043488197) - present_state_Q ( -0.003910552470275261)) * f1( 0.13267141046260125)
w2 ( 0.815940418330302 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.017152163043488197) - present_state_Q (-0.003910552470275261)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9354014954751787 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.056438406142702596) - present_state_Q ( 0.015641385226187493)) * f1( 0.11358387208659228)
w2 ( 0.8217904551611231 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.056438406142702596) - present_state_Q (0.015641385226187493)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.927836992257827 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09001939891997049) - present_state_Q ( -0.10233879657701059)) * f1( 0.1533334295795689)
w2 ( 0.8242571394445483 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09001939891997049) - present_state_Q (-0.10233879657701059)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.9842545847440238 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.254347575337544) - present_state_Q ( -0.254347575337544)) * f1( 0.31854779964155727)
w2 ( 0.8154017035335672 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.254347575337544) - present_state_Q (-0.254347575337544)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.0283691878242323 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19750767548529613) - present_state_Q ( -0.19750767548529613)) * f1( 0.24208956133432047)
w2 ( 0.8062904880732511 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.19750767548529613) - present_state_Q (-0.19750767548529613)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.064556800263519 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1602139621779257) - present_state_Q ( -0.1602139621779257)) * f1( 0.1949965916480399)
w2 ( 0.7970114509030518 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1602139621779257) - present_state_Q (-0.1602139621779257)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.0964560744366922 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14148814982074068) - present_state_Q ( -0.14148814982074068)) * f1( 0.17034198862945116)
w2 ( 0.7876481475772451 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.14148814982074068) - present_state_Q (-0.14148814982074068)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1268990458572872 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13861779536698457) - present_state_Q ( -0.13861779536698457)) * f1( 0.16234138958762664)
w2 ( 0.7782719276563965 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13861779536698457) - present_state_Q (-0.13861779536698457)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1560056146666353 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13575812399780327) - present_state_Q ( -0.13575812399780327)) * f1( 0.15500210158376856)
w2 ( 0.7688828392143866 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13575812399780327) - present_state_Q (-0.13575812399780327)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1704755970145675 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16933511261121442) - present_state_Q ( -0.19243948477477332)) * f1( 0.1997253506437977)
w2 ( 0.7652603690819548 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.16933511261121442) - present_state_Q (-0.19243948477477332)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1692688493291163 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1075969860186631) - present_state_Q ( -0.1075969860186631)) * f1( 0.12461601492999387)
w2 ( 0.7657445555190389 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1075969860186631) - present_state_Q (-0.1075969860186631)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1668245543024263 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07027955808744794) - present_state_Q ( -0.07027955808744794)) * f1( 0.09285014812948411)
w2 ( 0.7670608135304324 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07027955808744794) - present_state_Q (-0.07027955808744794)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.161313574861623 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07042810644336175) - present_state_Q ( -0.09376286581722049)) * f1( 0.11322688231627609)
w2 ( 0.7694944138062968 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07042810644336175) - present_state_Q (-0.09376286581722049)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1574631401751871 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04677505190603056) - present_state_Q ( -0.05977937145737288)) * f1( 0.0846059964117747)
w2 ( 0.7717699231376306 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04677505190603056) - present_state_Q (-0.05977937145737288)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.158949574913475 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12991058287754118) - present_state_Q ( -0.12991058287754118)) * f1( 0.17891504964897684)
w2 ( 0.7709391183835285 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12991058287754118) - present_state_Q (-0.12991058287754118)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1522307852021596 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028158302704870436) - present_state_Q ( -0.028664444149824803)) * f1( 0.15777413598014556)
w2 ( 0.7794560906611152 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.028158302704870436) - present_state_Q (-0.028664444149824803)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1469820133172441 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003497171209501465) - present_state_Q ( 0.003497171209501465)) * f1( 0.13226000283960818)
w2 ( 0.7873931415793441 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.003497171209501465) - present_state_Q (0.003497171209501465)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.138539779719253 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11394100934980814) - present_state_Q ( -0.11394100934980814)) * f1( 0.16798896693286597)
w2 ( 0.7924186106634924 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11394100934980814) - present_state_Q (-0.11394100934980814)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1321247258011091 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05321729319767156) - present_state_Q ( -0.07595042112903198)) * f1( 0.13630817733364517)
w2 ( 0.797124897581585 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05321729319767156) - present_state_Q (-0.07595042112903198)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1676723730969965 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13446984131244374) - present_state_Q ( -0.13446984131244374)) * f1( 0.18918616137373334)
w2 ( 0.778335126153397 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13446984131244374) - present_state_Q (-0.13446984131244374)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.200431687791308 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12480834937445064) - present_state_Q ( -0.12480834937445064)) * f1( 0.17354342421609836)
w2 ( 0.759458401297767 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.12480834937445064) - present_state_Q (-0.12480834937445064)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.212935345827113 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1118279250786545) - present_state_Q ( -0.1118279250786545)) * f1( 0.1564218664986418)
w2 ( 0.7514648526234748 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.1118279250786545) - present_state_Q (-0.1118279250786545)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2170492424456174 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07487441754112825) - present_state_Q ( -0.07487441754112825)) * f1( 0.12368417106451371)
w2 ( 0.748138722381345 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07487441754112825) - present_state_Q (-0.07487441754112825)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2166441019865728 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04552616858639341) - present_state_Q ( -0.04552616858639341)) * f1( 0.09887853065230857)
w2 ( 0.7485484578986226 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.04552616858639341) - present_state_Q (-0.04552616858639341)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2130394908062103 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028234389016196293) - present_state_Q ( -0.028234389016196293)) * f1( 0.08473244939726529)
w2 ( 0.7528025673997684 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.028234389016196293) - present_state_Q (-0.028234389016196293)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2050485918092895 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12918840002871887) - present_state_Q ( -0.11701135259935797)) * f1( 0.15852048576879715)
w2 ( 0.7578434925257332 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12918840002871887) - present_state_Q (-0.11701135259935797)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2002749296052104 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06455723330119661) - present_state_Q ( -0.05302960778797916)) * f1( 0.1068952388443922)
w2 ( 0.7623092313703118 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06455723330119661) - present_state_Q (-0.05302960778797916)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1721893840374213 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3307314025197562) - present_state_Q ( -0.3307314025197562)) * f1( 0.4025688089250923)
w2 ( 0.7762623966156674 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3307314025197562) - present_state_Q (-0.3307314025197562)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1469660284747751 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2669112195750304) - present_state_Q ( -0.29029361251626884)) * f1( 0.38009736131954136)
w2 ( 0.7895344464268428 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2669112195750304) - present_state_Q (-0.29029361251626884)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1257761496818277 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21404186917066204) - present_state_Q ( -0.2369383275522814)) * f1( 0.3442518845677683)
w2 ( 0.8018451292395471 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.21404186917066204) - present_state_Q (-0.2369383275522814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.070875571783147 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5455674233872508) - present_state_Q ( -0.5613267396279464)) * f1( 0.6054520778455972)
w2 ( 0.8154466791988855 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5455674233872508) - present_state_Q (-0.5613267396279464)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.0276313062452873 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4620401518008852) - present_state_Q ( -0.4522245405252603)) * f1( 0.5365156863634556)
w2 ( 0.8275369870790631 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4620401518008852) - present_state_Q (-0.4522245405252603)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2951325009969519 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45069437837031423) - present_state_Q ( 0.43921438736456364)) * f1( 0.396206467826691)
w2 ( 0.47501183742700787 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.45069437837031423) - present_state_Q (0.43921438736456364)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.3094888207327362 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11095809629485415) - present_state_Q ( 0.01747330503897976)) * f1( 0.35327546180474156)
w2 ( 0.4343740878860584 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11095809629485415) - present_state_Q (0.01747330503897976)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3008311409219329 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12585522246578096) - present_state_Q ( 0.12585522246578096)) * f1( 0.30194506187249703)
w2 ( 0.4687817238597541 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12585522246578096) - present_state_Q (0.12585522246578096)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.29666125230084 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2509902485279568) - present_state_Q ( 0.2509902485279568)) * f1( 0.23949904818771953)
w2 ( 0.48967477701873474 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2509902485279568) - present_state_Q (0.2509902485279568)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2947154037608535 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36147186942724446) - present_state_Q ( 0.3359060478753312)) * f1( 0.194116762647545)
w2 ( 0.501703713706822 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.36147186942724446) - present_state_Q (0.3359060478753312)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.294798768772784 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43662201097325437) - present_state_Q ( 0.4507985228837996)) * f1( 0.11681789922715977)
w2 ( 0.500847355092445 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.43662201097325437) - present_state_Q (0.4507985228837996)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.271847602527267 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3133921288661841) - present_state_Q ( -0.32700567093219685)) * f1( 0.3299162404638092)
w2 ( 0.5147606842533566 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3133921288661841) - present_state_Q (-0.32700567093219685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.255481649069601 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25032253622803374) - present_state_Q ( -0.23711579777904612)) * f1( 0.2673810399563393)
w2 ( 0.5270023551364815 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25032253622803374) - present_state_Q (-0.23711579777904612)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2434011068002457 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18276664641308799) - present_state_Q ( -0.16966632991545183)) * f1( 0.2190926495393953)
w2 ( 0.5380301484419644 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18276664641308799) - present_state_Q (-0.16966632991545183)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1712708333435298 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8004719663162319) - present_state_Q ( -0.8004719663162319)) * f1( 0.6437761410524696)
w2 ( 0.5380301484419644 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.8004719663162319) - present_state_Q (-0.8004719663162319)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2474823332559637 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34830413394097637) - present_state_Q ( -0.4587702284286325)) * f1( 0.48355704077445355)
w2 ( 0.5065089447426551 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.34830413394097637) - present_state_Q (-0.4587702284286325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.3288181759264843 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2833340036796669) - present_state_Q ( -0.27469027349301434)) * f1( 0.4638106888684)
w2 ( 0.4012903571301579 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2833340036796669) - present_state_Q (-0.27469027349301434)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4090691479446702 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008534264691706372) - present_state_Q ( -0.07172380673432516)) * f1( 0.4163641386864383)
w2 ( 0.16999962511457736 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.008534264691706372) - present_state_Q (-0.07172380673432516)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4734785588868193 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28116869104740994) - present_state_Q ( -0.28166837794382693)) * f1( 0.3688022364709677)
w2 ( -0.07450316364795062 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.28116869104740994) - present_state_Q (-0.28166837794382693)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.51888216736081 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5282132495096243) - present_state_Q ( -0.5282132495096243)) * f1( 0.2978051159859391)
w2 ( -0.2574561327009113 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5282132495096243) - present_state_Q (-0.5282132495096243)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.514142309673797 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6125207238135721) - present_state_Q ( -0.6640119503537545)) * f1( 0.23376704180391855)
w2 ( -0.2331249473442236 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.6125207238135721) - present_state_Q (-0.6640119503537545)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4961984104874637 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5756793477240953) - present_state_Q ( -0.5756793477240953)) * f1( 0.19544359141168266)
w2 ( -0.12295157779002126 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5756793477240953) - present_state_Q (-0.5756793477240953)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4900325805130146 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20144159108232743) - present_state_Q ( -0.22603190664033168)) * f1( 0.10176521970552826)
w2 ( -0.08659831293809532 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.20144159108232743) - present_state_Q (-0.22603190664033168)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4745622271596581 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34270410761113135) - present_state_Q ( -0.34270410761113135)) * f1( 0.21837404717115863)
w2 ( -0.07242963900109495 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.34270410761113135) - present_state_Q (-0.34270410761113135)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4633072483603289 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27190483754519856) - present_state_Q ( -0.27190483754519856)) * f1( 0.17457310719319513)
w2 ( -0.05953535192528137 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.27190483754519856) - present_state_Q (-0.27190483754519856)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4544349794893991 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19675693181670542) - present_state_Q ( -0.2260229222553841)) * f1( 0.14632323601912708)
w2 ( -0.0474084073438071 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19675693181670542) - present_state_Q (-0.2260229222553841)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4493839673896527 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14740017547267045) - present_state_Q ( -0.14740017547267045)) * f1( 0.09482616682687828)
w2 ( -0.03675520418529903 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14740017547267045) - present_state_Q (-0.14740017547267045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4464834053057871 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09408921161797279) - present_state_Q ( -0.09408921161797279)) * f1( 0.05984485321521034)
w2 ( -0.02706159837617552 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09408921161797279) - present_state_Q (-0.09408921161797279)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4600468909665616 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34224333122734596) - present_state_Q ( -0.34224333122734596)) * f1( 0.2291202862481604)
w2 ( -0.05074083845199107 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.34224333122734596) - present_state_Q (-0.34224333122734596)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.4742043697434966 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5882723876196226) - present_state_Q ( -0.5882723876196226)) * f1( 0.38206162281482753)
w2 ( -0.07297412952053145 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.5882723876196226) - present_state_Q (-0.5882723876196226)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4546527333302903 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4921129132476801) - present_state_Q ( -0.4921129132476801)) * f1( 0.30411552477854065)
w2 ( -0.03440003220515672 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4921129132476801) - present_state_Q (-0.4921129132476801)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4378966989829454 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3580981892714937) - present_state_Q ( -0.3580981892714937)) * f1( 0.23198538195148544)
w2 ( 0.008937270015503943 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3580981892714937) - present_state_Q (-0.3580981892714937)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4263230639440132 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2579070703755122) - present_state_Q ( -0.2579070703755122)) * f1( 0.18309342567587128)
w2 ( 0.04686425181578161 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2579070703755122) - present_state_Q (-0.2579070703755122)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.420835452001253 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12468896260010193) - present_state_Q ( -0.12468896260010193)) * f1( 0.10713387278968448)
w2 ( 0.07759745579618713 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12468896260010193) - present_state_Q (-0.12468896260010193)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.415358521902915 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06992489176780323) - present_state_Q ( -0.08514882378944189)) * f1( 0.11454266527232282)
w2 ( 0.1254130892574533 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06992489176780323) - present_state_Q (-0.08514882378944189)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3855443754737211 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47561565318610305) - present_state_Q ( -0.48114060070775855)) * f1( 0.35766430252501996)
w2 ( 0.14208466996523625 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.47561565318610305) - present_state_Q (-0.48114060070775855)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.3612235659939915 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38115510180073797) - present_state_Q ( -0.3908404364637235)) * f1( 0.32310354859385654)
w2 ( 0.17219366701658223 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.38115510180073797) - present_state_Q (-0.3908404364637235)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.344327799627539 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29155278695912334) - present_state_Q ( -0.282916400600698)) * f1( 0.2584394482991809)
w2 ( 0.19834411189277365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.29155278695912334) - present_state_Q (-0.282916400600698)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3343482627438898 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1717795107321235) - present_state_Q ( -0.19322486340968725)) * f1( 0.17324173899607506)
w2 ( 0.20986505013950316 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1717795107321235) - present_state_Q (-0.19322486340968725)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.3251738085151472 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10258547181875957) - present_state_Q ( -0.1449898428774241)) * f1( 0.17157129763293777)
w2 ( 0.23125430196732508 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10258547181875957) - present_state_Q (-0.1449898428774241)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3213012476742985 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028151427773426566) - present_state_Q ( -0.028151427773426566)) * f1( 0.09104703683779265)
w2 ( 0.24826775336716844 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.028151427773426566) - present_state_Q (-0.028151427773426566)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3073075792532696 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19434532529501303) - present_state_Q ( -0.2124740189345229)) * f1( 0.2359652053838403)
w2 ( 0.2719893328233693 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19434532529501303) - present_state_Q (-0.2124740189345229)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2995857582525314 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11278492658541535) - present_state_Q ( -0.09842657669281109)) * f1( 0.15851075378949736)
w2 ( 0.2914752561847401 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11278492658541535) - present_state_Q (-0.09842657669281109)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.295144917213181 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034784719897657745) - present_state_Q ( -0.02147824462242867)) * f1( 0.10624027404084226)
w2 ( 0.3081952470900466 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.034784719897657745) - present_state_Q (-0.02147824462242867)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2929855851697496 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.045463750756622026) - present_state_Q ( 0.04540722554657324)) * f1( 0.06012522016223752)
w2 ( 0.32256081307121015 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.045463750756622026) - present_state_Q (0.04540722554657324)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2663895743981466 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3921892280265895) - present_state_Q ( -0.3921892280265895)) * f1( 0.3532146033792585)
w2 ( 0.33762021917568874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3921892280265895) - present_state_Q (-0.3921892280265895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2467919756700916 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3040898287408445) - present_state_Q ( -0.34655308427371617)) * f1( 0.2736544040473612)
w2 ( 0.33762021917568874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3040898287408445) - present_state_Q (-0.34655308427371617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2323352079157568 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22084965220182384) - present_state_Q ( -0.275747511290077)) * f1( 0.22116561276541405)
w2 ( 0.33762021917568874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22084965220182384) - present_state_Q (-0.275747511290077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2226639275264377 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14069925957712037) - present_state_Q ( -0.15344157905224487)) * f1( 0.1793064269105001)
w2 ( 0.3484076522375794 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14069925957712037) - present_state_Q (-0.15344157905224487)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2172227778954936 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08578815946705325) - present_state_Q ( -0.07343231822556855)) * f1( 0.11705084729424954)
w2 ( 0.3577047222831567 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08578815946705325) - present_state_Q (-0.07343231822556855)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2145887632262482 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020966351767772937) - present_state_Q ( -0.007537899607669166)) * f1( 0.06496661539724319)
w2 ( 0.36581354757177453 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.020966351767772937) - present_state_Q (-0.007537899607669166)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1909915810641907 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26626106742668276) - present_state_Q ( -0.33942377694103765)) * f1( 0.4601655493662679)
w2 ( 0.3965814077836767 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.26626106742668276) - present_state_Q (-0.33942377694103765)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1774896551123555 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15561795091630903) - present_state_Q ( -0.15561795091630903)) * f1( 0.3970498907479376)
w2 ( 0.4237859002496509 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15561795091630903) - present_state_Q (-0.15561795091630903)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1624177773843007 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055439877920510416) - present_state_Q ( -0.055439877920510416)) * f1( 0.33500812207355757)
w2 ( 0.45977757145992765 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.055439877920510416) - present_state_Q (-0.055439877920510416)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.149698189727363 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009917926883782902) - present_state_Q ( -0.0009917926883782902)) * f1( 0.31728166673967584)
w2 ( 0.4918489805334909 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0009917926883782902) - present_state_Q (-0.0009917926883782902)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1419578653776492 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10348193848535037) - present_state_Q ( 0.10348193848535037)) * f1( 0.25223771641339343)
w2 ( 0.5163982809625457 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10348193848535037) - present_state_Q (0.10348193848535037)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1371713654183462 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18184338033538613) - present_state_Q ( 0.18184338033538613)) * f1( 0.2025251994373426)
w2 ( 0.5353055575783979 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18184338033538613) - present_state_Q (0.18184338033538613)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.13520460259685 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26696196889253543) - present_state_Q ( 0.27791744487167924)) * f1( 0.13219379748955984)
w2 ( 0.5472078577398038 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.26696196889253543) - present_state_Q (0.27791744487167924)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1646219779583027 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41416133433048335) - present_state_Q ( -0.4125129946783888)) * f1( 0.5561958930839014)
w2 ( 0.5260517321896174 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.41416133433048335) - present_state_Q (-0.4125129946783888)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2501197483982558 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3830729025760983) - present_state_Q ( -0.39580378048768894)) * f1( 0.520533258719973)
w2 ( 0.46035159179882057 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3830729025760983) - present_state_Q (-0.39580378048768894)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3363532501485689 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3826986810404458) - present_state_Q ( -0.3699684668987724)) * f1( 0.5168940197977008)
w2 ( 0.3602535077265042 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3826986810404458) - present_state_Q (-0.3699684668987724)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.31732500831634 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41664543116003055) - present_state_Q ( -0.43323857642002944)) * f1( 0.48594238161484327)
w2 ( 0.3837479497247458 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.41664543116003055) - present_state_Q (-0.43323857642002944)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2921943122423765 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23465508920647654) - present_state_Q ( -0.23465508920647654)) * f1( 0.4111767753339437)
w2 ( 0.4326431161476121 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23465508920647654) - present_state_Q (-0.23465508920647654)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.273694346352597 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12244094315865062) - present_state_Q ( -0.12244094315865062)) * f1( 0.3626044718178991)
w2 ( 0.47345886405503496 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12244094315865062) - present_state_Q (-0.12244094315865062)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2584731018412127 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05368108704419755) - present_state_Q ( -0.05368108704419755)) * f1( 0.3395227273533888)
w2 ( 0.5093239023222171 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05368108704419755) - present_state_Q (-0.05368108704419755)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2477839033105922 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05060622902765871) - present_state_Q ( 0.03948729092198455)) * f1( 0.2923954674894735)
w2 ( 0.5385697688806796 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05060622902765871) - present_state_Q (0.03948729092198455)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2414017472776886 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13190579776971179) - present_state_Q ( 0.13968734991598275)) * f1( 0.23334847036897924)
w2 ( 0.5604500272695587 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.13190579776971179) - present_state_Q (0.13968734991598275)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.235405748252444 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20542033387568734) - present_state_Q ( 0.10234465510419757)) * f1( 0.1884364685087003)
w2 ( 0.579541869966561 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20542033387568734) - present_state_Q (0.10234465510419757)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2336343351696413 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29596403674554994) - present_state_Q ( 0.2977063538219248)) * f1( 0.1343098349558742)
w2 ( 0.5900930739547714 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.29596403674554994) - present_state_Q (0.2977063538219248)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2324843666125627 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.34627493160243694) - present_state_Q ( 0.3327815945616691)) * f1( 0.11291260354146465)
w2 ( 0.5982407458426574 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.34627493160243694) - present_state_Q (0.3327815945616691)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.22663988152762 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04415781022797503) - present_state_Q ( -0.04415781022797503)) * f1( 0.13290712956199283)
w2 ( 0.6070355864267609 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04415781022797503) - present_state_Q (-0.04415781022797503)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2220024215045224 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08634616555304134) - present_state_Q ( 0.07321891294279662)) * f1( 0.1382600746819832)
w2 ( 0.6204522145712612 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08634616555304134) - present_state_Q (0.07321891294279662)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.211244177349362 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1301357530373526) - present_state_Q ( -0.1301357530373526)) * f1( 0.20804066463191045)
w2 ( 0.6307946581259336 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1301357530373526) - present_state_Q (-0.1301357530373526)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1608917081679015 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5531394093052702) - present_state_Q ( -0.5531394093052702)) * f1( 0.5608269196529853)
w2 ( 0.6487511674934284 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5531394093052702) - present_state_Q (-0.5531394093052702)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.112369824145767 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5195203101592771) - present_state_Q ( -0.5195203101592771)) * f1( 0.5592860549263721)
w2 ( 0.6661025330762954 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5195203101592771) - present_state_Q (-0.5195203101592771)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0680090072365822 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45492274901733454) - present_state_Q ( -0.4672605914262218)) * f1( 0.5398214559646242)
w2 ( 0.6825378994067852 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.45492274901733454) - present_state_Q (-0.4672605914262218)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0339153186601733 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36396124840745536) - present_state_Q ( -0.36396124840745536)) * f1( 0.4685998197559677)
w2 ( 0.6970892018781194 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.36396124840745536) - present_state_Q (-0.36396124840745536)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.004535345684003 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3087151001085645) - present_state_Q ( -0.3087151001085645)) * f1( 0.43343292472435113)
w2 ( 0.7106460736800736 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3087151001085645) - present_state_Q (-0.3087151001085645)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9814667670177208 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23617389030974253) - present_state_Q ( -0.23617389030974253)) * f1( 0.37659511601173684)
w2 ( 0.7228972037056489 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23617389030974253) - present_state_Q (-0.23617389030974253)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9622767159480495 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16662677512007087) - present_state_Q ( -0.18614750075825867)) * f1( 0.3369721243892276)
w2 ( 0.734286900170574 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.16662677512007087) - present_state_Q (-0.18614750075825867)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9504445497310678 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08989883120328676) - present_state_Q ( -0.08989883120328676)) * f1( 0.24603755584395054)
w2 ( 0.7439050791322331 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08989883120328676) - present_state_Q (-0.08989883120328676)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9420420685627595 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0338959068350316) - present_state_Q ( -0.035874824544074924)) * f1( 0.19428365434129816)
w2 ( 0.7525547838094446 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0338959068350316) - present_state_Q (-0.035874824544074924)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9341062878822012 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017818171560819995) - present_state_Q ( -0.02522459631846219)) * f1( 0.18654745785234908)
w2 ( 0.7610628393701261 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0017818171560819995) - present_state_Q (-0.02522459631846219)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9296453907692088 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05736232481056147) - present_state_Q ( 0.03868690038331138)) * f1( 0.12153399346887858)
w2 ( 0.768403826012081 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05736232481056147) - present_state_Q (0.03868690038331138)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9264152585241823 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07634612532877535) - present_state_Q ( 0.06582772197493116)) * f1( 0.0945016713897689)
w2 ( 0.7752399638232399 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07634612532877535) - present_state_Q (0.06582772197493116)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9478807866436841 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0869488198608542) - present_state_Q ( -0.0869488198608542)) * f1( 0.26121850908523797)
w2 ( 0.7588050425807352 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.0869488198608542) - present_state_Q (-0.0869488198608542)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0395873024676676 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37894688122562714) - present_state_Q ( 0.389091933527587)) * f1( 0.39004177979200494)
w2 ( 0.5236853180402328 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.37894688122562714) - present_state_Q (0.389091933527587)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1188618020334768 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13571517596295135) - present_state_Q ( 0.1352541681937297)) * f1( 0.37363975966663343)
w2 ( 0.3115170529804893 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.13571517596295135) - present_state_Q (0.1352541681937297)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.184241173351892 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054123093936036204) - present_state_Q ( -0.06557984958599855)) * f1( 0.3370361754071259)
w2 ( 0.11753380699972876 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.054123093936036204) - present_state_Q (-0.06557984958599855)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.233491860012371 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21569005326183877) - present_state_Q ( -0.20322303398981634)) * f1( 0.2708543227573068)
w2 ( -0.06430079013390802 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21569005326183877) - present_state_Q (-0.20322303398981634)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2726032505062417 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3251899537384019) - present_state_Q ( -0.35124785435825956)) * f1( 0.23262988068804416)
w2 ( -0.2324279042354661 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3251899537384019) - present_state_Q (-0.35124785435825956)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3002473531356014 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45341825528670454) - present_state_Q ( -0.45341825528670454)) * f1( 0.17365219754336514)
w2 ( -0.3916202612596627 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.45341825528670454) - present_state_Q (-0.45341825528670454)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2990979009389163 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3576204343794067) - present_state_Q ( -0.3576204343794067)) * f1( 0.0943268812105924)
w2 ( -0.38430875780317475 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3576204343794067) - present_state_Q (-0.3576204343794067)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2985461616410199 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2494373750017284) - present_state_Q ( -0.2633736397770243)) * f1( 0.14357030989087805)
w2 ( -0.3835401597576377 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2494373750017284) - present_state_Q (-0.2633736397770243)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2764548585470483 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42472657188928076) - present_state_Q ( -0.437522744213174)) * f1( 0.2778605204189829)
w2 ( -0.3676391580171528 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.42472657188928076) - present_state_Q (-0.437522744213174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2559809817369243 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4659971130188325) - present_state_Q ( -0.4659971130188325)) * f1( 0.2498650443267639)
w2 ( -0.3348632619484748 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4659971130188325) - present_state_Q (-0.4659971130188325)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2405122935721853 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3662452127750052) - present_state_Q ( -0.3913637328767441)) * f1( 0.2049540811847043)
w2 ( -0.30467369348450507 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3662452127750052) - present_state_Q (-0.3913637328767441)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2317738046847393 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2687842525409838) - present_state_Q ( -0.28626721947214434)) * f1( 0.13252407326407203)
w2 ( -0.2782981417157832 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2687842525409838) - present_state_Q (-0.28626721947214434)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2206185356301071 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26950545052418384) - present_state_Q ( -0.26950545052418384)) * f1( 0.1736080288180499)
w2 ( -0.2654470436063479 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.26950545052418384) - present_state_Q (-0.26950545052418384)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2127630225646877 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25787576469136814) - present_state_Q ( -0.25787576469136814)) * f1( 0.12427875115833796)
w2 ( -0.24016351607745864 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25787576469136814) - present_state_Q (-0.25787576469136814)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3661929041502776 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.083883396674927) - present_state_Q ( -1.1097166169278885)) * f1( 0.6773671564834729)
w2 ( -0.443460463122014 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.083883396674927) - present_state_Q (-1.1097166169278885)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.3494557293879974 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1795792900892823) - present_state_Q ( -1.2682713827136851)) * f1( 0.668648628931534)
w2 ( -0.4234353868256334 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.1795792900892823) - present_state_Q (-1.2682713827136851)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3395368291601133 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1773475486329958) - present_state_Q ( -1.1773475486329958)) * f1( 0.6214351615319822)
w2 ( -0.4106663633240577 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.1773475486329958) - present_state_Q (-1.1773475486329958)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3339985973678312 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1060205138941879) - present_state_Q ( -1.1060205138941879)) * f1( 0.5804151153667232)
w2 ( -0.4030328863236762 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.1060205138941879) - present_state_Q (-1.1060205138941879)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3322909004621784 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.051132935017936) - present_state_Q ( -1.0369936382075169)) * f1( 0.5356582312444096)
w2 ( -0.40048245874721833 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.051132935017936) - present_state_Q (-1.0369936382075169)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2947121317971504 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0625057318423332) - present_state_Q ( -1.0625057318423332)) * f1( 0.496905948141998)
w2 ( -0.3248569428814083 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -1.0625057318423332) - present_state_Q (-1.0625057318423332)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.243190528295988 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8831431456270771) - present_state_Q ( -0.8831431456270771)) * f1( 0.43120489028764153)
w2 ( -0.20537405977497136 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.8831431456270771) - present_state_Q (-0.8831431456270771)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.197983648790342 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7348762776570767) - present_state_Q ( -0.7348762776570767)) * f1( 0.42592201744641794)
w2 ( -0.09923519478583444 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.7348762776570767) - present_state_Q (-0.7348762776570767)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1652892717130485 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5483461556807386) - present_state_Q ( -0.554457304208984)) * f1( 0.36342321608946876)
w2 ( 0.008719527851074799 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5483461556807386) - present_state_Q (-0.554457304208984)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1445923336176111 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.333849693231852) - present_state_Q ( -0.333849693231852)) * f1( 0.29547438134994564)
w2 ( 0.09277529472011484 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.333849693231852) - present_state_Q (-0.333849693231852)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1322581784389434 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1644561522655514) - present_state_Q ( -0.15218530170738506)) * f1( 0.2302266471929376)
w2 ( 0.15706405709781446 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1644561522655514) - present_state_Q (-0.15218530170738506)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1243985389669717 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03799624942969557) - present_state_Q ( -0.0447522193873138)) * f1( 0.1782422775372438)
w2 ( 0.2011593165422489 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03799624942969557) - present_state_Q (-0.0447522193873138)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1197882606032883 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08233130716195328) - present_state_Q ( 0.08233130716195328)) * f1( 0.14146218371546432)
w2 ( 0.24026753536875795 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08233130716195328) - present_state_Q (0.08233130716195328)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.3076197346392102 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23398889304148074) - present_state_Q ( -0.252269647416741)) * f1( 0.13519859412879856)
w2 ( -0.3969627445953859 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.23398889304148074) - present_state_Q (-0.252269647416741)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2902511927075841 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.383999182532775) - present_state_Q ( -0.383999182532775)) * f1( 0.23294741242012748)
w2 ( -0.38205075930979593 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.383999182532775) - present_state_Q (-0.383999182532775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.275129748472145 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4069569217410206) - present_state_Q ( -0.40731898259347543)) * f1( 0.1972473889642319)
w2 ( -0.351385827693021 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4069569217410206) - present_state_Q (-0.40731898259347543)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2658194194526653 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.242664369986698) - present_state_Q ( -0.31294153552530224)) * f1( 0.13519189294630402)
w2 ( -0.3238388237519557 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.242664369986698) - present_state_Q (-0.31294153552530224)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2381899936630665 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43622671770026555) - present_state_Q ( -0.43945997155669275)) * f1( 0.3471743005386292)
w2 ( -0.3238388237519557 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.43622671770026555) - present_state_Q (-0.43945997155669275)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.212758956486324 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45045693858252017) - present_state_Q ( -0.45401123755447326)) * f1( 0.3143648994065463)
w2 ( -0.3076595128780313 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.45045693858252017) - present_state_Q (-0.45401123755447326)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1939900773543644 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3580537494566862) - present_state_Q ( -0.3710976117195389)) * f1( 0.2552574091399205)
w2 ( -0.29295366814255386 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3580537494566862) - present_state_Q (-0.3710976117195389)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1812517280526287 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28900169452531127) - present_state_Q ( -0.28900169452531127)) * f1( 0.19297560781019524)
w2 ( -0.2797516376410982 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.28900169452531127) - present_state_Q (-0.28900169452531127)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1744121445578208 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13903781264376672) - present_state_Q ( -0.19498814017198637)) * f1( 0.11770379618659244)
w2 ( -0.268129950462946 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13903781264376672) - present_state_Q (-0.19498814017198637)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1692653233842445 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16400631446715708) - present_state_Q ( -0.16400631446715708)) * f1( 0.09398772388898217)
w2 ( -0.2571778368025372 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.16400631446715708) - present_state_Q (-0.16400631446715708)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1440511596790528 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.391749291078759) - present_state_Q ( -0.391749291078759)) * f1( 0.33503883442374366)
w2 ( -0.2571778368025372 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.391749291078759) - present_state_Q (-0.391749291078759)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1314396043265167 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5462260988161742) - present_state_Q ( -0.5462260988161742)) * f1( 0.4324898648714916)
w2 ( -0.25134576702384603 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5462260988161742) - present_state_Q (-0.5462260988161742)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1217155135537196 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49494020426130536) - present_state_Q ( -0.49621213866335184)) * f1( 0.3941376840207284)
w2 ( -0.2464114046591016 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.49494020426130536) - present_state_Q (-0.49621213866335184)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1142946740421145 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4519274109755965) - present_state_Q ( -0.4519274109755965)) * f1( 0.3589547663187359)
w2 ( -0.24227671126154088 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4519274109755965) - present_state_Q (-0.4519274109755965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1109020305517232 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3579443243763266) - present_state_Q ( -0.3579443243763266)) * f1( 0.2777442891307595)
w2 ( -0.239833713422767 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3579443243763266) - present_state_Q (-0.3579443243763266)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.109464631281318 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2942597229900072) - present_state_Q ( -0.2942597229900072)) * f1( 0.22170540113526824)
w2 ( -0.23853703840894686 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2942597229900072) - present_state_Q (-0.2942597229900072)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1095087892060722 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21904515387530965) - present_state_Q ( -0.21904515387530965)) * f1( 0.1544328150376842)
w2 ( -0.23859422563919128 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21904515387530965) - present_state_Q (-0.21904515387530965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1100081578364929 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17205598562257812) - present_state_Q ( -0.17297025873130703)) * f1( 0.11288906840755589)
w2 ( -0.23947893243581028 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17205598562257812) - present_state_Q (-0.17297025873130703)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1105321884845136 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16618694262947747) - present_state_Q ( -0.1683137510144025)) * f1( 0.10848385543575285)
w2 ( -0.24044503130078118 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16618694262947747) - present_state_Q (-0.1683137510144025)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1761799714360124 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5929496557814943) - present_state_Q ( -0.5935699039331894)) * f1( 0.44788606451078394)
w2 ( -0.2990740337665796 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5929496557814943) - present_state_Q (-0.5935699039331894)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2325912905699539 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7349435362359648) - present_state_Q ( -0.7349435362359648)) * f1( 0.42143576770612245)
w2 ( -0.4061580991575901 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7349435362359648) - present_state_Q (-0.7349435362359648)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2397794821637125 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7869039198048642) - present_state_Q ( -0.7869039198048642)) * f1( 0.3748018049561037)
w2 ( -0.4215010169316399 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.7869039198048642) - present_state_Q (-0.7869039198048642)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2245429100307137 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7414406499924906) - present_state_Q ( -0.7414406499924906)) * f1( 0.3260578532415161)
w2 ( -0.38411729013218054 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7414406499924906) - present_state_Q (-0.7414406499924906)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2627924676306965 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6352240984799086) - present_state_Q ( -0.6352240984799086)) * f1( 0.2677981013878387)
w2 ( -0.4983811550416271 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6352240984799086) - present_state_Q (-0.6352240984799086)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2945143737711815 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.684218059454913) - present_state_Q ( -0.6891308392300344)) * f1( 0.22998705063678576)
w2 ( -0.6087244323788636 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.684218059454913) - present_state_Q (-0.6891308392300344)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3165910028036953 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6948829546776958) - present_state_Q ( -0.6948829546776958)) * f1( 0.16060339922602807)
w2 ( -0.7186928596420695 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6948829546776958) - present_state_Q (-0.6948829546776958)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0864106103138933 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6744305915842125) - present_state_Q ( -0.6868283542612167)) * f1( 0.5751650916779957)
w2 ( -0.23205732539872528 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6744305915842125) - present_state_Q (-0.6868283542612167)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0739117908862563 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.670073260004082) - present_state_Q ( -0.6809896330974257)) * f1( 0.5841052747398462)
w2 ( -0.22777767925678494 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.670073260004082) - present_state_Q (-0.6809896330974257)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0449060893684088 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7169669426591028) - present_state_Q ( -0.7280825264824647)) * f1( 0.635552190061947)
w2 ( -0.21864996261245384 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7169669426591028) - present_state_Q (-0.7280825264824647)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0124357840369158 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7923798440319645) - present_state_Q ( -0.7923798440319645)) * f1( 0.6327744408726214)
w2 ( -0.18786145103472776 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7923798440319645) - present_state_Q (-0.7923798440319645)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9998366880908287 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6899705402624232) - present_state_Q ( -0.6899705402624232)) * f1( 0.5701632426897096)
w2 ( -0.17460304186055692 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.6899705402624232) - present_state_Q (-0.6899705402624232)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0168230078931526 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6451130094185369) - present_state_Q ( -0.6569346944763571)) * f1( 0.5522630604947971)
w2 ( -0.1930576382484867 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.6451130094185369) - present_state_Q (-0.6569346944763571)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9707430155978635 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6097050434306122) - present_state_Q ( -0.6097050434306122)) * f1( 0.4856995333974738)
w2 ( -0.13613356590323364 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.6097050434306122) - present_state_Q (-0.6097050434306122)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9333808098874692 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.50577996244094) - present_state_Q ( -0.50577996244094)) * f1( 0.4368816628959254)
w2 ( -0.08482144793142286 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.50577996244094) - present_state_Q (-0.50577996244094)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8997864659762133 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42453561303279214) - present_state_Q ( -0.4427175307022754)) * f1( 0.419790783989507)
w2 ( -0.03680560976748309 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.42453561303279214) - present_state_Q (-0.4427175307022754)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8826527069392975 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3314606911619756) - present_state_Q ( -0.3314606911619756)) * f1( 0.3438341617706266)
w2 ( -0.006906732444736399 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3314606911619756) - present_state_Q (-0.3314606911619756)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8706912414601465 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2516817576613925) - present_state_Q ( -0.2516817576613925)) * f1( 0.28044746959754635)
w2 ( 0.0186840824689788 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2516817576613925) - present_state_Q (-0.2516817576613925)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8625297741902586 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18343089384736633) - present_state_Q ( -0.18343089384736633)) * f1( 0.2235480662494556)
w2 ( 0.040589350736736585 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18343089384736633) - present_state_Q (-0.18343089384736633)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8544177364337545 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.114688529905738) - present_state_Q ( -0.114688529905738)) * f1( 0.16120271381740123)
w2 ( 0.07078253135164644 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.114688529905738) - present_state_Q (-0.114688529905738)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8444333245365006 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1462980408209317) - present_state_Q ( -0.1462980408209317)) * f1( 0.18779402656244076)
w2 ( 0.08141589608642322 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1462980408209317) - present_state_Q (-0.1462980408209317)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8450813049344436 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18585471621322674) - present_state_Q ( -0.19448639147903557)) * f1( 0.2688817971960444)
w2 ( 0.08045193288073173 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18585471621322674) - present_state_Q (-0.19448639147903557)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8427583976999052 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1423619189187697) - present_state_Q ( -0.13304562525025818)) * f1( 0.19551538702582963)
w2 ( 0.08520431021506697 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1423619189187697) - present_state_Q (-0.13304562525025818)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8386367912536983 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09801828852928886) - present_state_Q ( -0.08992011945829516)) * f1( 0.14713806932420187)
w2 ( 0.09640904183928162 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09801828852928886) - present_state_Q (-0.08992011945829516)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8681446604386123 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11095253951539456) - present_state_Q ( -0.11095253951539456)) * f1( 0.15529291016264674)
w2 ( 0.058406187550558715 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.11095253951539456) - present_state_Q (-0.11095253951539456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8909163889256144 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09140467545278992) - present_state_Q ( -0.09140467545278992)) * f1( 0.11874278292608471)
w2 ( 0.020051471708708933 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09140467545278992) - present_state_Q (-0.09140467545278992)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.123156831003176 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7439383459519994) - present_state_Q ( -0.764665605929189)) * f1( 0.6019524695822885)
w2 ( -0.24883416044742163 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.7439383459519994) - present_state_Q (-0.764665605929189)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.195082743568912 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7060379655208698) - present_state_Q ( -0.770819734693606)) * f1( 0.553368168424019)
w2 ( -0.3268212041589305 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7060379655208698) - present_state_Q (-0.770819734693606)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2384930757261907 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6444254846637334) - present_state_Q ( -0.7097897254955196)) * f1( 0.3204535613935136)
w2 ( -0.4622864864560159 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6444254846637334) - present_state_Q (-0.7097897254955196)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2096212686013506 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7853527113650861) - present_state_Q ( -0.7853527113650861)) * f1( 0.2608542843242303)
w2 ( -0.35160474243315815 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.7853527113650861) - present_state_Q (-0.7853527113650861)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.191243022305542 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5456256951169789) - present_state_Q ( -0.5340521858256285)) * f1( 0.20896490367714074)
w2 ( -0.28124557312804366 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5456256951169789) - present_state_Q (-0.5340521858256285)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1621439762067776 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6981943598099276) - present_state_Q ( -0.7100510585693929)) * f1( 0.3795630827987778)
w2 ( -0.40361723659017323 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6981943598099276) - present_state_Q (-0.7100510585693929)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1685519483410969 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6759247392690262) - present_state_Q ( -0.7566481865870608)) * f1( 0.3037755710348178)
w2 ( -0.42471166532415744 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.6759247392690262) - present_state_Q (-0.7566481865870608)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1487856737884194 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5507276132798724) - present_state_Q ( -0.6356699463447039)) * f1( 0.2532199056323043)
w2 ( -0.3622638905228201 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5507276132798724) - present_state_Q (-0.6356699463447039)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.140278745259342 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6746967333536301) - present_state_Q ( -0.6746967333536301)) * f1( 0.2088989009889454)
w2 ( -0.31339664332062805 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6746967333536301) - present_state_Q (-0.6746967333536301)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.129604752310396 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40914971327335997) - present_state_Q ( -0.40914971327335997)) * f1( 0.13894181512681278)
w2 ( -0.2519378639649461 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.40914971327335997) - present_state_Q (-0.40914971327335997)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1073619231434362 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4106245417111391) - present_state_Q ( -0.42236235544904766)) * f1( 0.2846900291498619)
w2 ( -0.22068586791382874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4106245417111391) - present_state_Q (-0.42236235544904766)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0805441825338864 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42406111357537096) - present_state_Q ( -0.42406111357537096)) * f1( 0.34308922137590403)
w2 ( -0.20505276786947207 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.42406111357537096) - present_state_Q (-0.42406111357537096)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.059671817717483 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35462038097202186) - present_state_Q ( -0.35462038097202186)) * f1( 0.29023322920744377)
w2 ( -0.19066960101197566 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.35462038097202186) - present_state_Q (-0.35462038097202186)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0374229478423613 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3626906399317961) - present_state_Q ( -0.3626906399317961)) * f1( 0.3062804108808812)
w2 ( -0.17614116949320333 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3626906399317961) - present_state_Q (-0.3626906399317961)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0116361396282156 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39101256871920753) - present_state_Q ( -0.39101256871920753)) * f1( 0.3429501299932967)
w2 ( -0.1611029432562576 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.39101256871920753) - present_state_Q (-0.39101256871920753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9913868532440259 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32719145530298355) - present_state_Q ( -0.32719145530298355)) * f1( 0.2915780240513513)
w2 ( -0.1472134970608039 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.32719145530298355) - present_state_Q (-0.32719145530298355)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9741354080561315 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2886583651818107) - present_state_Q ( -0.2886583651818107)) * f1( 0.26146772566273385)
w2 ( -0.13401764648753128 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2886583651818107) - present_state_Q (-0.2886583651818107)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9527467155399836 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3269257758763754) - present_state_Q ( -0.3269257758763754)) * f1( 0.30809089177628535)
w2 ( -0.12013298252175653 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3269257758763754) - present_state_Q (-0.3269257758763754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9380421793836901 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2486261182972601) - present_state_Q ( -0.2486261182972601)) * f1( 0.23573896202372488)
w2 ( -0.10765771239240585 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2486261182972601) - present_state_Q (-0.2486261182972601)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9278770977272289 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18883208119499395) - present_state_Q ( -0.18883208119499395)) * f1( 0.17835076331688213)
w2 ( -0.09625873493089596 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18883208119499395) - present_state_Q (-0.18883208119499395)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.916494974449958 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2010464126846419) - present_state_Q ( -0.2010464126846419)) * f1( 0.1959253721680988)
w2 ( -0.0846398995025724 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2010464126846419) - present_state_Q (-0.2010464126846419)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8967409820261392 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2905992946376696) - present_state_Q ( -0.2905992946376696)) * f1( 0.29860645433588023)
w2 ( -0.07140911219909435 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2905992946376696) - present_state_Q (-0.2905992946376696)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8797392672757329 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2560969356402334) - present_state_Q ( -0.2560969356402334)) * f1( 0.26965993307682445)
w2 ( -0.058799367357570145 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2560969356402334) - present_state_Q (-0.2560969356402334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8669668218802826 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2042284285627629) - present_state_Q ( -0.2042284285627629)) * f1( 0.21877908858980633)
w2 ( -0.047123255643440415 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2042284285627629) - present_state_Q (-0.2042284285627629)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8577871941284958 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15655736862604086) - present_state_Q ( -0.15655736862604086)) * f1( 0.169709744114833)
w2 ( -0.036305223008171676 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.15655736862604086) - present_state_Q (-0.15655736862604086)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8509063927405954 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11974173530173401) - present_state_Q ( -0.12280375302695337)) * f1( 0.13469857001387087)
w2 ( -0.026088631418236075 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11974173530173401) - present_state_Q (-0.12280375302695337)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8436285295924589 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1258684591158754) - present_state_Q ( -0.1258684591158754)) * f1( 0.14179084075703893)
w2 ( -0.015822999154150315 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1258684591158754) - present_state_Q (-0.1258684591158754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8380929507339994 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10139174980630802) - present_state_Q ( -0.10139174980630802)) * f1( 0.11268294848986536)
w2 ( 0.0038271038388767754 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10139174980630802) - present_state_Q (-0.10139174980630802)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8339566378376178 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07358877074859753) - present_state_Q ( -0.07358877074859753)) * f1( 0.08871831155631807)
w2 ( 0.013151701712351532 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07358877074859753) - present_state_Q (-0.07358877074859753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8306567429551611 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0582024905860633) - present_state_Q ( -0.0582024905860633)) * f1( 0.07294483689975563)
w2 ( 0.02219934654290067 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0582024905860633) - present_state_Q (-0.0582024905860633)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9099164491155113 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38481054564181216) - present_state_Q ( -0.38481054564181216)) * f1( 0.47929563799260416)
w2 ( -0.0770208839924415 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.38481054564181216) - present_state_Q (-0.38481054564181216)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.909858911865595 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4458998623825781) - present_state_Q ( -0.4458998623825781)) * f1( 0.43925717836580624)
w2 ( -0.07694229142378228 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4458998623825781) - present_state_Q (-0.4458998623825781)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8795526523483465 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3978079078150771) - present_state_Q ( -0.40602841812144297)) * f1( 0.3955152151329731)
w2 ( -0.030967433783386157 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3978079078150771) - present_state_Q (-0.40602841812144297)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8573886161271072 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3068784247296418) - present_state_Q ( -0.3068784247296418)) * f1( 0.3277779490402012)
w2 ( 0.009604001152014506 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3068784247296418) - present_state_Q (-0.3068784247296418)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8425519400881306 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21015819334703367) - present_state_Q ( -0.21015819334703367)) * f1( 0.2518351538344105)
w2 ( 0.04495254359275433 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.21015819334703367) - present_state_Q (-0.21015819334703367)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8334928225087316 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12259292696755611) - present_state_Q ( -0.12259292696755611)) * f1( 0.17751362973250565)
w2 ( 0.07557256164900236 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12259292696755611) - present_state_Q (-0.12259292696755611)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8276300352258324 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05797602545601877) - present_state_Q ( -0.04963951084415478)) * f1( 0.13209179154293593)
w2 ( 0.11107991431288661 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05797602545601877) - present_state_Q (-0.04963951084415478)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.824696034137331 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010960341210004082) - present_state_Q ( 0.005296938644084086)) * f1( 0.07412854455784372)
w2 ( 0.1348278600415016 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.010960341210004082) - present_state_Q (0.005296938644084086)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8000533849189457 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2645561912091374) - present_state_Q ( -0.2645561912091374)) * f1( 0.38618754309748815)
w2 ( 0.16035188292503055 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2645561912091374) - present_state_Q (-0.2645561912091374)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7802536588976846 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20632332965413674) - present_state_Q ( -0.20632332965413674)) * f1( 0.33805754456191695)
w2 ( 0.18377952279257947 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.20632332965413674) - present_state_Q (-0.20632332965413674)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7658161227996422 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1404761998374789) - present_state_Q ( -0.1404761998374789)) * f1( 0.27425441266988165)
w2 ( 0.20483666598672873 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1404761998374789) - present_state_Q (-0.1404761998374789)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7579860732653283 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05221456467057181) - present_state_Q ( -0.05221456467057181)) * f1( 0.17517159416133146)
w2 ( 0.2227163903148693 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05221456467057181) - present_state_Q (-0.05221456467057181)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7538007763731267 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008272444187862063) - present_state_Q ( 0.008272444187862063)) * f1( 0.10661688227324093)
w2 ( 0.2384185823241063 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.008272444187862063) - present_state_Q (0.008272444187862063)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7497972145464853 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0568614759800774) - present_state_Q ( 0.05660004262719631)) * f1( 0.1146869431247106)
w2 ( 0.259363748622355 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0568614759800774) - present_state_Q (0.05660004262719631)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7470539501981807 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09280290853779659) - present_state_Q ( 0.09099424260872946)) * f1( 0.08618864582441985)
w2 ( 0.27846091151705804 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09280290853779659) - present_state_Q (0.09099424260872946)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7390884320891955 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07226793270714316) - present_state_Q ( -0.07226793270714316)) * f1( 0.17128631068292877)
w2 ( 0.2877617343057866 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07226793270714316) - present_state_Q (-0.07226793270714316)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7337707530218004 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03376580944900102) - present_state_Q ( -0.03376580944900102)) * f1( 0.12355511511934715)
w2 ( 0.29636951887586865 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03376580944900102) - present_state_Q (-0.03376580944900102)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7321129337608882 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019898081792588805) - present_state_Q ( -0.028272503852571667)) * f1( 0.038530431658853115)
w2 ( 0.29636951887586865 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.019898081792588805) - present_state_Q (-0.028272503852571667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7256764087758871 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01453998324045061) - present_state_Q ( -0.029140799147385543)) * f1( 0.28269205600525804)
w2 ( 0.31003072692526906 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.01453998324045061) - present_state_Q (-0.029140799147385543)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7175049787366162 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1219831479112774) - present_state_Q ( 0.11230195273271779)) * f1( 0.27247513051456584)
w2 ( 0.34002036313111006 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1219831479112774) - present_state_Q (0.11230195273271779)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7211447663069696 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15506053320373717) - present_state_Q ( 0.16321235652340732)) * f1( 0.24642059894695995)
w2 ( 0.3252497328108067 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.15506053320373717) - present_state_Q (0.16321235652340732)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7165145865022231 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18322323733007267) - present_state_Q ( 0.18322323733007267)) * f1( 0.19694588675733055)
w2 ( 0.34875964145110017 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18322323733007267) - present_state_Q (0.18322323733007267)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7139461342167339 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3764634253029112) - present_state_Q ( 0.25044954481667714)) * f1( 0.13720599480652448)
w2 ( 0.3674793212224616 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3764634253029112) - present_state_Q (0.25044954481667714)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.716331898530739 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42010059635955893) - present_state_Q ( 0.4173004533789911)) * f1( 0.13610354013480375)
w2 ( 0.34293866609843665 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.42010059635955893) - present_state_Q (0.4173004533789911)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7001186400955001 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14890016107754886) - present_state_Q ( -0.14890016107754886)) * f1( 0.30361330375392104)
w2 ( 0.3536188689978325 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14890016107754886) - present_state_Q (-0.14890016107754886)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6866552353659213 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11611349965994504) - present_state_Q ( -0.11611349965994504)) * f1( 0.26686516078764294)
w2 ( 0.3637089119917115 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11611349965994504) - present_state_Q (-0.11611349965994504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6641492546350959 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1813714821379088) - present_state_Q ( -0.1951044227255598)) * f1( 0.39007378277858146)
w2 ( 0.3752482574819469 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1813714821379088) - present_state_Q (-0.1951044227255598)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6489904461600124 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12232177790710315) - present_state_Q ( -0.12232177790710315)) * f1( 0.29717932833024785)
w2 ( 0.38545004948427475 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12232177790710315) - present_state_Q (-0.12232177790710315)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6325771619230052 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12922456010778488) - present_state_Q ( -0.12922456010778488)) * f1( 0.3179007814758674)
w2 ( 0.39577609156621485 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12922456010778488) - present_state_Q (-0.12922456010778488)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6033240989270825 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22704386653247477) - present_state_Q ( -0.22704386653247477)) * f1( 0.48405017328619127)
w2 ( 0.4078628811637994 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22704386653247477) - present_state_Q (-0.22704386653247477)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5805483122847462 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16780326591100042) - present_state_Q ( -0.16780326591100042)) * f1( 0.41333645148144454)
w2 ( 0.4188833399501974 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.16780326591100042) - present_state_Q (-0.16780326591100042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5628555227867423 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11883630830921678) - present_state_Q ( -0.11883630830921678)) * f1( 0.3490027823901054)
w2 ( 0.4290223934997633 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11883630830921678) - present_state_Q (-0.11883630830921678)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.549191008094232 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07782283959547542) - present_state_Q ( -0.07782283959547542)) * f1( 0.2907092702676102)
w2 ( 0.43842320461248185 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07782283959547542) - present_state_Q (-0.07782283959547542)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5373322240019788 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04545358013024435) - present_state_Q ( -0.056437018358046556)) * f1( 0.26242538052592085)
w2 ( 0.4474610378193823 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04545358013024435) - present_state_Q (-0.056437018358046556)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.530035885444643 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007001610699796254) - present_state_Q ( -0.007001610699796254)) * f1( 0.17957943699151266)
w2 ( 0.45558706681197864 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.007001610699796254) - present_state_Q (-0.007001610699796254)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5302313679004684 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015155691235105045) - present_state_Q ( 0.015155691235105045)) * f1( 0.1433143004337659)
w2 ( 0.45531426436974676 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.015155691235105045) - present_state_Q (0.015155691235105045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.799347000474445 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1426432584823413) - present_state_Q ( -0.1447439454988387)) * f1( 0.35963270269881426)
w2 ( 0.22158870366189282 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1426432584823413) - present_state_Q (-0.1447439454988387)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8021078272154631 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12840053283303268) - present_state_Q ( -0.12840053283303268)) * f1( 0.3269590739379072)
w2 ( 0.2165223324348766 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12840053283303268) - present_state_Q (-0.12840053283303268)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7865794201974264 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08106772126895612) - present_state_Q ( -0.0869160982963077)) * f1( 0.3243129606991499)
w2 ( 0.25482707852842956 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08106772126895612) - present_state_Q (-0.0869160982963077)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.7784394999788915 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09953339676874023) - present_state_Q ( 0.09953339676874023)) * f1( 0.26222285019051933)
w2 ( 0.2920774716774056 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09953339676874023) - present_state_Q (0.09953339676874023)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7729841015236135 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17522503384144392) - present_state_Q ( 0.17522503384144392)) * f1( 0.225152927332433)
w2 ( 0.3211531680225297 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17522503384144392) - present_state_Q (0.17522503384144392)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7709168978548653 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27843461053258955) - present_state_Q ( 0.27843461053258955)) * f1( 0.1383588496628077)
w2 ( 0.33908223008501004 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.27843461053258955) - present_state_Q (0.27843461053258955)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7702317303330917 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37359815115604345) - present_state_Q ( 0.3809979660583517)) * f1( 0.12156583455549821)
w2 ( 0.3469728889530254 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.37359815115604345) - present_state_Q (0.3809979660583517)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7390282543129089 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2928158937533243) - present_state_Q ( -0.2928158937533243)) * f1( 0.4702616852557984)
w2 ( 0.36024357504058524 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2928158937533243) - present_state_Q (-0.2928158937533243)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7133827604629451 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23698639109806302) - present_state_Q ( -0.23698639109806302)) * f1( 0.41816412877677717)
w2 ( 0.37250933008035036 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23698639109806302) - present_state_Q (-0.23698639109806302)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6934770391545783 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13504883583353527) - present_state_Q ( -0.20955070184960536)) * f1( 0.5026115764965555)
w2 ( 0.38835116281100046 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13504883583353527) - present_state_Q (-0.20955070184960536)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6898387228691699 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09594651949677713) - present_state_Q ( -0.08815499764599993)) * f1( 0.46312376214234163)
w2 ( 0.3930647835527798 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.09594651949677713) - present_state_Q (-0.08815499764599993)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6779819459879775 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06425130810109334) - present_state_Q ( -0.07206690817563147)) * f1( 0.44634458475549316)
w2 ( 0.40900329019471116 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06425130810109334) - present_state_Q (-0.07206690817563147)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6696857461574128 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016449164772359193) - present_state_Q ( -0.016449164772359193)) * f1( 0.38622140373901537)
w2 ( 0.42189154509241855 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.016449164772359193) - present_state_Q (-0.016449164772359193)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.66320181772624 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024736923690975493) - present_state_Q ( 0.01789103078595472)) * f1( 0.35127505344006715)
w2 ( 0.4329665047874071 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.024736923690975493) - present_state_Q (0.01789103078595472)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6514592707964059 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.060572794922483575) - present_state_Q ( -0.012758569258333713)) * f1( 0.28037494199096424)
w2 ( 0.4497191387374304 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.060572794922483575) - present_state_Q (-0.012758569258333713)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6448392434372928 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10789612640411006) - present_state_Q ( 0.12100692161245086)) * f1( 0.22844799099730384)
w2 ( 0.46710610019910803 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10789612640411006) - present_state_Q (0.12100692161245086)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6398432170467082 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16653494611048836) - present_state_Q ( 0.1564510631447616)) * f1( 0.19200536914398164)
w2 ( 0.48271824608708525 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.16653494611048836) - present_state_Q (0.1564510631447616)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.636185025009463 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19981690152194792) - present_state_Q ( 0.11606937973167476)) * f1( 0.12036998541400036)
w2 ( 0.49487473850390606 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.19981690152194792) - present_state_Q (0.11606937973167476)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6343972376465004 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24191660363456088) - present_state_Q ( 0.15560383755290877)) * f1( 0.06656248761596328)
w2 ( 0.505618251416328 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24191660363456088) - present_state_Q (0.15560383755290877)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6337830946437312 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2801664427333773) - present_state_Q ( 0.2774881091564225)) * f1( 0.040799108440943053)
w2 ( 0.5146499635233429 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2801664427333773) - present_state_Q (0.2774881091564225)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6326149508643345 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08033743489783803) - present_state_Q ( 0.08033743489783803)) * f1( 0.035647144895100936)
w2 ( 0.5212038896951818 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08033743489783803) - present_state_Q (0.08033743489783803)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6217584219409753 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06097824049337672) - present_state_Q ( -0.06097824049337672)) * f1( 0.4259459818382251)
w2 ( 0.5313991063529434 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06097824049337672) - present_state_Q (-0.06097824049337672)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6082822931797084 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0020951028167187635) - present_state_Q ( 0.0020951028167187635)) * f1( 0.33849889651263687)
w2 ( 0.5473236826515415 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0020951028167187635) - present_state_Q (0.0020951028167187635)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5984859990853343 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05075211481280739) - present_state_Q ( 0.05075211481280739)) * f1( 0.2764791284136946)
w2 ( 0.5614966065182805 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05075211481280739) - present_state_Q (0.05075211481280739)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5914635220246679 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16117322608895332) - present_state_Q ( 0.16770757454539456)) * f1( 0.28269732228347416)
w2 ( 0.5764011914020906 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.16117322608895332) - present_state_Q (0.16770757454539456)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5867280181917057 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20561082110443113) - present_state_Q ( 0.211723316625445)) * f1( 0.22675514756464699)
w2 ( 0.5889314573311905 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20561082110443113) - present_state_Q (0.211723316625445)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5838312936431569 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24769042818330478) - present_state_Q ( 0.25389462934110846)) * f1( 0.1695235986243753)
w2 ( 0.5991839221398237 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24769042818330478) - present_state_Q (0.25389462934110846)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5819440775933558 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2913276063144638) - present_state_Q ( 0.2837323270181673)) * f1( 0.12979438939092433)
w2 ( 0.6079079481566205 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2913276063144638) - present_state_Q (0.2837323270181673)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6819059403471124 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09520233464066535) - present_state_Q ( 0.08437745821431464)) * f1( 0.48177706668847586)
w2 ( 0.4834165146716056 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09520233464066535) - present_state_Q (0.08437745821431464)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7196568898109825 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0036060206673000916) - present_state_Q ( 0.005711607558387566)) * f1( 0.4169758384856398)
w2 ( 0.42909545434210616 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.0036060206673000916) - present_state_Q (0.005711607558387566)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.726687030200113 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01689579376827166) - present_state_Q ( -0.018062516109106808)) * f1( 0.38284881672811566)
w2 ( 0.4180778305460429 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.01689579376827166) - present_state_Q (-0.018062516109106808)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7203665810120108 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010176237189389037) - present_state_Q ( 0.010176237189389037)) * f1( 0.33118860133221534)
w2 ( 0.4295283137378159 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.010176237189389037) - present_state_Q (0.010176237189389037)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7112970417960129 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0659315609953291) - present_state_Q ( 0.0659315609953291)) * f1( 0.2662330989562699)
w2 ( 0.4499680094440682 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0659315609953291) - present_state_Q (0.0659315609953291)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7054205276637273 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12507372749851617) - present_state_Q ( 0.12473097929371732)) * f1( 0.20420417608650568)
w2 ( 0.4672345930514362 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12507372749851617) - present_state_Q (0.12473097929371732)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7019034535841154 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17011859345618013) - present_state_Q ( 0.1769791609471042)) * f1( 0.14652479029222387)
w2 ( 0.4816365549553471 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17011859345618013) - present_state_Q (0.1769791609471042)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7001860147668694 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21698584349158473) - present_state_Q ( 0.22705078224337383)) * f1( 0.08823314718512451)
w2 ( 0.4933154230816942 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21698584349158473) - present_state_Q (0.22705078224337383)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6796535322634718 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23507662902604745) - present_state_Q ( -0.23507662902604745)) * f1( 0.33573453920572444)
w2 ( 0.4933154230816942 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23507662902604745) - present_state_Q (-0.23507662902604745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6615775920675396 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11189530053805315) - present_state_Q ( -0.2064029004438764)) * f1( 0.30368840982329076)
w2 ( 0.4933154230816942 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11189530053805315) - present_state_Q (-0.2064029004438764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6503753933182199 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07050364305522742) - present_state_Q ( -0.0636486127901499)) * f1( 0.24534037934875905)
w2 ( 0.5024473880513868 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07050364305522742) - present_state_Q (-0.0636486127901499)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6411938990085034 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030492537604388845) - present_state_Q ( -0.037088391585088074)) * f1( 0.21153609224580616)
w2 ( 0.5111281708078798 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.030492537604388845) - present_state_Q (-0.037088391585088074)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6349944731691556 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008884614547529035) - present_state_Q ( 0.00245824942257708)) * f1( 0.15559627889983366)
w2 ( 0.5190967750485233 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.008884614547529035) - present_state_Q (0.00245824942257708)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6307452441861241 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.031259972357370894) - present_state_Q ( 0.031259972357370894)) * f1( 0.11426773888314574)
w2 ( 0.5265340955460905 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.031259972357370894) - present_state_Q (0.031259972357370894)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.614533176513513 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07076520729784361) - present_state_Q ( -0.025678853721105926)) * f1( 0.37462429422585636)
w2 ( 0.5438443105241262 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07076520729784361) - present_state_Q (-0.025678853721105926)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.601215084445894 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12143066105181685) - present_state_Q ( 0.012661798946991598)) * f1( 0.3333846456020489)
w2 ( 0.5598235612104538 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12143066105181685) - present_state_Q (0.012661798946991598)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5968996586007078 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37719501583961673) - present_state_Q ( 0.2816349239183985)) * f1( 0.2764799642430193)
w2 ( 0.5723103274236989 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.37719501583961673) - present_state_Q (0.2816349239183985)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5967593919829651 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42434199319634036) - present_state_Q ( 0.4362793444186319)) * f1( 0.227895896814483)
w2 ( 0.5729258129137992 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.42434199319634036) - present_state_Q (0.4362793444186319)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5970157660133435 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.46637871798878694) - present_state_Q ( 0.46021143875829107)) * f1( 0.188877419726853)
w2 ( 0.5715684562178579 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.46637871798878694) - present_state_Q (0.46021143875829107)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5976136311428818 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4917325843189993) - present_state_Q ( 0.49711201018698575)) * f1( 0.12471437149485265)
w2 ( 0.5667745810423493 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4917325843189993) - present_state_Q (0.49711201018698575)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5891360241219248 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010405804481858766) - present_state_Q ( -0.010405804481858766)) * f1( 0.20709152910994935)
w2 ( 0.5749618855230229 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.010405804481858766) - present_state_Q (-0.010405804481858766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5807129688961787 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09850947119236239) - present_state_Q ( -0.0047087306634480824)) * f1( 0.2031807644872178)
w2 ( 0.5832530790786765 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09850947119236239) - present_state_Q (-0.0047087306634480824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.576465656572205 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14252982422408694) - present_state_Q ( 0.14252982422408694)) * f1( 0.15631028110138867)
w2 ( 0.5941220054066094 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14252982422408694) - present_state_Q (0.14252982422408694)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5734928315658256 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17234535176749738) - present_state_Q ( 0.1686963567158594)) * f1( 0.11961240823398084)
w2 ( 0.6040635325450451 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17234535176749738) - present_state_Q (0.1686963567158594)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.541306195684155 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28223842919782) - present_state_Q ( -0.28223842919782)) * f1( 0.49213941947140905)
w2 ( 0.6040635325450451 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.28223842919782) - present_state_Q (-0.28223842919782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5168871568098248 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13336782976691783) - present_state_Q ( -0.13336782976691783)) * f1( 0.46956886564852446)
w2 ( 0.6144641534808496 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13336782976691783) - present_state_Q (-0.13336782976691783)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.49670492073555733 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09284071054051814) - present_state_Q ( -0.09284071054051814)) * f1( 0.41737067441987463)
w2 ( 0.6241352862705789 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09284071054051814) - present_state_Q (-0.09284071054051814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.48484001047606445 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01897485052944002) - present_state_Q ( -0.01710919338077309)) * f1( 0.28575567647829864)
w2 ( 0.6324395204371355 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.01897485052944002) - present_state_Q (-0.01710919338077309)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.47534634353997385 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002074586820544802) - present_state_Q ( 0.008868420128789578)) * f1( 0.24259442582543247)
w2 ( 0.6402663012082007 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.002074586820544802) - present_state_Q (0.008868420128789578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.46813805668064484 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02819805125923007) - present_state_Q ( 0.0349186557965334)) * f1( 0.19592999022885021)
w2 ( 0.6476243241947885 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.02819805125923007) - present_state_Q (0.0349186557965334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.462654908446947 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05620090110036663) - present_state_Q ( 0.05608761527267439)) * f1( 0.1568709241179699)
w2 ( 0.6546149736915358 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05620090110036663) - present_state_Q (0.05608761527267439)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4563651285570678 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05585118370351541) - present_state_Q ( 0.049256805588589206)) * f1( 0.17651642219436794)
w2 ( 0.661741539947171 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05585118370351541) - present_state_Q (0.049256805588589206)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4513938429631955 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06577783468807044) - present_state_Q ( 0.06577783468807044)) * f1( 0.14587107808137276)
w2 ( 0.6685575389227857 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06577783468807044) - present_state_Q (0.06577783468807044)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4349844454674335 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10292589497721116) - present_state_Q ( -0.11188676751828089)) * f1( 0.544088669199821)
w2 ( 0.6745894224831969 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10292589497721116) - present_state_Q (-0.11188676751828089)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.43117222991230747 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28741709177175) - present_state_Q ( 0.28741709177175)) * f1( 0.2697488678982038)
w2 ( 0.6830688995275224 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.28741709177175) - present_state_Q (0.28741709177175)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43363468004123934 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7341291906171672) - present_state_Q ( 0.5975154107116626)) * f1( 0.1984206840808365)
w2 ( 0.6706586503625278 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.7341291906171672) - present_state_Q (0.5975154107116626)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43695379179957283 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7483271415861161) - present_state_Q ( 0.7530599279924644)) * f1( 0.11929500758022753)
w2 ( 0.6372713847024655 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.7483271415861161) - present_state_Q (0.7530599279924644)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.45267459899881823 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08472614995803454) - present_state_Q ( -0.08472614995803454)) * f1( 0.4855900804171373)
w2 ( 0.6307964554017101 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08472614995803454) - present_state_Q (-0.08472614995803454)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.44062589507145056 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07127034175372143) - present_state_Q ( -0.0763950853276954)) * f1( 0.44746132620656776)
w2 ( 0.6361818164247566 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07127034175372143) - present_state_Q (-0.0763950853276954)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.43210441079629375 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03497390193236341) - present_state_Q ( -0.03497390193236341)) * f1( 0.36813602430472503)
w2 ( 0.6408113466595391 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03497390193236341) - present_state_Q (-0.03497390193236341)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.42237424606069246 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11466568953162146) - present_state_Q ( 0.11466568953162146)) * f1( 0.3278347676922377)
w2 ( 0.6526833818364007 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11466568953162146) - present_state_Q (0.11466568953162146)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41523032612975547 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1479834298592601) - present_state_Q ( 0.1479834298592601)) * f1( 0.26774814972750466)
w2 ( 0.6633559783614674 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1479834298592601) - present_state_Q (0.1479834298592601)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41026608867566633 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17904478636096222) - present_state_Q ( 0.17904478636096222)) * f1( 0.20783068950666572)
w2 ( 0.6729103660524728 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17904478636096222) - present_state_Q (0.17904478636096222)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40698144041388495 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2114875966265033) - present_state_Q ( 0.20640981843067685)) * f1( 0.15296006597299433)
w2 ( 0.6814999237017517 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2114875966265033) - present_state_Q (0.20640981843067685)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4049350796410747 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22954039793617478) - present_state_Q ( 0.22954039793617478)) * f1( 0.10580229776752459)
w2 ( 0.6892364693760494 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22954039793617478) - present_state_Q (0.22954039793617478)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41866167899763906 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.033499201711158944) - present_state_Q ( 0.02560984183577003)) * f1( 0.617592198078564)
w2 ( 0.6803460725094632 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.033499201711158944) - present_state_Q (0.02560984183577003)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4691182135025413 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04822272763761007) - present_state_Q ( 0.04822272763761007)) * f1( 0.534836868524186)
w2 ( 0.6426100543145092 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.04822272763761007) - present_state_Q (0.04822272763761007)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4761371128111726 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1687038138758444) - present_state_Q ( 0.1687038138758444)) * f1( 0.4622762716751487)
w2 ( 0.6335000483652137 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.1687038138758444) - present_state_Q (0.1687038138758444)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4745882279393207 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1810545473366828) - present_state_Q ( 0.1810545473366828)) * f1( 0.41804235865433015)
w2 ( 0.6357231028090328 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1810545473366828) - present_state_Q (0.1810545473366828)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4714979680725341 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33999249547547594) - present_state_Q ( 0.3445745216895554)) * f1( 0.3455710675121083)
w2 ( 0.6428770810376722 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.33999249547547594) - present_state_Q (0.3445745216895554)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.46980304552174124 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3741597053629537) - present_state_Q ( 0.37854808916804744)) * f1( 0.28791974696528555)
w2 ( 0.647586511547132 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3741597053629537) - present_state_Q (0.37854808916804744)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.46862677806605274 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.40032902083404764) - present_state_Q ( 0.3950953730722061)) * f1( 0.2617561493858145)
w2 ( 0.6511815138680279 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.40032902083404764) - present_state_Q (0.3950953730722061)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.46833951776358784 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42829955283001114) - present_state_Q ( 0.42829955283001114)) * f1( 0.19769604000596158)
w2 ( 0.6523439460642672 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.42829955283001114) - present_state_Q (0.42829955283001114)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4684954600815271 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45150402372315473) - present_state_Q ( 0.4562853579997566)) * f1( 0.14004754321151705)
w2 ( 0.6514531496140719 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.45150402372315473) - present_state_Q (0.4562853579997566)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4374010072373562 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14453000625731766) - present_state_Q ( -0.14453000625731766)) * f1( 0.5866025598888579)
w2 ( 0.6620546897267036 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14453000625731766) - present_state_Q (-0.14453000625731766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4021884880875699 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14395377665158396) - present_state_Q ( -0.15335937894086035)) * f1( 0.653337125790219)
w2 ( 0.6728339697522177 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14395377665158396) - present_state_Q (-0.15335937894086035)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3718356279058481 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11005128130920122) - present_state_Q ( -0.11005128130920122)) * f1( 0.6082174962859286)
w2 ( 0.6828148928157832 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11005128130920122) - present_state_Q (-0.11005128130920122)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3461214983915426 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06996245160618822) - present_state_Q ( -0.06996245160618822)) * f1( 0.5554213062704116)
w2 ( 0.6920742169446946 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06996245160618822) - present_state_Q (-0.06996245160618822)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.325024793774132 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03190782409039858) - present_state_Q ( -0.03190782409039858)) * f1( 0.49208924689983746)
w2 ( 0.7006485577783218 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03190782409039858) - present_state_Q (-0.03190782409039858)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3052045610732005 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015495941184852063) - present_state_Q ( -0.015495941184852063)) * f1( 0.4788116344400011)
w2 ( 0.7089274847196492 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.015495941184852063) - present_state_Q (-0.015495941184852063)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2793282737829182 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03952791040157977) - present_state_Q ( -0.03952791040157977)) * f1( 0.5940717488229911)
w2 ( 0.7176389871068776 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03952791040157977) - present_state_Q (-0.03952791040157977)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2631821117616086 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12846367855245538) - present_state_Q ( 0.12846367855245538)) * f1( 0.5677617741394357)
w2 ( 0.7290142946789891 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12846367855245538) - present_state_Q (0.12846367855245538)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.25016280096826554 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15847417443043024) - present_state_Q ( 0.15847417443043024)) * f1( 0.5058533140799345)
w2 ( 0.7393092243994936 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.15847417443043024) - present_state_Q (0.15847417443043024)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24003540971062964 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1865645108273478) - present_state_Q ( 0.1865645108273478)) * f1( 0.43635256125189087)
w2 ( 0.7485929020097092 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1865645108273478) - present_state_Q (0.1865645108273478)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2321771103274294 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2100292993946538) - present_state_Q ( 0.2100292993946538)) * f1( 0.37247780032543476)
w2 ( 0.7570318472315016 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2100292993946538) - present_state_Q (0.2100292993946538)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2262061093931361 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22831193091637625) - present_state_Q ( 0.23066900626761044)) * f1( 0.3107271536080755)
w2 ( 0.7647183347044627 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22831193091637625) - present_state_Q (0.23066900626761044)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.22120398189853865 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24638183474507402) - present_state_Q ( 0.2434409254044611)) * f1( 0.27605977860127096)
w2 ( 0.7719662250272645 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24638183474507402) - present_state_Q (0.2434409254044611)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21767136778799018 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26136223775791356) - present_state_Q ( 0.26136223775791356)) * f1( 0.21439149442953867)
w2 ( 0.7785571844679796 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.26136223775791356) - present_state_Q (0.26136223775791356)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2148937361947912 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27237996140936327) - present_state_Q ( 0.27237996140936327)) * f1( 0.17936632077332276)
w2 ( 0.7847515058572425 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.27237996140936327) - present_state_Q (0.27237996140936327)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21297551336952206 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2874446052948199) - present_state_Q ( 0.28518648417049897)) * f1( 0.13362007976989165)
w2 ( 0.7904938249116018 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2874446052948199) - present_state_Q (0.28518648417049897)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21155569354322382 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29388215491039155) - present_state_Q ( 0.29388215491039155)) * f1( 0.10477906450931287)
w2 ( 0.7959140673348277 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.29388215491039155) - present_state_Q (0.29388215491039155)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.20964943895087726 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14642706047238127) - present_state_Q ( 0.14426729259175713)) * f1( 0.0705039917640456)
w2 ( 0.8013215756039374 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14642706047238127) - present_state_Q (0.14426729259175713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2229406130137036 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18600938914343132) - present_state_Q ( 0.025745074022643838)) * f1( 0.6416389272077313)
w2 ( 0.7971786929017713 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18600938914343132) - present_state_Q (0.025745074022643838)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30425333265122567 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5016141920907786) - present_state_Q ( 0.5038256410849039)) * f1( 0.6006860366364994)
w2 ( 0.6888855551517052 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.5016141920907786) - present_state_Q (0.5038256410849039)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.43246752478968215 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3788142644627608) - present_state_Q ( 0.3848992447396773)) * f1( 0.5462855507065792)
w2 ( 0.5011241296882332 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.3788142644627608) - present_state_Q (0.3848992447396773)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4507282458183822 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18210099179997313) - present_state_Q ( 0.18643115455210058)) * f1( 0.4959173507948054)
w2 ( 0.4716664452584649 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18210099179997313) - present_state_Q (0.18643115455210058)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.43964935341973793 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17296005167377995) - present_state_Q ( 0.17296005167377995)) * f1( 0.4534286600164452)
w2 ( 0.4912133215379528 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17296005167377995) - present_state_Q (0.17296005167377995)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.42790969622221164 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.215930958291226) - present_state_Q ( 0.12228554454239601)) * f1( 0.39222723072162247)
w2 ( 0.5091717746151564 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.215930958291226) - present_state_Q (0.12228554454239601)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4186019094868024 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2502252503843916) - present_state_Q ( 0.15693577483611013)) * f1( 0.3471930906090836)
w2 ( 0.5252569796272961 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2502252503843916) - present_state_Q (0.15693577483611013)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4116307254162366 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2886527906397505) - present_state_Q ( 0.19197067459214742)) * f1( 0.29427365330285)
w2 ( 0.5394706558956058 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2886527906397505) - present_state_Q (0.19197067459214742)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40894981282929177 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3263040591078748) - present_state_Q ( 0.3270523463485861)) * f1( 0.25392705625219025)
w2 ( 0.547916900660582 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3263040591078748) - present_state_Q (0.3270523463485861)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4088921611361405 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44197181657040524) - present_state_Q ( 0.44197181657040524)) * f1( 0.25906622467241835)
w2 ( 0.5481394371692455 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.44197181657040524) - present_state_Q (0.44197181657040524)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.39357056127209566 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049476726283086886) - present_state_Q ( 0.04513882177806822)) * f1( 0.4258260970467903)
w2 ( 0.5625317912032551 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.049476726283086886) - present_state_Q (0.04513882177806822)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3765246887425716 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06635511375271541) - present_state_Q ( -0.0382805816714685)) * f1( 0.3831255554906017)
w2 ( 0.5714301130641899 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06635511375271541) - present_state_Q (-0.0382805816714685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3660516085133698 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09700525743415542) - present_state_Q ( 0.10088043940378641)) * f1( 0.3391320931658529)
w2 ( 0.583782916517775 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09700525743415542) - present_state_Q (0.10088043940378641)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3571033937947553 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.128465994892156) - present_state_Q ( 0.12120165662749488)) * f1( 0.30681878556890163)
w2 ( 0.5954487142322439 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.128465994892156) - present_state_Q (0.12120165662749488)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.34102202292329564 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015036864110396383) - present_state_Q ( -0.018595019639852065)) * f1( 0.3855599383226108)
w2 ( 0.6037905408968202 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.015036864110396383) - present_state_Q (-0.018595019639852065)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30766435639499295 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10802625181699523) - present_state_Q ( -0.10802625181699523)) * f1( 0.6708785492361546)
w2 ( 0.6137350134295261 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10802625181699523) - present_state_Q (-0.10802625181699523)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2777862115205092 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06784913011777477) - present_state_Q ( -0.07400185877001247)) * f1( 0.6394918922727691)
w2 ( 0.6230793523446908 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06784913011777477) - present_state_Q (-0.07400185877001247)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26017910520198795 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09115162096682858) - present_state_Q ( 0.09401225001718228)) * f1( 0.5587732021365434)
w2 ( 0.6356834688278709 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09115162096682858) - present_state_Q (0.09401225001718228)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2458018051585114 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12163587923638144) - present_state_Q ( 0.12431884806681989)) * f1( 0.4994810761741967)
w2 ( 0.6471972584221436 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12163587923638144) - present_state_Q (0.12431884806681989)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23378143145718183 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14543463265233653) - present_state_Q ( 0.1480205387513963)) * f1( 0.45100712155458483)
w2 ( 0.6578581754026971 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14543463265233653) - present_state_Q (0.1480205387513963)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.22078799698261553 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14893072368448285) - present_state_Q ( 0.14893072368448285)) * f1( 0.48854413186153567)
w2 ( 0.6684966693500557 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14893072368448285) - present_state_Q (0.14893072368448285)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.20982508444034398 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17198686368901633) - present_state_Q ( 0.1696290317535567)) * f1( 0.4428213368599191)
w2 ( 0.6783994555346695 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17198686368901633) - present_state_Q (0.1696290317535567)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.19829260510138558 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17249369904563722) - present_state_Q ( 0.17249369904563722)) * f1( 0.47118333554794556)
w2 ( 0.6881896823690266 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17249369904563722) - present_state_Q (0.17249369904563722)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1888640869630132 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.192743747190461) - present_state_Q ( 0.192743747190461)) * f1( 0.41621383568465165)
w2 ( 0.6972509074701699 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.192743747190461) - present_state_Q (0.192743747190461)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18268233339764747 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21921933233978222) - present_state_Q ( 0.2208391418039779)) * f1( 0.30742330168604615)
w2 ( 0.7052942191273699 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21921933233978222) - present_state_Q (0.2208391418039779)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1779057310753051 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23407812559261448) - present_state_Q ( 0.2356442911469501)) * f1( 0.2543945856156682)
w2 ( 0.7128047599838624 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.23407812559261448) - present_state_Q (0.2356442911469501)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17138623444641585 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2457835833087382) - present_state_Q ( 0.10613794292244083)) * f1( 0.2047320727341509)
w2 ( 0.719173568292031 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2457835833087382) - present_state_Q (0.10613794292244083)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16908460652153467 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2617902583018808) - present_state_Q ( 0.2634314426193086)) * f1( 0.1414231707452671)
w2 ( 0.7256834716204662 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2617902583018808) - present_state_Q (0.2634314426193086)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1670017358383348 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2698807847848409) - present_state_Q ( 0.26810703333577324)) * f1( 0.13109623500581724)
w2 ( 0.7320387134261747 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2698807847848409) - present_state_Q (0.26810703333577324)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16492276592848634 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2689449358004064) - present_state_Q ( 0.2706013567894137)) * f1( 0.1330173514038226)
w2 ( 0.7382904388977998 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2689449358004064) - present_state_Q (0.2706013567894137)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1615851996623495 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1283117576179201) - present_state_Q ( 0.1283117576179201)) * f1( 0.11730539475689361)
w2 ( 0.7439808272606772 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1283117576179201) - present_state_Q (0.1283117576179201)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.18423512960750368 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4880659376185883) - present_state_Q ( 0.19047360671431737)) * f1( 0.6629241069961246)
w2 ( 0.7303141467425788 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.4880659376185883) - present_state_Q (0.19047360671431737)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2107293322544428 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.468521149701737) - present_state_Q ( 0.4684825734555776)) * f1( 0.6283749694486619)
w2 ( 0.6965837100637464 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.468521149701737) - present_state_Q (0.4684825734555776)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34656527244198176 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4376950934622367) - present_state_Q ( 0.4376950934622367)) * f1( 0.5674192259309434)
w2 ( 0.5050696633344653 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.4376950934622367) - present_state_Q (0.4376950934622367)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.45817225796402794 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2287067762849938) - present_state_Q ( 0.2287067762849938)) * f1( 0.5059622770251266)
w2 ( 0.3286027754419458 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2287067762849938) - present_state_Q (0.2287067762849938)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4604643362632386 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.056552882335722254) - present_state_Q ( 0.056552882335722254)) * f1( 0.45033136430978277)
w2 ( 0.32453096791377384 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.056552882335722254) - present_state_Q (0.056552882335722254)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.44938891027931116 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1393203394287736) - present_state_Q ( 0.13902176091500032)) * f1( 0.40287421280922275)
w2 ( 0.3520219952165615 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1393203394287736) - present_state_Q (0.13902176091500032)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4414729680311275 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19914037991415584) - present_state_Q ( 0.19432851395516693)) * f1( 0.3509064813445941)
w2 ( 0.3745805476201864 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.19914037991415584) - present_state_Q (0.19432851395516693)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43631964954681646 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24183171676060178) - present_state_Q ( 0.24652371252114882)) * f1( 0.29006721673162217)
w2 ( 0.3923464935356775 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24183171676060178) - present_state_Q (0.24652371252114882)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43287733349313784 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2822628771159178) - present_state_Q ( 0.2864262224934774)) * f1( 0.2427584252788392)
w2 ( 0.40652650005748897 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2822628771159178) - present_state_Q (0.2864262224934774)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4306330240925759 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3242912803486975) - present_state_Q ( 0.32006532377195973)) * f1( 0.19973597505746932)
w2 ( 0.41776288048377996 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3242912803486975) - present_state_Q (0.32006532377195973)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4295422774328347 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3531060029747567) - present_state_Q ( 0.35744190741668236)) * f1( 0.14007512125714267)
w2 ( 0.4255497497718593 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3531060029747567) - present_state_Q (0.35744190741668236)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4173164533244962 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0340849289339529) - present_state_Q ( 0.029696882156326154)) * f1( 0.3271459531114267)
w2 ( 0.44049821420134205 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0340849289339529) - present_state_Q (0.029696882156326154)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.375592127207666 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1996619817553065) - present_state_Q ( -0.20800664965440382)) * f1( 0.7095485695226745)
w2 ( 0.45225902323091954 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1996619817553065) - present_state_Q (-0.20800664965440382)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3421074415065633 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1461549740429399) - present_state_Q ( -0.1461549740429399)) * f1( 0.6299567044926456)
w2 ( 0.46288981276369245 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1461549740429399) - present_state_Q (-0.1461549740429399)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3211877017513296 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06726961507024654) - present_state_Q ( 0.0670435976418989)) * f1( 0.6158600031863806)
w2 ( 0.4832708145956 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06726961507024654) - present_state_Q (0.0670435976418989)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.30528261579678456 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1167489202004722) - present_state_Q ( 0.1167489202004722)) * f1( 0.5392907873259527)
w2 ( 0.5009663729047745 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1167489202004722) - present_state_Q (0.1167489202004722)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2925367966265578 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15249293868673247) - present_state_Q ( 0.15249293868673247)) * f1( 0.48508128990452665)
w2 ( 0.516731754215691 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.15249293868673247) - present_state_Q (0.15249293868673247)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2820987204938174 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18116977854357103) - present_state_Q ( 0.18116977854357103)) * f1( 0.4405232964602178)
w2 ( 0.5309485861743382 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18116977854357103) - present_state_Q (0.18116977854357103)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.27325116170619856 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2034809487269338) - present_state_Q ( 0.2034809487269338)) * f1( 0.40797137532636)
w2 ( 0.5439606149430838 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2034809487269338) - present_state_Q (0.2034809487269338)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2663722516541783 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22930069642102252) - present_state_Q ( 0.22930069642102252)) * f1( 0.3552617011348855)
w2 ( 0.5555783773363486 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22930069642102252) - present_state_Q (0.22930069642102252)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.26162278770365005 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25796595257468047) - present_state_Q ( 0.25796595257468047)) * f1( 0.2829914653610143)
w2 ( 0.5656482158973158 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25796595257468047) - present_state_Q (0.25796595257468047)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.25830948663506376 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2758783163035673) - present_state_Q ( 0.2804684203669114)) * f1( 0.22521168621679713)
w2 ( 0.5744753805731225 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2758783163035673) - present_state_Q (0.2804684203669114)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2558102081907918 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2963504587349841) - present_state_Q ( 0.2962756434171945)) * f1( 0.18740924136120282)
w2 ( 0.5824769447205008 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2963504587349841) - present_state_Q (0.2962756434171945)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2542899898612378 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3158698809524272) - present_state_Q ( 0.3158772854312382)) * f1( 0.13138209627661027)
w2 ( 0.589419526880341 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3158698809524272) - present_state_Q (0.3158772854312382)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2489055386963393 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0765330105123869) - present_state_Q ( 0.0765330105123869)) * f1( 0.1626131444900599)
w2 ( 0.596041932691118 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0765330105123869) - present_state_Q (0.0765330105123869)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24381960265629096 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0805545222590731) - present_state_Q ( 0.0805545222590731)) * f1( 0.15529531597248866)
w2 ( 0.6025919512904547 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0805545222590731) - present_state_Q (0.0805545222590731)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23810486496852332 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03695396982267951) - present_state_Q ( 0.03695396982267951)) * f1( 0.3427305250481071)
w2 ( 0.6059267798336465 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03695396982267951) - present_state_Q (0.03695396982267951)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3165927033566409 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028945132635759346) - present_state_Q ( 0.028945132635759346)) * f1( 0.38739327456901734)
w2 ( 0.5654057674462029 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.028945132635759346) - present_state_Q (0.028945132635759346)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32968703119309695 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012213897515807814) - present_state_Q ( 0.012213897515807814)) * f1( 0.31860259223917126)
w2 ( 0.5571859172909183 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.012213897515807814) - present_state_Q (0.012213897515807814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3353503946392335 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028786918352255303) - present_state_Q ( 0.028786918352255303)) * f1( 0.2506931037196919)
w2 ( 0.5526677527605777 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.028786918352255303) - present_state_Q (0.028786918352255303)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3320585864799348 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04293946869695507) - present_state_Q ( 0.0423622618348895)) * f1( 0.20328375874006058)
w2 ( 0.5559063864612738 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04293946869695507) - present_state_Q (0.0423622618348895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32744092132637237 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06614532853976472) - present_state_Q ( 0.06614532853976472)) * f1( 0.13562651467593176)
w2 ( 0.5627157705475581 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06614532853976472) - present_state_Q (0.06614532853976472)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3211486015572295 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05036166616111975) - present_state_Q ( 0.053870968154952334)) * f1( 0.17918403636568858)
w2 ( 0.5697390745167813 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05036166616111975) - present_state_Q (0.053870968154952334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.31511984865864684 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06161049632403963) - present_state_Q ( 0.058291206523339044)) * f1( 0.1733048442687958)
w2 ( 0.5766964713789626 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06161049632403963) - present_state_Q (0.058291206523339044)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30952815499836006 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07027632818803961) - present_state_Q ( 0.06397530370028179)) * f1( 0.1629982712740851)
w2 ( 0.583557517961333 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07027632818803961) - present_state_Q (0.06397530370028179)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3061939518283497 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08482502406770456) - present_state_Q ( 0.08482502406770456)) * f1( 0.10301641065489175)
w2 ( 0.5900306675281144 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08482502406770456) - present_state_Q (0.08482502406770456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3994061404747641 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5925124804935533) - present_state_Q ( 0.4717246659426597)) * f1( 0.38637602369029245)
w2 ( 0.34878332573878396 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5925124804935533) - present_state_Q (0.4717246659426597)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46785051725298044 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30160629198548966) - present_state_Q ( 0.29799734243839016)) * f1( 0.3018046951027458)
w2 ( 0.07664292015000296 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.30160629198548966) - present_state_Q (0.29799734243839016)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5141016637144226 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00093874990730336) - present_state_Q ( -0.00093874990730336)) * f1( 0.23135346467679466)
w2 ( -0.20323879736167683 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.00093874990730336) - present_state_Q (-0.00093874990730336)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5003470868346275 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32835927641149576) - present_state_Q ( -0.3792682957017397)) * f1( 0.1842709060906983)
w2 ( -0.09873826583319417 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.32835927641149576) - present_state_Q (-0.3792682957017397)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4930424427187888 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18318857984819725) - present_state_Q ( -0.18318857984819725)) * f1( 0.12931555424394722)
w2 ( -0.030953899209588853 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18318857984819725) - present_state_Q (-0.18318857984819725)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4835040092403563 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09710575047680481) - present_state_Q ( -0.10695907128098446)) * f1( 0.19182428002672391)
w2 ( -0.01106395936025669 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09710575047680481) - present_state_Q (-0.10695907128098446)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4719582373434359 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11340269095409972) - present_state_Q ( -0.11340269095409972)) * f1( 0.2299668605783461)
w2 ( -0.0010227109230828944 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11340269095409972) - present_state_Q (-0.11340269095409972)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45961941240533133 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11571541801635632) - present_state_Q ( -0.11571541801635632)) * f1( 0.24474808720773417)
w2 ( 0.009060166601211523 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11571541801635632) - present_state_Q (-0.11571541801635632)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4502084445075825 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08838671869193303) - present_state_Q ( -0.08838671869193303)) * f1( 0.1962466109517377)
w2 ( 0.018651127537666318 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08838671869193303) - present_state_Q (-0.08838671869193303)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4355043561609336 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1253575860510474) - present_state_Q ( -0.1253575860510474)) * f1( 0.28672898772427746)
w2 ( 0.028907564086585173 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1253575860510474) - present_state_Q (-0.1253575860510474)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4228443834317737 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10559400048206809) - present_state_Q ( -0.10559400048206809)) * f1( 0.255739148699188)
w2 ( 0.0388082560952624 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10559400048206809) - present_state_Q (-0.10559400048206809)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.41342896970658316 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07478289882409511) - present_state_Q ( -0.07046701622173586)) * f1( 0.20336161961511645)
w2 ( 0.05732780514883545 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07478289882409511) - present_state_Q (-0.07046701622173586)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40663446753516674 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04544767630814549) - present_state_Q ( -0.041374166057818254)) * f1( 0.15554132107140647)
w2 ( 0.0748009810859156 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04544767630814549) - present_state_Q (-0.041374166057818254)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40238374645811076 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016230550742418366) - present_state_Q ( -0.012180637390875639)) * f1( 0.103535320260575)
w2 ( 0.09122328437858096 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.016230550742418366) - present_state_Q (-0.012180637390875639)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3915279971353213 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07098160514809825) - present_state_Q ( -0.0750934081066063)) * f1( 0.23196281113218184)
w2 ( 0.10058318933041689 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07098160514809825) - present_state_Q (-0.0750934081066063)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.38307701076617984 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04934213983303436) - present_state_Q ( -0.053626501075924926)) * f1( 0.18834703898970712)
w2 ( 0.10955703507226933 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04934213983303436) - present_state_Q (-0.053626501075924926)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37642105980706425 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03694057853036033) - present_state_Q ( -0.03694057853036033)) * f1( 0.15362964597407255)
w2 ( 0.11822196548581582 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03694057853036033) - present_state_Q (-0.03694057853036033)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37240488100659885 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013070256650188627) - present_state_Q ( -0.013070256650188627)) * f1( 0.09753612023240676)
w2 ( 0.1264572301055192 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.013070256650188627) - present_state_Q (-0.013070256650188627)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3739820041959831 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004711011278986224) - present_state_Q ( -0.004711011278986224)) * f1( 0.08056408181062062)
w2 ( 0.12254202830854095 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.004711011278986224) - present_state_Q (-0.004711011278986224)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4246270752310684 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05824860961084133) - present_state_Q ( 0.054603424286556776)) * f1( 0.24719641225101854)
w2 ( -0.12331139929051584 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05824860961084133) - present_state_Q (0.054603424286556776)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4140976683800309 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22246446570514733) - present_state_Q ( -0.22246446570514733)) * f1( 0.17542637034153508)
w2 ( -0.0512852369943599 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22246446570514733) - present_state_Q (-0.22246446570514733)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.40859195278401855 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08763936552827459) - present_state_Q ( -0.09789641292714657)) * f1( 0.11256082680960684)
w2 ( -0.002371989356927985 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08763936552827459) - present_state_Q (-0.09789641292714657)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3998715584421926 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07682143404778108) - present_state_Q ( -0.07729583191916668)) * f1( 0.18569293347075808)
w2 ( 0.025804831953935334 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07682143404778108) - present_state_Q (-0.07729583191916668)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3923382901983813 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056051921235037994) - present_state_Q ( -0.05199879604762522)) * f1( 0.16875842703812083)
w2 ( 0.05258844818938263 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.056051921235037994) - present_state_Q (-0.05199879604762522)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38643378880264023 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01552141273570156) - present_state_Q ( -0.023365990444690045)) * f1( 0.13997884155163762)
w2 ( 0.07789727913964983 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.01552141273570156) - present_state_Q (-0.023365990444690045)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3841085365698635 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02304593464568234) - present_state_Q ( 0.02304593464568234)) * f1( 0.061310458672670015)
w2 ( 0.10065279866878299 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.02304593464568234) - present_state_Q (0.02304593464568234)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3786248072749173 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02927401333293046) - present_state_Q ( -0.02927401333293046)) * f1( 0.1286213878709283)
w2 ( 0.10917973090877574 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02927401333293046) - present_state_Q (-0.02927401333293046)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37551385770664875 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0071451996704072415) - present_state_Q ( -0.0071451996704072415)) * f1( 0.07654317756078603)
w2 ( 0.11730834450284307 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0071451996704072415) - present_state_Q (-0.0071451996704072415)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37341248333122806 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0035743494302744594) - present_state_Q ( 0.0035743494302744594)) * f1( 0.05296028112451211)
w2 ( 0.1252440062130981 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0035743494302744594) - present_state_Q (0.0035743494302744594)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37192255843142874 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01079369050217731) - present_state_Q ( 0.01079369050217731)) * f1( 0.038175238849199385)
w2 ( 0.1330497197840589 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.01079369050217731) - present_state_Q (0.01079369050217731)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3714017667862488 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004440371302893939) - present_state_Q ( -0.004790304174218767)) * f1( 0.012879843036200113)
w2 ( 0.1330497197840589 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.004440371302893939) - present_state_Q (-0.004790304174218767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37026554355910796 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011273784550175914) - present_state_Q ( 0.01566225281805171)) * f1( 0.02947668029016337)
w2 ( 0.14075902229679824 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.011273784550175914) - present_state_Q (0.01566225281805171)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37133168176267 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19918759140009218) - present_state_Q ( -0.20282836369127122)) * f1( 0.6238230161261493)
w2 ( 0.14041721438782348 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19918759140009218) - present_state_Q (-0.20282836369127122)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.340439319718678 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1481578620363362) - present_state_Q ( -0.17624130491390091)) * f1( 0.5502486262996975)
w2 ( 0.15164572476202884 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1481578620363362) - present_state_Q (-0.17624130491390091)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3167878109942865 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10281963332765459) - present_state_Q ( -0.10281963332765459)) * f1( 0.4801969507151997)
w2 ( 0.1713472315618244 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10281963332765459) - present_state_Q (-0.10281963332765459)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3019310687009274 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04155161956651557) - present_state_Q ( 0.04155161956651557)) * f1( 0.40972413549601444)
w2 ( 0.20760758580083802 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.04155161956651557) - present_state_Q (0.04155161956651557)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2892616351056739 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1016284067309631) - present_state_Q ( 0.08863509616186835)) * f1( 0.3940385802324166)
w2 ( 0.23976036025196082 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1016284067309631) - present_state_Q (0.08863509616186835)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.28175928696162783 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1561277352108212) - present_state_Q ( 0.1561277352108212)) * f1( 0.2891244980019099)
w2 ( 0.2657088640829869 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1561277352108212) - present_state_Q (0.1561277352108212)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2772114181822061 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2033086933957671) - present_state_Q ( 0.20593947507104282)) * f1( 0.2121292598958199)
w2 ( 0.28714800350984027 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2033086933957671) - present_state_Q (0.20593947507104282)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.27432472249169976 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23996255964121305) - present_state_Q ( 0.24294837064412744)) * f1( 0.15944376734388765)
w2 ( 0.30525279204183964 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.23996255964121305) - present_state_Q (0.24294837064412744)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2726052478397238 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2721336930882652) - present_state_Q ( 0.2743878866302859)) * f1( 0.11251229977089516)
w2 ( 0.3205353403096937 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2721336930882652) - present_state_Q (0.2743878866302859)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2718175734434908 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3035963522267629) - present_state_Q ( 0.3035963522267629)) * f1( 0.062137424782409134)
w2 ( 0.3332116686092851 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3035963522267629) - present_state_Q (0.3035963522267629)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2678579277610145 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10173876085854328) - present_state_Q ( 0.09872660913589697)) * f1( 0.12713695391369342)
w2 ( 0.3456695592872834 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10173876085854328) - present_state_Q (0.09872660913589697)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2595322298441204 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01728003523354285) - present_state_Q ( 0.011923129271353174)) * f1( 0.21358629578120059)
w2 ( 0.3534656567723234 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.01728003523354285) - present_state_Q (0.011923129271353174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24629043940725762 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007881855467765778) - present_state_Q ( -0.012732498942069695)) * f1( 0.32144612769921205)
w2 ( 0.36170454304022925 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.007881855467765778) - present_state_Q (-0.012732498942069695)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23525520678105677 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0038070497267080755) - present_state_Q ( 0.0038070497267080755)) * f1( 0.27826438998719105)
w2 ( 0.3696360161451485 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0038070497267080755) - present_state_Q (0.0038070497267080755)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.22521929667426907 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013108415210445507) - present_state_Q ( 0.013108415210445507)) * f1( 0.25852260126674254)
w2 ( 0.3774000646713605 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.013108415210445507) - present_state_Q (0.013108415210445507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21648847777566257 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02356847041901146) - present_state_Q ( 0.02356847041901146)) * f1( 0.23049331598943518)
w2 ( 0.3849758322038183 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.02356847041901146) - present_state_Q (0.02356847041901146)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20677414665593874 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021718001065643693) - present_state_Q ( 0.021718001065643693)) * f1( 0.25533536908325094)
w2 ( 0.3925849081846367 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.021718001065643693) - present_state_Q (0.021718001065643693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20060661925007867 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04329553747689573) - present_state_Q ( 0.04320291001026825)) * f1( 0.17078572054474414)
w2 ( 0.39980744105938515 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.04329553747689573) - present_state_Q (0.04320291001026825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19508026313830257 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.048825374890506) - present_state_Q ( 0.048825374890506)) * f1( 0.1552098003434093)
w2 ( 0.40692858431135603 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.048825374890506) - present_state_Q (0.048825374890506)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1901019388626966 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05669084935673059) - present_state_Q ( 0.05378638319592656)) * f1( 0.1414768117612085)
w2 ( 0.413966238346151 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05669084935673059) - present_state_Q (0.05378638319592656)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.187938254872908 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15006171239565802) - present_state_Q ( 0.15006171239565802)) * f1( 0.08166556867163424)
w2 ( 0.4245640166999073 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.15006171239565802) - present_state_Q (0.15006171239565802)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18442699105053756 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06933953832225075) - present_state_Q ( 0.06558088971330929)) * f1( 0.10286311129016952)
w2 ( 0.4313910779822856 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06933953832225075) - present_state_Q (0.06558088971330929)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19688709358833784 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3148042409364716) - present_state_Q ( 0.2284405642614571)) * f1( 0.6326205153582987)
w2 ( 0.4156342667688608 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.3148042409364716) - present_state_Q (0.2284405642614571)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.20978770794448215 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4662270212688421) - present_state_Q ( 0.4662270212688421)) * f1( 0.5874481160730285)
w2 ( 0.3848896620889867 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.4662270212688421) - present_state_Q (0.4662270212688421)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20924895443054708 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4302672035787568) - present_state_Q ( 0.43240792897129476)) * f1( 0.50735860073105)
w2 ( 0.386376292883108 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4302672035787568) - present_state_Q (0.43240792897129476)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2091457937963658 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44201947489426313) - present_state_Q ( 0.44201947489426313)) * f1( 0.4726777986119733)
w2 ( 0.38668183904643083 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.44201947489426313) - present_state_Q (0.44201947489426313)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20951232079237472 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45421950889293417) - present_state_Q ( 0.45421950889293417)) * f1( 0.41662356287646846)
w2 ( 0.38545018092592115 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.45421950889293417) - present_state_Q (0.45421950889293417)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2100931341169667 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4594789239439714) - present_state_Q ( 0.46152854325587406)) * f1( 0.3727786019697328)
w2 ( 0.38326888980531437 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4594789239439714) - present_state_Q (0.46152854325587406)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.21082136430477869 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.46738587463137127) - present_state_Q ( 0.46957248616245556)) * f1( 0.3189250322082441)
w2 ( 0.3800721439874098 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.46738587463137127) - present_state_Q (0.46957248616245556)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20958949830916923 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.472322458944096) - present_state_Q ( 0.4005063402770676)) * f1( 0.2636366228399576)
w2 ( 0.38567925266149083 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.472322458944096) - present_state_Q (0.4005063402770676)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2105625626099992 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.49233864349114503) - present_state_Q ( 0.4903548401682767)) * f1( 0.23663453540334584)
w2 ( 0.37992231604680815 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.49233864349114503) - present_state_Q (0.4903548401682767)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.21136760482327105 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4954179070517159) - present_state_Q ( 0.49120696225051724)) * f1( 0.1932170643761067)
w2 ( 0.37408919203045976 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4954179070517159) - present_state_Q (0.49120696225051724)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.211993143804156 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.49419653193606156) - present_state_Q ( 0.49419653193606156)) * f1( 0.13970133659446718)
w2 ( 0.367820429006516 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.49419653193606156) - present_state_Q (0.49419653193606156)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.18052959970514984 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0025532330294018857) - present_state_Q ( -0.06867554943238897)) * f1( 0.6709633749528067)
w2 ( 0.3771990464612226 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0025532330294018857) - present_state_Q (-0.06867554943238897)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15139206232420732 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029209100057395576) - present_state_Q ( -0.04262249241053645)) * f1( 0.6539775299762829)
w2 ( 0.3861099145095481 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.029209100057395576) - present_state_Q (-0.04262249241053645)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12990456892372526 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057252015744611084) - present_state_Q ( 0.06027574796279149)) * f1( 0.6220155561350751)
w2 ( 0.3999278926540149 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.057252015744611084) - present_state_Q (0.06027574796279149)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11134775787529518 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08403664088287) - present_state_Q ( 0.08535123922942947)) * f1( 0.57442104192648)
w2 ( 0.4128499896483692 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08403664088287) - present_state_Q (0.08535123922942947)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09558848524035894 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10730785758860328) - present_state_Q ( 0.10730785758860328)) * f1( 0.5193830515699649)
w2 ( 0.42498690677517953 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10730785758860328) - present_state_Q (0.10730785758860328)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08124094889113453 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12456948770589832) - present_state_Q ( 0.1226685343839305)) * f1( 0.49510386326489736)
w2 ( 0.4365784433506459 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12456948770589832) - present_state_Q (0.1226685343839305)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07037693921658048 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14218658363017725) - present_state_Q ( 0.14218658363017725)) * f1( 0.39936502654047246)
w2 ( 0.4474597263399595 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14218658363017725) - present_state_Q (0.14218658363017725)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06125197610648053 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1543841409718493) - present_state_Q ( 0.1543841409718493)) * f1( 0.3495427598581742)
w2 ( 0.45790189726497293 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1543841409718493) - present_state_Q (0.1543841409718493)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05351397248042797 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16558081970894492) - present_state_Q ( 0.1643667721669793)) * f1( 0.3068307005530463)
w2 ( 0.4679895496571295 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.16558081970894492) - present_state_Q (0.1643667721669793)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.048120767647842275 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17528183966139105) - present_state_Q ( 0.17528183966139105)) * f1( 0.22263307411570246)
w2 ( 0.47767940342931947 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17528183966139105) - present_state_Q (0.17528183966139105)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0445802912675758 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18380873174596501) - present_state_Q ( 0.18380873174596501)) * f1( 0.15093336995193274)
w2 ( 0.48706228908646476 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18380873174596501) - present_state_Q (0.18380873174596501)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.041088126271111525 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18857746601197817) - present_state_Q ( 0.1880789858029869)) * f1( 0.15132090077898322)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18857746601197817) - present_state_Q (0.1880789858029869)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03169005188717525 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009452686778321442) - present_state_Q ( -0.009452686778321442)) * f1( 0.2300588426921646)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.009452686778321442) - present_state_Q (-0.009452686778321442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.007955210508061465 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01806933215108104) - present_state_Q ( -0.01806933215108104)) * f1( 0.5701894151329421)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.01806933215108104) - present_state_Q (-0.01806933215108104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.015504651265915059 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004617725840685779) - present_state_Q ( -0.004617725840685779)) * f1( 0.5804655748589401)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.004617725840685779) - present_state_Q (-0.004617725840685779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03942093796438138 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11163874029570388) - present_state_Q ( 0.11163874029570388)) * f1( 0.7984734503020494)
w2 ( 0.5022839421930706 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11163874029570388) - present_state_Q (0.11163874029570388)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0633888457857827 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13430691062781272) - present_state_Q ( 0.13430691062781272)) * f1( 0.8586838349656652)
w2 ( 0.50786641780177 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.13430691062781272) - present_state_Q (0.13430691062781272)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08655981742398318 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1592437900113608) - present_state_Q ( 0.15866715802135867)) * f1( 0.9006927599525731)
w2 ( 0.5130115622213656 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1592437900113608) - present_state_Q (0.15866715802135867)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1090048644862839 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18619120310540443) - present_state_Q ( 0.18619120310540443)) * f1( 0.965677760752431)
w2 ( 0.5176601205654683 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18619120310540443) - present_state_Q (0.18619120310540443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1300705770549889 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21447972707131685) - present_state_Q ( 0.21447972707131685)) * f1( 1.0178234107358004)
w2 ( 0.5217994854781846 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21447972707131685) - present_state_Q (0.21447972707131685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1491135049420526 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2369373699803818) - present_state_Q ( 0.2371143824517675)) * f1( 1.020634246129369)
w2 ( 0.5255310725691099 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2369373699803818) - present_state_Q (0.2371143824517675)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1663018089143689 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25724921374549475) - present_state_Q ( 0.25710856463093273)) * f1( 1.0193734643698489)
w2 ( 0.5289033997039823 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25724921374549475) - present_state_Q (0.25710856463093273)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18182034255538623 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2742049490494577) - present_state_Q ( 0.27404642388000394)) * f1( 1.0118094627933352)
w2 ( 0.5319708811244811 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2742049490494577) - present_state_Q (0.27404642388000394)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19583303345306036 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2911747705344102) - present_state_Q ( 0.29141295311385607)) * f1( 1.0175911797801134)
w2 ( 0.5347249716032728 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2911747705344102) - present_state_Q (0.29141295311385607)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.20848872073874583 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3063325032762877) - present_state_Q ( 0.3063325032762877)) * f1( 1.0181505409986142)
w2 ( 0.5372109865442997 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3063325032762877) - present_state_Q (0.3063325032762877)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2199103614231936 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31957871235903385) - present_state_Q ( 0.31984754772718355)) * f1( 1.0187858108855954)
w2 ( 0.539453193014474 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.31957871235903385) - present_state_Q (0.31984754772718355)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2318159747884683 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33116585981906543) - present_state_Q ( 0.25433877913766956)) * f1( 0.6659447039557687)
w2 ( 0.5430287491513588 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.33116585981906543) - present_state_Q (0.25433877913766956)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.24093991133417755 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3450530641574562) - present_state_Q ( 0.3450530641574562)) * f1( 1.0199785176277958)
w2 ( 0.5448177939965246 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3450530641574562) - present_state_Q (0.3450530641574562)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.24921900579509612 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3542471585706867) - present_state_Q ( 0.35402676476653505)) * f1( 1.0171133732482107)
w2 ( 0.5464457530183353 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3542471585706867) - present_state_Q (0.35402676476653505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.25668932081685386 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36286806460548815) - present_state_Q ( 0.36286806460548815)) * f1( 1.017494284566361)
w2 ( 0.5479141278554365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.36286806460548815) - present_state_Q (0.36286806460548815)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.26339385039701796 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37103042387294843) - present_state_Q ( 0.37136110404242784)) * f1( 1.0198253579007193)
w2 ( 0.5492289666223338 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.37103042387294843) - present_state_Q (0.37136110404242784)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2490999353770471 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37814167708368074) - present_state_Q ( 0.37814167708368074)) * f1( 1.0186110395318915)
w2 ( 0.5464224164348275 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.37814167708368074) - present_state_Q (0.37814167708368074)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.23609185132650495 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3633790884253367) - present_state_Q ( 0.3637006200778246)) * f1( 1.0213416410797747)
w2 ( 0.5438751622101217 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.3633790884253367) - present_state_Q (0.3637006200778246)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16319674192223652 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3498168710986534) - present_state_Q ( 0.3496015503782833)) * f1( 1.0200543414910417)
w2 ( 0.5295827649447533 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3498168710986534) - present_state_Q (0.3496015503782833)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0464176508967683 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2723455480197595) - present_state_Q ( 0.2723455480197595)) * f1( 1.019805867877635)
w2 ( 0.5066805450803976 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.2723455480197595) - present_state_Q (0.2723455480197595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.058847810175996246 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14864568466930633) - present_state_Q ( 0.14860314105984845)) * f1( 1.0182986672222947)
w2 ( 0.48600577362853925 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.14864568466930633) - present_state_Q (0.14860314105984845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05032784140854994 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.062354241724235565) - present_state_Q ( 0.062354241724235565)) * f1( 0.5921530962198179)
w2 ( 0.488883397277503 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.062354241724235565) - present_state_Q (0.062354241724235565)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04290604283901264 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07093050116401399) - present_state_Q ( 0.07044249784026134)) * f1( 0.5431224715828088)
w2 ( 0.49161640832302583 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07093050116401399) - present_state_Q (0.07044249784026134)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0357224735471057 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27529331932298595) - present_state_Q ( 0.27479040900763885)) * f1( 0.4703168749887231)
w2 ( 0.5007807436985054 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.27529331932298595) - present_state_Q (0.27479040900763885)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0820690876351184 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28616567770983103) - present_state_Q ( 0.28616567770983103)) * f1( 0.4003857261007366)
w2 ( 0.43132779710217456 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.28616567770983103) - present_state_Q (0.28616567770983103)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1197464955897118 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23112682579153324) - present_state_Q ( 0.2308834067899675)) * f1( 0.340119188304377)
w2 ( 0.36486155364952566 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.23112682579153324) - present_state_Q (0.2308834067899675)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13327749253061225 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25575160677004594) - present_state_Q ( 0.25407628583815217)) * f1( 0.31577506210308787)
w2 ( 0.3305814636366339 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.25575160677004594) - present_state_Q (0.25407628583815217)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1290191210349127 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23306694332878305) - present_state_Q ( 0.23441854869790343)) * f1( 0.22544408392514073)
w2 ( 0.3456925152874319 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.23306694332878305) - present_state_Q (0.23441854869790343)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1261113119800293 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25330568279508964) - present_state_Q ( 0.25458231105445767)) * f1( 0.17029802248879278)
w2 ( 0.359352375865436 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25330568279508964) - present_state_Q (0.25458231105445767)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.12350985053028603 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2705161586097297) - present_state_Q ( 0.20109227714683514)) * f1( 0.1151296275049911)
w2 ( 0.3729099361882843 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2705161586097297) - present_state_Q (0.20109227714683514)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1160605857477476 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1166339088810936) - present_state_Q ( 0.11784965928544149)) * f1( 0.25353698555560633)
w2 ( 0.384662485452391 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1166339088810936) - present_state_Q (0.11784965928544149)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10847613425248866 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1987149288960932) - present_state_Q ( 0.12410331128647056)) * f1( 0.25643229958507646)
w2 ( 0.39649321271651655 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1987149288960932) - present_state_Q (0.12410331128647056)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1041992587223707 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21434286815157613) - present_state_Q ( 0.2153804994883081)) * f1( 0.20756112205469116)
w2 ( 0.4088564399561275 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21434286815157613) - present_state_Q (0.2153804994883081)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10101803468287406 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2282756752008138) - present_state_Q ( 0.2282756752008138)) * f1( 0.16351545089451577)
w2 ( 0.4205295534952836 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2282756752008138) - present_state_Q (0.2282756752008138)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09827607643981162 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1574867696184717) - present_state_Q ( 0.1574867696184717)) * f1( 0.10616967369550295)
w2 ( 0.4308600297890186 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1574867696184717) - present_state_Q (0.1574867696184717)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09408811874029803 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07383276390602816) - present_state_Q ( 0.07383276390602816)) * f1( 0.1255569259455797)
w2 ( 0.4375310400387101 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07383276390602816) - present_state_Q (0.07383276390602816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09267150213562918 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07728802373294133) - present_state_Q ( 0.07728802373294133)) * f1( 0.1086022806238151)
w2 ( 0.44013985561151714 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07728802373294133) - present_state_Q (0.07728802373294133)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09115544459479218 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07725929239839656) - present_state_Q ( 0.07725929239839656)) * f1( 0.1162026995974058)
w2 ( 0.442749188348346 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07725929239839656) - present_state_Q (0.07725929239839656)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0920496493638673 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07801222454867132) - present_state_Q ( 0.07672318113794832)) * f1( 0.12974163621595186)
w2 ( 0.44137074917468433 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.07801222454867132) - present_state_Q (0.07672318113794832)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11880524603683673 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07636906447766512) - present_state_Q ( 0.07636906447766512)) * f1( 0.12933330479306426)
w2 ( 0.39999610601408636 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07636906447766512) - present_state_Q (0.07636906447766512)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15875240163353024 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1735947709419733) - present_state_Q ( 0.1735947709419733)) * f1( 0.5372621398279367)
w2 ( 0.4053669965691148 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1735947709419733) - present_state_Q (0.1735947709419733)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.265031472195016 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16641768738965998) - present_state_Q ( 0.16467360756970112)) * f1( 0.49477418649128574)
w2 ( 0.2764850862392707 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16641768738965998) - present_state_Q (0.16467360756970112)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.35694445025356725 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.043213975374475105) - present_state_Q ( -0.012220948952930591)) * f1( 0.4633977332256937)
w2 ( 0.1971467800988858 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.043213975374475105) - present_state_Q (-0.012220948952930591)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4459006692638245 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03658719569768906) - present_state_Q ( -0.04371911088958616)) * f1( 0.4538722449216693)
w2 ( 0.07955040357807483 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03658719569768906) - present_state_Q (-0.04371911088958616)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5182416096585617 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12300466161786555) - present_state_Q ( -0.12300466161786555)) * f1( 0.3828989627815345)
w2 ( -0.033807344694560454 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.12300466161786555) - present_state_Q (-0.12300466161786555)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5762527403210735 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1768379829633055) - present_state_Q ( -0.1768379829633055)) * f1( 0.3151330229795323)
w2 ( -0.10744117730788147 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1768379829633055) - present_state_Q (-0.1768379829633055)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6230226960566936 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21354761736477834) - present_state_Q ( -0.21354761736477834)) * f1( 0.2587109796596962)
w2 ( -0.21590960597018347 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21354761736477834) - present_state_Q (-0.21354761736477834)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6376237414010703 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3370989438406318) - present_state_Q ( -0.3802808650346685)) * f1( 0.2638286857041363)
w2 ( -0.2712525089051229 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.3370989438406318) - present_state_Q (-0.3802808650346685)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6293691529775057 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3240561311644117) - present_state_Q ( -0.3240561311644117)) * f1( 0.16789544850554028)
w2 ( -0.23192046746128528 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3240561311644117) - present_state_Q (-0.3240561311644117)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.6240613148963313 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1970165355239039) - present_state_Q ( -0.1970165355239039)) * f1( 0.09194008758354393)
w2 ( -0.19728157454299447 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1970165355239039) - present_state_Q (-0.1970165355239039)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6050236927962596 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22462341948225722) - present_state_Q ( -0.26407973439085614)) * f1( 0.29671299943406715)
w2 ( -0.17161687884528926 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22462341948225722) - present_state_Q (-0.26407973439085614)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5906384553803422 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18022500063311075) - present_state_Q ( -0.21454837640216862)) * f1( 0.24115026667754802)
w2 ( -0.14775584379173495 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18022500063311075) - present_state_Q (-0.21454837640216862)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5919956814136058 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1065596430404146) - present_state_Q ( -0.1065596430404146)) * f1( 0.13038174805681746)
w2 ( -0.1498377702170075 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1065596430404146) - present_state_Q (-0.1065596430404146)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.638668167127143 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17482242967497133) - present_state_Q ( -0.18036580544974568)) * f1( 0.25405295364184655)
w2 ( -0.18658009896736255 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.17482242967497133) - present_state_Q (-0.18036580544974568)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13759126286577905 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2769077005576931) - present_state_Q ( 0.2769077005576931)) * f1( 0.4209697105811708)
w2 ( 0.35666339249291035 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2769077005576931) - present_state_Q (0.2769077005576931)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14818445265368843 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3018944201249379) - present_state_Q ( 0.30329454947774315)) * f1( 0.38787959281418033)
w2 ( 0.32935288174638544 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.3018944201249379) - present_state_Q (0.30329454947774315)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1496749387306943 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3392183602275418) - present_state_Q ( 0.2733849357927816)) * f1( 0.3776910799434715)
w2 ( 0.3254065717693827 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.3392183602275418) - present_state_Q (0.2733849357927816)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.19291509250129185 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3382847254131409) - present_state_Q ( 0.33668280610891993)) * f1( 0.3594795526267043)
w2 ( 0.18106405174126997 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.3382847254131409) - present_state_Q (0.33668280610891993)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1912549333428326 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1580161162388164) - present_state_Q ( 0.15994204905382348)) * f1( 0.29720231990307644)
w2 ( 0.18776719924967694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1580161162388164) - present_state_Q (0.15994204905382348)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.18471911278777758 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17611732068372746) - present_state_Q ( 0.14235479516434119)) * f1( 0.23744435393952465)
w2 ( 0.2152928929400801 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17611732068372746) - present_state_Q (0.14235479516434119)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18083346243074883 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2224301642308409) - present_state_Q ( 0.2224301642308409)) * f1( 0.19446448586251605)
w2 ( 0.2392704352031493 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2224301642308409) - present_state_Q (0.2224301642308409)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.17828208052836236 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25945701198001353) - present_state_Q ( 0.25941877856586937)) * f1( 0.15321137639843563)
w2 ( 0.25925366591900517 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25945701198001353) - present_state_Q (0.25941877856586937)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1698329618845657 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015469996574508912) - present_state_Q ( 0.013075051734919817)) * f1( 0.21749623593108403)
w2 ( 0.2670231048774558 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.015469996574508912) - present_state_Q (0.013075051734919817)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.163413288399875 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024854705393412807) - present_state_Q ( 0.02455605513741451)) * f1( 0.16986435093609697)
w2 ( 0.2745816931854943 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.024854705393412807) - present_state_Q (0.02455605513741451)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15722258972169234 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028169491954936027) - present_state_Q ( 0.027931000010910825)) * f1( 0.1651355216606037)
w2 ( 0.282079412169186 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.028169491954936027) - present_state_Q (0.027931000010910825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15331505403101683 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.037937777271147194) - present_state_Q ( 0.03954938056045758)) * f1( 0.10727785303139879)
w2 ( 0.2893643001125191 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.037937777271147194) - present_state_Q (0.03954938056045758)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15114760883063097 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03651469065119381) - present_state_Q ( -0.03822941890355225)) * f1( 0.6268287190285556)
w2 ( 0.2900558591092878 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.03651469065119381) - present_state_Q (-0.03822941890355225)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13118474943179195 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0816285659660814) - present_state_Q ( 0.0816285659660814)) * f1( 0.6113556821334568)
w2 ( 0.3096479165471194 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0816285659660814) - present_state_Q (0.0816285659660814)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11776128605173493 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1750800943785704) - present_state_Q ( 0.1750800943785704)) * f1( 0.5537094759394465)
w2 ( 0.3290421497518623 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1750800943785704) - present_state_Q (0.1750800943785704)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.10754757338024135 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20695969884522303) - present_state_Q ( 0.20695969884522303)) * f1( 0.47786520377795894)
w2 ( 0.34614105143500623 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20695969884522303) - present_state_Q (0.20695969884522303)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.9671473617849645 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8671055202890097) - present_state_Q ( 0.6583489089897506)) * f1( 0.2583489089897506)
w2 ( 0.949134465721566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.8671055202890097) - present_state_Q (0.6583489089897506)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.8970328911514085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.1501240652995643) - present_state_Q ( 1.1501240652995643)) * f1( 0.40409198035863086)
w2 ( 0.8103255330199973 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 1.1501240652995643) - present_state_Q (1.1501240652995643)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.7965680382457591 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.4083622657561623) - present_state_Q ( 1.4262926961204374)) * f1( 0.506003805405422)
w2 ( 0.5720707566746187 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 1.4083622657561623) - present_state_Q (1.4262926961204374)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.7026590202235096 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.1236191919652194) - present_state_Q ( 1.1236191919652194)) * f1( 0.5487720608503893)
w2 ( 0.366719883942375 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 1.1236191919652194) - present_state_Q (1.1236191919652194)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.6142734122663496 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8688447300065194) - present_state_Q ( 0.861285409980809)) * f1( 0.5994679312819071)
w2 ( 0.1897917715047561 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.8688447300065194) - present_state_Q (0.861285409980809)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.5300134695401837 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6291790128043037) - present_state_Q ( 0.6347207655841205)) * f1( 0.6625236118830263)
w2 ( 0.03717542778831326 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.6291790128043037) - present_state_Q (0.6347207655841205)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.4542919579394976 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.41160354399794535) - present_state_Q ( 0.4174851456817025)) * f1( 0.703519162747347)
w2 ( -0.09198354716551574 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.41160354399794535) - present_state_Q (0.4174851456817025)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.3940764910188457 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1851329708247668) - present_state_Q ( 0.1864136303616257)) * f1( 0.6938062426262995)
w2 ( -0.21348959382459662 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.1851329708247668) - present_state_Q (0.1864136303616257)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3467042577029737 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02491209305620451) - present_state_Q ( -0.023815967632427937)) * f1( 0.6980103355336994)
w2 ( -0.3085041276588436 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.02491209305620451) - present_state_Q (-0.023815967632427937)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.30967832216199 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1904397639537702) - present_state_Q ( -0.18949225992477728)) * f1( 0.6991939481899379)
w2 ( -0.3826413679647276 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1904397639537702) - present_state_Q (-0.18949225992477728)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2812972008328702 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3208322697465861) - present_state_Q ( -0.3215894695310667)) * f1( 0.691389840027465)
w2 ( -0.44011049400683044 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3208322697465861) - present_state_Q (-0.3215894695310667)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.25885856114652306 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.419357880103693) - present_state_Q ( -0.4200578146410685)) * f1( 0.697116346653599)
w2 ( -0.48517341027853256 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.419357880103693) - present_state_Q (-0.4200578146410685)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.24149891025232337 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49903747619454375) - present_state_Q ( -0.4996658720921263)) * f1( 0.6937259540594158)
w2 ( -0.5202067128523585 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.49903747619454375) - present_state_Q (-0.4996658720921263)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.22778067959762563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5595841257023294) - present_state_Q ( -0.5595841257023294)) * f1( 0.698575708332206)
w2 ( -0.547699113013865 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5595841257023294) - present_state_Q (-0.5595841257023294)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.21717003652583858 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6089936134021143) - present_state_Q ( -0.6083474628187191)) * f1( 0.6955431675792723)
w2 ( -0.5690563788068739 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6089936134021143) - present_state_Q (-0.6083474628187191)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.20894294646698283 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6454445049338945) - present_state_Q ( -0.6459828276650026)) * f1( 0.693908354372318)
w2 ( -0.585655006002848 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6454445049338945) - present_state_Q (-0.6459828276650026)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2023996921449109 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6744181310464845) - present_state_Q ( -0.6738458599190396)) * f1( 0.6990958582467864)
w2 ( -0.5987584394488332 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6744181310464845) - present_state_Q (-0.6738458599190396)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1974237032846913 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6974601588617935) - present_state_Q ( -0.697961519205039)) * f1( 0.6931843350970992)
w2 ( -0.6088082689841928 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6974601588617935) - present_state_Q (-0.697961519205039)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.19337886730663428 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7155920744812267) - present_state_Q ( -0.7138815919470202)) * f1( 0.7012834949773007)
w2 ( -0.6168831351543472 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7155920744812267) - present_state_Q (-0.7138815919470202)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.19027257163800235 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7284179717052295) - present_state_Q ( -0.7284179717052295)) * f1( 0.6992409222071067)
w2 ( -0.6231024707194883 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7284179717052295) - present_state_Q (-0.7284179717052295)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18792517881870427 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7397203383537859) - present_state_Q ( -0.7401777548190648)) * f1( 0.6946124869729884)
w2 ( -0.6278336697817722 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7397203383537859) - present_state_Q (-0.7401777548190648)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1860074127134492 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7478966985206428) - present_state_Q ( -0.7473975785124618)) * f1( 0.700116716711748)
w2 ( -0.6316685625693166 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7478966985206428) - present_state_Q (-0.7473975785124618)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18460922136875013 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7548543576154431) - present_state_Q ( -0.7553261944938802)) * f1( 0.693573397001695)
w2 ( -0.6344908563467896 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7548543576154431) - present_state_Q (-0.7553261944938802)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18350367560241893 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7595955087473713) - present_state_Q ( -0.7600448443872834)) * f1( 0.694669277880019)
w2 ( -0.6367189152550331 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7595955087473713) - present_state_Q (-0.7600448443872834)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18260701696925333 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7640221223705057) - present_state_Q ( -0.7635346300971682)) * f1( 0.6968353676845519)
w2 ( -0.6385203767546166 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7640221223705057) - present_state_Q (-0.7635346300971682)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1820055569729915 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7676621069609658) - present_state_Q ( -0.7680416466256501)) * f1( 0.6893868752700205)
w2 ( -0.6397418157244791 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7676621069609658) - present_state_Q (-0.7680416466256501)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18144475786846917 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7683594381602434) - present_state_Q ( -0.7687894863301512)) * f1( 0.6969515535338476)
w2 ( -0.6408683197725014 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7683594381602434) - present_state_Q (-0.7687894863301512)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1810155636323815 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7714972049915608) - present_state_Q ( -0.7709806662963539)) * f1( 0.6957212920786444)
w2 ( -0.6417319873608937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7714972049915608) - present_state_Q (-0.7709806662963539)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18086804553529104 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730924788848237) - present_state_Q ( -0.7751432246806862)) * f1( 0.6810550162135972)
w2 ( -0.6420352306099851 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730924788848237) - present_state_Q (-0.7751432246806862)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1805613050280298 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7723794391504399) - present_state_Q ( -0.7728352947137114)) * f1( 0.6967180287005418)
w2 ( -0.6426516014981717 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7723794391504399) - present_state_Q (-0.7728352947137114)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18032530731699067 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7745462365386429) - present_state_Q ( -0.7740632665416891)) * f1( 0.695879859398701)
w2 ( -0.6431263914938762 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7745462365386429) - present_state_Q (-0.7740632665416891)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18016970256404052 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7752854270517293) - present_state_Q ( -0.7752854270517293)) * f1( 0.6936991978602383)
w2 ( -0.6434404276853583 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7752854270517293) - present_state_Q (-0.7752854270517293)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18001064680315565 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775798708209821) - present_state_Q ( -0.7752968023372412)) * f1( 0.6966753823531731)
w2 ( -0.6437600572730819 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775798708209821) - present_state_Q (-0.7752968023372412)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17995498331435877 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7763952893395345) - present_state_Q ( -0.7768342541105469)) * f1( 0.691235925660741)
w2 ( -0.6438727957483589 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7763952893395345) - present_state_Q (-0.7768342541105469)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17987952710533744 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7760858774101492) - present_state_Q ( -0.7765214246005421)) * f1( 0.6940651886754087)
w2 ( -0.6440249985880251 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7760858774101492) - present_state_Q (-0.7765214246005421)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17976597319923915 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775494894020988) - present_state_Q ( -0.7759246413411249)) * f1( 0.6988586122338106)
w2 ( -0.6442524773165614 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775494894020988) - present_state_Q (-0.7759246413411249)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17970580532215638 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7773562810875727) - present_state_Q ( -0.7768709082803232)) * f1( 0.6958077645997598)
w2 ( -0.6443735380925422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7773562810875727) - present_state_Q (-0.7768709082803232)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17983110079643694 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775673414316004) - present_state_Q ( -0.7795943726752124)) * f1( 0.6818287279851161)
w2 ( -0.6441162686980548 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775673414316004) - present_state_Q (-0.7795943726752124)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17977244752581178 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7763600592585884) - present_state_Q ( -0.7767919940661543)) * f1( 0.6949341996887708)
w2 ( -0.6442344303584134 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7763600592585884) - present_state_Q (-0.7767919940661543)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17968474845049368 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7759060472834582) - present_state_Q ( -0.7763352888859322)) * f1( 0.6986215927099397)
w2 ( -0.6444101745763513 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7759060472834582) - present_state_Q (-0.7763352888859322)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17987094069897724 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.776871066276019) - present_state_Q ( -0.7804352740579914)) * f1( 0.6775142097407434)
w2 ( -0.6440254311360968 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.776871066276019) - present_state_Q (-0.7804352740579914)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17983139769255088 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7766620777609047) - present_state_Q ( -0.7770950973815303)) * f1( 0.6923881407693858)
w2 ( -0.6441053865913352 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7766620777609047) - present_state_Q (-0.7770950973815303)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.179764673466358 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777265891881208) - present_state_Q ( -0.7767665145928573)) * f1( 0.694990019755539)
w2 ( -0.644239797034672 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777265891881208) - present_state_Q (-0.7767665145928573)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1796888025384592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7760796190042984) - present_state_Q ( -0.7765204616069612)) * f1( 0.6976635165476521)
w2 ( -0.6443920470757577 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7760796190042984) - present_state_Q (-0.7765204616069612)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17963489627651588 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777483469915576) - present_state_Q ( -0.7769745181812637)) * f1( 0.696617407186549)
w2 ( -0.6445003831091989 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777483469915576) - present_state_Q (-0.7769745181812637)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17959511307025666 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777010737003313) - present_state_Q ( -0.7771988558806092)) * f1( 0.696421926170167)
w2 ( -0.6445803583177182 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777010737003313) - present_state_Q (-0.7771988558806092)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17956426361315542 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778418595957145) - present_state_Q ( -0.7773412056823128)) * f1( 0.6964070114400361)
w2 ( -0.6446423755565344 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778418595957145) - present_state_Q (-0.7773412056823128)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17953369592211293 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778500442081882) - present_state_Q ( -0.7773464316677576)) * f1( 0.6969810784901712)
w2 ( -0.6447037757419629 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778500442081882) - present_state_Q (-0.7773464316677576)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17958244347201754 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7780732567597861) - present_state_Q ( -0.77851270536211)) * f1( 0.6910824179237335)
w2 ( -0.6446050225859045 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7780732567597861) - present_state_Q (-0.77851270536211)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17954476165685135 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767117677760106) - present_state_Q ( -0.777131181840496)) * f1( 0.6978179345204025)
w2 ( -0.6446806218770992 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767117677760106) - present_state_Q (-0.777131181840496)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17948596556323027 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7768447056528248) - present_state_Q ( -0.7768447056528248)) * f1( 0.7001494435987473)
w2 ( -0.6447981889648432 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7768447056528248) - present_state_Q (-0.7768447056528248)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1794627075088961 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769600461901902) - present_state_Q ( -0.7773629894366338)) * f1( 0.6984082277451727)
w2 ( -0.6448448110903772 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769600461901902) - present_state_Q (-0.7773629894366338)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17945852489686137 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7782764307636755) - present_state_Q ( -0.7777676005159089)) * f1( 0.696607873278754)
w2 ( -0.6448532170488414 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7782764307636755) - present_state_Q (-0.7777676005159089)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17946609231541205 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7784660933497257) - present_state_Q ( -0.7779553923721239)) * f1( 0.6956432499821443)
w2 ( -0.6448379874236402 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7784660933497257) - present_state_Q (-0.7779553923721239)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17937262831453743 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779932537956408) - present_state_Q ( -0.7764712679950755)) * f1( 0.7037647767804792)
w2 ( -0.6450239154574686 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779932537956408) - present_state_Q (-0.7764712679950755)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17960853113136613 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777026093498527) - present_state_Q ( -0.7812446778742125)) * f1( 0.6789709495290545)
w2 ( -0.6445374970859769 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777026093498527) - present_state_Q (-0.7812446778742125)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1795810003773858 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779023768771041) - present_state_Q ( -0.7773945239764186)) * f1( 0.6957240346927311)
w2 ( -0.6445928970055577 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779023768771041) - present_state_Q (-0.7773945239764186)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1796189766200095 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778902912166117) - present_state_Q ( -0.7783386086290316)) * f1( 0.6910054344166339)
w2 ( -0.6445159558745258 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778902912166117) - present_state_Q (-0.7783386086290316)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17965160376052144 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778226306707153) - present_state_Q ( -0.7782546223385348)) * f1( 0.6907272172487164)
w2 ( -0.644449825576521 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778226306707153) - present_state_Q (-0.7782546223385348)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1796182531787607 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767702566690765) - present_state_Q ( -0.7771978292197925)) * f1( 0.6959688862783916)
w2 ( -0.6445169130791171 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767702566690765) - present_state_Q (-0.7771978292197925)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17959270615343562 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7768902005009477) - present_state_Q ( -0.7773219282246087)) * f1( 0.6959301066231299)
w2 ( -0.6445683059346852 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7768902005009477) - present_state_Q (-0.7773219282246087)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17956488326669207 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7768450225130591) - present_state_Q ( -0.7772851123610609)) * f1( 0.6966347276966239)
w2 ( -0.6446242205193194 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7768450225130591) - present_state_Q (-0.7772851123610609)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.179469225237126 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7784936527656505) - present_state_Q ( -0.7764859945889823)) * f1( 0.7016289145519964)
w2 ( -0.644815092415581 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7784936527656505) - present_state_Q (-0.7764859945889823)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1795515278974381 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7786601690333955) - present_state_Q ( -0.7790602838586381)) * f1( 0.689147932520244)
w2 ( -0.6446478950418392 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7786601690333955) - present_state_Q (-0.7790602838586381)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17957519237456593 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776823963759724) - present_state_Q ( -0.7781098061215682)) * f1( 0.6928219903985658)
w2 ( -0.6446000757340832 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776823963759724) - present_state_Q (-0.7781098061215682)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1795396156536121 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777719689555055) - present_state_Q ( -0.7772668087657267)) * f1( 0.6970522799212584)
w2 ( -0.6446715300722585 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777719689555055) - present_state_Q (-0.7772668087657267)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17952884567393218 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7770942744959938) - present_state_Q ( -0.7775547183679661)) * f1( 0.6961439862627969)
w2 ( -0.6446931893436871 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7770942744959938) - present_state_Q (-0.7775547183679661)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17949476866714992 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767378284952308) - present_state_Q ( -0.7771858596574399)) * f1( 0.6984092442250253)
w2 ( -0.6447614985905787 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767378284952308) - present_state_Q (-0.7771858596574399)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17938674231766025 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777931373037366) - present_state_Q ( -0.7762455312841147)) * f1( 0.7043133773838633)
w2 ( -0.644976228133055 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777931373037366) - present_state_Q (-0.7762455312841147)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17943498485059622 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7780767971170504) - present_state_Q ( -0.778502988357283)) * f1( 0.6938290389854568)
w2 ( -0.6448788849226741 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7780767971170504) - present_state_Q (-0.778502988357283)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17942830162047294 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7782402646165735) - present_state_Q ( -0.7777281684654882)) * f1( 0.697201108972256)
w2 ( -0.6448923050421378 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7782402646165735) - present_state_Q (-0.7777281684654882)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17948646448590946 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7782226626667234) - present_state_Q ( -0.7786626196980471)) * f1( 0.6921238524768828)
w2 ( -0.6447746555617453 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7782226626667234) - present_state_Q (-0.7786626196980471)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.13292533957320515 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00828282750188293) - present_state_Q ( -0.00828282750188293)) * f1( 0.6723186840639979)
w2 ( -0.6586255646667114 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.00828282750188293) - present_state_Q (-0.00828282750188293)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1431365782169447 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17700141434824604) - present_state_Q ( -0.1796258220289611)) * f1( 0.630612674053463)
w2 ( -0.652148537442946 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17700141434824604) - present_state_Q (-0.1796258220289611)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1592520916713551 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02799981660224128) - present_state_Q ( -0.02799981660224128)) * f1( 0.715609470076197)
w2 ( -0.6476445407441056 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02799981660224128) - present_state_Q (-0.02799981660224128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1879721171811898 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15992051661340104) - present_state_Q ( -0.2926283579799709)) * f1( 0.6025563963361914)
w2 ( -0.6190463623649878 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15992051661340104) - present_state_Q (-0.2926283579799709)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.21863144362745446 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11013993480125833) - present_state_Q ( -0.2357370930581096)) * f1( 0.7218662341824257)
w2 ( -0.5935629763903087 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11013993480125833) - present_state_Q (-0.2357370930581096)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.24624886191600748 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19500477163451113) - present_state_Q ( -0.19558990660712916)) * f1( 0.7343311490941256)
w2 ( -0.570997610623688 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19500477163451113) - present_state_Q (-0.19558990660712916)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.27149464492000264 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16095920451347318) - present_state_Q ( -0.16380910292239348)) * f1( 0.7260519381113011)
w2 ( -0.5501348196754252 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16095920451347318) - present_state_Q (-0.16380910292239348)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2930967024227805 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15926100610972976) - present_state_Q ( -0.15926100610972976)) * f1( 0.6291832597503292)
w2 ( -0.5295347253454998 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15926100610972976) - present_state_Q (-0.15926100610972976)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3214253100028791 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11484614735192047) - present_state_Q ( -0.22075309242102045)) * f1( 0.692176630369388)
w2 ( -0.49679324713063355 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11484614735192047) - present_state_Q (-0.22075309242102045)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.34030888679497845 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053317937543832544) - present_state_Q ( -0.053317937543832544)) * f1( 0.7614770931771219)
w2 ( -0.4819140785032666 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.053317937543832544) - present_state_Q (-0.053317937543832544)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3575588262014547 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021242248761016824) - present_state_Q ( -0.021242248761016824)) * f1( 0.7872442029477332)
w2 ( -0.4687669970701717 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.021242248761016824) - present_state_Q (-0.021242248761016824)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3732841495519156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005886372033391818) - present_state_Q ( 0.00010073764021856224)) * f1( 0.7868941143793724)
w2 ( -0.45677657315180487 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0005886372033391818) - present_state_Q (0.00010073764021856224)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.34053915609452534 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022418491676037322) - present_state_Q ( 0.01913275384333729)) * f1( 0.7854571325526984)
w2 ( -0.4817900274323489 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.022418491676037322) - present_state_Q (0.01913275384333729)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2873340163974587 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01947898222286598) - present_state_Q ( -0.022435335179030624)) * f1( 0.7829897869552669)
w2 ( -0.5225607812149442 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.01947898222286598) - present_state_Q (-0.022435335179030624)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.24579260940678643 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19133526857926494) - present_state_Q ( -0.19172820601365367)) * f1( 0.7876561981622144)
w2 ( -0.564753206882486 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.19133526857926494) - present_state_Q (-0.19172820601365367)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.20894702586179986 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25815125523716487) - present_state_Q ( -0.25815125523716487)) * f1( 0.7878646584866645)
w2 ( -0.6021663165054101 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.25815125523716487) - present_state_Q (-0.25815125523716487)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.17609316047478452 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3168185600194189) - present_state_Q ( -0.3164217752942893)) * f1( 0.7911635843018779)
w2 ( -0.6353871229620223 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3168185600194189) - present_state_Q (-0.3164217752942893)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.14716919673126772 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3696472876374574) - present_state_Q ( -0.3696472876374574)) * f1( 0.7874377991643577)
w2 ( -0.6647725182521254 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3696472876374574) - present_state_Q (-0.3696472876374574)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.12133790489340038 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41529064542738714) - present_state_Q ( -0.41529064542738714)) * f1( 0.7917918407008311)
w2 ( -0.6908715917813535 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41529064542738714) - present_state_Q (-0.41529064542738714)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09857439833560562 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.457018785220006) - present_state_Q ( -0.457018785220006)) * f1( 0.7885292587599374)
w2 ( -0.7139662392455131 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.457018785220006) - present_state_Q (-0.457018785220006)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.07839927945523581 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4933262178126793) - present_state_Q ( -0.49345138116167153)) * f1( 0.7884563491843847)
w2 ( -0.7344367384950807 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4933262178126793) - present_state_Q (-0.49345138116167153)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06069130868983474 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5253483003161821) - present_state_Q ( -0.5262089769704119)) * f1( 0.7824104284105904)
w2 ( -0.7525428067399772 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5253483003161821) - present_state_Q (-0.5262089769704119)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.044798589582265305 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5541169522266371) - present_state_Q ( -0.5541169522266371)) * f1( 0.7895247968737636)
w2 ( -0.7686463861796593 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5541169522266371) - present_state_Q (-0.5541169522266371)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0346299700861601 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.579299660888216) - present_state_Q ( -0.5880946636213542)) * f1( 0.5987341470453729)
w2 ( -0.7822332103770567 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.579299660888216) - present_state_Q (-0.5880946636213542)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.021825446256248972 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5982872174493664) - present_state_Q ( -0.5983299889510633)) * f1( 0.7928559938766827)
w2 ( -0.7951531090005666 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5982872174493664) - present_state_Q (-0.5983299889510633)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.023091092929382433 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6189453690621183) - present_state_Q ( -0.7779759908622316)) * f1( 0.7870225395009708)
w2 ( -0.7935449636049646 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6189453690621183) - present_state_Q (-0.7779759908622316)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023211504267452287 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7795469471857558) - present_state_Q ( -0.7800087641894627)) * f1( 0.5862086934082561)
w2 ( -0.7933395566578759 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7795469471857558) - present_state_Q (-0.7800087641894627)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023290012281800093 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7792108657322897) - present_state_Q ( -0.7792108657322897)) * f1( 0.6086934634993822)
w2 ( -0.7932105787419699 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7792108657322897) - present_state_Q (-0.7792108657322897)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02308004201955786 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7748227911468492) - present_state_Q ( -0.7748227911468492)) * f1( 0.7895138642537235)
w2 ( -0.7934765275387534 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7748227911468492) - present_state_Q (-0.7748227911468492)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.022900959945980386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7752571298765395) - present_state_Q ( -0.7752571298765395)) * f1( 0.789400541245763)
w2 ( -0.7937033858498649 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7752571298765395) - present_state_Q (-0.7752571298765395)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.1669117853948076 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17787555175645992) - present_state_Q ( -0.43578541398115805)) * f1( 0.44590722034374525)
w2 ( -0.6673348268573043 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.17787555175645992) - present_state_Q (-0.43578541398115805)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1673680657560419 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4546886989491476) - present_state_Q ( -0.4551428717586302)) * f1( 0.47165626765659324)
w2 ( -0.666560906708207 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4546886989491476) - present_state_Q (-0.4551428717586302)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.16697334105519282 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4368312221466278) - present_state_Q ( -0.4368312221466278)) * f1( 0.5760806446821067)
w2 ( -0.6671090587136498 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4368312221466278) - present_state_Q (-0.4368312221466278)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.14742084516313567 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42952464882771013) - present_state_Q ( -0.42952464882771013)) * f1( 0.6238277169573969)
w2 ( -0.6921832839980546 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42952464882771013) - present_state_Q (-0.42952464882771013)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.12882389615781076 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4553520150168735) - present_state_Q ( -0.45829961272774183)) * f1( 0.6474458504499845)
w2 ( -0.7151621310999703 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4553520150168735) - present_state_Q (-0.45829961272774183)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.109895033228522 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48087356617678034) - present_state_Q ( -0.48087356617678034)) * f1( 0.7083789686923154)
w2 ( -0.7365392343352422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.48087356617678034) - present_state_Q (-0.48087356617678034)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09277607812921107 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5108654838341486) - present_state_Q ( -0.5109039690294226)) * f1( 0.7127475750054384)
w2 ( -0.7557538406835616 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5108654838341486) - present_state_Q (-0.5109039690294226)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09681809795641308 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8412495795703799) - present_state_Q ( -0.8411923773872025)) * f1( 0.7082885239183401)
w2 ( -0.7489057503519418 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8412495795703799) - present_state_Q (-0.8411923773872025)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.10014777919819802 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8297813463593222) - present_state_Q ( -0.8297241615011117)) * f1( 0.7122918170966878)
w2 ( -0.7432962271281203 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8297813463593222) - present_state_Q (-0.8297241615011117)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.11238148322823752 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9696466156839703) - present_state_Q ( -0.969677356231507)) * f1( 0.7083268577276448)
w2 ( -0.7191164498752849 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9696466156839703) - present_state_Q (-0.969677356231507)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1219302038246413 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9266745182569136) - present_state_Q ( -0.9267010758913429)) * f1( 0.7124123266059478)
w2 ( -0.7003517425060937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9266745182569136) - present_state_Q (-0.9267010758913429)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.12935179437649674 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8938979455972911) - present_state_Q ( -0.8939352673385963)) * f1( 0.7098911463678068)
w2 ( -0.6857153763170523 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8938979455972911) - present_state_Q (-0.8939352673385963)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1351272357412844 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8678264163253051) - present_state_Q ( -0.86786410902572)) * f1( 0.7123010412207679)
w2 ( -0.6743639708820057 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8678264163253051) - present_state_Q (-0.86786410902572)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1397560509636658 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8484590314550441) - present_state_Q ( -0.8551805283765554)) * f1( 0.6581132987025419)
w2 ( -0.6645171233496586 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8484590314550441) - present_state_Q (-0.8551805283765554)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14315786874108954 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8309188707268026) - present_state_Q ( -0.8309188707268026)) * f1( 0.7112758358388587)
w2 ( -0.6578213456380815 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8309188707268026) - present_state_Q (-0.8309188707268026)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14580975205210878 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8192088680468516) - present_state_Q ( -0.8192502043131292)) * f1( 0.7104023025385733)
w2 ( -0.6525952411868993 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8192088680468516) - present_state_Q (-0.8192502043131292)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14788447641531996 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8102453289389064) - present_state_Q ( -0.8103002528043043)) * f1( 0.7086843191422897)
w2 ( -0.6484966403994413 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8102453289389064) - present_state_Q (-0.8103002528043043)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1498781556474834 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.803370376341645) - present_state_Q ( -0.8106582045001457)) * f1( 0.6575206162003835)
w2 ( -0.644251677038204 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.803370376341645) - present_state_Q (-0.8106582045001457)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15097732528398872 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7948138374655288) - present_state_Q ( -0.7948652673143711)) * f1( 0.7144942508565797)
w2 ( -0.6420979333387093 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7948138374655288) - present_state_Q (-0.7948652673143711)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1518323891197951 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7910233093440181) - present_state_Q ( -0.7910703615575068)) * f1( 0.7144565908409676)
w2 ( -0.6404224090514746 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7910233093440181) - present_state_Q (-0.7910703615575068)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15299272207904482 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7880666560963743) - present_state_Q ( -0.7963886324242859)) * f1( 0.6599562901478093)
w2 ( -0.6379609336974238 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7880666560963743) - present_state_Q (-0.7963886324242859)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1533821272357262 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.783833454839814) - present_state_Q ( -0.783833454839814)) * f1( 0.7144905381845719)
w2 ( -0.6371979183876072 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.783833454839814) - present_state_Q (-0.783833454839814)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1536916202198097 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.782550149701013) - present_state_Q ( -0.7825907802562119)) * f1( 0.7138139720684258)
w2 ( -0.6365909112475517 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.782550149701013) - present_state_Q (-0.7825907802562119)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15397522736520447 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7821605865996873) - present_state_Q ( -0.7822144926455732)) * f1( 0.7092955552494623)
w2 ( -0.636031130489567 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7821605865996873) - present_state_Q (-0.7822144926455732)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1540931543823626 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7813410081620278) - present_state_Q ( -0.7797748386264881)) * f1( 0.7187438262157416)
w2 ( -0.6358014271961271 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7813410081620278) - present_state_Q (-0.7797748386264881)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1543327506544156 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7798882526723695) - present_state_Q ( -0.781384160901904)) * f1( 0.7056629972208558)
w2 ( -0.6353260802072737 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7798882526723695) - present_state_Q (-0.781384160901904)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15446998340105875 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7798842719183878) - present_state_Q ( -0.7799220196854589)) * f1( 0.7097294134930288)
w2 ( -0.6350553772581669 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7798842719183878) - present_state_Q (-0.7799220196854589)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15508586448681666 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7786733483760067) - present_state_Q ( -0.7872060791055737)) * f1( 0.6594902570253122)
w2 ( -0.6337479530606506 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7786733483760067) - present_state_Q (-0.7872060791055737)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15503892400496977 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7770438006659955) - present_state_Q ( -0.7770438006659955)) * f1( 0.7105956044645408)
w2 ( -0.6338404341767351 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7770438006659955) - present_state_Q (-0.7770438006659955)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1549202639615205 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777032834741909) - present_state_Q ( -0.7761168176384557)) * f1( 0.7176248862860207)
w2 ( -0.6340719256759899 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777032834741909) - present_state_Q (-0.7761168176384557)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15541335158575473 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779445093412534) - present_state_Q ( -0.7852506996047228)) * f1( 0.6613079123536093)
w2 ( -0.6330280508621062 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779445093412534) - present_state_Q (-0.7852506996047228)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15581138588447926 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761461331195492) - present_state_Q ( -0.7836441230419242)) * f1( 0.6601437207176768)
w2 ( -0.6321839194999105 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761461331195492) - present_state_Q (-0.7836441230419242)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15561496708535444 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774641502745992) - present_state_Q ( -0.7746911767863133)) * f1( 0.7083327696949477)
w2 ( -0.6325721357882705 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774641502745992) - present_state_Q (-0.7746911767863133)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15544053054053142 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774996720634731) - present_state_Q ( -0.7750443746513658)) * f1( 0.7104497563629145)
w2 ( -0.6329158774259656 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774996720634731) - present_state_Q (-0.7750443746513658)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15543215269581262 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7759485214717485) - present_state_Q ( -0.7774749470531729)) * f1( 0.6987063217392935)
w2 ( -0.6329326641391259 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7759485214717485) - present_state_Q (-0.7774749470531729)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1553015231344652 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7756781758931268) - present_state_Q ( -0.775728307880372)) * f1( 0.7101324918945031)
w2 ( -0.6331901954983775 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7756781758931268) - present_state_Q (-0.775728307880372)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15521186099173656 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7763191659999439) - present_state_Q ( -0.7763671771273719)) * f1( 0.7089376481841019)
w2 ( -0.6333672590245446 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7763191659999439) - present_state_Q (-0.7763671771273719)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1556958648290034 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761717681451734) - present_state_Q ( -0.7850031134361027)) * f1( 0.6553046174974659)
w2 ( -0.6323332278975227 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761717681451734) - present_state_Q (-0.7850031134361027)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15604062902286747 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7748987103817038) - present_state_Q ( -0.7827246456263576)) * f1( 0.6586037050039402)
w2 ( -0.6316003594551765 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7748987103817038) - present_state_Q (-0.7827246456263576)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1563081629942199 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7727197505678187) - present_state_Q ( -0.781328465441075)) * f1( 0.6595207827641514)
w2 ( -0.6310324508013754 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7727197505678187) - present_state_Q (-0.781328465441075)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15597539139032998 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7725082600933914) - present_state_Q ( -0.7725599574597627)) * f1( 0.7094029610357798)
w2 ( -0.6316891723983161 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7725082600933914) - present_state_Q (-0.7725599574597627)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1558962921361502 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730461710876589) - present_state_Q ( -0.7761643691159643)) * f1( 0.6937022005664061)
w2 ( -0.6318488071173083 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730461710876589) - present_state_Q (-0.7761643691159643)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15579937051819778 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7745322090063911) - present_state_Q ( -0.7760609709642321)) * f1( 0.6961509957223256)
w2 ( -0.6320437221084053 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7745322090063911) - present_state_Q (-0.7760609709642321)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15610454293601975 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7738943807567578) - present_state_Q ( -0.7820123039863627)) * f1( 0.6601368582127343)
w2 ( -0.6313965208809091 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7738943807567578) - present_state_Q (-0.7820123039863627)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1558300617224592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7734687727031009) - present_state_Q ( -0.7734687727031009)) * f1( 0.7077715641847491)
w2 ( -0.6319394555203184 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7734687727031009) - present_state_Q (-0.7734687727031009)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15579232759401337 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7737098683631635) - present_state_Q ( -0.7768259732436448)) * f1( 0.6923520615486697)
w2 ( -0.6320157574232924 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7737098683631635) - present_state_Q (-0.7768259732436448)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15609796821863267 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.773517590472544) - present_state_Q ( -0.781981902086819)) * f1( 0.6601105451982626)
w2 ( -0.6313675373977534 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.773517590472544) - present_state_Q (-0.781981902086819)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15590467999420526 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7729312454313515) - present_state_Q ( -0.7745346739440376)) * f1( 0.7007130179914869)
w2 ( -0.631753720481627 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7729312454313515) - present_state_Q (-0.7745346739440376)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15562344571671727 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7733321153914525) - present_state_Q ( -0.7733856197231306)) * f1( 0.7124198513814706)
w2 ( -0.6323063833358691 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7733321153914525) - present_state_Q (-0.7733856197231306)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1559675438431759 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7751953185797253) - present_state_Q ( -0.7827446999232194)) * f1( 0.6585398252494062)
w2 ( -0.6315748598067346 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7751953185797253) - present_state_Q (-0.7827446999232194)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1556664896869908 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730827715190455) - present_state_Q ( -0.7730827715190455)) * f1( 0.7124689500920479)
w2 ( -0.6321664305953348 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730827715190455) - present_state_Q (-0.7730827715190455)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15545638985084775 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7744360548208845) - present_state_Q ( -0.7744851135393623)) * f1( 0.7101585544608395)
w2 ( -0.6325806194673165 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7744360548208845) - present_state_Q (-0.7744851135393623)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15582573059803376 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7753418012492613) - present_state_Q ( -0.7831371028775996)) * f1( 0.6591930024553099)
w2 ( -0.6317962102819422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7753418012492613) - present_state_Q (-0.7831371028775996)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15611610841014584 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7739932467352783) - present_state_Q ( -0.7818047739755181)) * f1( 0.6591332511326414)
w2 ( -0.6311794473796636 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7739932467352783) - present_state_Q (-0.7818047739755181)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15581395622553582 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7729692081763886) - present_state_Q ( -0.7730326442101881)) * f1( 0.7085661002433237)
w2 ( -0.6317764461047066 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7729692081763886) - present_state_Q (-0.7730326442101881)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18101900429740825 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13443885675900863) - present_state_Q ( -0.13600894596300328)) * f1( 0.6791649532506372)
w2 ( -0.6438720531502612 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13443885675900863) - present_state_Q (-0.13600894596300328)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.18134910183425165 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11579480578377926) - present_state_Q ( -0.11579480578377926)) * f1( 0.783089134903361)
w2 ( -0.6437034401420452 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.11579480578377926) - present_state_Q (-0.11579480578377926)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.18939625790834488 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10464849970466605) - present_state_Q ( -0.10826619222616155)) * f1( 0.8228063018863767)
w2 ( -0.6397913864518173 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.10464849970466605) - present_state_Q (-0.10826619222616155)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.20533320553172116 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08421663715182318) - present_state_Q ( -0.08421663715182318)) * f1( 0.9065644660835644)
w2 ( -0.6327595875143517 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08421663715182318) - present_state_Q (-0.08421663715182318)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.14237787515666295 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054515630718379754) - present_state_Q ( -0.054515630718379754)) * f1( 0.9671509475201846)
w2 ( -0.65879702480849 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.054515630718379754) - present_state_Q (-0.054515630718379754)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09449229708260848 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25137081996775196) - present_state_Q ( -0.25137081996775196)) * f1( 1.0107426786570328)
w2 ( -0.6872230005302314 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.25137081996775196) - present_state_Q (-0.25137081996775196)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05240663994697412 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3165352994393962) - present_state_Q ( -0.3165352994393962)) * f1( 1.0138233891699375)
w2 ( -0.712130094360504 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3165352994393962) - present_state_Q (-0.3165352994393962)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.029304464034896203 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3740857305347213) - present_state_Q ( -0.39220571083043043)) * f1( 0.6692347729478325)
w2 ( -0.7328422660938865 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3740857305347213) - present_state_Q (-0.39220571083043043)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.004237987019841235 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41000849401629347) - present_state_Q ( -0.41000849401629347)) * f1( 1.0133905061247672)
w2 ( -0.7527018074170067 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41000849401629347) - present_state_Q (-0.41000849401629347)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.041080295172927314 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.604741945912993) - present_state_Q ( -0.604741945912993)) * f1( 0.6088975655910247)
w2 ( -0.6931603873112712 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.604741945912993) - present_state_Q (-0.604741945912993)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.08712663011865748 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3879921063511181) - present_state_Q ( -0.5270439435965844)) * f1( 0.6690401355865925)
w2 ( -0.6381008086743534 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3879921063511181) - present_state_Q (-0.5270439435965844)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1277366660861809 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4513299018138477) - present_state_Q ( -0.4521965418736348)) * f1( 0.6689585605052207)
w2 ( -0.5895357245389734 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4513299018138477) - present_state_Q (-0.4521965418736348)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1649211273810678 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3846675336695) - present_state_Q ( -0.3846675336695)) * f1( 0.6807837453891914)
w2 ( -0.5458396621147694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3846675336695) - present_state_Q (-0.3846675336695)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.19835161463634227 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3246537779143852) - present_state_Q ( -0.3246537779143852)) * f1( 0.6792213560279694)
w2 ( -0.5064645901049337 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3246537779143852) - present_state_Q (-0.3246537779143852)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.22826145007802898 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27184383826765135) - present_state_Q ( -0.2717115767898892)) * f1( 0.6728460241614036)
w2 ( -0.4709024146678838 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.27184383826765135) - present_state_Q (-0.2717115767898892)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.255467681946701 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22106236010661065) - present_state_Q ( -0.22106236010661065)) * f1( 0.6819354366428746)
w2 ( -0.4389859247402078 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.22106236010661065) - present_state_Q (-0.22106236010661065)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.27993430838303573 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17740263094516456) - present_state_Q ( -0.17740263094516456)) * f1( 0.6802665116883913)
w2 ( -0.41021293531215597 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17740263094516456) - present_state_Q (-0.17740263094516456)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.30198660017503987 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13748418966767298) - present_state_Q ( -0.13748418966767298)) * f1( 0.6811818089876102)
w2 ( -0.3843140736560835 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13748418966767298) - present_state_Q (-0.13748418966767298)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3150450990764305 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10221916732744915) - present_state_Q ( -0.10524901087606839)) * f1( 0.6695735768792269)
w2 ( -0.3687119061246176 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.10221916732744915) - present_state_Q (-0.10524901087606839)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.33351826429832243 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0809343028953044) - present_state_Q ( -0.08413947047748183)) * f1( 0.6692059487364523)
w2 ( -0.3466282229095815 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0809343028953044) - present_state_Q (-0.08413947047748183)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3298099506614802 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0505161501863734) - present_state_Q ( -0.0505161501863734)) * f1( 0.6799820352220288)
w2 ( -0.3509910600961626 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0505161501863734) - present_state_Q (-0.0505161501863734)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3332659083307144 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056452342050468424) - present_state_Q ( -0.056452342050468424)) * f1( 0.6802114538282282)
w2 ( -0.3469264914685289 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.056452342050468424) - present_state_Q (-0.056452342050468424)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.2887186604451064 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05067353829018417) - present_state_Q ( -0.05067353829018417)) * f1( 0.6807406614765654)
w2 ( -0.39927799671163566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.05067353829018417) - present_state_Q (-0.05067353829018417)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.2485583983821561 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12274103032310632) - present_state_Q ( -0.12274103032310632)) * f1( 0.681221528054287)
w2 ( -0.446440642528372 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.12274103032310632) - present_state_Q (-0.12274103032310632)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.21248171746801933 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18816772070443172) - present_state_Q ( -0.18816772070443172)) * f1( 0.6798595196065492)
w2 ( -0.4888925666376529 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.18816772070443172) - present_state_Q (-0.18816772070443172)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.23436639201637705 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24672460046495204) - present_state_Q ( -0.24672460046495204)) * f1( 0.6795382424697428)
w2 ( -0.46312839540417633 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.24672460046495204) - present_state_Q (-0.24672460046495204)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.2541212368593853 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21271766084793492) - present_state_Q ( -0.210334076316936)) * f1( 0.6834112972785485)
w2 ( -0.44000341058560494 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.21271766084793492) - present_state_Q (-0.210334076316936)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.27869063648608383 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17917135843781276) - present_state_Q ( -0.17917135843781276)) * f1( 0.6801138392314107)
w2 ( -0.41110307277808245 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17917135843781276) - present_state_Q (-0.17917135843781276)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3008139424205616 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1394863124717507) - present_state_Q ( -0.1394863124717507)) * f1( 0.6795927848123904)
w2 ( -0.3850600582801164 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1394863124717507) - present_state_Q (-0.1394863124717507)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.32064648962758635 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10373783906513656) - present_state_Q ( -0.10675932014918313)) * f1( 0.6691469313396801)
w2 ( -0.36134921538070286 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10373783906513656) - present_state_Q (-0.10675932014918313)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.34301618366318276 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14333021943858149) - present_state_Q ( -0.14333021943858149)) * f1( 0.6799357017609602)
w2 ( -0.32844949563123055 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14333021943858149) - present_state_Q (-0.14333021943858149)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.34202106383247516 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0948736000639189) - present_state_Q ( -0.0948736000639189)) * f1( 0.6809471584485541)
w2 ( -0.32991087162547783 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0948736000639189) - present_state_Q (-0.0948736000639189)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.3548214060119455 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09761921752350294) - present_state_Q ( -0.1010902364567132)) * f1( 0.6690249793528592)
w2 ( -0.3107780401550415 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.09761921752350294) - present_state_Q (-0.1010902364567132)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.37264854443044215 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06958246317304148) - present_state_Q ( -0.07315425458494296)) * f1( 0.669699689883137)
w2 ( -0.2841584393282776 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06958246317304148) - present_state_Q (-0.07315425458494296)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.38813079629705577 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030282533140306045) - present_state_Q ( -0.030282533140306045)) * f1( 0.6812743803306591)
w2 ( -0.26143301134565006 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.030282533140306045) - present_state_Q (-0.030282533140306045)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.34115441379192757 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0031010796824981757) - present_state_Q ( -0.0006165789627977092)) * f1( 0.6719807726445819)
w2 ( -0.3313403426525453 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0031010796824981757) - present_state_Q (-0.0006165789627977092)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.3336162989672151 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09915610470624844) - present_state_Q ( -0.09915610470624844)) * f1( 0.6805840070060112)
w2 ( -0.3424162932289829 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09915610470624844) - present_state_Q (-0.09915610470624844)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2931566745019818 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11584195772540334) - present_state_Q ( -0.11584195772540334)) * f1( 0.6791464811671126)
w2 ( -0.4019905170336966 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.11584195772540334) - present_state_Q (-0.11584195772540334)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.25873894688178484 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20307348149086438) - present_state_Q ( -0.20586134911378762)) * f1( 0.669025081052975)
w2 ( -0.4534351169372265 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20307348149086438) - present_state_Q (-0.20586134911378762)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2054349324019668 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18940597984230034) - present_state_Q ( -0.19180031223738397)) * f1( 1.0111921991372284)
w2 ( -0.5061491455119111 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.18940597984230034) - present_state_Q (-0.19180031223738397)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.16147235163487264 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2972969985549132) - present_state_Q ( -0.2972969985549132)) * f1( 1.0166340481391198)
w2 ( -0.549392415641969 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2972969985549132) - present_state_Q (-0.2972969985549132)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.12546209579839845 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38493242860569954) - present_state_Q ( -0.38493242860569954)) * f1( 1.0185024579821105)
w2 ( -0.584748497067456 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38493242860569954) - present_state_Q (-0.38493242860569954)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0960863844123754 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.457220721446309) - present_state_Q ( -0.45706826961837776)) * f1( 1.017679695501373)
w2 ( -0.6136138773200813 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.457220721446309) - present_state_Q (-0.45706826961837776)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07207105133639129 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.515760137940619) - present_state_Q ( -0.515760137940619)) * f1( 1.0183933965035241)
w2 ( -0.6371954649054257 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.515760137940619) - present_state_Q (-0.515760137940619)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.05240112273583149 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5636383743536254) - present_state_Q ( -0.5636383743536254)) * f1( 1.0206190861358864)
w2 ( -0.6564680112135994 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5636383743536254) - present_state_Q (-0.5636383743536254)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03644387167266776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6032375130601775) - present_state_Q ( -0.6032375130601775)) * f1( 1.015827436022155)
w2 ( -0.6721766350381835 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6032375130601775) - present_state_Q (-0.6032375130601775)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023405943983689503 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.635158648940249) - present_state_Q ( -0.635158648940249)) * f1( 1.0157533873026257)
w2 ( -0.6850123566335611 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.635158648940249) - present_state_Q (-0.635158648940249)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.012714616845467675 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6611682418696392) - present_state_Q ( -0.6611682418696392)) * f1( 1.018720491706625)
w2 ( -0.6955072148652935 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6611682418696392) - present_state_Q (-0.6611682418696392)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.004049942897442416 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6826406410464486) - present_state_Q ( -0.6826406410464486)) * f1( 1.0119513608018325)
w2 ( -0.7040695571711132 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6826406410464486) - present_state_Q (-0.6826406410464486)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0006400999490107301 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6999723730894634) - present_state_Q ( -0.7013044340695703)) * f1( 0.6827560712742505)
w2 ( -0.7109388374950508 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6999723730894634) - present_state_Q (-0.7013044340695703)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.004126545170202827 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7113119067230622) - present_state_Q ( -0.7113119067230622)) * f1( 0.5828296480697277)
w2 ( -0.7169207658899752 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7113119067230622) - present_state_Q (-0.7113119067230622)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006920601278943454 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7191041820113911) - present_state_Q ( -0.7191041820113911)) * f1( 0.5291147997559955)
w2 ( -0.72220138950895 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7191041820113911) - present_state_Q (-0.7191041820113911)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.009222223623137474 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7255079658080955) - present_state_Q ( -0.7255935405605354)) * f1( 0.490152649294558)
w2 ( -0.7268971151109773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7255079658080955) - present_state_Q (-0.7255935405605354)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.011068288987061593 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.730935444034751) - present_state_Q ( -0.730935444034751)) * f1( 0.4378910216015487)
w2 ( -0.7311129251478498 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.730935444034751) - present_state_Q (-0.730935444034751)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.012528047247819885 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7352338520375605) - present_state_Q ( -0.7353448962746616)) * f1( 0.3823509787067203)
w2 ( -0.7349307740407592 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7352338520375605) - present_state_Q (-0.7353448962746616)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.020867249187679977 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2976362454847291) - present_state_Q ( -0.444622400292881)) * f1( 0.29245865664044024)
w2 ( -0.7520392474960947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2976362454847291) - present_state_Q (-0.444622400292881)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.028164567537425987 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4562589011421503) - present_state_Q ( -0.4564901037843221)) * f1( 0.2523837828023145)
w2 ( -0.7693873946758883 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4562589011421503) - present_state_Q (-0.4564901037843221)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03083662288453948 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4676412219992018) - present_state_Q ( -0.6215187009343794)) * f1( 0.21334555148713186)
w2 ( -0.7794070283771316 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4676412219992018) - present_state_Q (-0.6215187009343794)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.033055831148909955 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6286234668150289) - present_state_Q ( -0.6286234668150289)) * f1( 0.1653178473016086)
w2 ( -0.7901461387664496 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6286234668150289) - present_state_Q (-0.6286234668150289)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.025577556305449636 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6354559618810325) - present_state_Q ( -0.6357941312319356)) * f1( 0.11124270940914306)
w2 ( -0.7363662559629429 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.6354559618810325) - present_state_Q (-0.6357941312319356)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.004111550259857474 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16308969189714803) - present_state_Q ( -0.16310525407756227)) * f1( 0.6189802769235023)
w2 ( -0.729430330265186 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16308969189714803) - present_state_Q (-0.16310525407756227)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.014953498510526516 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14823710853576272) - present_state_Q ( -0.14823710853576272)) * f1( 0.5718141173366109)
w2 ( -0.7227620623115423 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14823710853576272) - present_state_Q (-0.14823710853576272)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03643971259592599 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13454610344360238) - present_state_Q ( -0.13454610344360238)) * f1( 0.6691617357411138)
w2 ( -0.7163402324495575 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13454610344360238) - present_state_Q (-0.13454610344360238)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05878823027631499 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11655877021194362) - present_state_Q ( -0.11655877021194362)) * f1( 0.7329716503020396)
w2 ( -0.7102421745857425 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11655877021194362) - present_state_Q (-0.11655877021194362)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.034819558737144124 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3861659282464145) - present_state_Q ( -0.3861659282464145)) * f1( 0.6800574930920857)
w2 ( -0.731389214460436 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3861659282464145) - present_state_Q (-0.3861659282464145)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.020923305507355466 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5596788261153198) - present_state_Q ( -0.5603733849961352)) * f1( 0.7104623800365425)
w2 ( -0.7470367742696677 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5596788261153198) - present_state_Q (-0.5603733849961352)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.00928963035431976 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5836941365612475) - present_state_Q ( -0.5836941365612475)) * f1( 0.6660172719644072)
w2 ( -0.7610107964372579 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5836941365612475) - present_state_Q (-0.5836941365612475)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0125257185254163 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44974216465846556) - present_state_Q ( -0.44974216465846556)) * f1( 0.7389221036870721)
w2 ( -0.7787247195457008 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.44974216465846556) - present_state_Q (-0.44974216465846556)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.031177622951663146 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4758319782336577) - present_state_Q ( -0.4758319782336577)) * f1( 0.6863595480604514)
w2 ( -0.7950297927210832 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4758319782336577) - present_state_Q (-0.4758319782336577)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.023741023535207614 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4982244155268364) - present_state_Q ( -0.657230374071053)) * f1( 0.6801846287981732)
w2 ( -0.7304371581196136 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4982244155268364) - present_state_Q (-0.657230374071053)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06773190279370589 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4201988835672784) - present_state_Q ( -0.4201988835672784)) * f1( 0.7608522554936208)
w2 ( -0.6957464184069806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4201988835672784) - present_state_Q (-0.4201988835672784)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09554580459952926 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5031230262968762) - present_state_Q ( -0.5032143803477289)) * f1( 0.7881478620266407)
w2 ( -0.6675142521895373 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.5031230262968762) - present_state_Q (-0.5032143803477289)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.14140210409942203 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6098915171572422) - present_state_Q ( -0.6089354905282339)) * f1( 0.6130961155942997)
w2 ( -0.5927196183082863 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6098915171572422) - present_state_Q (-0.6089354905282339)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.18006388802298995 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5069445434625248) - present_state_Q ( -0.5097725844893565)) * f1( 0.5866039571844597)
w2 ( -0.526811805293976 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5069445434625248) - present_state_Q (-0.5097725844893565)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.21557044364953046 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38378454282634067) - present_state_Q ( -0.4161559037465659)) * f1( 0.6145368888918021)
w2 ( -0.46903406034758277 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.38378454282634067) - present_state_Q (-0.4161559037465659)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.22105527050937085 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29952649927963704) - present_state_Q ( -0.29983135148415446)) * f1( 0.7849068081824531)
w2 ( -0.4620461901919637 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.29952649927963704) - present_state_Q (-0.29983135148415446)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.23297181363768893 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.288061903291431) - present_state_Q ( -0.33311010897230314)) * f1( 0.5832753090327008)
w2 ( -0.4416157983276477 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.288061903291431) - present_state_Q (-0.33311010897230314)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.24339046518405577 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2579550660198787) - present_state_Q ( -0.2579550660198787)) * f1( 0.7883388528424853)
w2 ( -0.4283998423858586 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2579550660198787) - present_state_Q (-0.2579550660198787)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2522824063051197 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2358657414451635) - present_state_Q ( -0.23617304671927888)) * f1( 0.7897876998641452)
w2 ( -0.4171411951283823 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2358657414451635) - present_state_Q (-0.23617304671927888)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.21255060396938455 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21780693874863105) - present_state_Q ( -0.2181239936266639)) * f1( 0.7888667482464854)
w2 ( -0.4675068651532023 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21780693874863105) - present_state_Q (-0.2181239936266639)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.18908332587920268 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29864286054704187) - present_state_Q ( -0.3396726504604798)) * f1( 0.6014295528002147)
w2 ( -0.5065260287126248 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.29864286054704187) - present_state_Q (-0.3396726504604798)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.15888483503708312 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3564629783796668) - present_state_Q ( -0.35608741746576666)) * f1( 0.7956207166726429)
w2 ( -0.5444819167498447 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3564629783796668) - present_state_Q (-0.35608741746576666)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.13328284509543 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4186341538109213) - present_state_Q ( -0.4186341538109213)) * f1( 0.7920690663117789)
w2 ( -0.5768048429068617 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4186341538109213) - present_state_Q (-0.4186341538109213)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.11145468547132115 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47111989010254085) - present_state_Q ( -0.4713165444233289)) * f1( 0.791461935014995)
w2 ( -0.6043843873655542 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.47111989010254085) - present_state_Q (-0.4713165444233289)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.09283262664438714 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5160534853126484) - present_state_Q ( -0.51621211254635)) * f1( 0.7911042451588288)
w2 ( -0.6279237109640456 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5160534853126484) - present_state_Q (-0.51621211254635)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07695326312001223 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5547439894963481) - present_state_Q ( -0.5545548798873636)) * f1( 0.7903345378531096)
w2 ( -0.6480156628702728 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5547439894963481) - present_state_Q (-0.5545548798873636)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06336549109125657 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.586987917631509) - present_state_Q ( -0.5870863250375297)) * f1( 0.7917706847300413)
w2 ( -0.6651769095428349 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.586987917631509) - present_state_Q (-0.5870863250375297)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.051784693063745835 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.615066219888959) - present_state_Q ( -0.615066219888959)) * f1( 0.7908198735761142)
w2 ( -0.6798209497528286 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.615066219888959) - present_state_Q (-0.615066219888959)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04197028223730806 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6391001190159404) - present_state_Q ( -0.6391001190159404)) * f1( 0.7863487901098829)
w2 ( -0.6923019390413939 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6391001190159404) - present_state_Q (-0.6391001190159404)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03348808032650661 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6589392223197285) - present_state_Q ( -0.6589983651645471)) * f1( 0.7935036912199442)
w2 ( -0.7029914947481365 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6589392223197285) - present_state_Q (-0.6589983651645471)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02625922556636085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6764469320512553) - present_state_Q ( -0.6764469320512553)) * f1( 0.7926570420900054)
w2 ( -0.7121112708635235 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6764469320512553) - present_state_Q (-0.6764469320512553)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02010101968208075 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6912896480519748) - present_state_Q ( -0.6913265936186928)) * f1( 0.791519048888356)
w2 ( -0.719891507982174 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6912896480519748) - present_state_Q (-0.6913265936186928)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.014826841433211054 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7039109390727387) - present_state_Q ( -0.7039379557828933)) * f1( 0.7936688014639738)
w2 ( -0.7265368217946121 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7039109390727387) - present_state_Q (-0.7039379557828933)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.010333523619675426 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7147643989696038) - present_state_Q ( -0.714785154751097)) * f1( 0.7925941001292505)
w2 ( -0.7322059503091984 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7147643989696038) - present_state_Q (-0.714785154751097)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.006546953006169547 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7240919677849723) - present_state_Q ( -0.7241054206935756)) * f1( 0.7839077853559137)
w2 ( -0.7370363279176906 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7240919677849723) - present_state_Q (-0.7241054206935756)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0032850000679503417 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7318814104890666) - present_state_Q ( -0.7318679464942642)) * f1( 0.789433102476212)
w2 ( -0.7411683473731548 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7318814104890666) - present_state_Q (-0.7318679464942642)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.000499731311048733 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7385813459678351) - present_state_Q ( -0.7385751410871878)) * f1( 0.7894082899015135)
w2 ( -0.7446966467241144 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7385813459678351) - present_state_Q (-0.7385751410871878)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.001882607346692251 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7443024739672623) - present_state_Q ( -0.7443014994636181)) * f1( 0.7907194361445954)
w2 ( -0.7477095215174252 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7443024739672623) - present_state_Q (-0.7443014994636181)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.003364978304981357 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7487788015875967) - present_state_Q ( -0.7487788015875967)) * f1( 0.567978273350612)
w2 ( -0.7503194293745414 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7487788015875967) - present_state_Q (-0.7487788015875967)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.004594388308874359 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.75204351485894) - present_state_Q ( -0.7521108126813817)) * f1( 0.5323610271686845)
w2 ( -0.7526287832549927 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.75204351485894) - present_state_Q (-0.7521108126813817)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023448477072519312 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7547606652194617) - present_state_Q ( -0.7548525499514415)) * f1( 0.4840180121809557)
w2 ( -0.6946911349120432 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.7547606652194617) - present_state_Q (-0.7548525499514415)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.01868383341495288 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6817622496117843) - present_state_Q ( -0.6817622496117843)) * f1( 0.5513742005620896)
w2 ( -0.7033325324469826 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6817622496117843) - present_state_Q (-0.6817622496117843)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.013941545188146592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6920574960163405) - present_state_Q ( -0.6918747560535783)) * f1( 0.6132454801397702)
w2 ( -0.7110656318017882 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6920574960163405) - present_state_Q (-0.6918747560535783)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.008547049361641381 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7000009196237534) - present_state_Q ( -0.7002787475342307)) * f1( 0.7737222898885467)
w2 ( -0.7180377662446026 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7000009196237534) - present_state_Q (-0.7002787475342307)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.00385560840352056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7112570424816731) - present_state_Q ( -0.7113317417902889)) * f1( 0.7846011144395536)
w2 ( -0.7240171624903904 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7112570424816731) - present_state_Q (-0.7113317417902889)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0007574345283958566 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7209905853998699) - present_state_Q ( -0.7216493878233197)) * f1( 0.6141118130432406)
w2 ( -0.7290621295620571 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7209905853998699) - present_state_Q (-0.7216493878233197)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0027503897209610077 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.728462507780427) - present_state_Q ( -0.7284634856518695)) * f1( 0.790357301845586)
w2 ( -0.7335004060746744 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.728462507780427) - present_state_Q (-0.7284634856518695)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.004916020384695175 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7350215732332043) - present_state_Q ( -0.7350494053019934)) * f1( 0.5631926324890983)
w2 ( -0.7373456812768071 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7350215732332043) - present_state_Q (-0.7350494053019934)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006575979243410415 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7397286818564539) - present_state_Q ( -0.7397286818564539)) * f1( 0.4847418019391536)
w2 ( -0.7407700999097262 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7397286818564539) - present_state_Q (-0.7397286818564539)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.007262608138289658 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.595381855488391) - present_state_Q ( -0.7436037468608827)) * f1( 0.4309087432105257)
w2 ( -0.7423635437785219 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.595381855488391) - present_state_Q (-0.7436037468608827)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.017111286178606328 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29948189682582343) - present_state_Q ( -0.4479546055815279)) * f1( 0.3492518481125203)
w2 ( -0.7592831588245852 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.29948189682582343) - present_state_Q (-0.4479546055815279)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.029025351022755795 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3085406013486344) - present_state_Q ( -0.3085406013486344)) * f1( 0.2821142588822928)
w2 ( -0.7761756971760343 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3085406013486344) - present_state_Q (-0.3085406013486344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03577871817997404 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47284800469870986) - present_state_Q ( -0.47284800469870986)) * f1( 0.24608096513594102)
w2 ( -0.7926419049223039 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.47284800469870986) - present_state_Q (-0.47284800469870986)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03840289978079497 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6413395896749136) - present_state_Q ( -0.6417876285582942)) * f1( 0.2144879696877006)
w2 ( -0.8024296113550397 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6413395896749136) - present_state_Q (-0.6417876285582942)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04283092426978645 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.487594969646026) - present_state_Q ( -0.48797852558534793)) * f1( 0.16979860399982133)
w2 ( -0.8180764696377949 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.487594969646026) - present_state_Q (-0.48797852558534793)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0385788186686811 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49529459500622436) - present_state_Q ( -0.6589098889337833)) * f1( 0.10386685086517032)
w2 ( -0.785326035283142 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.49529459500622436) - present_state_Q (-0.6589098889337833)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.028649233102187963 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16720174008485736) - present_state_Q ( -0.1679711326243319)) * f1( 0.28269205600525804)
w2 ( -0.7783010161108251 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16720174008485736) - present_state_Q (-0.1679711326243319)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.020144072521056593 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.162693951936499) - present_state_Q ( -0.162693951936499)) * f1( 0.24551263516358482)
w2 ( -0.7713725249759682 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.162693951936499) - present_state_Q (-0.162693951936499)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.000704822739399754 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46795736605929206) - present_state_Q ( -0.6225121334354736)) * f1( 0.2687695573494197)
w2 ( -0.7093152132296047 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.46795736605929206) - present_state_Q (-0.6225121334354736)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.026827359561413095 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7090955512685636) - present_state_Q ( -0.7090955512685636)) * f1( 0.31165561035688016)
w2 ( -0.625496613615434 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7090955512685636) - present_state_Q (-0.7090955512685636)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.057566066308049196 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7408517027770329) - present_state_Q ( -0.7410845297925426)) * f1( 0.35454128551879266)
w2 ( -0.5214566904736532 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7408517027770329) - present_state_Q (-0.7410845297925426)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09560679275139478 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8102055143754004) - present_state_Q ( -0.8107777272954959)) * f1( 0.4091468980407984)
w2 ( -0.37269554233638025 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.8102055143754004) - present_state_Q (-0.8107777272954959)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.11494307381012923 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4758817585377821) - present_state_Q ( -0.5504208670050581)) * f1( 0.4800077422582594)
w2 ( -0.3082423117521754 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.4758817585377821) - present_state_Q (-0.5504208670050581)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.10575133197072153 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45663345026890106) - present_state_Q ( -0.45663345026890106)) * f1( 0.31802045415074254)
w2 ( -0.35448709491345365 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.45663345026890106) - present_state_Q (-0.45663345026890106)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12481648044723884 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3840198814998861) - present_state_Q ( -0.5258147194652676)) * f1( 0.3911499895595702)
w2 ( -0.276501057903009 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.3840198814998861) - present_state_Q (-0.5258147194652676)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.10729167246244187 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27653848254037305) - present_state_Q ( -0.3318386941209748)) * f1( 0.4427523252155626)
w2 ( -0.33191517948163773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.27653848254037305) - present_state_Q (-0.3318386941209748)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09165176013351004 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34512150664695423) - present_state_Q ( -0.4125606026466597)) * f1( 0.48578465999659315)
w2 ( -0.37698839620416275 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.34512150664695423) - present_state_Q (-0.4125606026466597)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.07755582564617432 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4030494790962682) - present_state_Q ( -0.4784471583371008)) * f1( 0.5383049521019351)
w2 ( -0.41364848674431637 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4030494790962682) - present_state_Q (-0.4784471583371008)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06472151596431386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5330137203864392) - present_state_Q ( -0.5337674798316653)) * f1( 0.5846163229211162)
w2 ( -0.44438323165329335 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5330137203864392) - present_state_Q (-0.5337674798316653)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05883530814226406 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6719214729322295) - present_state_Q ( -0.6712892215873859)) * f1( 0.6137672838161956)
w2 ( -0.4597276997662273 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6719214729322295) - present_state_Q (-0.6712892215873859)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.05254488634698355 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6891602787153404) - present_state_Q ( -0.6891602787153404)) * f1( 0.788710765284311)
w2 ( -0.4724886196312183 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6891602787153404) - present_state_Q (-0.6891602787153404)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.04804136477111965 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7145691608143971) - present_state_Q ( -0.7144624743830196)) * f1( 0.7901685570838295)
w2 ( -0.4816077303029655 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7145691608143971) - present_state_Q (-0.7144624743830196)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.044800720179711734 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7324280649645161) - present_state_Q ( -0.7324280649645161)) * f1( 0.7939887574376213)
w2 ( -0.4881380889480752 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7324280649645161) - present_state_Q (-0.7324280649645161)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.042492434880062074 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.745463226990012) - present_state_Q ( -0.745463226990012)) * f1( 0.7936862439771862)
w2 ( -0.4927913842615135 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.745463226990012) - present_state_Q (-0.745463226990012)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.040852527997054744 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7547878596665616) - present_state_Q ( -0.7547878596665616)) * f1( 0.792572966150789)
w2 ( -0.4961019324695286 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7547878596665616) - present_state_Q (-0.7547878596665616)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03969802188293028 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7615231155607685) - present_state_Q ( -0.7615231155607685)) * f1( 0.7891794699412914)
w2 ( -0.4984426038287779 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7615231155607685) - present_state_Q (-0.7615231155607685)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.039557040262784345 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7663542656081489) - present_state_Q ( -0.7742310514796799)) * f1( 0.5863545220215046)
w2 ( -0.4988273038417595 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7663542656081489) - present_state_Q (-0.7742310514796799)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.038802343216658874 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7670289549393258) - present_state_Q ( -0.7670847792554178)) * f1( 0.7846620142761062)
w2 ( -0.5003662024399218 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7670289549393258) - present_state_Q (-0.7670847792554178)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03881903853295989 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7698255985640153) - present_state_Q ( -0.7772602874955894)) * f1( 0.6011398919401155)
w2 ( -0.5003217660176518 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7698255985640153) - present_state_Q (-0.7772602874955894)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.038243275670255164 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7696568644105539) - present_state_Q ( -0.7697101194968782)) * f1( 0.7935463446682094)
w2 ( -0.5014826567287202 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7696568644105539) - present_state_Q (-0.7697101194968782)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.038346908713538 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7723401520726456) - present_state_Q ( -0.7789242520090957)) * f1( 0.6131273628083592)
w2 ( -0.5012122188404272 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7723401520726456) - present_state_Q (-0.7789242520090957)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03792367735498215 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7717381644449541) - present_state_Q ( -0.7717906613949171)) * f1( 0.7862143197770367)
w2 ( -0.5020735236483598 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7717381644449541) - present_state_Q (-0.7717906613949171)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03764511743782363 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7733463451337604) - present_state_Q ( -0.7737605308541916)) * f1( 0.7793839902843968)
w2 ( -0.5026453802338293 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7733463451337604) - present_state_Q (-0.7737605308541916)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03742040499890215 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7746918541389005) - present_state_Q ( -0.774613179577017)) * f1( 0.7868066515141208)
w2 ( -0.503102341167729 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7746918541389005) - present_state_Q (-0.774613179577017)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.037253261399210945 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775425086489328) - present_state_Q ( -0.775425086489328)) * f1( 0.789373054083867)
w2 ( -0.5034411287132657 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775425086489328) - present_state_Q (-0.775425086489328)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03758369424577063 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7760943748070698) - present_state_Q ( -0.7831045257548375)) * f1( 0.6013240007722463)
w2 ( -0.5025619145894049 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7760943748070698) - present_state_Q (-0.7831045257548375)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03785349302423527 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774596294837352) - present_state_Q ( -0.7820607162968423)) * f1( 0.586380543170945)
w2 ( -0.5018257406993077 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774596294837352) - present_state_Q (-0.7820607162968423)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03751912397199917 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730670778820333) - present_state_Q ( -0.7730670778820333)) * f1( 0.788675095789583)
w2 ( -0.5025040814842949 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730670778820333) - present_state_Q (-0.7730670778820333)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.0377715056511194 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774484352370388) - present_state_Q ( -0.7816919026217161)) * f1( 0.5947534321379514)
w2 ( -0.5018251267027466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774484352370388) - present_state_Q (-0.7816919026217161)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.037436826721354954 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7731587243987031) - present_state_Q ( -0.7730795897890137)) * f1( 0.7900297438764281)
w2 ( -0.5025029319268837 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7731587243987031) - present_state_Q (-0.7730795897890137)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.0371921786754632 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7742903087365315) - present_state_Q ( -0.7743414252922661)) * f1( 0.7923552391748804)
w2 ( -0.5029969488199055 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7742903087365315) - present_state_Q (-0.7743414252922661)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.037061758872345 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7754570528317503) - present_state_Q ( -0.7758688238360911)) * f1( 0.7777520786874819)
w2 ( -0.503265249851439 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7754570528317503) - present_state_Q (-0.7758688238360911)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036920388235531613 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7757993272742577) - present_state_Q ( -0.7757993272742577)) * f1( 0.7939470058449182)
w2 ( -0.5035501467239458 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7757993272742577) - present_state_Q (-0.7757993272742577)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036822625479824386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.776407725218264) - present_state_Q ( -0.776407725218264)) * f1( 0.7928548679744938)
w2 ( -0.5037474342925158 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.776407725218264) - present_state_Q (-0.776407725218264)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03678131900558946 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767617248686141) - present_state_Q ( -0.7771489033504191)) * f1( 0.7834039844174554)
w2 ( -0.5038317973543466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767617248686141) - present_state_Q (-0.7771489033504191)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03675096331342496 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.776918250100674) - present_state_Q ( -0.7773044983676414)) * f1( 0.783723318756801)
w2 ( -0.5038937696171347 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.776918250100674) - present_state_Q (-0.7773044983676414)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03670777011769673 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7771708210267938) - present_state_Q ( -0.7771708210267938)) * f1( 0.7907060860635076)
w2 ( -0.5039811713892764 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7771708210267938) - present_state_Q (-0.7771708210267938)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03667424489861787 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7773072851560834) - present_state_Q ( -0.7773072851560834)) * f1( 0.7917285352276933)
w2 ( -0.5040489223268004 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7773072851560834) - present_state_Q (-0.7773072851560834)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036680738652547784 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7774205979902918) - present_state_Q ( -0.77782517594485)) * f1( 0.7812867001689959)
w2 ( -0.5040356237434691 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7774205979902918) - present_state_Q (-0.77782517594485)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036672162635184774 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775937959697112) - present_state_Q ( -0.7776501781463131)) * f1( 0.7853391425975736)
w2 ( -0.5040530959755744 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775937959697112) - present_state_Q (-0.7776501781463131)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03664783480487691 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7773874250196) - present_state_Q ( -0.777431667376636)) * f1( 0.792243601047079)
w2 ( -0.5041022279956262 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7773874250196) - present_state_Q (-0.777431667376636)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03663344358813071 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775213038875104) - present_state_Q ( -0.7775702241570484)) * f1( 0.7911337952247948)
w2 ( -0.5041313329926986 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775213038875104) - present_state_Q (-0.7775702241570484)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03661801171137196 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777561541327718) - present_state_Q ( -0.777561541327718)) * f1( 0.7929527943698862)
w2 ( -0.5041624710415072 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777561541327718) - present_state_Q (-0.777561541327718)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036614012333857346 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777215475240956) - present_state_Q ( -0.7777215475240956)) * f1( 0.7902779203418361)
w2 ( -0.5041705681980374 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777215475240956) - present_state_Q (-0.7777215475240956)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036618196532859464 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779174439719032) - present_state_Q ( -0.7778448872436)) * f1( 0.7873494336102113)
w2 ( -0.5041620653426119 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779174439719032) - present_state_Q (-0.7778448872436)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03661107816352295 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777595068614324) - present_state_Q ( -0.7776859845217134)) * f1( 0.7912273888329342)
w2 ( -0.5041764599289207 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777595068614324) - present_state_Q (-0.7776859845217134)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036601601045713056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776450109179283) - present_state_Q ( -0.7776450109179283)) * f1( 0.793129468590081)
w2 ( -0.504195578356739 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776450109179283) - present_state_Q (-0.7776450109179283)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03660545272640044 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779125792453478) - present_state_Q ( -0.7778400850153716)) * f1( 0.7888409121598404)
w2 ( -0.504187766022205 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779125792453478) - present_state_Q (-0.7778400850153716)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036601151454856 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777174167447259) - present_state_Q ( -0.7777174167447259)) * f1( 0.7917675300297329)
w2 ( -0.5041964580109645 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777174167447259) - present_state_Q (-0.7777174167447259)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03664107316740671 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779115494197024) - present_state_Q ( -0.778305495114088)) * f1( 0.7761733326476037)
w2 ( -0.5041141635834256 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779115494197024) - present_state_Q (-0.778305495114088)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03666968927701091 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777476531973855) - present_state_Q ( -0.7781434552714693)) * f1( 0.7761564824282825)
w2 ( -0.5040551731911487 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777476531973855) - present_state_Q (-0.7781434552714693)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03665432056676713 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776409601357144) - present_state_Q ( -0.7775692190476297)) * f1( 0.788636572285827)
w2 ( -0.5040863535056994 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776409601357144) - present_state_Q (-0.7775692190476297)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036641665344662094 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775388218533898) - present_state_Q ( -0.7775936210463613)) * f1( 0.789662558607088)
w2 ( -0.5041119952879358 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775388218533898) - present_state_Q (-0.7775936210463613)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03662450127529658 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776156935596702) - present_state_Q ( -0.7775449560868779)) * f1( 0.7923830999687113)
w2 ( -0.5041466534109901 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776156935596702) - present_state_Q (-0.7775449560868779)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03704304289977718 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777171748796406) - present_state_Q ( -0.7847884414133166)) * f1( 0.5964915093329302)
w2 ( -0.5030239775829337 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777171748796406) - present_state_Q (-0.7847884414133166)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03689073582846109 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7757198594749631) - present_state_Q ( -0.7756397319496602)) * f1( 0.7882352500585036)
w2 ( -0.5033331382225875 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7757198594749631) - present_state_Q (-0.7756397319496602)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036780315774585286 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761697751823707) - present_state_Q ( -0.7762178849602329)) * f1( 0.7892262255567404)
w2 ( -0.5035569930318682 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761697751823707) - present_state_Q (-0.7762178849602329)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036688025979739664 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7765651273592873) - present_state_Q ( -0.7764939218018259)) * f1( 0.7938286127858138)
w2 ( -0.5037430075813246 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7765651273592873) - present_state_Q (-0.7764939218018259)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.03681862705142815 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3169286169560931) - present_state_Q ( -0.31777957909257526)) * f1( 0.09458985799067265)
w2 ( -0.7778825665872634 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.3169286169560931) - present_state_Q (-0.31777957909257526)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.02507712704420416 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3200115653460611) - present_state_Q ( -0.3200115653460611)) * f1( 0.24059937647273363)
w2 ( -0.7583621502348052 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3200115653460611) - present_state_Q (-0.3200115653460611)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.015327160501790331 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3084640418489483) - present_state_Q ( -0.3084640418489483)) * f1( 0.2041374893544395)
w2 ( -0.7392574447282431 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3084640418489483) - present_state_Q (-0.3084640418489483)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0015548060392794605 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44765848404062014) - present_state_Q ( -0.4478449917805238)) * f1( 0.2799295370513504)
w2 ( -0.7030726961256554 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.44765848404062014) - present_state_Q (-0.4478449917805238)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.020067639069564158 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42135949334410516) - present_state_Q ( -0.4213466678835842)) * f1( 0.3196217272473178)
w2 ( -0.6683200530127049 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.42135949334410516) - present_state_Q (-0.4213466678835842)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04129924347884324 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39348739916924513) - present_state_Q ( -0.3933006022242195)) * f1( 0.38327526007126644)
w2 ( -0.6350829412742672 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.39348739916924513) - present_state_Q (-0.3933006022242195)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06972505995309733 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48974521685470396) - present_state_Q ( -0.48974521685470396)) * f1( 0.44361917123482936)
w2 ( -0.5838212856607286 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.48974521685470396) - present_state_Q (-0.48974521685470396)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09773339827085735 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.433055019358015) - present_state_Q ( -0.43399600847182396)) * f1( 0.4741626623053231)
w2 ( -0.5365660451378468 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.433055019358015) - present_state_Q (-0.43399600847182396)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.10933504332359775 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3828984488008277) - present_state_Q ( -0.3828984488008277)) * f1( 0.47429423441292495)
w2 ( -0.5169973568241872 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.3828984488008277) - present_state_Q (-0.3828984488008277)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.08978700502621886 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3571414704492897) - present_state_Q ( -0.3571414704492897)) * f1( 0.5163615735072854)
w2 ( -0.5472831709518384 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3571414704492897) - present_state_Q (-0.3571414704492897)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06964149852494446 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38646634567387245) - present_state_Q ( -0.38646634567387245)) * f1( 0.5720225446054308)
w2 ( -0.5754575940633196 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38646634567387245) - present_state_Q (-0.38646634567387245)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04958380124321263 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3008138688287525) - present_state_Q ( -0.4159053876414165)) * f1( 0.6384223279358958)
w2 ( -0.6005916740026364 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3008138688287525) - present_state_Q (-0.4159053876414165)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.029689799747563913 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3259303124139126) - present_state_Q ( -0.4460486472144399)) * f1( 0.6942729505310269)
w2 ( -0.6235152247247925 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3259303124139126) - present_state_Q (-0.4460486472144399)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.010809333396103269 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47811898461747593) - present_state_Q ( -0.47803372818941714)) * f1( 0.6998515236574389)
w2 ( -0.6450974783465789 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.47811898461747593) - present_state_Q (-0.47803372818941714)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.006069049946306276 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5085215715150407) - present_state_Q ( -0.5085484217950527)) * f1( 0.6965795767687969)
w2 ( -0.664481777175095 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5085215715150407) - present_state_Q (-0.5085484217950527)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.019936133443185424 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.535324981594253) - present_state_Q ( -0.535444413975581)) * f1( 0.6358478295031217)
w2 ( -0.6819288239098026 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.535324981594253) - present_state_Q (-0.535444413975581)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.031343220246500776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5567371888077559) - present_state_Q ( -0.5569889920026092)) * f1( 0.5741300291446253)
w2 ( -0.6978236020600559 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5567371888077559) - present_state_Q (-0.5569889920026092)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04061281105617881 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5741091860797227) - present_state_Q ( -0.5741091860797227)) * f1( 0.5057012108845972)
w2 ( -0.7124877406623159 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5741091860797227) - present_state_Q (-0.5741091860797227)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04970830678571123 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5918547887760842) - present_state_Q ( -0.5920979909060314)) * f1( 0.5443552859613072)
w2 ( -0.7258547397000421 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5918547887760842) - present_state_Q (-0.5920979909060314)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05006635281488157 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6100689144444894) - present_state_Q ( -0.7548737096315034)) * f1( 0.5837851218018714)
w2 ( -0.7264680578813366 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6100689144444894) - present_state_Q (-0.7548737096315034)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.044385656254551674 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.898038909642595) - present_state_Q ( -0.898038909642595)) * f1( 0.5248483004573968)
w2 ( -0.7134798556399363 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.898038909642595) - present_state_Q (-0.898038909642595)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04027687405261641 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8766669011505632) - present_state_Q ( -0.8766669011505632)) * f1( 0.46165982688468626)
w2 ( -0.7027998303156755 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8766669011505632) - present_state_Q (-0.8766669011505632)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03736113040165598 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.859355120089889) - present_state_Q ( -0.859355120089889)) * f1( 0.39713418896864816)
w2 ( -0.6939894773459675 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.859355120089889) - present_state_Q (-0.859355120089889)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03491556871309493 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7062643429472353) - present_state_Q ( -0.8450622384164289)) * f1( 0.3285464189467823)
w2 ( -0.6850571808513628 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7062643429472353) - present_state_Q (-0.8450622384164289)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04327458906276099 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42010083743918014) - present_state_Q ( -0.42010083743918014)) * f1( 0.2596700916677904)
w2 ( -0.7043717356296471 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42010083743918014) - present_state_Q (-0.42010083743918014)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04422319811081129 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5729384334340217) - present_state_Q ( -0.7138127805599511)) * f1( 0.21816602155624765)
w2 ( -0.7087198419079922 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5729384334340217) - present_state_Q (-0.7138127805599511)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.05028102389071824 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2895851758106581) - present_state_Q ( -0.2895851758106581)) * f1( 0.13787422230710664)
w2 ( -0.7262947755788085 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2895851758106581) - present_state_Q (-0.2895851758106581)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05386970281892531 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2946679314672811) - present_state_Q ( -0.2946679314672811)) * f1( 0.08253652997952876)
w2 ( -0.7436867300459864 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2946679314672811) - present_state_Q (-0.2946679314672811)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06535781611713654 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16407092008290625) - present_state_Q ( -0.3128082660921035)) * f1( 0.28464189092058684)
w2 ( -0.7598306830826339 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.16407092008290625) - present_state_Q (-0.3128082660921035)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0770755778983067 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16587256273487094) - present_state_Q ( -0.16587256273487094)) * f1( 0.2127737269161589)
w2 ( -0.7708449769534063 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.16587256273487094) - present_state_Q (-0.16587256273487094)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08598964459647593 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3247959200456069) - present_state_Q ( -0.32520772569353934)) * f1( 0.2188726360824007)
w2 ( -0.7871358516058471 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3247959200456069) - present_state_Q (-0.32520772569353934)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09406305326473031 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3321645577354742) - present_state_Q ( -0.3321645577354742)) * f1( 0.2013058336776142)
w2 ( -0.80317792752737 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3321645577354742) - present_state_Q (-0.3321645577354742)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10150255220313048 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3389071580155412) - present_state_Q ( -0.3389917010387572)) * f1( 0.18838990881931744)
w2 ( -0.818973888117882 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3389071580155412) - present_state_Q (-0.3389917010387572)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10678246480532398 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34123002833570965) - present_state_Q ( -0.34123002833570965)) * f1( 0.13438551831937234)
w2 ( -0.8346896070977964 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.34123002833570965) - present_state_Q (-0.34123002833570965)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10749237950960243 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3421063434118387) - present_state_Q ( -0.3421063434118387)) * f1( 0.07707726720604591)
w2 ( -0.8383737787349702 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3421063434118387) - present_state_Q (-0.3421063434118387)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11317822099547213 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19488665013549172) - present_state_Q ( -0.19488665013549172)) * f1( 0.25315184678804875)
w2 ( -0.8428658190325313 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19488665013549172) - present_state_Q (-0.19488665013549172)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1185959283486275 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19599509465847217) - present_state_Q ( -0.19599509465847217)) * f1( 0.24228982052176762)
w2 ( -0.8473379073286789 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19599509465847217) - present_state_Q (-0.19599509465847217)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12325214259857685 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19396298761264752) - present_state_Q ( -0.19396298761264752)) * f1( 0.2065450853835759)
w2 ( -0.8518465735516512 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19396298761264752) - present_state_Q (-0.19396298761264752)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12372629860310265 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19129739128262205) - present_state_Q ( -0.19158925596313411)) * f1( 0.1721669157664518)
w2 ( -0.8523973832149537 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19129739128262205) - present_state_Q (-0.19158925596313411)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1215246717056534 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18669155041565386) - present_state_Q ( -0.18669155041565386)) * f1( 0.1310317527938764)
w2 ( -0.8490369353074719 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.18669155041565386) - present_state_Q (-0.18669155041565386)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1205465975844583 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17725793197326103) - present_state_Q ( -0.17725793197326103)) * f1( 0.061308907954202886)
w2 ( -0.8458462925319532 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17725793197326103) - present_state_Q (-0.17725793197326103)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11366143173139547 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5368213845907144) - present_state_Q ( -0.5368213845907144)) * f1( 0.24317243007215056)
w2 ( -0.8288579377640546 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5368213845907144) - present_state_Q (-0.5368213845907144)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10274115720935444 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3531373296334255) - present_state_Q ( -0.3554083341413755)) * f1( 0.20996708093693364)
w2 ( -0.8080541537169332 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3531373296334255) - present_state_Q (-0.3554083341413755)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09813125314188985 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17486368309090028) - present_state_Q ( -0.17486368309090028)) * f1( 0.12899263262636237)
w2 ( -0.800906607421297 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17486368309090028) - present_state_Q (-0.17486368309090028)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08697570769553417 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3419234199556287) - present_state_Q ( -0.3419234199556287)) * f1( 0.21971366202706782)
w2 ( -0.7805973643028944 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3419234199556287) - present_state_Q (-0.3419234199556287)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07868421248690764 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.326832879934316) - present_state_Q ( -0.326832879934316)) * f1( 0.1677932218067773)
w2 ( -0.760831380625259 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.326832879934316) - present_state_Q (-0.326832879934316)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07307809109965709 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31348174397407214) - present_state_Q ( -0.31348174397407214)) * f1( 0.11627735011633789)
w2 ( -0.7415460378421924 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.31348174397407214) - present_state_Q (-0.31348174397407214)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06922055328479561 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3025867560324397) - present_state_Q ( -0.3025867560324397)) * f1( 0.08167072792615276)
w2 ( -0.7226529146250246 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3025867560324397) - present_state_Q (-0.3025867560324397)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06049009193223638 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16135109895640745) - present_state_Q ( -0.16200334574662595)) * f1( 0.2524216001240624)
w2 ( -0.7157355499080048 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16135109895640745) - present_state_Q (-0.16200334574662595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05300787401100444 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15642819770478364) - present_state_Q ( -0.15642819770478364)) * f1( 0.21955806808924547)
w2 ( -0.7089198423493187 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15642819770478364) - present_state_Q (-0.15642819770478364)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04779352066399717 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15003401413061768) - present_state_Q ( -0.15003401413061768)) * f1( 0.15563811631157337)
w2 ( -0.7022192300949676 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15003401413061768) - present_state_Q (-0.15003401413061768)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0435411558579271 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14656705031550002) - present_state_Q ( -0.14656705031550002)) * f1( 0.12811787479634473)
w2 ( -0.6955810231892886 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14656705031550002) - present_state_Q (-0.14656705031550002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.007836095711667969 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5839946271219039) - present_state_Q ( -0.7231108317597615)) * f1( 0.6322709635982439)
w2 ( -0.6391098862845315 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.5839946271219039) - present_state_Q (-0.7231108317597615)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.022915327809909333 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6439029251365763) - present_state_Q ( -0.7716820120009604)) * f1( 0.6061881623586602)
w2 ( -0.5782348799460552 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6439029251365763) - present_state_Q (-0.7716820120009604)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01786132679102108 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5632732001456238) - present_state_Q ( -0.6789201761348349)) * f1( 0.6529114453235721)
w2 ( -0.5875237372116224 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5632732001456238) - present_state_Q (-0.6789201761348349)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.02064946159369213 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6929218974449953) - present_state_Q ( -0.8104266448873197)) * f1( 0.6778100725998567)
w2 ( -0.5817649134916276 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6929218974449953) - present_state_Q (-0.8104266448873197)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.022800850525219563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6845687767216448) - present_state_Q ( -0.8006873183018427)) * f1( 0.667502177908918)
w2 ( -0.5772526518034726 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6845687767216448) - present_state_Q (-0.8006873183018427)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.016768222963681842 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5601480198148041) - present_state_Q ( -0.6755985501754987)) * f1( 0.7501751730598553)
w2 ( -0.5869026020201904 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5601480198148041) - present_state_Q (-0.6755985501754987)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.010346701681167688 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.69056479083587) - present_state_Q ( -0.69056479083587)) * f1( 0.8181148126471758)
w2 ( -0.5963216046099165 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.69056479083587) - present_state_Q (-0.69056479083587)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0038100181531799115 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4678820354969089) - present_state_Q ( -0.5871463564188921)) * f1( 0.8867800071712166)
w2 ( -0.6122857893229964 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4678820354969089) - present_state_Q (-0.5871463564188921)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006969788128973433 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7381155304909717) - present_state_Q ( -0.7381155304909717)) * f1( 0.8851882504971882)
w2 ( -0.6165693120299715 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7381155304909717) - present_state_Q (-0.7381155304909717)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.009493957822099315 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7461030321929246) - present_state_Q ( -0.7460412005203096)) * f1( 0.8835313169341373)
w2 ( -0.6199976043538494 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7461030321929246) - present_state_Q (-0.7460412005203096)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0018403174361156845 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8761997947250076) - present_state_Q ( -0.8761997947250076)) * f1( 0.8640388743379123)
w2 ( -0.6075964302184984 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8761997947250076) - present_state_Q (-0.8761997947250076)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.003771970628087408 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.852140773101061) - present_state_Q ( -0.8521773976834672)) * f1( 0.8381137662993977)
w2 ( -0.5982215653662278 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.852140773101061) - present_state_Q (-0.8521773976834672)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01917074344127595 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8341415087625996) - present_state_Q ( -0.9537444405946469)) * f1( 0.9040536970052641)
w2 ( -0.5709687190112859 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8341415087625996) - present_state_Q (-0.9537444405946469)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.01959137047235456 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7829419783093828) - present_state_Q ( -0.7833238637916178)) * f1( 0.8362921799716864)
w2 ( -0.5702645657767907 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7829419783093828) - present_state_Q (-0.7833238637916178)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03805086184220825 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.009105648040035) - present_state_Q ( -1.009105648040035)) * f1( 0.8866439631009965)
w2 ( -0.5327894507943051 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -1.009105648040035) - present_state_Q (-1.009105648040035)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.05024036311620418 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9229720999393294) - present_state_Q ( -0.923731932579087)) * f1( 0.9274186481504973)
w2 ( -0.5091312007289773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9229720999393294) - present_state_Q (-0.923731932579087)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04897868699947644 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7638913351327323) - present_state_Q ( -0.7638913351327323)) * f1( 1.0095186994632406)
w2 ( -0.5111308484698639 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7638913351327323) - present_state_Q (-0.7638913351327323)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.056326241957732705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7850633733540074) - present_state_Q ( -0.8868061418151221)) * f1( 0.6784458193211285)
w2 ( -0.49163688366351405 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7850633733540074) - present_state_Q (-0.8868061418151221)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06051215756161575 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.84675462835985) - present_state_Q ( -0.8472927449797321)) * f1( 0.6684920617080874)
w2 ( -0.4803657728776396 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.84675462835985) - present_state_Q (-0.8472927449797321)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0633111536947372 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.823484082279646) - present_state_Q ( -0.823484082279646)) * f1( 0.6804303558037919)
w2 ( -0.47296135154833696 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.823484082279646) - present_state_Q (-0.823484082279646)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.07078763400467838 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9031832466762598) - present_state_Q ( -0.9044486669674495)) * f1( 0.6550826151296625)
w2 ( -0.45013528308837225 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9031832466762598) - present_state_Q (-0.9044486669674495)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07534094852066793 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8520551226347486) - present_state_Q ( -0.8520551226347486)) * f1( 0.681128056050147)
w2 ( -0.4367653610141175 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8520551226347486) - present_state_Q (-0.8520551226347486)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07808562899359352 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8224718877624092) - present_state_Q ( -0.8239771069161274)) * f1( 0.6577248639033774)
w2 ( -0.4284193773861402 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8224718877624092) - present_state_Q (-0.8239771069161274)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07970243133919587 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8036818216901737) - present_state_Q ( -0.804480903639112)) * f1( 0.6705184015033561)
w2 ( -0.4235968330921213 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8036818216901737) - present_state_Q (-0.804480903639112)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08061622673577637 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7935446831316648) - present_state_Q ( -0.7927260249232796)) * f1( 0.6833874493635063)
w2 ( -0.4209225217700987 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7935446831316648) - present_state_Q (-0.7927260249232796)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08118587646228817 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7870980235910825) - present_state_Q ( -0.7870980235910825)) * f1( 0.6791067030282983)
w2 ( -0.4192448775237039 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7870980235910825) - present_state_Q (-0.7870980235910825)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08150771846288188 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7839413115330499) - present_state_Q ( -0.7831124986966667)) * f1( 0.6821045576377371)
w2 ( -0.4183012040150315 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7839413115330499) - present_state_Q (-0.7831124986966667)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08171333927602066 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7811350439264334) - present_state_Q ( -0.7811350439264334)) * f1( 0.680516706266157)
w2 ( -0.4176968961082735 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7811350439264334) - present_state_Q (-0.7811350439264334)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07938526747869001 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7522510923132502) - present_state_Q ( -0.7523244263360376)) * f1( 1.01659492338097)
w2 ( -0.42227703268733097 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7522510923132502) - present_state_Q (-0.7523244263360376)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07813886160397138 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7642211390160113) - present_state_Q ( -0.7641205003068177)) * f1( 1.0132051906159505)
w2 ( -0.42473735540628765 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7642211390160113) - present_state_Q (-0.7641205003068177)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07739585395606148 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7696922152896654) - present_state_Q ( -0.7696922152896654)) * f1( 1.021034781991948)
w2 ( -0.4261927566541479 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7696922152896654) - present_state_Q (-0.7696922152896654)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0770427724858414 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7740272251752717) - present_state_Q ( -0.7739200389393641)) * f1( 1.0138201254743893)
w2 ( -0.4268892933697805 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7740272251752717) - present_state_Q (-0.7739200389393641)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07868137680160563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775460681077772) - present_state_Q ( -0.8018629423178425)) * f1( 0.6738548308507376)
w2 ( -0.42202591852776744 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775460681077772) - present_state_Q (-0.8018629423178425)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07740639349849243 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7638754518272591) - present_state_Q ( -0.7638754518272591)) * f1( 1.0190007913872647)
w2 ( -0.4245283371988608 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7638754518272591) - present_state_Q (-0.7638754518272591)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07674889948645004 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7705021927432515) - present_state_Q ( -0.7705660981655076)) * f1( 1.0140063718863566)
w2 ( -0.4258251614206243 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7705021927432515) - present_state_Q (-0.7705660981655076)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0763524247345688 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7734540425740793) - present_state_Q ( -0.7734540425740793)) * f1( 1.0188586519207983)
w2 ( -0.42660343375729004 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7734540425740793) - present_state_Q (-0.7734540425740793)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07628173832918861 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7752982347107481) - present_state_Q ( -0.7768232480969699)) * f1( 1.0004085617863452)
w2 ( -0.426744748832111 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7752982347107481) - present_state_Q (-0.7768232480969699)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07611197734621457 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7758533517659686) - present_state_Q ( -0.7759159941900771)) * f1( 1.0169341335587003)
w2 ( -0.42707861702941496 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7758533517659686) - present_state_Q (-0.7759159941900771)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0778160747775712 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769995488503453) - present_state_Q ( -0.8031064794454112)) * f1( 0.6707322079046953)
w2 ( -0.4219973121173396 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769995488503453) - present_state_Q (-0.8031064794454112)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07664960202573282 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7649375785365845) - present_state_Q ( -0.7650026719563038)) * f1( 1.015110984512716)
w2 ( -0.4242955292968105 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7649375785365845) - present_state_Q (-0.7650026719563038)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07801573030647466 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.770692089783159) - present_state_Q ( -0.7976080494100043)) * f1( 0.6651438211838431)
w2 ( -0.42018776121047285 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.770692089783159) - present_state_Q (-0.7976080494100043)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07649411826386639 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7611327683762165) - present_state_Q ( -0.7611327683762165)) * f1( 1.015727901712057)
w2 ( -0.4231838629027539 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7611327683762165) - present_state_Q (-0.7611327683762165)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07581618791358237 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7685301009721802) - present_state_Q ( -0.770057380466779)) * f1( 0.9975975548276327)
w2 ( -0.42454298882884167 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7685301009721802) - present_state_Q (-0.770057380466779)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0752753291829999 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7718766690617115) - present_state_Q ( -0.7718766690617115)) * f1( 1.018374976647169)
w2 ( -0.4256051883977336 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7718766690617115) - present_state_Q (-0.7718766690617115)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07499444826759867 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7747069769435134) - present_state_Q ( -0.7747069769435134)) * f1( 1.0163143845703864)
w2 ( -0.4261579325479012 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7747069769435134) - present_state_Q (-0.7747069769435134)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07663282750030487 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761761592824994) - present_state_Q ( -0.8020724145864295)) * f1( 0.6699622661412465)
w2 ( -0.4212669728162653 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761761592824994) - present_state_Q (-0.8020724145864295)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07756720280587412 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7650056488990391) - present_state_Q ( -0.7901765306896598)) * f1( 0.6832243654674306)
w2 ( -0.4185317796563141 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7650056488990391) - present_state_Q (-0.7901765306896598)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07578588125330046 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7582887353103561) - present_state_Q ( -0.7582887353103561)) * f1( 1.0155687088449006)
w2 ( -0.42203980730045 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7582887353103561) - present_state_Q (-0.7582887353103561)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07477979097173085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7669138809644944) - present_state_Q ( -0.76682212620458)) * f1( 1.0194179591064054)
w2 ( -0.4240136596788239 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7669138809644944) - present_state_Q (-0.76682212620458)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07425297701827438 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7720189068519905) - present_state_Q ( -0.7720189068519905)) * f1( 1.0164298605005582)
w2 ( -0.42505025644546557 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7720189068519905) - present_state_Q (-0.7720189068519905)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07396422760377076 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774728311379785) - present_state_Q ( -0.7746318501454459)) * f1( 1.0163722153107977)
w2 ( -0.4256184526439721 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774728311379785) - present_state_Q (-0.7746318501454459)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07377218664188274 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7756887265478628) - present_state_Q ( -0.7756887265478628)) * f1( 1.0214150973737723)
w2 ( -0.42599448186535677 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7756887265478628) - present_state_Q (-0.7756887265478628)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07370102536123732 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769999272891429) - present_state_Q ( -0.7769999272891429)) * f1( 1.0164946961053887)
w2 ( -0.426134494953311 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769999272891429) - present_state_Q (-0.7769999272891429)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07368447022184607 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775962255287677) - present_state_Q ( -0.7775962255287677)) * f1( 1.0131848778474128)
w2 ( -0.4261671743581328 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775962255287677) - present_state_Q (-0.7775962255287677)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07362025621937676 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7771858332314465) - present_state_Q ( -0.7770897574272604)) * f1( 1.0211729970027865)
w2 ( -0.4262929395373096 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7771858332314465) - present_state_Q (-0.7770897574272604)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07361960286131174 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777706341968392) - present_state_Q ( -0.7777706341968392)) * f1( 1.0162317916259673)
w2 ( -0.42629422538187856 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777706341968392) - present_state_Q (-0.7777706341968392)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07533433101436647 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778974051554628) - present_state_Q ( -0.8035128073908565)) * f1( 0.6666110854380961)
w2 ( -0.4211496120068165 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778974051554628) - present_state_Q (-0.8035128073908565)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07425472000743996 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7659426890498827) - present_state_Q ( -0.7659426890498827)) * f1( 1.0135688992736773)
w2 ( -0.4232799279778376 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7659426890498827) - present_state_Q (-0.7659426890498827)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07364110382712108 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7710712609881699) - present_state_Q ( -0.7710712609881699)) * f1( 1.0166167882653339)
w2 ( -0.424487100999967 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7710712609881699) - present_state_Q (-0.7710712609881699)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07329503252789457 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7740008685876807) - present_state_Q ( -0.7740008685876807)) * f1( 1.018090842150598)
w2 ( -0.4251669446541845 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7740008685876807) - present_state_Q (-0.7740008685876807)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07311064859824681 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7757640900420922) - present_state_Q ( -0.7757640900420922)) * f1( 1.0173922664935986)
w2 ( -0.42552940844660786 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7757640900420922) - present_state_Q (-0.7757640900420922)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07303079137545199 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769029813572714) - present_state_Q ( -0.7769029813572714)) * f1( 1.0142959604070934)
w2 ( -0.42568687180229897 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769029813572714) - present_state_Q (-0.7769029813572714)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07298481133771369 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7772742567405565) - present_state_Q ( -0.7772742567405565)) * f1( 1.0146334918253224)
w2 ( -0.4257775055889988 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7772742567405565) - present_state_Q (-0.7772742567405565)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07292641820167961 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7771414226737454) - present_state_Q ( -0.7771414226737454)) * f1( 1.019576363086387)
w2 ( -0.42589204950772463 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7771414226737454) - present_state_Q (-0.7771414226737454)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07288585657096686 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777336302960516) - present_state_Q ( -0.777336302960516)) * f1( 1.0208618205962918)
w2 ( -0.42597151497483177 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777336302960516) - present_state_Q (-0.777336302960516)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07287814400372337 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776936562945128) - present_state_Q ( -0.7776936562945128)) * f1( 1.0187075675354966)
w2 ( -0.42598665684181947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776936562945128) - present_state_Q (-0.7776936562945128)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07289426697649705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778860259387614) - present_state_Q ( -0.7779473323141486)) * f1( 1.0157500905306862)
w2 ( -0.42595491089776494 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778860259387614) - present_state_Q (-0.7779473323141486)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07291204596070418 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777972535326635) - present_state_Q ( -0.777972535326635)) * f1( 1.0143086628847526)
w2 ( -0.42591985453897063 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777972535326635) - present_state_Q (-0.777972535326635)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0729490392728677 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7774369678621745) - present_state_Q ( -0.7781095249894221)) * f1( 1.0112208911029041)
w2 ( -0.4258466888983297 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7774369678621745) - present_state_Q (-0.7781095249894221)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07293211883832935 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777592694846688) - present_state_Q ( -0.777592694846688)) * f1( 1.0157869615362023)
w2 ( -0.4258800038259259 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777592694846688) - present_state_Q (-0.777592694846688)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07291325392200974 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775049184640356) - present_state_Q ( -0.7775650535863189)) * f1( 1.0173152137538037)
w2 ( -0.4259170914779428 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775049184640356) - present_state_Q (-0.7775650535863189)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07460289760017383 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779331590709669) - present_state_Q ( -0.8030435194890837)) * f1( 0.6691604179260726)
w2 ( -0.4208670507615454 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779331590709669) - present_state_Q (-0.8030435194890837)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07350423381893935 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7657865365417194) - present_state_Q ( -0.7657865365417194)) * f1( 1.0180243318215894)
w2 ( -0.4230254741840359 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7657865365417194) - present_state_Q (-0.7657865365417194)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07306380402929039 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7712411197286824) - present_state_Q ( -0.7727100079210438)) * f1( 0.9977784494385243)
w2 ( -0.4239082949944008 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7712411197286824) - present_state_Q (-0.7727100079210438)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07267745738311085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7735543150332561) - present_state_Q ( -0.7735543150332561)) * f1( 1.0164030732067364)
w2 ( -0.4246685182884147 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7735543150332561) - present_state_Q (-0.7735543150332561)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06883372784721851 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04014884435358565) - present_state_Q ( -0.04076630804501646)) * f1( 0.6077179527600028)
w2 ( -0.42593348981622153 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.04014884435358565) - present_state_Q (-0.04076630804501646)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08247836080288024 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046327280489330915) - present_state_Q ( -0.046327280489330915)) * f1( 0.5645403596353914)
w2 ( -0.4210995987674136 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.046327280489330915) - present_state_Q (-0.046327280489330915)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09598770338546712 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03633820839195061) - present_state_Q ( -0.03633820839195061)) * f1( 0.5805366510128319)
w2 ( -0.41644551101635846 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03633820839195061) - present_state_Q (-0.03633820839195061)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10998961733069733 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022189484697615766) - present_state_Q ( -0.022189484697615766)) * f1( 0.6365358827296063)
w2 ( -0.4120461002918014 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.022189484697615766) - present_state_Q (-0.022189484697615766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1355718052330098 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16687768686690174) - present_state_Q ( -0.16687768686690174)) * f1( 0.730523255359613)
w2 ( -0.3910347052009887 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16687768686690174) - present_state_Q (-0.16687768686690174)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1369147173133834 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13054857029800754) - present_state_Q ( -0.13054857029800754)) * f1( 0.7676541050974044)
w2 ( -0.3899850824048963 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13054857029800754) - present_state_Q (-0.13054857029800754)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.11347964419128732 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12306076644231242) - present_state_Q ( -0.12306076644231242)) * f1( 0.8102144544966459)
w2 ( -0.40733980101701145 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12306076644231242) - present_state_Q (-0.12306076644231242)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06784742868645308 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15343767421186447) - present_state_Q ( -0.15241486469579668)) * f1( 0.8106212930959551)
w2 ( -0.4411155351805348 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.15343767421186447) - present_state_Q (-0.15241486469579668)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.026298268197360128 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21027511275526575) - present_state_Q ( -0.2095539819616884)) * f1( 0.8123423424245)
w2 ( -0.4718039469393651 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21027511275526575) - present_state_Q (-0.2095539819616884)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.011303537565763244 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2617885892612889) - present_state_Q ( -0.2617885892612889)) * f1( 0.8097027052324195)
w2 ( -0.4996673631192555 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2617885892612889) - present_state_Q (-0.2617885892612889)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03561212610598583 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3083184001277999) - present_state_Q ( -0.4082518727516509)) * f1( 0.7535678283625328)
w2 ( -0.5254737605001458 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3083184001277999) - present_state_Q (-0.4082518727516509)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05850246435804646 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3357762311631896) - present_state_Q ( -0.3357762311631896)) * f1( 0.5754212708928296)
w2 ( -0.5493418440173335 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3357762311631896) - present_state_Q (-0.3357762311631896)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04670771697111542 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24920553594815414) - present_state_Q ( -0.35907390475162093)) * f1( 0.5037189230331554)
w2 ( -0.5352926429479252 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.24920553594815414) - present_state_Q (-0.35907390475162093)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.028560062230701875 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23472799488390975) - present_state_Q ( -0.23472799488390975)) * f1( 0.44127478372547485)
w2 ( -0.5188424351321045 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23472799488390975) - present_state_Q (-0.23472799488390975)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.017501812298539813 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11419932588845294) - present_state_Q ( -0.11419932588845294)) * f1( 0.3652246545464093)
w2 ( -0.5127868472661123 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11419932588845294) - present_state_Q (-0.11419932588845294)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.00865468701640929 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10777096947638547) - present_state_Q ( -0.10777096947638547)) * f1( 0.29788915194788007)
w2 ( -0.5068469698155375 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10777096947638547) - present_state_Q (-0.10777096947638547)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.00881685098912512 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10338875144485711) - present_state_Q ( -0.10338875144485711)) * f1( 0.23332530430284912)
w2 ( -0.50698597228953 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.10338875144485711) - present_state_Q (-0.10338875144485711)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.00032598649630154533 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2045472132458368) - present_state_Q ( -0.30585230519035334)) * f1( 0.18835770488620718)
w2 ( -0.4778621172575838 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2045472132458368) - present_state_Q (-0.30585230519035334)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.009950950831767622 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2866474077848789) - present_state_Q ( -0.2866487612306371)) * f1( 0.21015939215441917)
w2 ( -0.45038307603045485 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2866474077848789) - present_state_Q (-0.2866487612306371)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.019441773401974664 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26808964977461) - present_state_Q ( -0.26808964977461)) * f1( 0.21507450693360508)
w2 ( -0.4239062349426259 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.26808964977461) - present_state_Q (-0.26808964977461)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.028618715022202527 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25014702360042523) - present_state_Q ( -0.25014702360042523)) * f1( 0.21586083112789112)
w2 ( -0.39839829566820295 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.25014702360042523) - present_state_Q (-0.25014702360042523)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03750656125251609 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23296560194811436) - present_state_Q ( -0.23282800776883947)) * f1( 0.21702475555816678)
w2 ( -0.3738264088137613 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23296560194811436) - present_state_Q (-0.23282800776883947)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04575882874625263 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21631250118695716) - present_state_Q ( -0.21645657674717814)) * f1( 0.20901059119496782)
w2 ( -0.35013688921605235 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21631250118695716) - present_state_Q (-0.21645657674717814)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05394440563589928 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2002306086650841) - present_state_Q ( -0.2002306086650841)) * f1( 0.21529233012447077)
w2 ( -0.3273244363481378 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2002306086650841) - present_state_Q (-0.2002306086650841)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06183369253719069 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18477619194745173) - present_state_Q ( -0.18477619194745173)) * f1( 0.215378586981762)
w2 ( -0.3053465219829754 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18477619194745173) - present_state_Q (-0.18477619194745173)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06906579381761596 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16995265923241587) - present_state_Q ( -0.1705599436632456)) * f1( 0.20454818413007442)
w2 ( -0.2841326413185752 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16995265923241587) - present_state_Q (-0.1705599436632456)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0755873744201198 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025431282689401934) - present_state_Q ( 0.025431282689401934)) * f1( 0.3682182059118738)
w2 ( -0.2841326413185752 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.025431282689401934) - present_state_Q (0.025431282689401934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08065572941765507 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029999022958442655) - present_state_Q ( -0.027461166622092668)) * f1( 0.388495590260983)
w2 ( -0.28152341994021646 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.029999022958442655) - present_state_Q (-0.027461166622092668)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08145218255916833 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01927579154940967) - present_state_Q ( -0.01927579154940967)) * f1( 0.45909810382953664)
w2 ( -0.2811764556923271 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.01927579154940967) - present_state_Q (-0.01927579154940967)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07656806757691424 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01174906465894826) - present_state_Q ( -0.01174906465894826)) * f1( 0.5461637132584085)
w2 ( -0.28296497252846603 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.01174906465894826) - present_state_Q (-0.01174906465894826)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05270521965531741 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009876547021645865) - present_state_Q ( -0.009876547021645865)) * f1( 0.6101296397107017)
w2 ( -0.2907871946820764 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.009876547021645865) - present_state_Q (-0.009876547021645865)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02676638440352726 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02269163466464129) - present_state_Q ( -0.022188447313141166)) * f1( 0.6824559665722828)
w2 ( -0.29838880900514286 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02269163466464129) - present_state_Q (-0.022188447313141166)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.017560767762550198 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04148934801222825) - present_state_Q ( -0.04176551404229886)) * f1( 0.669206848735582)
w2 ( -0.31163647742032136 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.04148934801222825) - present_state_Q (-0.04176551404229886)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04976361310810863 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0712190352718967) - present_state_Q ( -0.07122028572939644)) * f1( 0.5064123827374573)
w2 ( -0.3243545097762772 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0712190352718967) - present_state_Q (-0.07122028572939644)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07840453168736441 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08781876842864499) - present_state_Q ( -0.08782374729904688)) * f1( 0.46123751693687665)
w2 ( -0.33677367236715355 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.08781876842864499) - present_state_Q (-0.08782374729904688)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10414968749663867 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10039410968846552) - present_state_Q ( -0.10046881333968838)) * f1( 0.42234904225050424)
w2 ( -0.3489650843197367 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.10039410968846552) - present_state_Q (-0.10046881333968838)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08278570252391922 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13679193935640066) - present_state_Q ( -0.1369661817368687)) * f1( 0.4527019348662979)
w2 ( -0.41573703877636553 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13679193935640066) - present_state_Q (-0.1369661817368687)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09835787084863486 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12504757433717012) - present_state_Q ( -0.12504757433717012)) * f1( 0.4982411203366737)
w2 ( -0.4032353261002274 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12504757433717012) - present_state_Q (-0.12504757433717012)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11533531284124646 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10450684768879329) - present_state_Q ( -0.10450684768879329)) * f1( 0.5773537212765506)
w2 ( -0.3914730795834308 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10450684768879329) - present_state_Q (-0.10450684768879329)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.12617434304637937 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08538118781257312) - present_state_Q ( -0.08623964083706602)) * f1( 0.6099570830760148)
w2 ( -0.38436501870119844 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08538118781257312) - present_state_Q (-0.08623964083706602)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1303732020049852 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07208527322043332) - present_state_Q ( -0.07208527322043332)) * f1( 0.6472055434441937)
w2 ( -0.38176994886526283 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.07208527322043332) - present_state_Q (-0.07208527322043332)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.12005061727711872 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06033814782483099) - present_state_Q ( -0.06033814782483099)) * f1( 0.7085032069530833)
w2 ( -0.3875977755435689 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06033814782483099) - present_state_Q (-0.06033814782483099)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11676808008705042 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06335438436791539) - present_state_Q ( -0.06335438436791539)) * f1( 0.7637172380202912)
w2 ( -0.38931701770632393 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.06335438436791539) - present_state_Q (-0.06335438436791539)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1212516150018599 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062189610485789776) - present_state_Q ( -0.062189610485789776)) * f1( 0.8010510794303373)
w2 ( -0.3870781917288355 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.062189610485789776) - present_state_Q (-0.062189610485789776)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11747803363276654 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05707463130051128) - present_state_Q ( -0.05830538156842226)) * f1( 0.7960792532258752)
w2 ( -0.38897427499130066 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.05707463130051128) - present_state_Q (-0.05830538156842226)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.06710879599816447 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06116550608385152) - present_state_Q ( -0.06350759747611447)) * f1( 0.7838240875588047)
w2 ( -0.4146786331165915 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06116550608385152) - present_state_Q (-0.06350759747611447)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04281827240325643 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11147512394073797) - present_state_Q ( -0.11147512394073797)) * f1( 0.810569292695796)
w2 ( -0.4266655286547249 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11147512394073797) - present_state_Q (-0.11147512394073797)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.02029265005554711 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13631846906460893) - present_state_Q ( -0.13593392959672512)) * f1( 0.8111556098775105)
w2 ( -0.4377734453471143 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13631846906460893) - present_state_Q (-0.13593392959672512)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.00022377926674871026 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15872973042158484) - present_state_Q ( -0.1589074856273196)) * f1( 0.7984118617911746)
w2 ( -0.44805206484370785 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.15872973042158484) - present_state_Q (-0.1589074856273196)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01703741857822743 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17937854536958714) - present_state_Q ( -0.17937854536958714)) * f1( 0.7047991281565282)
w2 ( -0.4575944372104027 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.17937854536958714) - present_state_Q (-0.17937854536958714)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.026515319723336726 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2858713889122021) - present_state_Q ( -0.2858713889122021)) * f1( 0.6641103835072668)
w2 ( -0.4661573822091438 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2858713889122021) - present_state_Q (-0.2858713889122021)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.015935143295568027 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29597816248629155) - present_state_Q ( -0.29650261913544845)) * f1( 0.6339048514345795)
w2 ( -0.45614309403593467 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.29597816248629155) - present_state_Q (-0.29650261913544845)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0071882088875172626 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28180868134066545) - present_state_Q ( -0.28180868134066545)) * f1( 0.5097428224171532)
w2 ( -0.4289254252435387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.28180868134066545) - present_state_Q (-0.28180868134066545)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.00410850850377887 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25265093850481013) - present_state_Q ( -0.25265093850481013)) * f1( 0.6544490727700593)
w2 ( -0.439282274564279 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25265093850481013) - present_state_Q (-0.25265093850481013)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03510769586588475 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2662913823619289) - present_state_Q ( -0.2663363082272368)) * f1( 0.6734666573342609)
w2 ( -0.46689984436481635 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2662913823619289) - present_state_Q (-0.2663363082272368)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05652367659090132 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3026340619840447) - present_state_Q ( -0.3960140308570079)) * f1( 0.6407186461647884)
w2 ( -0.4936397943921281 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3026340619840447) - present_state_Q (-0.3960140308570079)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07996425845171289 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32898676857893283) - present_state_Q ( -0.32898676857893283)) * f1( 0.5803389645205114)
w2 ( -0.5178745088888657 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32898676857893283) - present_state_Q (-0.32898676857893283)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09971540805365726 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3519340311017036) - present_state_Q ( -0.3519340311017036)) * f1( 0.5153468132674405)
w2 ( -0.5408700712093737 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3519340311017036) - present_state_Q (-0.3519340311017036)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08479974807680872 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47776764011390693) - present_state_Q ( -0.47776764011390693)) * f1( 0.45200219330351377)
w2 ( -0.5144708011211724 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.47776764011390693) - present_state_Q (-0.47776764011390693)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.08085041852025848 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3402185141759284) - present_state_Q ( -0.3402185141759284)) * f1( 0.3718882923409234)
w2 ( -0.5080990013556722 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3402185141759284) - present_state_Q (-0.3402185141759284)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06433953118260063 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3315811062110684) - present_state_Q ( -0.33163900589071477)) * f1( 0.3312240748710697)
w2 ( -0.47819014763949574 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3315811062110684) - present_state_Q (-0.33163900589071477)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05199741429396999 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3036909601910256) - present_state_Q ( -0.3036909601910256)) * f1( 0.2607552666138334)
w2 ( -0.44979083578918033 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3036909601910256) - present_state_Q (-0.3036909601910256)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04529136711659286 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18933046877615717) - present_state_Q ( -0.18933046877615717)) * f1( 0.18105005005175337)
w2 ( -0.4349749389132387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18933046877615717) - present_state_Q (-0.18933046877615717)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.043369484516852366 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037921320609013917) - present_state_Q ( -0.004269198905538686)) * f1( 0.09426076485058518)
w2 ( -0.4349749389132387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0037921320609013917) - present_state_Q (-0.004269198905538686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.029204477403658884 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27763352041784806) - present_state_Q ( -0.2785003403173678)) * f1( 0.40386408011417585)
w2 ( -0.4139307196167037 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.27763352041784806) - present_state_Q (-0.2785003403173678)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.013882470046179365 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2583988348292332) - present_state_Q ( -0.25869607341764855)) * f1( 0.35397454659918526)
w2 ( -0.38795934822062017 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2583988348292332) - present_state_Q (-0.25869607341764855)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0057438606505103695 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23831262919932192) - present_state_Q ( -0.31590449884344596)) * f1( 0.3988497903133343)
w2 ( -0.34859348934673906 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23831262919932192) - present_state_Q (-0.31590449884344596)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.025461144581880917 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27635084779985825) - present_state_Q ( -0.27635084779985825)) * f1( 0.4394158965727558)
w2 ( -0.31269622830514926 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.27635084779985825) - present_state_Q (-0.27635084779985825)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.03644587554659488 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17490200677639348) - present_state_Q ( -0.23744125243742328)) * f1( 0.49941706924460477)
w2 ( -0.29510014416436653 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17490200677639348) - present_state_Q (-0.23744125243742328)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.042043999424977135 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15701050712759024) - present_state_Q ( -0.21567116293193814)) * f1( 0.5599797533595509)
w2 ( -0.2871025351868322 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.15701050712759024) - present_state_Q (-0.21567116293193814)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.058265301669218346 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20584567300193368) - present_state_Q ( -0.2057672360929822)) * f1( 0.5688039288259645)
w2 ( -0.2642879216834091 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.20584567300193368) - present_state_Q (-0.2057672360929822)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.061802234733905524 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1766456505212308) - present_state_Q ( -0.17626070410041622)) * f1( 0.6036119652477697)
w2 ( -0.2596002305595456 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1766456505212308) - present_state_Q (-0.17626070410041622)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.024684987706236478 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21456117348380635) - present_state_Q ( -0.21436336233745956)) * f1( 0.7319616906549902)
w2 ( -0.31030950606063773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21456117348380635) - present_state_Q (-0.21436336233745956)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006903122330766967 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29217105475428695) - present_state_Q ( -0.2924564310564828)) * f1( 0.7232361310694324)
w2 ( -0.35398557350253235 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.29217105475428695) - present_state_Q (-0.2924564310564828)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.011326556932755454 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3577853619390448) - present_state_Q ( -0.35790683227064213)) * f1( 0.568041326840299)
w2 ( -0.3617727438948586 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3577853619390448) - present_state_Q (-0.35790683227064213)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.030013667400602354 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36738816490829695) - present_state_Q ( -0.36750517443516256)) * f1( 0.5061053040510745)
w2 ( -0.3986961081004253 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.36738816490829695) - present_state_Q (-0.36750517443516256)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.033974153989218284 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25192734005043593) - present_state_Q ( -0.331666561670521)) * f1( 0.4234629184278106)
w2 ( -0.40617820188718706 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25192734005043593) - present_state_Q (-0.331666561670521)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.029881127907435478 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2561483742961679) - present_state_Q ( -0.33738401467360524)) * f1( 0.36620347243389517)
w2 ( -0.39723666770766797 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2561483742961679) - present_state_Q (-0.33738401467360524)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.01397163512406854 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32739984885371043) - present_state_Q ( -0.32739984885371043)) * f1( 0.32162489707038816)
w2 ( -0.3576638785902008 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32739984885371043) - present_state_Q (-0.32739984885371043)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0032082720939278844 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4329857829570973) - present_state_Q ( -0.43326521312066296)) * f1( 0.2912013358703549)
w2 ( -0.2868678824112064 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4329857829570973) - present_state_Q (-0.43326521312066296)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01777989211539393 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34332297995984895) - present_state_Q ( -0.34332297995984895)) * f1( 0.2862846126228409)
w2 ( -0.22578900057554271 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.34332297995984895) - present_state_Q (-0.34332297995984895)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.001559514836887245 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2647013256014551) - present_state_Q ( -0.2647013256014551)) * f1( 0.35126619715475393)
w2 ( -0.28120125741058555 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2647013256014551) - present_state_Q (-0.2647013256014551)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014387918846081114 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33679989460166504) - present_state_Q ( -0.33681484113052856)) * f1( 0.40183507546806185)
w2 ( -0.3288250752101421 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33679989460166504) - present_state_Q (-0.33681484113052856)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.025817799431819523 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3994194985230809) - present_state_Q ( -0.3994194985230809)) * f1( 0.33565718034514147)
w2 ( -0.36968776936964937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3994194985230809) - present_state_Q (-0.3994194985230809)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03479432839552472 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4515179496585037) - present_state_Q ( -0.4515179496585037)) * f1( 0.30570484660272934)
w2 ( -0.404923830806531 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4515179496585037) - present_state_Q (-0.4515179496585037)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04081724726554909 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4985325339728083) - present_state_Q ( -0.5791713752621804)) * f1( 0.35287395099185803)
w2 ( -0.42881929374544503 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4985325339728083) - present_state_Q (-0.5791713752621804)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.046388793264365444 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5312963211756909) - present_state_Q ( -0.6170601799247799)) * f1( 0.40946339601061554)
w2 ( -0.4478690170524355 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5312963211756909) - present_state_Q (-0.6170601799247799)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05569343517307377 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5584887362942307) - present_state_Q ( -0.5594163227099607)) * f1( 0.4736812643910146)
w2 ( -0.471440923162771 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5584887362942307) - present_state_Q (-0.5594163227099607)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.05930036132261619 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6837569665423431) - present_state_Q ( -0.6837569665423431)) * f1( 0.42625623721521083)
w2 ( -0.4832875453784357 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6837569665423431) - present_state_Q (-0.6837569665423431)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06192046182290122 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6983329294791042) - present_state_Q ( -0.6983329294791042)) * f1( 0.3664457595978688)
w2 ( -0.49329759626406855 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6983329294791042) - present_state_Q (-0.6983329294791042)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06349829291233114 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.610925083300081) - present_state_Q ( -0.7095846025528947)) * f1( 0.3063279443465539)
w2 ( -0.5005087030728644 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.610925083300081) - present_state_Q (-0.7095846025528947)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06890902688764043 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41562003471101344) - present_state_Q ( -0.5157217753255863)) * f1( 0.23958238174569205)
w2 ( -0.523092725887416 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41562003471101344) - present_state_Q (-0.5157217753255863)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0710842287054944 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5364022439264947) - present_state_Q ( -0.641020789103978)) * f1( 0.19314621959152806)
w2 ( -0.5366070581220566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5364022439264947) - present_state_Q (-0.641020789103978)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07243852373380107 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5461228102621651) - present_state_Q ( -0.6534442218865766)) * f1( 0.13386587029779537)
w2 ( -0.5487472252188134 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5461228102621651) - present_state_Q (-0.6534442218865766)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07339583082043705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5558217248958003) - present_state_Q ( -0.6662602653707212)) * f1( 0.10717494929459111)
w2 ( -0.5594658540730764 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5558217248958003) - present_state_Q (-0.6662602653707212)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07113058519834321 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11733359800224163) - present_state_Q ( -0.11733359800224163)) * f1( 0.07412447174194894)
w2 ( -0.553353849309036 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11733359800224163) - present_state_Q (-0.11733359800224163)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.052922034344414975 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2587146916821534) - present_state_Q ( -0.260093741078989)) * f1( 0.5448036347137669)
w2 ( -0.5399849584326051 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2587146916821534) - present_state_Q (-0.260093741078989)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.032057607445980484 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24148268953642482) - present_state_Q ( -0.24239441587039762)) * f1( 0.498855208882228)
w2 ( -0.5232551125559348 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.24148268953642482) - present_state_Q (-0.24239441587039762)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.00939568496628243 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22366102972966612) - present_state_Q ( -0.32831205224085314)) * f1( 0.44791192641210564)
w2 ( -0.4928983555998616 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.22366102972966612) - present_state_Q (-0.32831205224085314)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.005162493390810454 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2007525186510569) - present_state_Q ( -0.2007525186510569)) * f1( 0.38242836195623725)
w2 ( -0.47767126492842354 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2007525186510569) - present_state_Q (-0.2007525186510569)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.02300731966288137 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28458305276489965) - present_state_Q ( -0.28458305276489965)) * f1( 0.39122688190744037)
w2 ( -0.450303780079119 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.28458305276489965) - present_state_Q (-0.28458305276489965)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04356174299291975 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2592693697888378) - present_state_Q ( -0.2592693697888378)) * f1( 0.47432288587009175)
w2 ( -0.42430323411052173 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2592693697888378) - present_state_Q (-0.2592693697888378)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06647813584315332 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14572498345486917) - present_state_Q ( -0.2305856302769735)) * f1( 0.5508574391350629)
w2 ( -0.39934244619463255 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14572498345486917) - present_state_Q (-0.2305856302769735)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.07494023725808614 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03793057947471578) - present_state_Q ( -0.03793057947471578)) * f1( 0.630852674075547)
w2 ( -0.39665969576408766 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.03793057947471578) - present_state_Q (-0.03793057947471578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08348753532876853 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028269967036485356) - present_state_Q ( -0.028269967036485356)) * f1( 0.681369234800795)
w2 ( -0.3941508363574309 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.028269967036485356) - present_state_Q (-0.028269967036485356)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10800770618848875 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1799301673008547) - present_state_Q ( -0.1799301673008547)) * f1( 0.6774704067005083)
w2 ( -0.3724346073231848 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1799301673008547) - present_state_Q (-0.1799301673008547)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.12353426495491464 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22521859319627918) - present_state_Q ( -0.14961491248479242)) * f1( 0.6837091029435162)
w2 ( -0.3588090241332749 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22521859319627918) - present_state_Q (-0.14961491248479242)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14930160789189378 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2028591889658113) - present_state_Q ( -0.20411599646677311)) * f1( 0.671321619715094)
w2 ( -0.32810261792765955 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2028591889658113) - present_state_Q (-0.20411599646677311)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1727487511717309 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16097717340166195) - present_state_Q ( -0.16097717340166195)) * f1( 0.6798648880858893)
w2 ( -0.3005122614427399 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16097717340166195) - present_state_Q (-0.16097717340166195)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1870869968531053 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12426019758996759) - present_state_Q ( -0.12250697142241958)) * f1( 0.6825105069185954)
w2 ( -0.28370578530966606 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.12426019758996759) - present_state_Q (-0.12250697142241958)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.16599256542206398 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09975051747450223) - present_state_Q ( -0.09975051747450223)) * f1( 0.6799730227810278)
w2 ( -0.3085237480515019 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09975051747450223) - present_state_Q (-0.09975051747450223)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.12728761780366232 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13393717549135487) - present_state_Q ( -0.13562075942142143)) * f1( 0.6698989122617625)
w2 ( -0.354745584701719 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13393717549135487) - present_state_Q (-0.13562075942142143)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09174824109181584 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1972185869265387) - present_state_Q ( -0.1972185869265387)) * f1( 0.6801751995106119)
w2 ( -0.3965458464430082 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1972185869265387) - present_state_Q (-0.1972185869265387)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0656762058054047 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33447978964630093) - present_state_Q ( -0.33631286097190694)) * f1( 0.6565028904567655)
w2 ( -0.43625935824228057 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33447978964630093) - present_state_Q (-0.33631286097190694)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04244438246777929 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3916027784148615) - present_state_Q ( -0.39227435073597694)) * f1( 0.6697251609910138)
w2 ( -0.4709479509528315 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3916027784148615) - present_state_Q (-0.39227435073597694)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.021874320708990317 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4420525130459137) - present_state_Q ( -0.4420525130459137)) * f1( 0.6807835625563196)
w2 ( -0.5011632247786992 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4420525130459137) - present_state_Q (-0.4420525130459137)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.010108013212171807 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5866728506666308) - present_state_Q ( -0.586450733433368)) * f1( 0.6832274473752555)
w2 ( -0.5218292109746947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5866728506666308) - present_state_Q (-0.586450733433368)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.00039671013921653564 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6193122401587666) - present_state_Q ( -0.6193122401587666)) * f1( 0.6809263963544311)
w2 ( -0.538943489037548 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6193122401587666) - present_state_Q (-0.6193122401587666)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0076249232874886705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6464629219470619) - present_state_Q ( -0.6464629219470619)) * f1( 0.6787446837822009)
w2 ( -0.5531254934672653 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6464629219470619) - present_state_Q (-0.6464629219470619)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012629352422838922 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5574793107899884) - present_state_Q ( -0.6681044094834415)) * f1( 0.5709981803839261)
w2 ( -0.5636427160587321 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5574793107899884) - present_state_Q (-0.6681044094834415)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.016392829711441778 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5700458054477819) - present_state_Q ( -0.6827743486595285)) * f1( 0.5070006105357054)
w2 ( -0.5725503438849621 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5700458054477819) - present_state_Q (-0.6827743486595285)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.011276099895959284 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35072787580402237) - present_state_Q ( -0.46523794458101475)) * f1( 0.43907425378922)
w2 ( -0.522137131324913 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.35072787580402237) - present_state_Q (-0.46523794458101475)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.049886227468090925 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7258897777452152) - present_state_Q ( -0.7258897777452152)) * f1( 0.4524796832893827)
w2 ( -0.4026750193290159 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7258897777452152) - present_state_Q (-0.7258897777452152)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.08792180567051422 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6192246223181885) - present_state_Q ( -0.6192246223181885)) * f1( 0.5022510195677427)
w2 ( -0.2815066737151967 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6192246223181885) - present_state_Q (-0.6192246223181885)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.09494660417799951 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22978105263398246) - present_state_Q ( -0.3423837221200612)) * f1( 0.5883139078723579)
w2 ( -0.26478988735526393 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.22978105263398246) - present_state_Q (-0.3423837221200612)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05426057495739241 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04246564687973148) - present_state_Q ( -0.09542362435078429)) * f1( 0.6682735903163186)
w2 ( -0.3013192637754953 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.04246564687973148) - present_state_Q (-0.09542362435078429)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.030617951022009312 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3246697580601263) - present_state_Q ( -0.38493361081522537)) * f1( 0.6802979603414434)
w2 ( -0.3499739348742055 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3246697580601263) - present_state_Q (-0.38493361081522537)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01596642661130301 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5393966202697652) - present_state_Q ( -0.5390796055413437)) * f1( 0.6819101069949693)
w2 ( -0.38435154391190673 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5393966202697652) - present_state_Q (-0.5390796055413437)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.005336080164098563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6041037707821685) - present_state_Q ( -0.6041037707821685)) * f1( 0.6800957873188243)
w2 ( -0.40936060091927445 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6041037707821685) - present_state_Q (-0.6041037707821685)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.002425471098040881 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6513886843263306) - present_state_Q ( -0.6513376115321343)) * f1( 0.6820268486951511)
w2 ( -0.42756880202335423 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6513886843263306) - present_state_Q (-0.6513376115321343)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.0075188670314679275 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6855508474859964) - present_state_Q ( -0.6855992963306305)) * f1( 0.6139892140816261)
w2 ( -0.44084172817022926 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6855508474859964) - present_state_Q (-0.6855992963306305)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.01488719212938296 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6211075104595957) - present_state_Q ( -0.6211075104595957)) * f1( 0.5225642380468531)
w2 ( -0.4605821818523202 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6211075104595957) - present_state_Q (-0.6211075104595957)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.013014481756776292 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5596701512015569) - present_state_Q ( -0.6517865875720209)) * f1( 0.4682906567056985)
w2 ( -0.377167441709059 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.5596701512015569) - present_state_Q (-0.6517865875720209)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0021914393368761744 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5949094541964978) - present_state_Q ( -0.5949094541964978)) * f1( 0.6576099377557241)
w2 ( -0.4035004803047633 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5949094541964978) - present_state_Q (-0.5949094541964978)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.006094038407563392 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6440916678156468) - present_state_Q ( -0.6440916678156468)) * f1( 0.6886344725954265)
w2 ( -0.4227512801393102 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6440916678156468) - present_state_Q (-0.6440916678156468)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.011100839942471168 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6798644812500755) - present_state_Q ( -0.6798644812500755)) * f1( 0.5681672473350181)
w2 ( -0.4368507948392993 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6798644812500755) - present_state_Q (-0.6798644812500755)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.014528889256193283 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7047512906565085) - present_state_Q ( -0.7047512906565085)) * f1( 0.521583857044671)
w2 ( -0.44736660898476205 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7047512906565085) - present_state_Q (-0.7047512906565085)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.020226095926944604 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5434146481667551) - present_state_Q ( -0.7223612917606599)) * f1( 0.45252718697941563)
w2 ( -0.32448343667372453 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.5434146481667551) - present_state_Q (-0.7223612917606599)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.034549275178313216 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5079061111628326) - present_state_Q ( -0.5079061111628326)) * f1( 0.5570717925903138)
w2 ( -0.2833449566662766 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5079061111628326) - present_state_Q (-0.5079061111628326)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.005256923049940035 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20463504899790885) - present_state_Q ( -0.26130404033116417)) * f1( 0.637955969303451)
w2 ( -0.32926090312313927 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20463504899790885) - present_state_Q (-0.26130404033116417)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.013395676016932437 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32569743294801445) - present_state_Q ( -0.45740179419727023)) * f1( 0.6778623429090282)
w2 ( -0.3677844159967936 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32569743294801445) - present_state_Q (-0.45740179419727023)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.02348935446725266 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5967542730847651) - present_state_Q ( -0.5967542730847651)) * f1( 0.6195437601958262)
w2 ( -0.39385180067258746 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5967542730847651) - present_state_Q (-0.5967542730847651)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.10268530103382158 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27050426625548757) - present_state_Q ( -0.27050426625548757)) * f1( 0.6911615642523673)
w2 ( -0.5716156957238384 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.27050426625548757) - present_state_Q (-0.27050426625548757)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12526865892103423 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3026703727310281) - present_state_Q ( -0.4169935118757958)) * f1( 0.7208830640435216)
w2 ( -0.5904121072476768 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3026703727310281) - present_state_Q (-0.4169935118757958)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14599498405264388 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43712576921135887) - present_state_Q ( -0.43963085622038284)) * f1( 0.6816037834778774)
w2 ( -0.608657010489722 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.43712576921135887) - present_state_Q (-0.43963085622038284)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14542631797727978 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4547326000933923) - present_state_Q ( -0.4547443146337433)) * f1( 0.6133779795312656)
w2 ( -0.6081007472122577 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4547326000933923) - present_state_Q (-0.4547443146337433)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15682035069595915 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5791906663946144) - present_state_Q ( -0.5791906663946144)) * f1( 0.6375054385911942)
w2 ( -0.6223990192318455 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5791906663946144) - present_state_Q (-0.5791906663946144)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11375623588091988 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7257862076709145) - present_state_Q ( -0.7257862076709145)) * f1( 0.6592715038593076)
w2 ( -0.5570782605414631 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.7257862076709145) - present_state_Q (-0.7257862076709145)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06691758958039812 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6245143196620851) - present_state_Q ( -0.6267882133607406)) * f1( 0.6128011557296074)
w2 ( -0.4806445824020099 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6245143196620851) - present_state_Q (-0.6267882133607406)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0326084984253671 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5152360921278074) - present_state_Q ( -0.5152360921278074)) * f1( 0.5169270134011252)
w2 ( -0.41427333411050726 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5152360921278074) - present_state_Q (-0.5152360921278074)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.008903312088328768 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2634308489000288) - present_state_Q ( -0.3462855157221303)) * f1( 0.4559194428333157)
w2 ( -0.3726779396439371 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2634308489000288) - present_state_Q (-0.3462855157221303)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.014568181063258898 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37646257532786065) - present_state_Q ( -0.3765556606025488)) * f1( 0.43553690133978307)
w2 ( -0.3187869993369608 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.37646257532786065) - present_state_Q (-0.3765556606025488)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03921318107564048 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3113099362968758) - present_state_Q ( -0.3113099362968758)) * f1( 0.5132461635133179)
w2 ( -0.270769105070242 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3113099362968758) - present_state_Q (-0.3113099362968758)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06334908738608158 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2484252856204448) - present_state_Q ( -0.2484252856204448)) * f1( 0.5698037965014102)
w2 ( -0.22841082936440196 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2484252856204448) - present_state_Q (-0.2484252856204448)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.030186919210388394 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.142775921698888) - present_state_Q ( -0.18845808757176838)) * f1( 0.6306758856546938)
w2 ( -0.280992779824214 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.142775921698888) - present_state_Q (-0.18845808757176838)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.001122484720162159 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.204246895444949) - present_state_Q ( -0.2604454514097918)) * f1( 0.6806699375718718)
w2 ( -0.32699070363768434 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.204246895444949) - present_state_Q (-0.2604454514097918)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.025491724648212 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32765472647132243) - present_state_Q ( -0.3276659475658453)) * f1( 0.6015617994901359)
w2 ( -0.36750065614581306 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32765472647132243) - present_state_Q (-0.3276659475658453)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.04510926268873469 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38152320379399624) - present_state_Q ( -0.38152320379399624)) * f1( 0.5500823440428428)
w2 ( -0.4031635678043534 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38152320379399624) - present_state_Q (-0.38152320379399624)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06041169864444206 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42489846501982875) - present_state_Q ( -0.42489846501982875)) * f1( 0.48182780918969104)
w2 ( -0.4349227059525688 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42489846501982875) - present_state_Q (-0.42489846501982875)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.03394651387254661 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4609265859575262) - present_state_Q ( -0.4609265859575262)) * f1( 0.43044444351756)
w2 ( -0.37343931321639146 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4609265859575262) - present_state_Q (-0.4609265859575262)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.012143804066890217 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38626307240129726) - present_state_Q ( -0.38693759436179836)) * f1( 0.3976337952134557)
w2 ( -0.31860818450422457 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.38626307240129726) - present_state_Q (-0.38693759436179836)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.005226098639738559 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3229076026429813) - present_state_Q ( -0.3229076026429813)) * f1( 0.3540421201688342)
w2 ( -0.26954650026635624 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3229076026429813) - present_state_Q (-0.3229076026429813)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023988373527820343 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26732099024499867) - present_state_Q ( -0.26732099024499867)) * f1( 0.4258453915192324)
w2 ( -0.22548761114430635 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.26732099024499867) - present_state_Q (-0.26732099024499867)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0004350213358661967 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21394213631048367) - present_state_Q ( -0.21394213631048367)) * f1( 0.4812946080080373)
w2 ( -0.2762328188763628 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21394213631048367) - present_state_Q (-0.21394213631048367)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.019854509959482132 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27644617540640115) - present_state_Q ( -0.33169273918167375)) * f1( 0.4904507260857191)
w2 ( -0.3237470442794388 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.27644617540640115) - present_state_Q (-0.33169273918167375)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04203555801697811 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33439728204135905) - present_state_Q ( -0.33479430105389907)) * f1( 0.5564104476516832)
w2 ( -0.3636115869944625 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33439728204135905) - present_state_Q (-0.33479430105389907)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.05845101232779571 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38887341984953727) - present_state_Q ( -0.46118152120290645)) * f1( 0.5911094792536236)
w2 ( -0.39693628548830817 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38887341984953727) - present_state_Q (-0.46118152120290645)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07874698450557771 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4348708216752954) - present_state_Q ( -0.4354483265305088)) * f1( 0.658877229126905)
w2 ( -0.42774016105201024 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4348708216752954) - present_state_Q (-0.4354483265305088)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09182978678592892 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5670323024221264) - present_state_Q ( -0.5678361422750902)) * f1( 0.6926988932358941)
w2 ( -0.45040421160806493 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5670323024221264) - present_state_Q (-0.5678361422750902)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1021239573883991 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5984497067784066) - present_state_Q ( -0.5994050048834284)) * f1( 0.6416213411352345)
w2 ( -0.4696570075033944 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5984497067784066) - present_state_Q (-0.5994050048834284)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11032965673483512 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6232667875759531) - present_state_Q ( -0.6243021424189242)) * f1( 0.594510191021521)
w2 ( -0.48621995186403494 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6232667875759531) - present_state_Q (-0.6243021424189242)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11681219784225957 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.64198646755597) - present_state_Q ( -0.64198646755597)) * f1( 0.5304333127744628)
w2 ( -0.5008854133679902 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.64198646755597) - present_state_Q (-0.64198646755597)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12209074508614527 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6585012757104305) - present_state_Q ( -0.6585012757104305)) * f1( 0.4917190218987761)
w2 ( -0.5137672755912637 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6585012757104305) - present_state_Q (-0.6585012757104305)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12638023095368536 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.669829412914938) - present_state_Q ( -0.6711932867506375)) * f1( 0.4478026242082882)
w2 ( -0.5252620341361665 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.669829412914938) - present_state_Q (-0.6711932867506375)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12976720471401776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6779638085848898) - present_state_Q ( -0.6779638085848898)) * f1( 0.377031813139763)
w2 ( -0.5360419428089983 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6779638085848898) - present_state_Q (-0.6779638085848898)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.13214273484822206 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5790888242702151) - present_state_Q ( -0.6862972128320148)) * f1( 0.33172388629379773)
w2 ( -0.5446353431603991 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5790888242702151) - present_state_Q (-0.6862972128320148)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1173774851001619 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6897903266064158) - present_state_Q ( -0.6909427938693405)) * f1( 0.2828788288648359)
w2 ( -0.4819996918153552 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.6897903266064158) - present_state_Q (-0.6909427938693405)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.10288403608979076 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41112747199325517) - present_state_Q ( -0.5075274103563262)) * f1( 0.21748394523180867)
w2 ( -0.4153582254996551 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.41112747199325517) - present_state_Q (-0.5075274103563262)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09312253630502099 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3491016563017296) - present_state_Q ( -0.4321733014016606)) * f1( 0.16343717199556934)
w2 ( -0.3556319119225063 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3491016563017296) - present_state_Q (-0.4321733014016606)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.08353233829359843 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.101754870325119) - present_state_Q ( -0.101754870325119)) * f1( 0.32890521624426916)
w2 ( -0.3498003242566542 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.101754870325119) - present_state_Q (-0.101754870325119)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07522473093479 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09431897486872758) - present_state_Q ( -0.09431897486872758)) * f1( 0.2916105368890829)
w2 ( -0.3441025827090171 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09431897486872758) - present_state_Q (-0.09431897486872758)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0685806695619404 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08679158443987135) - present_state_Q ( -0.08679158443987135)) * f1( 0.2388984004960615)
w2 ( -0.33854033418909946 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08679158443987135) - present_state_Q (-0.08679158443987135)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06180060011309686 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08134707624065535) - present_state_Q ( -0.14905514307847526)) * f1( 0.19887541912254203)
w2 ( -0.32490351677092305 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08134707624065535) - present_state_Q (-0.14905514307847526)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05392176331301864 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08272225173507897) - present_state_Q ( -0.08272225173507897)) * f1( 0.2870772832048042)
w2 ( -0.31941451623969164 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08272225173507897) - present_state_Q (-0.08272225173507897)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.046610609774402983 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0784513171241595) - present_state_Q ( -0.0784513171241595)) * f1( 0.270176881858458)
w2 ( -0.3140023925314568 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0784513171241595) - present_state_Q (-0.0784513171241595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04038571021102096 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0736949085948949) - present_state_Q ( -0.0736949085948949)) * f1( 0.23373283768079778)
w2 ( -0.3086758841767487 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0736949085948949) - present_state_Q (-0.0736949085948949)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.035197427139202535 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06936085941910924) - present_state_Q ( -0.06970907467180448)) * f1( 0.1974435461154459)
w2 ( -0.30342042440215083 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06936085941910924) - present_state_Q (-0.06970907467180448)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03182282170922699 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06527456815691716) - present_state_Q ( -0.06527456815691716)) * f1( 0.1304209895323332)
w2 ( -0.2982454821753263 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06527456815691716) - present_state_Q (-0.06527456815691716)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.02858671871014707 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06365171267028479) - present_state_Q ( -0.06365171267028479)) * f1( 0.12577816862980942)
w2 ( -0.2930997513472612 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06365171267028479) - present_state_Q (-0.06365171267028479)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.024513972822287906 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06315302431405592) - present_state_Q ( -0.06315302431405592)) * f1( 0.15857273059445717)
w2 ( -0.2879629969096082 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06315302431405592) - present_state_Q (-0.06315302431405592)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09022050662425121 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13411072261236698) - present_state_Q ( -0.13411072261236698)) * f1( 0.32953409120983507)
w2 ( -0.5649398563020134 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13411072261236698) - present_state_Q (-0.13411072261236698)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10960902282918514 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3895824608955484) - present_state_Q ( -0.38893958506653414)) * f1( 0.5539280719566765)
w2 ( -0.5859409759633947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3895824608955484) - present_state_Q (-0.38893958506653414)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.126862586217234 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4063715334874307) - present_state_Q ( -0.40850398172580965)) * f1( 0.519477271834703)
w2 ( -0.6058689662607707 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4063715334874307) - present_state_Q (-0.40850398172580965)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14421311620547056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4334933219319954) - present_state_Q ( -0.4348774068630257)) * f1( 0.5624670695612037)
w2 ( -0.6243772817805812 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4334933219319954) - present_state_Q (-0.4348774068630257)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14900936074232965 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5305933863402967) - present_state_Q ( -0.5305933863402967)) * f1( 0.21559454322818639)
w2 ( -0.6421745579640799 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5305933863402967) - present_state_Q (-0.5305933863402967)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1530615878602274 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5392901632009552) - present_state_Q ( -0.5422672213394257)) * f1( 0.19144820718674388)
w2 ( -0.6591075015625334 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5392901632009552) - present_state_Q (-0.5422672213394257)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1562996623389592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41045667655960266) - present_state_Q ( -0.41045667655960266)) * f1( 0.09794864819887447)
w2 ( -0.6789428410283148 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41045667655960266) - present_state_Q (-0.41045667655960266)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15115884813934055 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16779844771733587) - present_state_Q ( -0.16779844771733587)) * f1( 0.20479813604622332)
w2 ( -0.6739224689694028 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.16779844771733587) - present_state_Q (-0.16779844771733587)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14530964634594667 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15822533139024028) - present_state_Q ( -0.29300982518412083)) * f1( 0.15507420098062402)
w2 ( -0.6588349772875989 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.15822533139024028) - present_state_Q (-0.29300982518412083)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.14074424085516377 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.278261896522285) - present_state_Q ( -0.278261896522285)) * f1( 0.10135531933084435)
w2 ( -0.6408175490127966 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.278261896522285) - present_state_Q (-0.278261896522285)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13708838335528312 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27127807268292026) - present_state_Q ( -0.27127807268292026)) * f1( 0.10622852478338574)
w2 ( -0.6270515383962115 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.27127807268292026) - present_state_Q (-0.27127807268292026)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13416231879789656 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2627423817262819) - present_state_Q ( -0.2627423817262819)) * f1( 0.08696408897682029)
w2 ( -0.6135928126540654 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2627423817262819) - present_state_Q (-0.2627423817262819)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13086012147296053 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2572080860007322) - present_state_Q ( -0.2587405539549929)) * f1( 0.09915920515213486)
w2 ( -0.6002720228398686 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2572080860007322) - present_state_Q (-0.2587405539549929)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13080696027753186 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00034729127404160235) - present_state_Q ( -0.00034729127404160235)) * f1( 0.002653912208948719)
w2 ( -0.6002720228398686 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.00034729127404160235) - present_state_Q (-0.00034729127404160235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.11584186722507861 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2734064654345547) - present_state_Q ( -0.3946677910275055)) * f1( 0.2637824260297488)
w2 ( -0.5662323941708256 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2734064654345547) - present_state_Q (-0.3946677910275055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10110112468339395 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3711708160633264) - present_state_Q ( -0.37168301949714705)) * f1( 0.2757516238285925)
w2 ( -0.5341584378973767 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3711708160633264) - present_state_Q (-0.37168301949714705)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08714477864926277 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4506273565909369) - present_state_Q ( -0.4506273565909369)) * f1( 0.23046831918045585)
w2 ( -0.48571326822282923 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4506273565909369) - present_state_Q (-0.4506273565909369)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.06922577601474353 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32323699007613294) - present_state_Q ( -0.32323699007613294)) * f1( 0.3650135973201465)
w2 ( -0.45625847075871806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32323699007613294) - present_state_Q (-0.32323699007613294)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.055093940661695054 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38296784235917714) - present_state_Q ( -0.38296784235917714)) * f1( 0.2594563295090747)
w2 ( -0.4126847861088573 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.38296784235917714) - present_state_Q (-0.38296784235917714)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04232149808562076 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3433407932066465) - present_state_Q ( -0.3439557998782216)) * f1( 0.25062594588983517)
w2 ( -0.3719150484642527 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3433407932066465) - present_state_Q (-0.3439557998782216)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.03276627934882375 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37938378249188115) - present_state_Q ( -0.37938378249188115)) * f1( 0.17647612597546505)
w2 ( -0.3177705080399834 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.37938378249188115) - present_state_Q (-0.37938378249188115)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.023536264955278146 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13647642701474547) - present_state_Q ( -0.13647642701474547)) * f1( 0.2859105148625427)
w2 ( -0.30485735666745256 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13647642701474547) - present_state_Q (-0.13647642701474547)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01691851455302088 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06671209825743009) - present_state_Q ( -0.06695558493275622)) * f1( 0.25425077473576535)
w2 ( -0.2996516691653123 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06671209825743009) - present_state_Q (-0.06695558493275622)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.011058733607485837 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12305101657962683) - present_state_Q ( -0.12305101657962683)) * f1( 0.1885714554610391)
w2 ( -0.2872218325684457 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12305101657962683) - present_state_Q (-0.12305101657962683)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.006483125186182592 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11654834312600877) - present_state_Q ( -0.11654834312600877)) * f1( 0.15007234621395224)
w2 ( -0.27502609221590935 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11654834312600877) - present_state_Q (-0.11654834312600877)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.002532795875755789 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.057328444481535935) - present_state_Q ( -0.057328444481535935)) * f1( 0.3583497112327133)
w2 ( -0.2699941802152417 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.057328444481535935) - present_state_Q (-0.057328444481535935)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.015210657423241469 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05272410165198314) - present_state_Q ( -0.05272410165198314)) * f1( 0.5032914034909388)
w2 ( -0.277045146385506 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05272410165198314) - present_state_Q (-0.05272410165198314)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0261838215135149 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11633889417921417) - present_state_Q ( -0.1164729062248025)) * f1( 0.37176878771588423)
w2 ( -0.2888515857132307 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11633889417921417) - present_state_Q (-0.1164729062248025)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03595348685635984 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12416086041441621) - present_state_Q ( -0.12442304235707126)) * f1( 0.3392326848544344)
w2 ( -0.30037130746060553 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12416086041441621) - present_state_Q (-0.12442304235707126)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05143773806215227 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12969310611512286) - present_state_Q ( -0.12969310611512286)) * f1( 0.26547030525892623)
w2 ( -0.3237023556404611 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.12969310611512286) - present_state_Q (-0.12969310611512286)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.046457452684342225 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1407809589936028) - present_state_Q ( -0.1407809589936028)) * f1( 0.21968339128296296)
w2 ( -0.3146342411166914 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1407809589936028) - present_state_Q (-0.1407809589936028)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.042619936202605194 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07364836933836748) - present_state_Q ( -0.07364836933836748)) * f1( 0.23078151072718442)
w2 ( -0.3113085704686008 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.07364836933836748) - present_state_Q (-0.07364836933836748)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.035396426332424964 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1336958606068975) - present_state_Q ( -0.13412166768345243)) * f1( 0.2252053933254225)
w2 ( -0.29847848720369025 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1336958606068975) - present_state_Q (-0.13412166768345243)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03168448473796018 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06478231722619697) - present_state_Q ( -0.06478231722619697)) * f1( 0.143704331552796)
w2 ( -0.2933124054936187 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06478231722619697) - present_state_Q (-0.06478231722619697)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.028264630174314853 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12083446734863565) - present_state_Q ( -0.12083446734863565)) * f1( 0.11076415413451689)
w2 ( -0.2809623646690678 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12083446734863565) - present_state_Q (-0.12083446734863565)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.024935203852082085 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11525734432456049) - present_state_Q ( -0.11548096118201667)) * f1( 0.10953673532240321)
w2 ( -0.2688041555990854 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11525734432456049) - present_state_Q (-0.11548096118201667)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.022695776492491052 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05599095861316432) - present_state_Q ( -0.05599095861316432)) * f1( 0.0894369064145838)
w2 ( -0.26379631834404843 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05599095861316432) - present_state_Q (-0.05599095861316432)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.014698718488947373 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2200245095067797) - present_state_Q ( -0.22019479687749963)) * f1( 0.4034998408312106)
w2 ( -0.2479409306699027 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.2200245095067797) - present_state_Q (-0.22019479687749963)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0026493959811257593 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15333408085994088) - present_state_Q ( -0.2029222669939214)) * f1( 0.31087896958059774)
w2 ( -0.2169338219572685 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15333408085994088) - present_state_Q (-0.2029222669939214)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.007807034477034116 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21763362760807833) - present_state_Q ( -0.21763362760807833)) * f1( 0.264137809446083)
w2 ( -0.17734679547254145 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21763362760807833) - present_state_Q (-0.21763362760807833)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0182347486095477 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2103093002800933) - present_state_Q ( -0.24577865937460155)) * f1( 0.3211019874871684)
w2 ( -0.13188211336401853 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2103093002800933) - present_state_Q (-0.24577865937460155)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01676284406167121 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1511227717169046) - present_state_Q ( -0.1774991943897083)) * f1( 0.3913278144224806)
w2 ( -0.13714794495349603 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1511227717169046) - present_state_Q (-0.1774991943897083)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.006993849957510319 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18465798052763707) - present_state_Q ( -0.18499318986253965)) * f1( 0.4184214233903408)
w2 ( -0.1698341101001274 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18465798052763707) - present_state_Q (-0.18499318986253965)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.01884943320490732 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2003231400413786) - present_state_Q ( -0.2003231400413786)) * f1( 0.4972643250717273)
w2 ( -0.2321992109756585 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2003231400413786) - present_state_Q (-0.2003231400413786)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.042797487471073065 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2887120606546992) - present_state_Q ( -0.2888989559058798)) * f1( 0.5443082889313843)
w2 ( -0.2849958809948093 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2887120606546992) - present_state_Q (-0.2888989559058798)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.05763318270558128 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41863777951837783) - present_state_Q ( -0.41863777951837783)) * f1( 0.4589883025007447)
w2 ( -0.33024752077549374 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41863777951837783) - present_state_Q (-0.41863777951837783)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06949689171958456 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28623358402843063) - present_state_Q ( -0.4183325923386282)) * f1( 0.38234167147429987)
w2 ( -0.36748241270319953 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.28623358402843063) - present_state_Q (-0.4183325923386282)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04567385877298358 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4649071483898337) - present_state_Q ( -0.5384036309304736)) * f1( 0.34430681076418734)
w2 ( -0.27061460445039087 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4649071483898337) - present_state_Q (-0.5384036309304736)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.021625609785252794 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34620751642723635) - present_state_Q ( -0.34620751642723635)) * f1( 0.4700717579716948)
w2 ( -0.20922419267624934 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.34620751642723635) - present_state_Q (-0.34620751642723635)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.011292939807083322 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13404731545373813) - present_state_Q ( -0.17589215398898797)) * f1( 0.39364438425193754)
w2 ( -0.1882251988807602 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13404731545373813) - present_state_Q (-0.17589215398898797)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0003999670668485226 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11665957949639907) - present_state_Q ( -0.1544325558753623)) * f1( 0.3411332068145596)
w2 ( -0.1608038710467024 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11665957949639907) - present_state_Q (-0.1544325558753623)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.009516523679182409 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.096367979122454) - present_state_Q ( -0.12852875333179448)) * f1( 0.28588230143152155)
w2 ( -0.1352925146131385 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.096367979122454) - present_state_Q (-0.12852875333179448)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.020343246509842705 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18660932438631614) - present_state_Q ( -0.18660932438631614)) * f1( 0.2942456895476705)
w2 ( -0.08377973974046266 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18660932438631614) - present_state_Q (-0.18660932438631614)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.02989831870229565 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05984078744066942) - present_state_Q ( -0.07659673538876195)) * f1( 0.35309036579905495)
w2 ( -0.05671847407599315 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05984078744066942) - present_state_Q (-0.07659673538876195)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.014182492092173024 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02131363347278119) - present_state_Q ( -0.03265732828797982)) * f1( 0.42535672655861534)
w2 ( -0.08627639688073702 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02131363347278119) - present_state_Q (-0.03265732828797982)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.019630498337902355 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02740956397010466) - present_state_Q ( -0.02740956397010466)) * f1( 0.5006873782153569)
w2 ( -0.11328965257781325 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.02740956397010466) - present_state_Q (-0.02740956397010466)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.047985051257065606 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12350637052191593) - present_state_Q ( -0.14577170417613713)) * f1( 0.5004519454196884)
w2 ( -0.1812791245229398 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.12350637052191593) - present_state_Q (-0.14577170417613713)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07668825197314796 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20794037937865822) - present_state_Q ( -0.20814598883693805)) * f1( 0.5599007109540645)
w2 ( -0.23254392943303256 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20794037937865822) - present_state_Q (-0.20814598883693805)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09482841817937179 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36281041852284396) - present_state_Q ( -0.36281041852284396)) * f1( 0.4857186903887296)
w2 ( -0.28482981669915425 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.36281041852284396) - present_state_Q (-0.36281041852284396)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10737122781079131 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3814826760091251) - present_state_Q ( -0.43844863934895595)) * f1( 0.41851268567056144)
w2 ( -0.32678776465442816 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3814826760091251) - present_state_Q (-0.43844863934895595)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12042503653973026 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36446736693279913) - present_state_Q ( -0.36446736693279913)) * f1( 0.3509282984522598)
w2 ( -0.3639857016304762 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.36446736693279913) - present_state_Q (-0.36446736693279913)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1297970302274853 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2529303368057258) - present_state_Q ( -0.3985246174579163)) * f1( 0.2868084313683814)
w2 ( -0.39666254325274186 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2529303368057258) - present_state_Q (-0.3985246174579163)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13756223281603547 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4287483523108929) - present_state_Q ( -0.4287483523108929)) * f1( 0.2471998704586438)
w2 ( -0.4280751915447615 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4287483523108929) - present_state_Q (-0.4287483523108929)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.14166018799710398 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4556706522470421) - present_state_Q ( -0.5412856905559945)) * f1( 0.2006034660631351)
w2 ( -0.45258895650500663 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4556706522470421) - present_state_Q (-0.5412856905559945)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1444429073799221 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4741609285935593) - present_state_Q ( -0.5646787198945608)) * f1( 0.15227970817738626)
w2 ( -0.47451744126078205 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4741609285935593) - present_state_Q (-0.5646787198945608)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1447147347570076 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2945191753312398) - present_state_Q ( -0.38942266358339617)) * f1( 0.0679071804403043)
w2 ( -0.47771978157676026 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2945191753312398) - present_state_Q (-0.38942266358339617)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.14239412094849882 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4876300693854038) - present_state_Q ( -0.4876300693854038)) * f1( 0.06848153939047073)
w2 ( -0.44383307533207395 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.4876300693854038) - present_state_Q (-0.4876300693854038)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13998009156110416 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18686945825230045) - present_state_Q ( -0.18686945825230045)) * f1( 0.06556610664317797)
w2 ( -0.4291057748349911 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18686945825230045) - present_state_Q (-0.18686945825230045)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12577803605719856 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1692876409832722) - present_state_Q ( -0.2551087959502704)) * f1( 0.5962739778594812)
w2 ( -0.4195785735609134 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1692876409832722) - present_state_Q (-0.2551087959502704)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09595997812121163 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32754590346749396) - present_state_Q ( -0.32754590346749396)) * f1( 0.6026390752076601)
w2 ( -0.3898910947736687 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32754590346749396) - present_state_Q (-0.32754590346749396)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06784943763604162 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2914757731268523) - present_state_Q ( -0.29219033239952913)) * f1( 0.6070830431176459)
w2 ( -0.36210852946845806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2914757731268523) - present_state_Q (-0.29219033239952913)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03969567126455298 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32824436073757324) - present_state_Q ( -0.32824436073757324)) * f1( 0.5682808657845834)
w2 ( -0.3224749354953528 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32824436073757324) - present_state_Q (-0.32824436073757324)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0157703294743345 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27823919498290905) - present_state_Q ( -0.27902875860998394)) * f1( 0.5302545477420268)
w2 ( -0.28637854836641735 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.27823919498290905) - present_state_Q (-0.27902875860998394)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.004231196998573095 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23669370577528537) - present_state_Q ( -0.23673909579955416)) * f1( 0.4842167133443813)
w2 ( -0.2533329703486553 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23669370577528537) - present_state_Q (-0.23673909579955416)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.014073304370776592 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20035694827884512) - present_state_Q ( -0.20035694827884512)) * f1( 0.5458096138889204)
w2 ( -0.23890727007257848 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.20035694827884512) - present_state_Q (-0.20035694827884512)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.019504581535300644 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1823082768861092) - present_state_Q ( -0.1823082768861092)) * f1( 0.6265436275409012)
w2 ( -0.2817810741367786 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1823082768861092) - present_state_Q (-0.1823082768861092)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04553132421695548 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23583265195910982) - present_state_Q ( -0.23583265195910982)) * f1( 0.5336075850102302)
w2 ( -0.32080112319572274 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.23583265195910982) - present_state_Q (-0.23583265195910982)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.03836630004275421 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2783193280471257) - present_state_Q ( -0.2783193280471257)) * f1( 0.4761212168407504)
w2 ( -0.3087621315763297 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2783193280471257) - present_state_Q (-0.2783193280471257)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.032379959665715885 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2637181129214805) - present_state_Q ( -0.2637304407522555)) * f1( 0.43581829554996504)
w2 ( -0.2977734412195211 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2637181129214805) - present_state_Q (-0.2637304407522555)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.01590791515763038 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2504421349024558) - present_state_Q ( -0.250747756904181)) * f1( 0.38693698379834274)
w2 ( -0.26371715774640625 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2504421349024558) - present_state_Q (-0.250747756904181)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.002663916920683306 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21599816849403514) - present_state_Q ( -0.21631140581412733)) * f1( 0.3355360877972781)
w2 ( -0.23214023062922834 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21599816849403514) - present_state_Q (-0.21631140581412733)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.007236430122700647 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13999230717919617) - present_state_Q ( -0.1864203533050418)) * f1( 0.26583742013902334)
w2 ( -0.20234654082225856 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13999230717919617) - present_state_Q (-0.1864203533050418)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.019236723756126072 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20006162996377003) - present_state_Q ( -0.20006162996377003)) * f1( 0.3157511120463633)
w2 ( -0.16434099412551925 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.20006162996377003) - present_state_Q (-0.20006162996377003)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03273745050418758 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1568905391119436) - present_state_Q ( -0.15672569057634667)) * f1( 0.39587320823003636)
w2 ( -0.130237330459004 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1568905391119436) - present_state_Q (-0.15672569057634667)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04654410318805684 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11536049567298931) - present_state_Q ( -0.11536049567298931)) * f1( 0.45442863011314044)
w2 ( -0.09985488584843498 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11536049567298931) - present_state_Q (-0.11536049567298931)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.05988921851665242 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03523342544109867) - present_state_Q ( -0.055204402610785665)) * f1( 0.5302391576489771)
w2 ( -0.07972040104310091 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03523342544109867) - present_state_Q (-0.055204402610785665)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.016915783569324556 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0046592661417533265) - present_state_Q ( 0.0046592661417533265)) * f1( 0.6102505169412344)
w2 ( -0.10788813462420402 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0046592661417533265) - present_state_Q (0.0046592661417533265)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.024782366750164056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05390596556438851) - present_state_Q ( -0.05390596556438851)) * f1( 0.6400481045269268)
w2 ( -0.14697721248372703 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.05390596556438851) - present_state_Q (-0.05390596556438851)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05704688392168389 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19152717330537444) - present_state_Q ( -0.19152717330537444)) * f1( 0.6115040777855347)
w2 ( -0.21029227776674658 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.19152717330537444) - present_state_Q (-0.19152717330537444)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.08046472979659662 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28230924860320955) - present_state_Q ( -0.28230924860320955)) * f1( 0.5251560334871538)
w2 ( -0.26380287891759996 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.28230924860320955) - present_state_Q (-0.28230924860320955)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09961228354008766 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24732421364954818) - present_state_Q ( -0.30008478943306816)) * f1( 0.4509045218592506)
w2 ( -0.3062676421107886 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24732421364954818) - present_state_Q (-0.30008478943306816)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09579439944093567 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22190149579048538) - present_state_Q ( -0.22190149579048538)) * f1( 0.38289364693324074)
w2 ( -0.3002849613381024 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22190149579048538) - present_state_Q (-0.22190149579048538)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0851507095743976 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08995593786181985) - present_state_Q ( -0.15001293012944034)) * f1( 0.3121157997616998)
w2 ( -0.2866442678843721 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08995593786181985) - present_state_Q (-0.15001293012944034)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07867039585371963 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0777667636196176) - present_state_Q ( -0.0777667636196176)) * f1( 0.240020431361012)
w2 ( -0.28124446613921894 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0777667636196176) - present_state_Q (-0.0777667636196176)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07329751628687818 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07220050602594491) - present_state_Q ( -0.07220050602594491)) * f1( 0.20276512689425988)
w2 ( -0.27594485703075194 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07220050602594491) - present_state_Q (-0.07220050602594491)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06993421338643128 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17250619307366163) - present_state_Q ( -0.17250619307366163)) * f1( 0.09467276937530716)
w2 ( -0.2546295226047742 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17250619307366163) - present_state_Q (-0.17250619307366163)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06561940212273937 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21004159219717242) - present_state_Q ( -0.21143238641104728)) * f1( 0.11051483891756213)
w2 ( -0.22339526442946783 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21004159219717242) - present_state_Q (-0.21143238641104728)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0631057796686474 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051377500329935516) - present_state_Q ( -0.051377500329935516)) * f1( 0.10208028764895895)
w2 ( -0.218470469423529 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.051377500329935516) - present_state_Q (-0.051377500329935516)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08060610408089916 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19586331623138215) - present_state_Q ( -0.19586331623138215)) * f1( 0.33415228847945777)
w2 ( -0.2603683106548695 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.19586331623138215) - present_state_Q (-0.19586331623138215)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0856347474562325 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22922063522751626) - present_state_Q ( -0.22922063522751626)) * f1( 0.25960796570218314)
w2 ( -0.27586442491848834 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22922063522751626) - present_state_Q (-0.22922063522751626)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09939001100776672 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24425142587857243) - present_state_Q ( -0.24527525696738922)) * f1( 0.2870764235646651)
w2 ( -0.3141964157681258 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24425142587857243) - present_state_Q (-0.24527525696738922)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11432478670114686 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2837218532630051) - present_state_Q ( -0.2848227975791849)) * f1( 0.3367105469187351)
w2 ( -0.34968036678789505 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2837218532630051) - present_state_Q (-0.2848227975791849)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1250611963787001 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3843449839619931) - present_state_Q ( -0.3843449839619931)) * f1( 0.3032117371424788)
w2 ( -0.38508931823131565 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3843449839619931) - present_state_Q (-0.3843449839619931)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1106211200640019 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4152547719164774) - present_state_Q ( -0.41649767545568683)) * f1( 0.25114390501481315)
w2 ( -0.3275920984049117 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4152547719164774) - present_state_Q (-0.41649767545568683)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10078891195843571 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34761584938203943) - present_state_Q ( -0.3487529569815115)) * f1( 0.19129130643729478)
w2 ( -0.27619296120058096 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.34761584938203943) - present_state_Q (-0.3487529569815115)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09501693526902297 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23345577150274094) - present_state_Q ( -0.2886943637428571)) * f1( 0.12403549457336739)
w2 ( -0.22965808254132264 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23345577150274094) - present_state_Q (-0.2886943637428571)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0934773836284514 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14225431892996887) - present_state_Q ( -0.14225431892996887)) * f1( 0.046933416580413924)
w2 ( -0.2099763493191043 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14225431892996887) - present_state_Q (-0.14225431892996887)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08562586349039947 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0699102869973473) - present_state_Q ( -0.0699102869973473)) * f1( 0.29862856714605396)
w2 ( -0.20471796415315205 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0699102869973473) - present_state_Q (-0.0699102869973473)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07017102393646531 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08829579215367736) - present_state_Q ( -0.08829579215367736)) * f1( 0.5530128093640325)
w2 ( -0.19912863989438587 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08829579215367736) - present_state_Q (-0.08829579215367736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05688164791076303 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07471955407329989) - present_state_Q ( -0.07471955407329989)) * f1( 0.4972683044502314)
w2 ( -0.19378368792106646 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07471955407329989) - present_state_Q (-0.07471955407329989)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04532479410364017 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06425330698954493) - present_state_Q ( -0.06425330698954493)) * f1( 0.4482389371934356)
w2 ( -0.18862712839525464 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06425330698954493) - present_state_Q (-0.06425330698954493)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03591845206467255 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054824072839927504) - present_state_Q ( -0.054824072839927504)) * f1( 0.37724710059969874)
w2 ( -0.18364029508413593 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.054824072839927504) - present_state_Q (-0.054824072839927504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.028401156427431257 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04783712434054713) - present_state_Q ( -0.04783712434054713)) * f1( 0.3092857482755003)
w2 ( -0.17877922684600608 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04783712434054713) - present_state_Q (-0.04783712434054713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.021339766345192385 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04336920849609761) - present_state_Q ( -0.0788182583238791)) * f1( 0.2572630309665708)
w2 ( -0.1677999733470353 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04336920849609761) - present_state_Q (-0.0788182583238791)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.015903192988256414 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03849526473674815) - present_state_Q ( -0.03850407964819824)) * f1( 0.2316841196297836)
w2 ( -0.16310688228354484 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03849526473674815) - present_state_Q (-0.03850407964819824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.011804386412060606 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03543237965964816) - present_state_Q ( -0.03543237965964816)) * f1( 0.17675715845333428)
w2 ( -0.15846909944967116 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03543237965964816) - present_state_Q (-0.03543237965964816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.009504847348649554 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0328761371620142) - present_state_Q ( -0.0328761371620142)) * f1( 0.10015914684663174)
w2 ( -0.1538773289807549 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0328761371620142) - present_state_Q (-0.0328761371620142)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.007677096378823286 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03611959326814658) - present_state_Q ( -0.03611959326814658)) * f1( 0.5622528459391716)
w2 ( -0.15322717630192825 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.03611959326814658) - present_state_Q (-0.03611959326814658)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05197529165648547 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03548377482045447) - present_state_Q ( -0.03573792087799532)) * f1( 0.6633348555655654)
w2 ( -0.16658338543400925 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.03548377482045447) - present_state_Q (-0.03573792087799532)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08934394617467695 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06526439511944562) - present_state_Q ( -0.09858107220624747)) * f1( 0.614671260409509)
w2 ( -0.19090120012623712 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06526439511944562) - present_state_Q (-0.09858107220624747)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12770057095780776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13475676479551085) - present_state_Q ( -0.13566989373995147)) * f1( 0.6638324836637548)
w2 ( -0.2140134314358211 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13475676479551085) - present_state_Q (-0.13566989373995147)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1602468191386135 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16040585266211238) - present_state_Q ( -0.16040585266211238)) * f1( 0.585748987077732)
w2 ( -0.23623882073998506 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.16040585266211238) - present_state_Q (-0.16040585266211238)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1622392122613749 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1792097348791092) - present_state_Q ( -0.1810218263345218)) * f1( 0.5399564153824639)
w2 ( -0.23771478662612064 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1792097348791092) - present_state_Q (-0.1810218263345218)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1473533721378017 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14797363410763176) - present_state_Q ( -0.15015541987573466)) * f1( 0.6324763361473742)
w2 ( -0.23300762549682122 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.14797363410763176) - present_state_Q (-0.15015541987573466)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11739389098807029 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3199937409404918) - present_state_Q ( -0.3229311330750449)) * f1( 0.6102575480534587)
w2 ( -0.18391444959872164 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3199937409404918) - present_state_Q (-0.3229311330750449)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.08781237251145385 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25905568590273415) - present_state_Q ( -0.2947642160516823)) * f1( 0.6309261573137829)
w2 ( -0.12765141190335255 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.25905568590273415) - present_state_Q (-0.2947642160516823)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07026595613168463 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18539101795699775) - present_state_Q ( -0.18539101795699775)) * f1( 0.6575338349514916)
w2 ( -0.10096622028722275 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.18539101795699775) - present_state_Q (-0.18539101795699775)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06860686429256316 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1420407325278824) - present_state_Q ( -0.142364263084914)) * f1( 0.5891621643930621)
w2 ( -0.09815020130401018 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1420407325278824) - present_state_Q (-0.142364263084914)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.04892861214754925 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1563302371887655) - present_state_Q ( -0.15729484596388568)) * f1( 0.5759570096451231)
w2 ( -0.05715078263460907 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1563302371887655) - present_state_Q (-0.15729484596388568)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03467599051940343 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09315016530365106) - present_state_Q ( -0.09315016530365106)) * f1( 0.5021443499772515)
w2 ( -0.023090564781814746 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09315016530365106) - present_state_Q (-0.09315016530365106)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.023396617031999065 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04361220812066732) - present_state_Q ( -0.04402813702345783)) * f1( 0.47062705465178695)
w2 ( 0.005669465163552191 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04361220812066732) - present_state_Q (-0.04402813702345783)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014717093499802504 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029581239391460717) - present_state_Q ( -0.003204657379286411)) * f1( 0.42775481437599666)
w2 ( 0.030018526561796814 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0029581239391460717) - present_state_Q (-0.003204657379286411)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.008309288381386466 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03084945350236279) - present_state_Q ( 0.030556227813911166)) * f1( 0.3714051324277016)
w2 ( 0.050721972666155836 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03084945350236279) - present_state_Q (0.030556227813911166)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0034230163846469793 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05821134178637736) - present_state_Q ( 0.05811752156106636)) * f1( 0.3308160112096132)
w2 ( 0.0684464061802644 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05821134178637736) - present_state_Q (0.05811752156106636)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 3.755925210222142e-05 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08123100513694258) - present_state_Q ( 0.08120237983333133)) * f1( 0.2726564754910518)
w2 ( 0.08367689266190795 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08123100513694258) - present_state_Q (0.08120237983333133)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.003925995970918139 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10042599541247328) - present_state_Q ( 0.10042559456277916)) * f1( 0.3547293341557466)
w2 ( 0.09683093325932413 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10042599541247328) - present_state_Q (0.10042559456277916)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.007983029400936134 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11785162888404008) - present_state_Q ( 0.1178935327736197)) * f1( 0.43209745374089203)
w2 ( 0.10809792887309824 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11785162888404008) - present_state_Q (0.1178935327736197)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01290531410803684 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13343482307989815) - present_state_Q ( 0.11197432937560348)) * f1( 0.4855801360384149)
w2 ( 0.11823484416633688 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13343482307989815) - present_state_Q (0.11197432937560348)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.016415348825385458 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1486130589412985) - present_state_Q ( 0.14873171143425834)) * f1( 0.5307812252619455)
w2 ( 0.12617039550152145 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1486130589412985) - present_state_Q (0.14873171143425834)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.021128760128462435 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16071680981446473) - present_state_Q ( 0.13581045376939524)) * f1( 0.5872588131033781)
w2 ( 0.13419651822272657 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16071680981446473) - present_state_Q (0.13581045376939524)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023867241924478158 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.174107062193841) - present_state_Q ( 0.174528889385428)) * f1( 0.6386114204581105)
w2 ( 0.13934233624280132 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.174107062193841) - present_state_Q (0.174528889385428)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.026153951085173058 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18253592097466953) - present_state_Q ( 0.18245760974040287)) * f1( 0.6388172666655799)
w2 ( 0.143637854125649 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18253592097466953) - present_state_Q (0.18245760974040287)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012840649727492714 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19022022351565274) - present_state_Q ( 0.19022022351565274)) * f1( 0.6826807355694706)
w2 ( 0.07509406998595848 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.19022022351565274) - present_state_Q (0.19022022351565274)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.029852798879266892 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08217112008321957) - present_state_Q ( 0.08213806324063123)) * f1( 0.6210605313408963)
w2 ( 0.04222355583808136 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08217112008321957) - present_state_Q (0.08213806324063123)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0538335632380853 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.034044332805340055) - present_state_Q ( 0.034044332805340055)) * f1( 0.5568635044100704)
w2 ( -0.009453232104895383 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.034044332805340055) - present_state_Q (0.034044332805340055)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04748458263836751 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031851225332333276) - present_state_Q ( -0.03374187175331235)) * f1( 0.4863004507729695)
w2 ( 0.0009913078327109404 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.031851225332333276) - present_state_Q (-0.03374187175331235)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.03803574383970052 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019469778960897546) - present_state_Q ( -0.0198020310680177)) * f1( 0.43372135101268944)
w2 ( 0.01841971208646518 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.019469778960897546) - present_state_Q (-0.0198020310680177)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.030010670591427084 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005985830750098054) - present_state_Q ( 0.00638680968826303)) * f1( 0.4132125003715699)
w2 ( 0.041725124892874804 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.005985830750098054) - present_state_Q (0.00638680968826303)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.023944039351240003 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0390276312297185) - present_state_Q ( 0.0390276312297185)) * f1( 0.3679530788254263)
w2 ( 0.061510140720065214 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0390276312297185) - present_state_Q (0.0390276312297185)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.019569370301933054 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06608602913163177) - present_state_Q ( 0.06634431991316206)) * f1( 0.3118875993047292)
w2 ( 0.07834185468006535 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06608602913163177) - present_state_Q (0.06634431991316206)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.016426695274867394 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08871580251017971) - present_state_Q ( 0.08888465296141361)) * f1( 0.2619181187530869)
w2 ( 0.09274028595481788 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08871580251017971) - present_state_Q (0.08888465296141361)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014245378811521728 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10787425501613439) - present_state_Q ( 0.10780880340144722)) * f1( 0.21182226163639306)
w2 ( 0.10509772060683784 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10787425501613439) - present_state_Q (0.10780880340144722)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012788949001203089 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12406070044667403) - present_state_Q ( 0.10319791789504427)) * f1( 0.13336273727287537)
w2 ( 0.11601853582180015 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12406070044667403) - present_state_Q (0.10319791789504427)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.011798159099521517 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1376922899694308) - present_state_Q ( 0.13755957161884935)) * f1( 0.1300084445683877)
w2 ( 0.1251636947071714 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1376922899694308) - present_state_Q (0.13755957161884935)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.010934591698306309 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09921383280171996) - present_state_Q ( 0.09921067392023177)) * f1( 0.07800215590775375)
w2 ( 0.1340205514559666 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09921383280171996) - present_state_Q (0.09921067392023177)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.007824728755188325 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07780868293942297) - present_state_Q ( 0.07779624591685042)) * f1( 0.23924852696006732)
w2 ( 0.14181962879859214 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07780868293942297) - present_state_Q (0.07779624591685042)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.041915605587297594 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11011757360953153) - present_state_Q ( 0.11011757360953153)) * f1( 0.4266127981917303)
w2 ( 0.07789116349870587 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.11011757360953153) - present_state_Q (0.11011757360953153)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.06952074508236214 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04671978258793348) - present_state_Q ( 0.04671978258793348)) * f1( 0.3720129529932564)
w2 ( 0.01852733915237466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.04671978258793348) - present_state_Q (0.04671978258793348)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09299626949528347 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00860628270068505) - present_state_Q ( -0.00875900006872033)) * f1( 0.3391918680198765)
w2 ( -0.03684079110373319 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.00860628270068505) - present_state_Q (-0.00875900006872033)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11835893808031236 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06502575754398576) - present_state_Q ( -0.06631551370110197)) * f1( 0.39617590058259256)
w2 ( -0.08805575606799693 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06502575754398576) - present_state_Q (-0.06631551370110197)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.14178949799767096 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11987406774209372) - present_state_Q ( -0.13622144062548527)) * f1( 0.4069458998086444)
w2 ( -0.14563235268286934 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.11987406774209372) - present_state_Q (-0.13622144062548527)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1665305570615257 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18192097117268463) - present_state_Q ( -0.18192097117268463)) * f1( 0.4613535554478347)
w2 ( -0.18853404275843605 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.18192097117268463) - present_state_Q (-0.18192097117268463)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18821995877139078 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26931877976308094) - present_state_Q ( -0.2670799358671761)) * f1( 0.47166054383473177)
w2 ( -0.23451923696934926 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.26931877976308094) - present_state_Q (-0.2670799358671761)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.20733648277136818 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32229137414533393) - present_state_Q ( -0.32229137414533393)) * f1( 0.46632746999265606)
w2 ( -0.2755130132962692 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32229137414533393) - present_state_Q (-0.32229137414533393)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.22080959897977995 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4165378411518833) - present_state_Q ( -0.4165378411518833)) * f1( 0.4144095821819618)
w2 ( -0.3145269264518658 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4165378411518833) - present_state_Q (-0.4165378411518833)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23135802489399074 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4583924920673832) - present_state_Q ( -0.4584903446416569)) * f1( 0.3670946973045341)
w2 ( -0.3490087949996756 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4583924920673832) - present_state_Q (-0.4584903446416569)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23973706972605996 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.491976979875213) - present_state_Q ( -0.49510348707761265)) * f1( 0.3297613433247438)
w2 ( -0.3795001003088646 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.491976979875213) - present_state_Q (-0.49510348707761265)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.24505482847593046 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3606002276466901) - present_state_Q ( -0.512400267770236)) * f1( 0.23776109161895828)
w2 ( -0.40633927090819655 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3606002276466901) - present_state_Q (-0.512400267770236)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23947243667077725 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20223931406748769) - present_state_Q ( -0.3647750224307663)) * f1( 0.16201927524194357)
w2 ( -0.37877518362627516 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.20223931406748769) - present_state_Q (-0.3647750224307663)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.2348100727813137 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1773870808891602) - present_state_Q ( -0.25292118982322964)) * f1( 0.1071358357736014)
w2 ( -0.3526642347222163 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1773870808891602) - present_state_Q (-0.25292118982322964)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23186771920471816 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15781205951948785) - present_state_Q ( -0.22834490646393113)) * f1( 0.07131877023945962)
w2 ( -0.32791041269149734 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15781205951948785) - present_state_Q (-0.22834490646393113)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.22456938515722485 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06537808215885538) - present_state_Q ( -0.06537808215885538)) * f1( 0.28196284667436805)
w2 ( -0.32791041269149734 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06537808215885538) - present_state_Q (-0.06537808215885538)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.21570205143663376 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12864305980483054) - present_state_Q ( -0.12864305980483054)) * f1( 0.2808084335377283)
w2 ( -0.3215948376150104 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12864305980483054) - present_state_Q (-0.12864305980483054)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20926557165441995 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11066036228002021) - present_state_Q ( -0.11066036228002021)) * f1( 0.21483984249742627)
w2 ( -0.31560295109397 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11066036228002021) - present_state_Q (-0.11066036228002021)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20440947061328846 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09537508974407707) - present_state_Q ( -0.09831127818106411)) * f1( 0.16816281667384747)
w2 ( -0.3098274757098369 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09537508974407707) - present_state_Q (-0.09831127818106411)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.198098756261311 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.212927096734084) - present_state_Q ( -0.27621764291190554)) * f1( 0.13871990499735992)
w2 ( -0.2734334810507571 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.212927096734084) - present_state_Q (-0.27621764291190554)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1946489986144209 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18126216751615482) - present_state_Q ( -0.1827999285775172)) * f1( 0.09459847351259133)
w2 ( -0.251553058341203 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18126216751615482) - present_state_Q (-0.1827999285775172)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19110959505936745 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17042734599925577) - present_state_Q ( -0.17042734599925577)) * f1( 0.10015726324465965)
w2 ( -0.23034998165724319 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17042734599925577) - present_state_Q (-0.17042734599925577)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.18786949383314083 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0696430647219564) - present_state_Q ( -0.0696430647219564)) * f1( 0.12334842938254818)
w2 ( -0.22509640649224796 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0696430647219564) - present_state_Q (-0.0696430647219564)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.181317743935904 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0869576667385078) - present_state_Q ( -0.08894065160985729)) * f1( 0.2337866005559004)
w2 ( -0.21949150879352783 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0869576667385078) - present_state_Q (-0.08894065160985729)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17527171461368052 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08003995514725568) - present_state_Q ( -0.08366625537776627)) * f1( 0.2193274235373164)
w2 ( -0.213978263596267 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08003995514725568) - present_state_Q (-0.08366625537776627)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15935638847070005 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1292432340585551) - present_state_Q ( -0.13060433946904465)) * f1( 0.5009860657969366)
w2 ( -0.20762466327500323 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1292432340585551) - present_state_Q (-0.13060433946904465)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14674447746359154 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10895420205416727) - present_state_Q ( -0.10895420205416727)) * f1( 0.423135024872658)
w2 ( -0.20166348763802822 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10895420205416727) - present_state_Q (-0.10895420205416727)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13674666769541735 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09218126598108939) - present_state_Q ( -0.09218126598108939)) * f1( 0.35332551759126873)
w2 ( -0.1960042248503686 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09218126598108939) - present_state_Q (-0.09218126598108939)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1293320274437365 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07686794547420417) - present_state_Q ( -0.07686794547420417)) * f1( 0.27545168843183987)
w2 ( -0.19062060183183294 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07686794547420417) - present_state_Q (-0.07686794547420417)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12519282734686724 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05925586739400639) - present_state_Q ( -0.05925586739400639)) * f1( 0.16339144638270495)
w2 ( -0.18555399621874083 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05925586739400639) - present_state_Q (-0.05925586739400639)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12359982923011709 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.050797019998638976) - present_state_Q ( -0.050797019998638976)) * f1( 0.10932112521886653)
w2 ( -0.18263964985876532 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.050797019998638976) - present_state_Q (-0.050797019998638976)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12198536856586871 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049980709457248765) - present_state_Q ( -0.05026455124770235)) * f1( 0.11113786614036947)
w2 ( -0.1797343202527258 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.049980709457248765) - present_state_Q (-0.05026455124770235)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11791170738273557 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054394811578919636) - present_state_Q ( -0.05579566703113412)) * f1( 0.16271462072823234)
w2 ( -0.17472719653526095 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.054394811578919636) - present_state_Q (-0.05579566703113412)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11365532517218187 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05371643058175604) - present_state_Q ( -0.05504666118667158)) * f1( 0.1704768960250216)
w2 ( -0.16973369617269102 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05371643058175604) - present_state_Q (-0.05504666118667158)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11098010016641609 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05436305581096014) - present_state_Q ( -0.05436305581096014)) * f1( 0.17963361193584448)
w2 ( -0.16675516116809375 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.05436305581096014) - present_state_Q (-0.05436305581096014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10790810267220118 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056016985685180126) - present_state_Q ( -0.056016985685180126)) * f1( 0.20423439353157447)
w2 ( -0.1637468554257605 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.056016985685180126) - present_state_Q (-0.056016985685180126)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10525174589722633 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04444626525027221) - present_state_Q ( -0.04468105129883603)) * f1( 0.1105726068590929)
w2 ( -0.15894212693028434 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04444626525027221) - present_state_Q (-0.04468105129883603)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1006940118172381 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19099194642940132) - present_state_Q ( -0.19250449550698556)) * f1( 0.6208998568685998)
w2 ( -0.1530697028611607 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.19099194642940132) - present_state_Q (-0.19250449550698556)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09145568582699715 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1789138576163385) - present_state_Q ( -0.1798816788943549)) * f1( 0.5703012082749834)
w2 ( -0.14011047941054303 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1789138576163385) - present_state_Q (-0.1798816788943549)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07886304313274446 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15753681061892616) - present_state_Q ( -0.15936351779348146)) * f1( 0.5169184817494606)
w2 ( -0.12062169247201592 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.15753681061892616) - present_state_Q (-0.15936351779348146)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07334309658819778 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13128054350258772) - present_state_Q ( -0.13285633205512987)) * f1( 0.46103950130756033)
w2 ( -0.11104343025562623 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.13128054350258772) - present_state_Q (-0.13285633205512987)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.061916050516877895 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09394685023843369) - present_state_Q ( -0.11615553628955894)) * f1( 0.3725066619215304)
w2 ( -0.08650256215436897 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09394685023843369) - present_state_Q (-0.11615553628955894)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05128637185470114 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09244191588618952) - present_state_Q ( -0.09244191588618952)) * f1( 0.37534477681775436)
w2 ( -0.06384674421056331 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09244191588618952) - present_state_Q (-0.09244191588618952)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04303119200194583 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05526114376286825) - present_state_Q ( -0.05526114376286825)) * f1( 0.330557546253416)
w2 ( -0.048862642447368425 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05526114376286825) - present_state_Q (-0.05526114376286825)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.036008446582477524 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0415193885033162) - present_state_Q ( -0.04202188507907166)) * f1( 0.29523466628756473)
w2 ( -0.03459044567364402 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0415193885033162) - present_state_Q (-0.04202188507907166)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.030874164008108048 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028933244797157254) - present_state_Q ( -0.028933244797157254)) * f1( 0.22714052310579003)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.028933244797157254) - present_state_Q (-0.028933244797157254)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.016101247469421702 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02084900740979341) - present_state_Q ( -0.02084900740979341)) * f1( 0.6752897796461186)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02084900740979341) - present_state_Q (-0.02084900740979341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0030588389200692633 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009603819072565318) - present_state_Q ( -0.010043792889355726)) * f1( 0.6237897348283202)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.009603819072565318) - present_state_Q (-0.010043792889355726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.009034908749766132 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001784910109888343) - present_state_Q ( -0.001834452391525477)) * f1( 0.5997218027695091)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.001784910109888343) - present_state_Q (-0.001834452391525477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.021573555879511347 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0014970849266015603) - present_state_Q ( 0.0014970849266015603)) * f1( 0.6311845725800694)
w2 ( -0.01705499798327635 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0014970849266015603) - present_state_Q (0.0014970849266015603)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03436939738446339 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007431441514679838) - present_state_Q ( 0.007460269878416641)) * f1( 0.6620266566853366)
w2 ( -0.009323683012354297 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.007431441514679838) - present_state_Q (0.007460269878416641)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04654237272831509 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019120944462622493) - present_state_Q ( 0.019164248138231716)) * f1( 0.6661077320349671)
w2 ( -0.0020137691600330757 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.019120944462622493) - present_state_Q (0.019164248138231716)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.05805163337109642 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030131167841807936) - present_state_Q ( 0.03018959631270977)) * f1( 0.6659545304587886)
w2 ( 0.0048991716588257656 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.030131167841807936) - present_state_Q (0.03018959631270977)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.06897486978373896 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04079429476559325) - present_state_Q ( 0.04079429476559325)) * f1( 0.6689669841641094)
w2 ( 0.011430577047264408 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04079429476559325) - present_state_Q (0.04079429476559325)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.07929490180555122 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050677718020557344) - present_state_Q ( 0.050677718020557344)) * f1( 0.6684389161764114)
w2 ( 0.017606179198524346 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.050677718020557344) - present_state_Q (0.050677718020557344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.08887681555259166 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06444242192332719) - present_state_Q ( 0.06384608787975798)) * f1( 0.6719521576721748)
w2 ( 0.026162068457278833 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06444242192332719) - present_state_Q (0.06384608787975798)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09782293114843459 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07619675302921276) - present_state_Q ( 0.07619675302921276)) * f1( 0.6807119672176564)
w2 ( 0.034047443793701344 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07619675302921276) - present_state_Q (0.07619675302921276)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10604623040919016 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08676733332549053) - present_state_Q ( 0.08600299142792864)) * f1( 0.6703389929321004)
w2 ( 0.041407868307978574 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08676733332549053) - present_state_Q (0.08600299142792864)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.11355986948272255 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09549679749012707) - present_state_Q ( 0.09351071806178599)) * f1( 0.6475100228649724)
w2 ( 0.04837020600921218 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09549679749012707) - present_state_Q (0.09351071806178599)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.12063798786140265 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10628978030135525) - present_state_Q ( 0.10542553689983472)) * f1( 0.6728029333102723)
w2 ( 0.05468241247703023 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10628978030135525) - present_state_Q (0.10542553689983472)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1271984086467269 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11426566937048055) - present_state_Q ( 0.11426566937048055)) * f1( 0.675212039990629)
w2 ( 0.06051206633102428 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11426566937048055) - present_state_Q (0.11426566937048055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1332689707093093 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12188467702593545) - present_state_Q ( 0.12079919564401853)) * f1( 0.6642532461240671)
w2 ( 0.06599542265453878 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12188467702593545) - present_state_Q (0.12079919564401853)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1389044118530296 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1300802335607672) - present_state_Q ( 0.12904660646158914)) * f1( 0.6711941451395746)
w2 ( 0.07103310766820804 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1300802335607672) - present_state_Q (0.12904660646158914)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14411477239224957 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13718268199252315) - present_state_Q ( 0.13718268199252315)) * f1( 0.6807761980350362)
w2 ( 0.0756252428406118 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13718268199252315) - present_state_Q (0.13718268199252315)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.148941070405695 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1433113369030328) - present_state_Q ( 0.1433113369030328)) * f1( 0.6795708002237575)
w2 ( 0.07988643064784803 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1433113369030328) - present_state_Q (0.1433113369030328)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15346107801575973 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14861124030964054) - present_state_Q ( 0.14669999457071303)) * f1( 0.6631356677709741)
w2 ( 0.0839760984154631 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14861124030964054) - present_state_Q (0.14669999457071303)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1575966515101251 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15447780260861782) - present_state_Q ( 0.15447780260861782)) * f1( 0.678296704970691)
w2 ( 0.08763429707459773 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15447780260861782) - present_state_Q (0.15447780260861782)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.16152921153591845 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1584954977318756) - present_state_Q ( 0.15581613682287998)) * f1( 0.6550618784656651)
w2 ( 0.09123630185161619 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1584954977318756) - present_state_Q (0.15581613682287998)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.16509894616682633 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16461628500776457) - present_state_Q ( 0.16338952689965164)) * f1( 0.6726197989552029)
w2 ( 0.09442062794768367 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16461628500776457) - present_state_Q (0.16338952689965164)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1684533564528398 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16763676120480453) - present_state_Q ( 0.1662175336026835)) * f1( 0.6636332900838856)
w2 ( 0.0974533964987515 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16763676120480453) - present_state_Q (0.1662175336026835)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17149409583861852 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17216169849999893) - present_state_Q ( 0.17216169849999893)) * f1( 0.6749029107803891)
w2 ( 0.10015666477975156 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17216169849999893) - present_state_Q (0.17216169849999893)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17429987597018273 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17615817267140113) - present_state_Q ( 0.17615817267140113)) * f1( 0.6767823302370148)
w2 ( 0.1026441234554959 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17615817267140113) - present_state_Q (0.17615817267140113)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17694594229333105 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17991208482021082) - present_state_Q ( 0.17856409004577903)) * f1( 0.6711285095377388)
w2 ( 0.10500975056167042 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17991208482021082) - present_state_Q (0.17856409004577903)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1793258301606257 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18333806590254692) - present_state_Q ( 0.18333806590254692)) * f1( 0.680050720609714)
w2 ( 0.10710949500293289 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18333806590254692) - present_state_Q (0.18333806590254692)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18155105110293845 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18571504050145055) - present_state_Q ( 0.18571504050145055)) * f1( 0.6772551583389087)
w2 ( 0.10908088281585455 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18571504050145055) - present_state_Q (0.18571504050145055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18373718687122118 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18818808389990793) - present_state_Q ( 0.18585618190567726)) * f1( 0.6632164974241544)
w2 ( 0.11105864040491337 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18818808389990793) - present_state_Q (0.18585618190567726)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.185691466063596 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1914698590783301) - present_state_Q ( 0.1900527729171993)) * f1( 0.6717071855505925)
w2 ( 0.1128042931843514 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1914698590783301) - present_state_Q (0.1900527729171993)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1874115984600572 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19416197297968452) - present_state_Q ( 0.19416197297968452)) * f1( 0.6811266007548066)
w2 ( 0.11431954664344843 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19416197297968452) - present_state_Q (0.19416197297968452)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18900840186056672 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19615613156862527) - present_state_Q ( 0.19615613156862527)) * f1( 0.6806644019406506)
w2 ( 0.11572711553874267 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19615613156862527) - present_state_Q (0.19615613156862527)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1905036615331047 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19774939552214682) - present_state_Q ( 0.19774939552214682)) * f1( 0.6788752507074212)
w2 ( 0.11704864818054674 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19774939552214682) - present_state_Q (0.19774939552214682)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17162297077003671 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1990720750057905) - present_state_Q ( 0.1990720750057905)) * f1( 0.6763276099817789)
w2 ( 0.10029875613023403 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1990720750057905) - present_state_Q (0.1990720750057905)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14828983318869324 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1766063157625806) - present_state_Q ( 0.172904652400104)) * f1( 0.6568199945274695)
w2 ( 0.07898411488080327 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1766063157625806) - present_state_Q (0.172904652400104)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14618041876597251 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14756313438628552) - present_state_Q ( 0.14636187700113343)) * f1( 0.6674187025803316)
w2 ( 0.07708778106705298 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.14756313438628552) - present_state_Q (0.14636187700113343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.13092697928178254 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1447873254321761) - present_state_Q ( 0.14357916288215)) * f1( 0.6657970681951119)
w2 ( 0.06334175524671704 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1447873254321761) - present_state_Q (0.14357916288215)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09612263130721385 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12666028749456837) - present_state_Q ( 0.12666028749456837)) * f1( 0.677134955933974)
w2 ( 0.03250209972201033 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12666028749456837) - present_state_Q (0.12666028749456837)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04473338077770984 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0844885227418714) - present_state_Q ( 0.08325489266867739)) * f1( 0.6632530962631542)
w2 ( -0.013986262701659084 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0844885227418714) - present_state_Q (0.08325489266867739)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.024666506257713116 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007449201495424474) - present_state_Q ( 0.007113577721413445)) * f1( 0.2840850072393674)
w2 ( -0.042241009004533925 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.007449201495424474) - present_state_Q (0.007113577721413445)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.031244995528035717 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8392964848674854e-05) - present_state_Q ( -0.00034875249822794753)) * f1( 0.3283581881461698)
w2 ( -0.03823411074049907 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -3.8392964848674854e-05) - present_state_Q (-0.00034875249822794753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03967169708029156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034377525399451304) - present_state_Q ( -0.034485347837053426)) * f1( 0.36471712858209093)
w2 ( -0.010508399304846063 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.034377525399451304) - present_state_Q (-0.034485347837053426)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.04794133900695029 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004101939953180259) - present_state_Q ( 0.004101939953180259)) * f1( 0.4212579836242467)
w2 ( 0.013048591180210473 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.004101939953180259) - present_state_Q (0.004101939953180259)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.05566816786274267 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04003391271188424) - present_state_Q ( 0.04099256219546621)) * f1( 0.4740070889525439)
w2 ( 0.03587010725081159 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04003391271188424) - present_state_Q (0.04099256219546621)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.040464416863205566 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08127129008323394) - present_state_Q ( 0.0812110072944228)) * f1( 0.5567428987370951)
w2 ( -0.0023616357092423254 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08127129008323394) - present_state_Q (0.0812110072944228)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0053179728870659596 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019292812649438648) - present_state_Q ( 0.016593684782764677)) * f1( 0.4917894861299496)
w2 ( -0.10241465220173726 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.019292812649438648) - present_state_Q (0.016593684782764677)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0323384843476342 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13989232119727946) - present_state_Q ( -0.13989232119727946)) * f1( 0.6559250976319391)
w2 ( -0.18278821973088005 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13989232119727946) - present_state_Q (-0.13989232119727946)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05511617560023471 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2720881384671995) - present_state_Q ( -0.2720881384671995)) * f1( 0.5004758624425584)
w2 ( -0.24650511428401292 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2720881384671995) - present_state_Q (-0.2720881384671995)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.050435652590258374 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12233399279034737) - present_state_Q ( -0.22093603850395255)) * f1( 0.43058043883293556)
w2 ( -0.2378089031460195 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.12233399279034737) - present_state_Q (-0.22093603850395255)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.030989231579279554 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3030896568732092) - present_state_Q ( -0.3977778864516081)) * f1( 0.3426869789589127)
w2 ( -0.14701387582373354 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3030896568732092) - present_state_Q (-0.3977778864516081)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.019004604286089407 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1857809751714606) - present_state_Q ( -0.21518375033620732)) * f1( 0.3021799414104103)
w2 ( -0.09148908442906496 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1857809751714606) - present_state_Q (-0.21518375033620732)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.010728295304329935 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15106431176847254) - present_state_Q ( -0.15106431176847254)) * f1( 0.24634960094357083)
w2 ( -0.03773582353440491 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15106431176847254) - present_state_Q (-0.15106431176847254)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.005934698092766598 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062385040545243076) - present_state_Q ( -0.062385040545243076)) * f1( 0.1871427690273309)
w2 ( 0.0032476223041101038 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.062385040545243076) - present_state_Q (-0.062385040545243076)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.003337250050781431 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004409837486120182) - present_state_Q ( 0.004409837486120182)) * f1( 0.1325018034892834)
w2 ( 0.0346126057061088 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.004409837486120182) - present_state_Q (0.004409837486120182)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.002001936734464543 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05508864245141058) - present_state_Q ( 0.05508392341360138)) * f1( 0.08876940944336488)
w2 ( 0.058680596239155156 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05508864245141058) - present_state_Q (0.05508392341360138)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.009109760443685513 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022267248894712648) - present_state_Q ( 0.022287049850688464)) * f1( 0.5920210287217691)
w2 ( 0.053878183240706465 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.022267248894712648) - present_state_Q (0.022287049850688464)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0008108401461516656 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02736111700244347) - present_state_Q ( 0.02717905120405366)) * f1( 0.5650926577260867)
w2 ( 0.06441160687047791 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02736111700244347) - present_state_Q (0.02717905120405366)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.011621132648235577 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052107063685400384) - present_state_Q ( 0.05210178113233624)) * f1( 0.7060524058546348)
w2 ( 0.07666032088937422 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.052107063685400384) - present_state_Q (0.05210178113233624)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.02273094305760669 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06956865434153733) - present_state_Q ( 0.05446265010969373)) * f1( 0.7285397931805419)
w2 ( 0.08580997380884182 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06956865434153733) - present_state_Q (0.05446265010969373)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.008037518857063717 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08734740292350501) - present_state_Q ( 0.08734740292350501)) * f1( 0.822641798408534)
w2 ( 0.07152096079834946 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.08734740292350501) - present_state_Q (0.08734740292350501)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.01170321759883184 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06427730199877199) - present_state_Q ( 0.06419277248910099)) * f1( 0.8679300135377719)
w2 ( 0.07489975741521156 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.06427730199877199) - present_state_Q (0.06419277248910099)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.023986014023726507 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07056408175505557) - present_state_Q ( 0.07044199268426282)) * f1( 0.8990849450790218)
w2 ( 0.08582891065451098 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07056408175505557) - present_state_Q (0.07044199268426282)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.001655953352000715 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09134766795174322) - present_state_Q ( 0.07466075129379372)) * f1( 0.9657046342995687)
w2 ( 0.06989735158459381 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09134766795174322) - present_state_Q (0.07466075129379372)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04115645361405206 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05446111511756052) - present_state_Q ( 0.05446111511756052)) * f1( 0.8797144849246351)
w2 ( 0.03397615129612945 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05446111511756052) - present_state_Q (0.05446111511756052)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04071121500365371 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0061150090424393275) - present_state_Q ( -0.0061150090424393275)) * f1( 0.8090087253770244)
w2 ( 0.034416431947185085 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.0061150090424393275) - present_state_Q (-0.0061150090424393275)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.026389398461907323 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010951402625697243) - present_state_Q ( -0.0014287449570423438)) * f1( 0.7113983336579655)
w2 ( 0.05052197042164792 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0010951402625697243) - present_state_Q (-0.0014287449570423438)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.015131829264541255 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02378327049518505) - present_state_Q ( 0.02378327049518505)) * f1( 0.6303404704788796)
w2 ( 0.0648095749459946 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02378327049518505) - present_state_Q (0.02378327049518505)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0056673749443135815 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06850794599133211) - present_state_Q ( 0.055356138716116964)) * f1( 0.6247384942433942)
w2 ( 0.07995904053429623 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06850794599133211) - present_state_Q (0.055356138716116964)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0009623378106376054 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09283971980565643) - present_state_Q ( 0.09272726178023363)) * f1( 0.5687971755170816)
w2 ( 0.09394584575833607 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09283971980565643) - present_state_Q (0.09272726178023363)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0008273987269102453 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1133729324002108) - present_state_Q ( 0.1133729324002108)) * f1( 0.6628831197901704)
w2 ( 0.09370156905911331 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1133729324002108) - present_state_Q (0.1133729324002108)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.05746705488283182 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11303584551767543) - present_state_Q ( 0.11304348392310327)) * f1( 0.7270993205583531)
w2 ( -0.0025072188654469973 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.11303584551767543) - present_state_Q (0.11304348392310327)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09803951868846883 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038029273482983) - present_state_Q ( -0.038029273482983)) * f1( 0.6094032644590762)
w2 ( -0.08240005732928485 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.038029273482983) - present_state_Q (-0.038029273482983)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12980297478121935 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1536237954910869) - present_state_Q ( -0.15439237659832358)) * f1( 0.5662237896085364)
w2 ( -0.14971645768337905 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1536237954910869) - present_state_Q (-0.15439237659832358)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11861253402321803 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24542190072042108) - present_state_Q ( -0.24542190072042108)) * f1( 0.5066305422599686)
w2 ( -0.12321089240557356 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.24542190072042108) - present_state_Q (-0.24542190072042108)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.10267826256311927 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15099105532023108) - present_state_Q ( -0.17563323380134577)) * f1( 0.4419629158711904)
w2 ( -0.08715747957864128 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15099105532023108) - present_state_Q (-0.17563323380134577)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10209594204670902 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12755604548856264) - present_state_Q ( -0.12755604548856264)) * f1( 0.39344808629856987)
w2 ( -0.08567743548467065 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.12755604548856264) - present_state_Q (-0.12755604548856264)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10287375737056093 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08422663053968343) - present_state_Q ( -0.08422663053968343)) * f1( 0.3214639934843422)
w2 ( -0.08712919743552774 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08422663053968343) - present_state_Q (-0.08422663053968343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09640319915888806 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0609687481799742) - present_state_Q ( -0.0609687481799742)) * f1( 0.2538749421943146)
w2 ( -0.07693432250104867 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0609687481799742) - present_state_Q (-0.0609687481799742)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.020531283228363402 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01124490975935563) - present_state_Q ( 0.011213726038920216)) * f1( 0.2609439149187399)
w2 ( 0.044478811537895464 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.01124490975935563) - present_state_Q (0.011213726038920216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04345745392125077 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019877019711174355) - present_state_Q ( 0.020132850050240104)) * f1( 0.3192414618995858)
w2 ( 0.0013901026531480953 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.019877019711174355) - present_state_Q (0.020132850050240104)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06436277795300234 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012353868873098868) - present_state_Q ( -0.012353868873098868)) * f1( 0.303467628105539)
w2 ( -0.039942788427704566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.012353868873098868) - present_state_Q (-0.012353868873098868)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08321123362746677 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02591649488070017) - present_state_Q ( -0.02591649488070017)) * f1( 0.27854511202500026)
w2 ( -0.05347629151985196 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.02591649488070017) - present_state_Q (-0.02591649488070017)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09970514622285967 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04211994253666179) - present_state_Q ( -0.04211994253666179)) * f1( 0.24911811813205148)
w2 ( -0.07995997358853214 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.04211994253666179) - present_state_Q (-0.04211994253666179)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11424689060232254 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07076167855533769) - present_state_Q ( -0.07076167855533769)) * f1( 0.22853077564610455)
w2 ( -0.11813884294654392 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.07076167855533769) - present_state_Q (-0.07076167855533769)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12380818423013767 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0884916698508929) - present_state_Q ( -0.0884916698508929)) * f1( 0.15412554328728986)
w2 ( -0.1553602927745957 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0884916698508929) - present_state_Q (-0.0884916698508929)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12127737441048855 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0816950787019774) - present_state_Q ( -0.08034291839130282)) * f1( 0.14699190844795984)
w2 ( -0.1484733563537515 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0816950787019774) - present_state_Q (-0.08034291839130282)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11872154412044914 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07112967481608254) - present_state_Q ( -0.07112967481608254)) * f1( 0.09680562703183476)
w2 ( -0.13791268806037252 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07112967481608254) - present_state_Q (-0.07112967481608254)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11318747458402363 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07785977323336069) - present_state_Q ( -0.07935803817389504)) * f1( 0.20377904557239432)
w2 ( -0.12704980562635015 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07785977323336069) - present_state_Q (-0.07935803817389504)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1101036014587217 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06435351026989795) - present_state_Q ( -0.06435351026989795)) * f1( 0.11956789449623559)
w2 ( -0.11673307925663383 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06435351026989795) - present_state_Q (-0.06435351026989795)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1053591093494036 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04296769072970294) - present_state_Q ( -0.045045034618525055)) * f1( 0.19707274312305867)
w2 ( -0.11191811394572274 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04296769072970294) - present_state_Q (-0.045045034618525055)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10065887506626946 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041923198838770534) - present_state_Q ( -0.04311087433871607)) * f1( 0.19672956308726477)
w2 ( -0.10713974285662596 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.041923198838770534) - present_state_Q (-0.04311087433871607)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09477130212474076 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04492045359515344) - present_state_Q ( -0.045970001165216406)) * f1( 0.24381409565459364)
w2 ( -0.10231018374051194 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04492045359515344) - present_state_Q (-0.045970001165216406)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08896924945085759 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06239058927875818) - present_state_Q ( -0.06239058927875818)) * f1( 0.2265086086323742)
w2 ( -0.09206412252647665 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06239058927875818) - present_state_Q (-0.06239058927875818)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08491991630966264 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05145235300926296) - present_state_Q ( -0.05145235300926296)) * f1( 0.16440179150607978)
w2 ( -0.08221183781814319 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05145235300926296) - present_state_Q (-0.05145235300926296)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08213253552025443 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04280829438742998) - present_state_Q ( -0.04280829438742998)) * f1( 0.11685785492282151)
w2 ( -0.07267073922019571 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04280829438742998) - present_state_Q (-0.04280829438742998)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08467465419992357 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059332457674734204) - present_state_Q ( -0.06097007154602327)) * f1( 0.5653779395442227)
w2 ( -0.07357000270462472 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.059332457674734204) - present_state_Q (-0.06097007154602327)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07128673510599143 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07125510188692409) - present_state_Q ( -0.07219482982306161)) * f1( 0.5050723754978185)
w2 ( -0.06296722991924994 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07125510188692409) - present_state_Q (-0.07219482982306161)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05805598146146966 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07181078590882878) - present_state_Q ( -0.07323141940558749)) * f1( 0.497302638440766)
w2 ( -0.04700420947036766 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07181078590882878) - present_state_Q (-0.07323141940558749)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04586482475743838 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0557706348245519) - present_state_Q ( -0.0564183571813481)) * f1( 0.4860107570113798)
w2 ( -0.031953731848434086 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0557706348245519) - present_state_Q (-0.0564183571813481)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03329934109326514 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04915889953338788) - present_state_Q ( -0.04915889953338788)) * f1( 0.5144664605049823)
w2 ( -0.012414291082030154 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04915889953338788) - present_state_Q (-0.04915889953338788)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.02183238491693934 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02658097041804618) - present_state_Q ( -0.026955362441267974)) * f1( 0.5112392322707843)
w2 ( 0.005529490149926916 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02658097041804618) - present_state_Q (-0.026955362441267974)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.012635397214318542 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003886067462798376) - present_state_Q ( -0.0043167326180980105)) * f1( 0.45099162576532925)
w2 ( 0.025922302737108736 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.003886067462798376) - present_state_Q (-0.0043167326180980105)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0059919080161648175 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021280648107047908) - present_state_Q ( 0.021280648107047908)) * f1( 0.3673532815257176)
w2 ( 0.044007044407474424 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.021280648107047908) - present_state_Q (0.021280648107047908)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0005525686922305704 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050797125052401464) - present_state_Q ( 0.042008408225488056)) * f1( 0.3335558851361703)
w2 ( 0.06031417483544964 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.050797125052401464) - present_state_Q (0.042008408225488056)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.003118216651404736 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07222675629363556) - present_state_Q ( 0.07222675629363556)) * f1( 0.2719182447660695)
w2 ( 0.076513685155737 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07222675629363556) - present_state_Q (0.07222675629363556)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.007577146803278397 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09301205481185379) - present_state_Q ( 0.09301205481185379)) * f1( 0.3834347508954327)
w2 ( 0.0904683832360568 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09301205481185379) - present_state_Q (0.09301205481185379)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.011996066696832102 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11187249453423742) - present_state_Q ( 0.11193558567317508)) * f1( 0.44522376001046743)
w2 ( 0.10237858288968664 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11187249453423742) - present_state_Q (0.11193558567317508)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.016100818272103423 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12870476680696621) - present_state_Q ( 0.12870476680696621)) * f1( 0.487698800548284)
w2 ( 0.1124784680745343 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12870476680696621) - present_state_Q (0.12870476680696621)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01983823289859737 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14346474820012795) - present_state_Q ( 0.14346357666465745)) * f1( 0.5272660576465971)
w2 ( 0.12098441585317694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14346474820012795) - present_state_Q (0.14346357666465745)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.023155127453993993 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1566519111444754) - present_state_Q ( 0.15625753601351564)) * f1( 0.558327803001366)
w2 ( 0.12811333446528878 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1566519111444754) - present_state_Q (0.15625753601351564)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01991976013628476 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16850695773159285) - present_state_Q ( 0.1682976644660803)) * f1( 0.6288742368905428)
w2 ( 0.12193969822213825 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16850695773159285) - present_state_Q (0.1682976644660803)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.037965891880429814 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16038845367962193) - present_state_Q ( 0.1599903693241754)) * f1( 0.6858883522759993)
w2 ( 0.020665515347392638 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.16038845367962193) - present_state_Q (0.1599903693241754)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07826230967054496 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0030278064373142205) - present_state_Q ( 0.0030278064373142205)) * f1( 0.573430805948723)
w2 ( -0.06366148774783731 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0030278064373142205) - present_state_Q (0.0030278064373142205)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.10931097074411258 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09107347581207034) - present_state_Q ( -0.10380577336163781)) * f1( 0.5129453217365156)
w2 ( -0.12419164516979422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.09107347581207034) - present_state_Q (-0.10380577336163781)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13373253442444344 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20040322795090093) - present_state_Q ( -0.20040322795090093)) * f1( 0.4699734472892767)
w2 ( -0.18654809655109694 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20040322795090093) - present_state_Q (-0.20040322795090093)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.15365935175255063 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24179127413505772) - present_state_Q ( -0.24179127413505772)) * f1( 0.4130870458839035)
w2 ( -0.23478688187894176 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24179127413505772) - present_state_Q (-0.24179127413505772)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16949381455842497 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24395938219197155) - present_state_Q ( -0.2909167585677599)) * f1( 0.36528773581713636)
w2 ( -0.2781347998440855 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24395938219197155) - present_state_Q (-0.2909167585677599)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18198497587026094 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33026938493465197) - present_state_Q ( -0.33076677173617525)) * f1( 0.31052444025293524)
w2 ( -0.3183608165198145 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33026938493465197) - present_state_Q (-0.33076677173617525)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18376986040690743 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36229412209319195) - present_state_Q ( -0.36229412209319195)) * f1( 0.24141171744143294)
w2 ( -0.32575434553142724 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.36229412209319195) - present_state_Q (-0.36229412209319195)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.17735967599816418 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22731340687267296) - present_state_Q ( -0.2924642759789584)) * f1( 0.17337336755477573)
w2 ( -0.29617571070809195 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22731340687267296) - present_state_Q (-0.2924642759789584)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17384566153834566 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13770985462728458) - present_state_Q ( -0.13770985462728458)) * f1( 0.10847770349020552)
w2 ( -0.28321815594150973 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13770985462728458) - present_state_Q (-0.13770985462728458)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1712921862946677 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17851377741755892) - present_state_Q ( -0.23716122096679376)) * f1( 0.06089709757439553)
w2 ( -0.24967336848350669 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17851377741755892) - present_state_Q (-0.23716122096679376)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.16041681907509944 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1317926327022797) - present_state_Q ( -0.1342139855124907)) * f1( 0.49202076077659923)
w2 ( -0.24525267403866144 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1317926327022797) - present_state_Q (-0.1342139855124907)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15101833469956885 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11884196893025352) - present_state_Q ( -0.12111017846705424)) * f1( 0.44920254668300763)
w2 ( -0.24106815440718085 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.11884196893025352) - present_state_Q (-0.12111017846705424)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14783243450092676 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20288895506546797) - present_state_Q ( -0.20288895506546797)) * f1( 0.3857019251141678)
w2 ( -0.2361121508336456 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.20288895506546797) - present_state_Q (-0.20288895506546797)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1446145316323902 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1985140411659382) - present_state_Q ( -0.20058833394795328)) * f1( 0.3985664150541777)
w2 ( -0.23126793504376403 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1985140411659382) - present_state_Q (-0.20058833394795328)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.16404138602779386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24332784692757348) - present_state_Q ( -0.24343427187794026)) * f1( 0.40396994121885565)
w2 ( -0.26973981606894937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24332784692757348) - present_state_Q (-0.24343427187794026)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.179502937686188 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2714505036428742) - present_state_Q ( -0.2714505036428742)) * f1( 0.33929639425434)
w2 ( -0.30619537980666245 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2714505036428742) - present_state_Q (-0.2714505036428742)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1751108048947271 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29309300572537167) - present_state_Q ( -0.29309300572537167)) * f1( 0.2681666523151595)
w2 ( -0.29309268339443567 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.29309300572537167) - present_state_Q (-0.29309300572537167)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.16478125066665206 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21149172435273772) - present_state_Q ( -0.32872879771051194)) * f1( 0.20350608483297172)
w2 ( -0.24233472086691185 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21149172435273772) - present_state_Q (-0.32872879771051194)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1608217909145627 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11822744945442186) - present_state_Q ( -0.11822744945442186)) * f1( 0.12922320362001258)
w2 ( -0.23007853268655265 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11822744945442186) - present_state_Q (-0.11822744945442186)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.15605342111692583 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2454743501801276) - present_state_Q ( -0.24818046787845494)) * f1( 0.11255897032958061)
w2 ( -0.18771522940050844 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2454743501801276) - present_state_Q (-0.24818046787845494)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1481311468245179 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0826065267990787) - present_state_Q ( -0.0826065267990787)) * f1( 0.2887695802914336)
w2 ( -0.18222831191812502 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0826065267990787) - present_state_Q (-0.0826065267990787)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1419691551934694 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07103029959422191) - present_state_Q ( -0.07103029959422191)) * f1( 0.23347309429506588)
w2 ( -0.17694976652542904 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07103029959422191) - present_state_Q (-0.07103029959422191)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13767094595528157 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059460136370648585) - present_state_Q ( -0.059460136370648585)) * f1( 0.1695451595296244)
w2 ( -0.17187948407075737 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.059460136370648585) - present_state_Q (-0.059460136370648585)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13154604727394226 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06679325338111108) - present_state_Q ( -0.06679325338111108)) * f1( 0.23546984690211584)
w2 ( -0.16667720550989737 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06679325338111108) - present_state_Q (-0.06679325338111108)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12592426069438237 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05914208674311498) - present_state_Q ( -0.09247752784509446)) * f1( 0.1961795597506145)
w2 ( -0.15521467274306605 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05914208674311498) - present_state_Q (-0.09247752784509446)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12190120103072599 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049429387828709745) - present_state_Q ( -0.08047232237732296)) * f1( 0.14601200101321524)
w2 ( -0.14419349739928797 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.049429387828709745) - present_state_Q (-0.08047232237732296)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11915196060884667 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0704026668469619) - present_state_Q ( -0.0704026668469619)) * f1( 0.10439001240060984)
w2 ( -0.13365900139279735 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0704026668469619) - present_state_Q (-0.0704026668469619)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11574499276965619 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0436939894507265) - present_state_Q ( -0.0436939894507265)) * f1( 0.14235761699172278)
w2 ( -0.12887250958268426 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0436939894507265) - present_state_Q (-0.0436939894507265)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11338021533046024 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03748475699890583) - present_state_Q ( -0.03748475699890583)) * f1( 0.10117288706971125)
w2 ( -0.12419778395670396 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03748475699890583) - present_state_Q (-0.03748475699890583)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11080138470004458 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035097074093147176) - present_state_Q ( -0.037343659370338754)) * f1( 0.11028469598997723)
w2 ( -0.11952110491748348 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.035097074093147176) - present_state_Q (-0.037343659370338754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.111328664872137 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09710767840272061) - present_state_Q ( -0.09809198073287911)) * f1( 0.45381688055623626)
w2 ( -0.1199858564017792 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.09710767840272061) - present_state_Q (-0.09809198073287911)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09877764544452361 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11766912511922462) - present_state_Q ( -0.11766912511922462)) * f1( 0.410295150225853)
w2 ( -0.10163172364534107 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11766912511922462) - present_state_Q (-0.11766912511922462)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08883919145377651 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09532863557035509) - present_state_Q ( -0.09532863557035509)) * f1( 0.3477467116022945)
w2 ( -0.08448397732454188 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09532863557035509) - present_state_Q (-0.09532863557035509)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07950084383482005 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09471480638525576) - present_state_Q ( -0.09649148280457079)) * f1( 0.32535529051923356)
w2 ( -0.06152237715125826 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09471480638525576) - present_state_Q (-0.09649148280457079)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07268236129923883 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06984038692612446) - present_state_Q ( -0.06984038692612446)) * f1( 0.2593995762858249)
w2 ( -0.0404938692925773 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06984038692612446) - present_state_Q (-0.06984038692612446)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0679110348903925 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046722346406765335) - present_state_Q ( -0.046722346406765335)) * f1( 0.19712142969209695)
w2 ( -0.02112986035129019 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.046722346406765335) - present_state_Q (-0.046722346406765335)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0644812807395913 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027276600498474846) - present_state_Q ( -0.027276600498474846)) * f1( 0.15273971651564594)
w2 ( -0.0031659451153999985 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.027276600498474846) - present_state_Q (-0.027276600498474846)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05244565099034786 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03425431973450566) - present_state_Q ( -0.03425431973450566)) * f1( 0.5214091644240929)
w2 ( 0.0014506326398211035 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03425431973450566) - present_state_Q (-0.03425431973450566)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04225913578658057 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023570141110588937) - present_state_Q ( -0.023570141110588937)) * f1( 0.4604842100436895)
w2 ( 0.010299157719802308 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.023570141110588937) - present_state_Q (-0.023570141110588937)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.034223766463945704 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011989581165407631) - present_state_Q ( -0.011989581165407631)) * f1( 0.38120145983780535)
w2 ( 0.018730782641756982 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.011989581165407631) - present_state_Q (-0.011989581165407631)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.027584682116040184 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0036831841896906815) - present_state_Q ( -0.0036831841896906815)) * f1( 0.3265420028554343)
w2 ( 0.02686337727258585 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0036831841896906815) - present_state_Q (-0.0036831841896906815)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.021943367259009708 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002863102474168879) - present_state_Q ( 0.002863102474168879)) * f1( 0.2857472999582628)
w2 ( 0.03476030558351577 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.002863102474168879) - present_state_Q (0.002863102474168879)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01772168455373595 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009075016568117128) - present_state_Q ( 0.009075016568117128)) * f1( 0.22007131395508137)
w2 ( 0.042433604987063556 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.009075016568117128) - present_state_Q (0.009075016568117128)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.014648870462713732 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01389511488863712) - present_state_Q ( 0.014066409102478424)) * f1( 0.1640381806555833)
w2 ( 0.04992652908251897 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01389511488863712) - present_state_Q (0.014066409102478424)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.012220936269262885 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018035212987900336) - present_state_Q ( 0.018035212987900336)) * f1( 0.1321193091326386)
w2 ( 0.05727726141495455 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.018035212987900336) - present_state_Q (0.018035212987900336)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.007773781769432169 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01652645448027494) - present_state_Q ( 0.01652645448027494)) * f1( 0.5224190638948456)
w2 ( 0.06068230905366465 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.01652645448027494) - present_state_Q (0.01652645448027494)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0025575400284336683 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03188510642600263) - present_state_Q ( 0.0317253790058759)) * f1( 0.602538965621753)
w2 ( 0.07097009695186812 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03188510642600263) - present_state_Q (0.0317253790058759)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01251984570216328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07265549696550674) - present_state_Q ( 0.05848864141500861)) * f1( 0.6696137047610351)
w2 ( 0.08287224961439149 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07265549696550674) - present_state_Q (0.05848864141500861)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.02052963330012951 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09138869607245184) - present_state_Q ( 0.09138869607245184)) * f1( 0.6802357361791452)
w2 ( 0.09464726696787083 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09138869607245184) - present_state_Q (0.09138869607245184)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02068194458731082 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10862560223584171) - present_state_Q ( 0.10862560223584171)) * f1( 0.6808857744128678)
w2 ( 0.09487096276664508 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.10862560223584171) - present_state_Q (0.10862560223584171)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.03465422661608197 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12788592784220426) - present_state_Q ( 0.12788592784220426)) * f1( 0.6788903462609939)
w2 ( -0.002940717440312976 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.12788592784220426) - present_state_Q (0.12788592784220426)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07655633649665114 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024958978975393193) - present_state_Q ( -0.02496074082928726)) * f1( 0.6184492338653376)
w2 ( -0.08424493628850324 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.024958978975393193) - present_state_Q (-0.02496074082928726)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.058000928416204266 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14417237490763224) - present_state_Q ( -0.14417237490763224)) * f1( 0.5627026230978638)
w2 ( -0.044674319798478954 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14417237490763224) - present_state_Q (-0.14417237490763224)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04376866980653407 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08360056203733804) - present_state_Q ( -0.08360056203733804)) * f1( 0.5170844518892963)
w2 ( -0.011645459098446435 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08360056203733804) - present_state_Q (-0.08360056203733804)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03295023557984872 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0338724551521897) - present_state_Q ( -0.034465786236384674)) * f1( 0.4681713062979557)
w2 ( 0.016083965788093457 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0338724551521897) - present_state_Q (-0.034465786236384674)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.02494658113490472 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005763543487705768) - present_state_Q ( 0.005763543487705768)) * f1( 0.41083819947816397)
w2 ( 0.03946150309142124 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.005763543487705768) - present_state_Q (0.005763543487705768)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01901556002783453 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03864437481713866) - present_state_Q ( 0.038411175588200924)) * f1( 0.35847108961124285)
w2 ( 0.059315894518642796 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03864437481713866) - present_state_Q (0.038411175588200924)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01424441894384262 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0649941157309887) - present_state_Q ( 0.06477739215529521)) * f1( 0.33665488987468806)
w2 ( 0.07632253684877924 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0649941157309887) - present_state_Q (0.06477739215529521)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.011142389299183328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10261358051297315) - present_state_Q ( 0.1027419126598286)) * f1( 0.2885087095980712)
w2 ( 0.09137525920358486 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10261358051297315) - present_state_Q (0.1027419126598286)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.008471440403141125 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12489069632432062) - present_state_Q ( 0.10683353919389327)) * f1( 0.2527978313067037)
w2 ( 0.10405392285620951 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12489069632432062) - present_state_Q (0.10683353919389327)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.006841105373444647 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14366293627242094) - present_state_Q ( 0.1437204876154299)) * f1( 0.23077591179636073)
w2 ( 0.11394433569786322 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14366293627242094) - present_state_Q (0.1437204876154299)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.005861781999811987 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15835649022021367) - present_state_Q ( 0.15835649022021367)) * f1( 0.17037886323448775)
w2 ( 0.1219914179301163 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15835649022021367) - present_state_Q (0.15835649022021367)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.004910388862028252 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1699985283521091) - present_state_Q ( 0.1456085338269476)) * f1( 0.13326454126356937)
w2 ( 0.13055837621110788 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1699985283521091) - present_state_Q (0.1456085338269476)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0008661583694440532 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02451754150232469) - present_state_Q ( 0.02451754150232469)) * f1( 0.32464511155607867)
w2 ( 0.13411706046406605 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02451754150232469) - present_state_Q (0.02451754150232469)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.013010542591551641 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0005271987250166057) - present_state_Q ( 0.0005271987250166057)) * f1( 0.6086632001893488)
w2 ( 0.13411706046406605 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0005271987250166057) - present_state_Q (0.0005271987250166057)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02461714997809615 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03582499194594185) - present_state_Q ( 0.03582499194594185)) * f1( 0.6918681361508933)
w2 ( 0.13747221060903908 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03582499194594185) - present_state_Q (0.03582499194594185)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03647896733774462 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04677193830072077) - present_state_Q ( 0.045883321537596694)) * f1( 0.7469946534083325)
w2 ( 0.1406480880548886 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04677193830072077) - present_state_Q (0.045883321537596694)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04897402713868215 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05919513543725207) - present_state_Q ( 0.05919513543725207)) * f1( 0.8516008015975551)
w2 ( 0.14358257561701807 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05919513543725207) - present_state_Q (0.05919513543725207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06096468425411333 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07221217788925653) - present_state_Q ( 0.07221217788925653)) * f1( 0.88813735171674)
w2 ( 0.14628275641501146 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07221217788925653) - present_state_Q (0.07221217788925653)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07246695490087053 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08678470076461443) - present_state_Q ( 0.08678470076461443)) * f1( 0.9436307295847378)
w2 ( 0.1487206318012484 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08678470076461443) - present_state_Q (0.08678470076461443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08328735442392121 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10254187954283528) - present_state_Q ( 0.10254187954283528)) * f1( 1.0045648155378901)
w2 ( 0.15087487796947735 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10254187954283528) - present_state_Q (0.10254187954283528)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0931062472591512 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11455602371324052) - present_state_Q ( 0.11466953150111629)) * f1( 1.0144944150483532)
w2 ( 0.15281059938688152 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11455602371324052) - present_state_Q (0.11466953150111629)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10110070528741562 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12501847235679867) - present_state_Q ( 0.09268451852095633)) * f1( 0.6672205192705171)
w2 ( 0.15520694596117598 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12501847235679867) - present_state_Q (0.09268451852095633)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10919029138706328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13371436291823297) - present_state_Q ( 0.13371436291823297)) * f1( 1.0155515081137407)
w2 ( 0.1568000874286478 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13371436291823297) - present_state_Q (0.13371436291823297)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11656701765702127 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1425327370680003) - present_state_Q ( 0.14035135658828052)) * f1( 0.9981779306384752)
w2 ( 0.15827812577101819 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1425327370680003) - present_state_Q (0.14035135658828052)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12316699911568062 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14977823686433564) - present_state_Q ( 0.14993167447469602)) * f1( 1.014661365606004)
w2 ( 0.15957904875525294 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14977823686433564) - present_state_Q (0.14993167447469602)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12912953127530832 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1569734463328783) - present_state_Q ( 0.1569734463328783)) * f1( 1.0153501950986998)
w2 ( 0.16075352672126114 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1569734463328783) - present_state_Q (0.1569734463328783)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13450522649140775 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1633004580221094) - present_state_Q ( 0.16346915745011142)) * f1( 1.016951357361346)
w2 ( 0.16181074448830313 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1633004580221094) - present_state_Q (0.16346915745011142)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13945188027299552 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16919961567627131) - present_state_Q ( 0.16779066336192114)) * f1( 1.006864327855035)
w2 ( 0.16279333045241726 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16919961567627131) - present_state_Q (0.16779066336192114)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14380711273652041 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17476959987998203) - present_state_Q ( 0.17476959987998203)) * f1( 1.0197849861264106)
w2 ( 0.16364747765457757 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17476959987998203) - present_state_Q (0.17476959987998203)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14774703513652526 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17904284668814446) - present_state_Q ( 0.17922919839361112)) * f1( 1.0187236227397771)
w2 ( 0.16442097938008163 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17904284668814446) - present_state_Q (0.17922919839361112)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1512956446123156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18371322592442868) - present_state_Q ( 0.18357949135092214)) * f1( 1.0199547851207722)
w2 ( 0.16511681600491204 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18371322592442868) - present_state_Q (0.18357949135092214)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15450280644168646 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18726851244119036) - present_state_Q ( 0.18726851244119036)) * f1( 1.0194949738008008)
w2 ( 0.1657459827809706 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18726851244119036) - present_state_Q (0.18726851244119036)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15742330139209837 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19048450957428414) - present_state_Q ( 0.19034356312401207)) * f1( 1.017420784697185)
w2 ( 0.16632008053763894 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19048450957428414) - present_state_Q (0.19034356312401207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1601353063095108 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1924161699507923) - present_state_Q ( 0.1924161699507923)) * f1( 1.0109821890144461)
w2 ( 0.1668565894785247 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1924161699507923) - present_state_Q (0.1924161699507923)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16250347577619156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19653620732582477) - present_state_Q ( 0.19639092563764157)) * f1( 1.0180116521390419)
w2 ( 0.1673218433804235 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19653620732582477) - present_state_Q (0.19639092563764157)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16462876924606082 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19880748501152354) - present_state_Q ( 0.19901957585808874)) * f1( 1.018779483892489)
w2 ( 0.16773906683328477 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19880748501152354) - present_state_Q (0.19901957585808874)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16650984706030397 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20152750344949777) - present_state_Q ( 0.20174057075909813)) * f1( 1.0216486350636167)
w2 ( 0.1681073104250018 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20152750344949777) - present_state_Q (0.20174057075909813)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16823286603475396 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20345188955050514) - present_state_Q ( 0.20345188955050514)) * f1( 1.0199422464426275)
w2 ( 0.1684451764130927 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20345188955050514) - present_state_Q (0.20345188955050514)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16982206549852832 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2050407734005427) - present_state_Q ( 0.20488736026536308)) * f1( 1.0176271082927277)
w2 ( 0.16875751075458653 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2050407734005427) - present_state_Q (0.20488736026536308)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17130013050876855 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2060338013616811) - present_state_Q ( 0.2060338013616811)) * f1( 1.0144871263048958)
w2 ( 0.16904890233007627 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2060338013616811) - present_state_Q (0.2060338013616811)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17264731374288306 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20763300600770418) - present_state_Q ( 0.20747493142923049)) * f1( 1.0138062968628363)
w2 ( 0.16931466971350706 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20763300600770418) - present_state_Q (0.20747493142923049)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17746145031899713 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20912850324779875) - present_state_Q ( 0.14825509908774173)) * f1( 0.6625771502903319)
w2 ( 0.17076782473824784 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20912850324779875) - present_state_Q (0.14825509908774173)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16807141447787863 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21403895491530464) - present_state_Q ( 0.21403895491530464)) * f1( 1.0136589644922926)
w2 ( 0.16891512354977234 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.21403895491530464) - present_state_Q (0.21403895491530464)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1390219676084861 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20525656349880997) - present_state_Q ( 0.20525656349880997)) * f1( 1.0202421352943671)
w2 ( 0.16322050540679375 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20525656349880997) - present_state_Q (0.20525656349880997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10290242490274736 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17373053645224365) - present_state_Q ( 0.17360363951611874)) * f1( 1.0139371558294332)
w2 ( 0.15609589368937588 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17373053645224365) - present_state_Q (0.17360363951611874)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01920810553456656 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13594734850986628) - present_state_Q ( 0.13594734850986628)) * f1( 1.0177424863502416)
w2 ( 0.13964884141619827 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.13594734850986628) - present_state_Q (0.13594734850986628)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05645006421782367 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04751300627868213) - present_state_Q ( 0.04749571953330335)) * f1( 1.018629932808999)
w2 ( 0.12479395303808956 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.04751300627868213) - present_state_Q (0.04749571953330335)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.055997734307367486 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008483595931084425) - present_state_Q ( -0.008483595931084425)) * f1( 0.5924242425953372)
w2 ( 0.12494665776484908 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.008483595931084425) - present_state_Q (-0.008483595931084425)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.050272863785771595 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005544872026694427) - present_state_Q ( -0.005544872026694427)) * f1( 0.545275696549868)
w2 ( 0.12704646546132958 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.005544872026694427) - present_state_Q (-0.005544872026694427)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04069690816091502 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001207274314575242) - present_state_Q ( 0.001207274314575242)) * f1( 0.4814131711458303)
w2 ( 0.13102473452366722 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.001207274314575242) - present_state_Q (0.001207274314575242)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.032665813098903464 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009160306413003527) - present_state_Q ( 0.009160306413003527)) * f1( 0.41881905191263286)
w2 ( 0.13485984900823317 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.009160306413003527) - present_state_Q (0.009160306413003527)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.026127215522921517 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01549222900430704) - present_state_Q ( 0.01549222900430704)) * f1( 0.3514298193827893)
w2 ( 0.13858098888615564 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01549222900430704) - present_state_Q (0.01549222900430704)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.02098964080183043 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020100854753246078) - present_state_Q ( 0.02032798921057851)) * f1( 0.28277826085871693)
w2 ( 0.14221463081145055 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.020100854753246078) - present_state_Q (0.02032798921057851)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03474368015227225 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.078838720213755) - present_state_Q ( 0.05036140018486967)) * f1( 0.31084153375038154)
w2 ( 0.12451552968491078 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.078838720213755) - present_state_Q (0.05036140018486967)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.04104587312214452 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06629391017020067) - present_state_Q ( 0.06627627309057459)) * f1( 0.24272168876216063)
w2 ( 0.1089367167604975 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06629391017020067) - present_state_Q (0.06627627309057459)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03961161538279629 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08246887634917825) - present_state_Q ( 0.08246887634917825)) * f1( 0.11403088065130235)
w2 ( 0.11899895766335666 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08246887634917825) - present_state_Q (0.08246887634917825)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05758552915577679 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04653194269987533) - present_state_Q ( 0.04653194269987533)) * f1( 0.49328665137528066)
w2 ( 0.11253948087425504 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.04653194269987533) - present_state_Q (0.04653194269987533)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08329489186012454 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03361565783360791) - present_state_Q ( 0.03307042502415996)) * f1( 0.5982972459485829)
w2 ( 0.08675694931980708 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.03361565783360791) - present_state_Q (0.03307042502415996)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07286994598890732 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0063232610321446725) - present_state_Q ( 0.0071697837794807776)) * f1( 0.5388612051718242)
w2 ( 0.0983647018592311 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0063232610321446725) - present_state_Q (0.0071697837794807776)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06426890374000242 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023197681581495128) - present_state_Q ( 0.023892119186809897)) * f1( 0.48204649327002325)
w2 ( 0.10907036079751148 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.023197681581495128) - present_state_Q (0.023892119186809897)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05702853976797205 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03693315048814684) - present_state_Q ( 0.03745111366123163)) * f1( 0.4355310451616272)
w2 ( 0.11904489288076646 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03693315048814684) - present_state_Q (0.03745111366123163)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.051189824068878315 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049950901488385185) - present_state_Q ( 0.049950901488385185)) * f1( 0.3765839758032156)
w2 ( 0.12834754420039365 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.049950901488385185) - present_state_Q (0.049950901488385185)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0457526582219271 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0591874248960909) - present_state_Q ( 0.05817055810174823)) * f1( 0.36800221061788774)
w2 ( 0.1372124352636653 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0591874248960909) - present_state_Q (0.05817055810174823)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04166909766629138 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0684100047916875) - present_state_Q ( 0.06879344725719744)) * f1( 0.29580825304955805)
w2 ( 0.1454952884569836 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0684100047916875) - present_state_Q (0.06879344725719744)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03649031530163034 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07317933648795305) - present_state_Q ( 0.0449108166411178)) * f1( 0.3188765652687634)
w2 ( 0.1519915731372907 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07317933648795305) - present_state_Q (0.0449108166411178)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03222877379224026 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08058523170684381) - present_state_Q ( 0.05090173420326995)) * f1( 0.2711649644530269)
w2 ( 0.15827784469598727 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08058523170684381) - present_state_Q (0.05090173420326995)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.029510919021593342 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08752200065039865) - present_state_Q ( 0.08772899356128225)) * f1( 0.2245730260470773)
w2 ( 0.16553923708621274 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08752200065039865) - present_state_Q (0.08772899356128225)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.02757764242206066 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09436549528247883) - present_state_Q ( 0.09436549528247883)) * f1( 0.16800720321928964)
w2 ( 0.17244350034095887 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09436549528247883) - present_state_Q (0.09436549528247883)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0136844509412437 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014039686890186101) - present_state_Q ( 0.014039686890186101)) * f1( 0.7415069375780847)
w2 ( 0.17619078597693552 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.014039686890186101) - present_state_Q (0.014039686890186101)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0010794026964448863 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025439854690706542) - present_state_Q ( -0.00952678352454749)) * f1( 0.6961757958322334)
w2 ( 0.17619078597693552 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.025439854690706542) - present_state_Q (-0.00952678352454749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.014770404041575575 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.036120476446634214) - present_state_Q ( 0.036120476446634214)) * f1( 0.8174143479102896)
w2 ( 0.1795406174008961 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.036120476446634214) - present_state_Q (0.036120476446634214)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02847609041123586 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04888463344658606) - present_state_Q ( 0.04888463344658606)) * f1( 0.8785480701733477)
w2 ( 0.18266069399885757 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04888463344658606) - present_state_Q (0.04888463344658606)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0416184429108157 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06313491698126869) - present_state_Q ( 0.06256706983649775)) * f1( 0.91427336620808)
w2 ( 0.18553562243609015 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06313491698126869) - present_state_Q (0.06256706983649775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05439675201031679 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07811020820075132) - present_state_Q ( 0.07811020820075132)) * f1( 0.9852142666989953)
w2 ( 0.18812963868847662 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07811020820075132) - present_state_Q (0.07811020820075132)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06621962412335441 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09286863959978042) - present_state_Q ( 0.09286863959978042)) * f1( 1.0155516610919686)
w2 ( 0.19045800317568057 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09286863959978042) - present_state_Q (0.09286863959978042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0769142049211454 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10544033172470542) - present_state_Q ( 0.10552821429636797)) * f1( 1.0183780798213293)
w2 ( 0.19255831955320263 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10544033172470542) - present_state_Q (0.10552821429636797)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08657876788600502 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11702397511472712) - present_state_Q ( 0.11702397511472712)) * f1( 1.0207777781045726)
w2 ( 0.19445188800113755 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11702397511472712) - present_state_Q (0.11702397511472712)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0953007644752685 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12711281254999554) - present_state_Q ( 0.12722541877078475)) * f1( 1.0202852653997643)
w2 ( 0.19616160525082185 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12711281254999554) - present_state_Q (0.12722541877078475)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10317852907808246 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.136338654603606) - present_state_Q ( 0.136251686999868)) * f1( 1.018033448985408)
w2 ( 0.1977092488200317 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.136338654603606) - present_state_Q (0.136251686999868)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11028883613336349 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14453099503826486) - present_state_Q ( 0.14466598019642385)) * f1( 1.0188566494572013)
w2 ( 0.19910499120617978 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14453099503826486) - present_state_Q (0.14466598019642385)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11672584535238886 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1515560125632857) - present_state_Q ( 0.15170096594138965)) * f1( 1.0144269503838648)
w2 ( 0.20037408391247855 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1515560125632857) - present_state_Q (0.15170096594138965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11669180974644418 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15845371151854662) - present_state_Q ( 0.11636611662143792)) * f1( 0.6535938943823709)
w2 ( 0.20036366900308689 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.15845371151854662) - present_state_Q (0.11636611662143792)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09208417477180006 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15855043903091415) - present_state_Q ( 0.1584427053611075)) * f1( 1.0143811448094975)
w2 ( 0.19551191577392657 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.15855043903091415) - present_state_Q (0.1584427053611075)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06986338199388856 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13232378523876392) - present_state_Q ( 0.13244485258026287)) * f1( 1.0136646134560663)
w2 ( 0.19112766629279884 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.13232378523876392) - present_state_Q (0.13244485258026287)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0801933545070333 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10912996887930251) - present_state_Q ( 0.10912996887930251)) * f1( 1.0149012772806396)
w2 ( 0.19316332685297138 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10912996887930251) - present_state_Q (0.10912996887930251)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08952195414891093 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11978633050444432) - present_state_Q ( 0.1197119790054435)) * f1( 1.011047787354228)
w2 ( 0.1950086599338714 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11978633050444432) - present_state_Q (0.1197119790054435)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08805527792564523 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1295359360124036) - present_state_Q ( 0.127748471896881)) * f1( 0.9913405125460653)
w2 ( 0.1947127623679586 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1295359360124036) - present_state_Q (0.127748471896881)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08648170434492398 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12833393244953364) - present_state_Q ( 0.12833393244953364)) * f1( 1.01517344651873)
w2 ( 0.194402751583867 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12833393244953364) - present_state_Q (0.12833393244953364)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08504459166254645 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12681519294067345) - present_state_Q ( 0.12681519294067345)) * f1( 1.0168005278108438)
w2 ( 0.19412007811093487 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12681519294067345) - present_state_Q (0.12681519294067345)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08375922436790216 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12526097675171424) - present_state_Q ( 0.12518399111338338)) * f1( 1.0154669897630804)
w2 ( 0.19386692024217064 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12526097675171424) - present_state_Q (0.12518399111338338)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08258836763680771 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1239101342977002) - present_state_Q ( 0.1239101342977002)) * f1( 1.0164462588062337)
w2 ( 0.19363653782481205 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1239101342977002) - present_state_Q (0.1239101342977002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.778082000750749e-05 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12298400560142385) - present_state_Q ( 0.12291013443504578)) * f1( 1.0193060993805747)
w2 ( 0.17742430314731397 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.12298400560142385) - present_state_Q (0.12291013443504578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0039907008748238364 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.035462505904040875) - present_state_Q ( 0.035462505904040875)) * f1( 0.5916950827821057)
w2 ( 0.17878597804104124 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.035462505904040875) - present_state_Q (0.035462505904040875)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.015228180930284465 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.038468887682960175) - present_state_Q ( 0.038468887682960175)) * f1( 0.679502713886476)
w2 ( 0.18209353806274794 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.038468887682960175) - present_state_Q (0.038468887682960175)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.025654692649631796 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.051818418107795014) - present_state_Q ( 0.04641962671258541)) * f1( 0.6567376067975969)
w2 ( 0.18526878236471184 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.051818418107795014) - present_state_Q (0.04641962671258541)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.019897068773276644 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0630742488393955) - present_state_Q ( 0.0630742488393955)) * f1( 1.0142585889379807)
w2 ( 0.1841334458856027 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0630742488393955) - present_state_Q (0.0630742488393955)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.035038193307969485 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05707325293612339) - present_state_Q ( 0.05709904519773873)) * f1( 1.018861433893498)
w2 ( 0.1871056114875202 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05707325293612339) - present_state_Q (0.05709904519773873)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.038522536047116196 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07302510144599351) - present_state_Q ( 0.07232510921059317)) * f1( 0.9961697113289846)
w2 ( 0.18780515950620033 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.07302510144599351) - present_state_Q (0.07232510921059317)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05179564129030785 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07659944439091462) - present_state_Q ( 0.07656406312152891)) * f1( 1.0124730929600525)
w2 ( 0.19042707713255158 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07659944439091462) - present_state_Q (0.07656406312152891)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06381622581804447 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09061115888313417) - present_state_Q ( 0.0906789850889552)) * f1( 1.0154053189082983)
w2 ( 0.19279471974853873 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09061115888313417) - present_state_Q (0.0906789850889552)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07469336460750035 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10347869410892355) - present_state_Q ( 0.10356132025132325)) * f1( 1.0185869732715476)
w2 ( 0.19493045073173013 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10347869410892355) - present_state_Q (0.10356132025132325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08452254234785851 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11515991394653458) - present_state_Q ( 0.11525613513499239)) * f1( 1.0211087074391572)
w2 ( 0.19685564785692336 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11515991394653458) - present_state_Q (0.11525613513499239)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09339186313700858 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1253905190203688) - present_state_Q ( 0.1255002159166626)) * f1( 1.0190072843621714)
w2 ( 0.19859642457663085 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1253905190203688) - present_state_Q (0.1255002159166626)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09123421090025993 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1347802235689266) - present_state_Q ( 0.1346947762361624)) * f1( 1.0169568111250167)
w2 ( 0.19817208949904544 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1347802235689266) - present_state_Q (0.1346947762361624)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08930639587653227 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1320886955068306) - present_state_Q ( 0.1322080830133588)) * f1( 1.0146814906390116)
w2 ( 0.19779210522979193 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1320886955068306) - present_state_Q (0.1322080830133588)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07732785274998015 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13058284362055955) - present_state_Q ( 0.13058284362055955)) * f1( 1.0192374429760225)
w2 ( 0.19544161404462185 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.13058284362055955) - present_state_Q (0.13058284362055955)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07676690899787823 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1173540771640113) - present_state_Q ( 0.11728268439752229)) * f1( 1.0112056498118394)
w2 ( 0.19533066851099942 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1173540771640113) - present_state_Q (0.11728268439752229)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07619940610600172 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11737546941381014) - present_state_Q ( 0.11730575426920266)) * f1( 1.0191841978314542)
w2 ( 0.19521930436444299 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.11737546941381014) - present_state_Q (0.11730575426920266)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06562073195635346 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11622704403062453) - present_state_Q ( 0.1161565551591569)) * f1( 1.0119855026034732)
w2 ( 0.1931286273493211 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.11622704403062453) - present_state_Q (0.1161565551591569)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04586862546042164 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1052031307735726) - present_state_Q ( 0.1052031307735726)) * f1( 1.0145788277398555)
w2 ( 0.1892349709953968 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1052031307735726) - present_state_Q (0.1052031307735726)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03604872496351936 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0665554190786198) - present_state_Q ( 0.06609716656842914)) * f1( 0.6158931532344657)
w2 ( 0.18604613850218546 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.0665554190786198) - present_state_Q (0.06609716656842914)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04637107862750385 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06321067126998278) - present_state_Q ( 0.06321067126998278)) * f1( 0.7212860814316917)
w2 ( 0.18890834641932577 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06321067126998278) - present_state_Q (0.06321067126998278)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.056796555314450395 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07402858441977207) - present_state_Q ( 0.07402858441977207)) * f1( 0.7816707354831285)
w2 ( 0.19157583189976987 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07402858441977207) - present_state_Q (0.07402858441977207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06686332682746077 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08392868480912186) - present_state_Q ( 0.08444352093887825)) * f1( 0.8121681729382647)
w2 ( 0.19405481885061054 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08392868480912186) - present_state_Q (0.08444352093887825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07628597384986119 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09297202815523802) - present_state_Q ( 0.09297202815523802)) * f1( 0.8100264667487642)
w2 ( 0.19638132234381625 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09297202815523802) - present_state_Q (0.09297202815523802)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08511628334361175 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10104163700824534) - present_state_Q ( 0.10104163700824534)) * f1( 0.8096556866540475)
w2 ( 0.19856257287766785 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10104163700824534) - present_state_Q (0.10104163700824534)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09339332309736299 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10798209190894213) - present_state_Q ( 0.10874869304183872)) * f1( 0.8110807445340191)
w2 ( 0.20060356320064895 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10798209190894213) - present_state_Q (0.10874869304183872)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10112666466084957 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11551775897777503) - present_state_Q ( 0.1146680891033848)) * f1( 0.7982088439613504)
w2 ( 0.2025412369365368 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11551775897777503) - present_state_Q (0.1146680891033848)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10839791735766907 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12170241253227841) - present_state_Q ( 0.12262646865370783)) * f1( 0.8120333202108653)
w2 ( 0.20433211238852722 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12170241253227841) - present_state_Q (0.12262646865370783)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1152187122459969 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12843046515143666) - present_state_Q ( 0.12747939251716411)) * f1( 0.7990279901196907)
w2 ( 0.20603938546848682 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12843046515143666) - present_state_Q (0.12747939251716411)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12160218683807004 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1336084321947289) - present_state_Q ( 0.13464667738273978)) * f1( 0.8109689690815721)
w2 ( 0.2076136687852215 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1336084321947289) - present_state_Q (0.13464667738273978)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12757908658029046 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13944805955762324) - present_state_Q ( 0.14055180221967878)) * f1( 0.8143691411940251)
w2 ( 0.20908152885994316 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13944805955762324) - present_state_Q (0.14055180221967878)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13327014425954015 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14444491954664884) - present_state_Q ( 0.14189714365779316)) * f1( 0.784461157141298)
w2 ( 0.21053247582588058 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14444491954664884) - present_state_Q (0.14189714365779316)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13853033355991629 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15008749817729788) - present_state_Q ( 0.15008749817729788)) * f1( 0.8102415106704736)
w2 ( 0.21183090085868922 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15008749817729788) - present_state_Q (0.15008749817729788)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14351932766151607 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15408667633235373) - present_state_Q ( 0.15285939600502554)) * f1( 0.7976102633543277)
w2 ( 0.2130818862912534 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15408667633235373) - present_state_Q (0.15285939600502554)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14812116769330244 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.157728494639151) - present_state_Q ( 0.15904860962623088)) * f1( 0.8112651742807799)
w2 ( 0.2142163710880071 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.157728494639151) - present_state_Q (0.15904860962623088)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15252031623711532 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16279266761820593) - present_state_Q ( 0.1612462935414788)) * f1( 0.7993659594220927)
w2 ( 0.2153170305524139 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16279266761820593) - present_state_Q (0.1612462935414788)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15658045160017636 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1664641095203545) - present_state_Q ( 0.1664641095203545)) * f1( 0.8090771541414006)
w2 ( 0.21632067658104753 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1664641095203545) - present_state_Q (0.1664641095203545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16040079032481666 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16962053809622923) - present_state_Q ( 0.16962053809622923)) * f1( 0.8069743156870383)
w2 ( 0.2172675068953154 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16962053809622923) - present_state_Q (0.16962053809622923)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1640605864644918 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17284689966786115) - present_state_Q ( 0.17140540397592727)) * f1( 0.7977011979664036)
w2 ( 0.21818509261513258 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17284689966786115) - present_state_Q (0.17140540397592727)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1674915991820491 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17609240681235291) - present_state_Q ( 0.1746418067127676)) * f1( 0.7985146890724723)
w2 ( 0.21904444129450193 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17609240681235291) - present_state_Q (0.1746418067127676)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17057015174549364 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17846696115325353) - present_state_Q ( 0.17998033760130522)) * f1( 0.8130046522177992)
w2 ( 0.21980176846478233 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17846696115325353) - present_state_Q (0.17998033760130522)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17347167657280937 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1807637750924295) - present_state_Q ( 0.18230152206121616)) * f1( 0.8110514468831419)
w2 ( 0.22051726557374288 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1807637750924295) - present_state_Q (0.18230152206121616)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17619377276469747 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18317426800853923) - present_state_Q ( 0.18474137732006737)) * f1( 0.8107255719425203)
w2 ( 0.2211887865633586 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18317426800853923) - present_state_Q (0.18474137732006737)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17890788903530266 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1864011466957165) - present_state_Q ( 0.18456088727786524)) * f1( 0.7964136743504078)
w2 ( 0.22187037111119273 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1864011466957165) - present_state_Q (0.18456088727786524)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18143473804077231 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18902942940043044) - present_state_Q ( 0.18726538418263267)) * f1( 0.7986864678292546)
w2 ( 0.22250312228634095 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18902942940043044) - present_state_Q (0.18726538418263267)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18368653354769532 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1912989197477921) - present_state_Q ( 0.1912989197477921)) * f1( 0.8090969616718886)
w2 ( 0.2230597417308807 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1912989197477921) - present_state_Q (0.1912989197477921)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18592574213215152 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19289651955623394) - present_state_Q ( 0.191237817337443)) * f1( 0.7982396213775494)
w2 ( 0.2236207784232443 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19289651955623394) - present_state_Q (0.191237817337443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18801982907236114 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19498363218089454) - present_state_Q ( 0.19329180554453793)) * f1( 0.7990698230172494)
w2 ( 0.22414490957671532 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19498363218089454) - present_state_Q (0.19329180554453793)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18985021638977276 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19711185388838845) - present_state_Q ( 0.19711185388838845)) * f1( 0.8099298500821311)
w2 ( 0.22459689620672432 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19711185388838845) - present_state_Q (0.19711185388838845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19156159029620506 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19875532526234652) - present_state_Q ( 0.19875532526234652)) * f1( 0.8103016627864575)
w2 ( 0.22501930035200207 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19875532526234652) - present_state_Q (0.19875532526234652)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19320080315855304 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19966290485544386) - present_state_Q ( 0.19966290485544386)) * f1( 0.8073593696204939)
w2 ( 0.2254253680646041 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19966290485544386) - present_state_Q (0.19966290485544386)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1946598733111175 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19998414089938177) - present_state_Q ( 0.20203803091632216)) * f1( 0.8123825301833535)
w2 ( 0.22578457572807642 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19998414089938177) - present_state_Q (0.20203803091632216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1962580634488516 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20214604235627073) - present_state_Q ( 0.20014140237527595)) * f1( 0.7961809724490821)
w2 ( 0.22618603976528345 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20214604235627073) - present_state_Q (0.20014140237527595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19757925396369974 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20408507118286118) - present_state_Q ( 0.20408507118286118)) * f1( 0.8093826079721973)
w2 ( 0.22651250848399196 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20408507118286118) - present_state_Q (0.20408507118286118)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1988193891988585 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20519517611348775) - present_state_Q ( 0.20519517611348775)) * f1( 0.8092584176173965)
w2 ( 0.22681899531394917 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20519517611348775) - present_state_Q (0.20519517611348775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19995120776346662 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20470916874993034) - present_state_Q ( 0.20650642590789361)) * f1( 0.8104975450051778)
w2 ( 0.22709828513329117 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20470916874993034) - present_state_Q (0.20650642590789361)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.193588708763774 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20588138072689705) - present_state_Q ( 0.20189236936050942)) * f1( 0.7825544745843769)
w2 ( 0.22547220050753478 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20588138072689705) - present_state_Q (0.20189236936050942)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1707259473943722 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20029308053207961) - present_state_Q ( 0.20203862987699217)) * f1( 0.8107094198711552)
w2 ( 0.2198320140710591 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20029308053207961) - present_state_Q (0.20203862987699217)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14978017543684602 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18187360632565963) - present_state_Q ( 0.1803630063697986)) * f1( 0.7989213452159936)
w2 ( 0.21458850115631445 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.18187360632565963) - present_state_Q (0.1803630063697986)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14610118090240065 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16379763785616092) - present_state_Q ( 0.1624712721978118)) * f1( 0.7981935634529818)
w2 ( 0.21366667098807055 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16379763785616092) - present_state_Q (0.1624712721978118)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1424680301921085 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16098614129976002) - present_state_Q ( 0.16098614129976002)) * f1( 0.8093898103475415)
w2 ( 0.21276892044467485 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16098614129976002) - present_state_Q (0.16098614129976002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1471595819932443 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15775654858013008) - present_state_Q ( 0.15775654858013008)) * f1( 0.8086218665047307)
w2 ( 0.21392930257023252 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15775654858013008) - present_state_Q (0.15775654858013008)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14363274838614878 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16187876355917863) - present_state_Q ( 0.16033881693476543)) * f1( 0.798812791042825)
w2 ( 0.21304628375865556 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16187876355917863) - present_state_Q (0.16033881693476543)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14830985349701803 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.158529979747982) - present_state_Q ( 0.15725779060723138)) * f1( 0.7982060856154751)
w2 ( 0.21421818790600688 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.158529979747982) - present_state_Q (0.15725779060723138)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15269878029782685 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1626320869940145) - present_state_Q ( 0.1613241014984066)) * f1( 0.7988711547044136)
w2 ( 0.21531697005002678 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1626320869940145) - present_state_Q (0.1613241014984066)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1567188149908044 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16568881013001902) - present_state_Q ( 0.16706522557162357)) * f1( 0.8120682517552693)
w2 ( 0.21630704315885435 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16568881013001902) - present_state_Q (0.16706522557162357)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1605282162846059 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1698006322565399) - present_state_Q ( 0.1698006322565399)) * f1( 0.8074284101254456)
w2 ( 0.21725063177823664 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1698006322565399) - present_state_Q (0.1698006322565399)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1641755768965308 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1730334718127394) - present_state_Q ( 0.1716222536893416)) * f1( 0.7984398649671256)
w2 ( 0.2181642536480753 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1730334718127394) - present_state_Q (0.1716222536893416)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1596160593653878 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17616702925477346) - present_state_Q ( 0.1747205621045037)) * f1( 0.7984604887821085)
w2 ( 0.21702217646449476 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.17616702925477346) - present_state_Q (0.1747205621045037)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13897126672545052 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17250205036866506) - present_state_Q ( 0.17250205036866506)) * f1( 0.8088009163303557)
w2 ( 0.2119171395578588 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.17250205036866506) - present_state_Q (0.17250205036866506)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11150718244189758 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15484992643099127) - present_state_Q ( 0.15484992643099127)) * f1( 0.8092787895616335)
w2 ( 0.20512984088210096 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15484992643099127) - present_state_Q (0.15484992643099127)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.046278749528718494 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13107725876977894) - present_state_Q ( 0.13005760803686223)) * f1( 0.798438610955247)
w2 ( 0.18879084323890327 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.13107725876977894) - present_state_Q (0.13005760803686223)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.01590122216930802 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07524088804087586) - present_state_Q ( 0.07524088804087586)) * f1( 0.8099337120125756)
w2 ( 0.1734365072541675 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.07524088804087586) - present_state_Q (0.07524088804087586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.069187247106902 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023104555554315457) - present_state_Q ( 0.02292917050764594)) * f1( 0.7394482523414267)
w2 ( 0.15902413295512322 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.023104555554315457) - present_state_Q (0.02292917050764594)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11781093483262664 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015986815675535018) - present_state_Q ( -0.01736144054933674)) * f1( 0.7106261514408)
w2 ( 0.14533938813475888 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.015986815675535018) - present_state_Q (-0.01736144054933674)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16013249455169604 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046710910538854206) - present_state_Q ( -0.046710910538854206)) * f1( 0.6432237234469321)
w2 ( 0.13218018452445826 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.046710910538854206) - present_state_Q (-0.046710910538854206)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19901727413353656 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0715258010787768) - present_state_Q ( -0.0715258010787768)) * f1( 0.6117548987038552)
w2 ( 0.11946764894387624 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0715258010787768) - present_state_Q (-0.0715258010787768)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2333253806706045 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0857315457896226) - present_state_Q ( -0.0857315457896226)) * f1( 0.5508319619775399)
w2 ( 0.10701081676808945 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0857315457896226) - present_state_Q (-0.0857315457896226)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26220880214603326 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08699757345722486) - present_state_Q ( -0.08699757345722486)) * f1( 0.46458613503292784)
w2 ( 0.09457677309031949 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.08699757345722486) - present_state_Q (-0.08699757345722486)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.028261155261721366 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17452595048479286) - present_state_Q ( 0.17403704244530113)) * f1( 0.4937812158449513)
w2 ( 0.16070821520365106 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17452595048479286) - present_state_Q (0.17403704244530113)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.021233519131748684 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20821938906464255) - present_state_Q ( 0.2086056848635567)) * f1( 0.5575082289900606)
w2 ( 0.05417416568879993 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.20821938906464255) - present_state_Q (0.2086056848635567)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01776769534907839 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08107989485821361) - present_state_Q ( 0.07049606603312877)) * f1( 0.25185490440890124)
w2 ( 0.0734398349721769 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08107989485821361) - present_state_Q (0.07049606603312877)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.015577163060207604 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1137226204931017) - present_state_Q ( 0.11352599500170311)) * f1( 0.2238748963008456)
w2 ( 0.08909523769979404 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1137226204931017) - present_state_Q (0.11352599500170311)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.014487953247148138 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1572996358572354) - present_state_Q ( 0.1574596862904532)) * f1( 0.18692373944612667)
w2 ( 0.09958388761294269 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1572996358572354) - present_state_Q (0.1574596862904532)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.013922572752118816 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1772282085677046) - present_state_Q ( 0.1772282085677046)) * f1( 0.13961869569053348)
w2 ( 0.10687291782497454 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1772282085677046) - present_state_Q (0.1772282085677046)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.007501803701976086 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01654534602618661) - present_state_Q ( 0.01654534602618661)) * f1( 0.34686387529010093)
w2 ( 0.11057510159650319 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01654534602618661) - present_state_Q (0.01654534602618661)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.002058817375956779 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01987290121951812) - present_state_Q ( 0.01987290121951812)) * f1( 0.29887733521898385)
w2 ( 0.11421738937455186 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01987290121951812) - present_state_Q (0.01987290121951812)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0027434399267236348 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02229400450827326) - present_state_Q ( 0.02229400450827326)) * f1( 0.2668878614752139)
w2 ( 0.11781609729340295 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02229400450827326) - present_state_Q (0.02229400450827326)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.00870267849494364 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024481865917120312) - present_state_Q ( 0.024481865917120312)) * f1( 0.33485204085981907)
w2 ( 0.12137542370689479 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.024481865917120312) - present_state_Q (0.024481865917120312)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.015507416739552384 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02774049675215959) - present_state_Q ( 0.02765678728612862)) * f1( 0.3885818080852313)
w2 ( 0.12487776895467653 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02774049675215959) - present_state_Q (0.02765678728612862)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02334971900041139 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032082238925886505) - present_state_Q ( 0.032082238925886505)) * f1( 0.45827653014736286)
w2 ( 0.12830028865401058 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.032082238925886505) - present_state_Q (0.032082238925886505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03171400074214884 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03740137563182358) - present_state_Q ( 0.03740137563182358)) * f1( 0.5028462184411981)
w2 ( 0.13162706389263776 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03740137563182358) - present_state_Q (0.03740137563182358)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04055210059466013 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04377822800089403) - present_state_Q ( 0.04377822800089403)) * f1( 0.5503189384482535)
w2 ( 0.13483905578862165 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04377822800089403) - present_state_Q (0.04377822800089403)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.048939013753725835 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049433176652038745) - present_state_Q ( 0.04874134889050931)) * f1( 0.5369274936068811)
w2 ( 0.13796309516411553 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.049433176652038745) - present_state_Q (0.04874134889050931)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.057409059430618144 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.055161877323364514) - present_state_Q ( 0.055161877323364514)) * f1( 0.5633390658274656)
w2 ( 0.14097018137229497 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.055161877323364514) - present_state_Q (0.055161877323364514)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06629906196726257 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06404446926461065) - present_state_Q ( 0.06404446926461065)) * f1( 0.6244734427930975)
w2 ( 0.14381738092553198 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06404446926461065) - present_state_Q (0.06404446926461065)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07548557989837858 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07460802033250227) - present_state_Q ( 0.07460802033250227)) * f1( 0.6914810373943628)
w2 ( 0.14647443655954695 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07460802033250227) - present_state_Q (0.07460802033250227)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08428885294306265 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0817104044178357) - present_state_Q ( 0.081936661999917)) * f1( 0.6973752438396298)
w2 ( 0.14899912412838429 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0817104044178357) - present_state_Q (0.081936661999917)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07182737132254255 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0883198738605232) - present_state_Q ( 0.0883198738605232)) * f1( 0.6942798127099534)
w2 ( 0.14540936639889487 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.0883198738605232) - present_state_Q (0.0883198738605232)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.053139234332072606 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07880946197465505) - present_state_Q ( 0.0786552530618347)) * f1( 0.6901739388379573)
w2 ( 0.1399938802616075 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07880946197465505) - present_state_Q (0.0786552530618347)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.021678672759364623 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06449697678178348) - present_state_Q ( 0.06449697678178348)) * f1( 0.6868409224976959)
w2 ( 0.13083293467953538 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06449697678178348) - present_state_Q (0.06449697678178348)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0296305176721735 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.041256381506798584) - present_state_Q ( 0.041256381506798584)) * f1( 0.6960663477137041)
w2 ( 0.11609031981241301 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.041256381506798584) - present_state_Q (0.041256381506798584)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07254316269243208 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00517346238307544) - present_state_Q ( 0.00517346238307544)) * f1( 0.6089870510886533)
w2 ( 0.10199719748951765 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.00517346238307544) - present_state_Q (0.00517346238307544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1095762804849304 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018906490466636318) - present_state_Q ( -0.018937046603318373)) * f1( 0.5422493952738237)
w2 ( 0.08833812544065074 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.018906490466636318) - present_state_Q (-0.018937046603318373)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12686017476510658 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03354541146223456) - present_state_Q ( -0.03354541146223456)) * f1( 0.46737337974716014)
w2 ( 0.08094194284697097 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03354541146223456) - present_state_Q (-0.03354541146223456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12967122598592384 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03802990693569579) - present_state_Q ( -0.03802990693569579)) * f1( 0.42738625896961124)
w2 ( 0.07962648117181349 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.03802990693569579) - present_state_Q (-0.03802990693569579)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1286795029373272 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030667260043242748) - present_state_Q ( -0.030667260043242748)) * f1( 0.3593129927117616)
w2 ( 0.08017849185259186 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.030667260043242748) - present_state_Q (-0.030667260043242748)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12463353651883352 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02469499819853701) - present_state_Q ( -0.026082334753739604)) * f1( 0.327309572720151)
w2 ( 0.08265074855126958 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.02469499819853701) - present_state_Q (-0.026082334753739604)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12427585632589938 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015475883423902975) - present_state_Q ( -0.015475883423902975)) * f1( 0.25680113096462137)
w2 ( 0.08292931445289983 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.015475883423902975) - present_state_Q (-0.015475883423902975)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12201990700294452 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009287462613477543) - present_state_Q ( -0.009287462613477543)) * f1( 0.20819269541951607)
w2 ( 0.08509648877994243 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.009287462613477543) - present_state_Q (-0.009287462613477543)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11851617121778481 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015183335277122213) - present_state_Q ( -0.003957788704967522)) * f1( 0.1719152798604395)
w2 ( 0.08917260788698636 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0015183335277122213) - present_state_Q (-0.003957788704967522)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11578365465851231 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015299014770073435) - present_state_Q ( 0.0015299014770073435)) * f1( 0.13757295677759138)
w2 ( 0.09314506966040023 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0015299014770073435) - present_state_Q (0.0015299014770073435)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11308468169252071 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002804459303211951) - present_state_Q ( 0.002804459303211951)) * f1( 0.13667347671431176)
w2 ( 0.09709458939294241 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.002804459303211951) - present_state_Q (0.002804459303211951)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11038928738534168 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003905864058347225) - present_state_Q ( 0.003905864058347225)) * f1( 0.13718085940606467)
w2 ( 0.10102428383989216 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.003905864058347225) - present_state_Q (0.003905864058347225)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10904258092708917 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004684330657123328) - present_state_Q ( 0.004684330657123328)) * f1( 0.14059811851739554)
w2 ( 0.10293996588806394 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.004684330657123328) - present_state_Q (0.004684330657123328)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10910652084681458 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00495566658755606) - present_state_Q ( 0.00495566658755606)) * f1( 0.143359836654171)
w2 ( 0.10285076388948793 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.00495566658755606) - present_state_Q (0.00495566658755606)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11052598971729823 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005858517110546484) - present_state_Q ( 0.005858517110546484)) * f1( 0.1348373640105913)
w2 ( 0.10074531058149809 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.005858517110546484) - present_state_Q (0.005858517110546484)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03350031965321713 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13969337792456) - present_state_Q ( 0.13877668721730724)) * f1( 0.5501735525483342)
w2 ( 0.17574653002990573 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.13969337792456) - present_state_Q (0.13877668721730724)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.025992091138875725 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12738701562423138) - present_state_Q ( 0.12738701562423138)) * f1( 0.6548921871012974)
w2 ( 0.16886763118619724 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.12738701562423138) - present_state_Q (0.12738701562423138)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03230404960271178 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11897651929295254) - present_state_Q ( 0.11897651929295254)) * f1( 0.6792812662474327)
w2 ( 0.1744428991443778 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11897651929295254) - present_state_Q (0.11897651929295254)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.023659385025481688 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12876317977968282) - present_state_Q ( 0.12876317977968282)) * f1( 0.7459572589014738)
w2 ( 0.16748968743627493 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.12876317977968282) - present_state_Q (0.12876317977968282)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.029781219608351162 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1161433141826252) - present_state_Q ( 0.1162082226116001)) * f1( 0.6641935169874599)
w2 ( 0.11921405396467466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.1161433141826252) - present_state_Q (0.1162082226116001)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0769750267517415 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052729148725046165) - present_state_Q ( 0.05272468900248543)) * f1( 0.6313960147906935)
w2 ( 0.07436694751687581 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.052729148725046165) - present_state_Q (0.05272468900248543)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11688597544571615 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0007761138625260663) - present_state_Q ( 0.0007761138625260663)) * f1( 0.5695880404043833)
w2 ( 0.0323250373682994 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0007761138625260663) - present_state_Q (0.0007761138625260663)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10415908641038511 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042939812151538165) - present_state_Q ( -0.042939812151538165)) * f1( 0.5332960976269319)
w2 ( 0.046643787224482466 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.042939812151538165) - present_state_Q (-0.042939812151538165)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09576478153463416 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03647262256540058) - present_state_Q ( -0.03738914434512874)) * f1( 0.627649674481976)
w2 ( 0.05466830014979779 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.03647262256540058) - present_state_Q (-0.03738914434512874)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11758862939515924 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010453536706820726) - present_state_Q ( -0.009664970974953828)) * f1( 0.5576122060642892)
w2 ( 0.02335786953413952 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.010453536706820726) - present_state_Q (-0.009664970974953828)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.12146002171999082 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027111042527242314) - present_state_Q ( -0.029366471309659005)) * f1( 0.5278358033145802)
w2 ( 0.013089620922110388 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.027111042527242314) - present_state_Q (-0.029366471309659005)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10995509951865666 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03879612461568653) - present_state_Q ( -0.04068460771266348)) * f1( 0.4858395064316517)
w2 ( 0.04624232025726367 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03879612461568653) - present_state_Q (-0.04068460771266348)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1021901494596884 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020030193814714548) - present_state_Q ( 0.010829908492074262)) * f1( 0.40617375648925047)
w2 ( 0.06918309356399134 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.020030193814714548) - present_state_Q (0.010829908492074262)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09618170789154502 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05663377140779358) - present_state_Q ( 0.044840778121278076)) * f1( 0.3736067943669289)
w2 ( 0.0884818054463315 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05663377140779358) - present_state_Q (0.044840778121278076)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09223251371362408 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09267619317851016) - present_state_Q ( 0.09159509441473207)) * f1( 0.3356088586670813)
w2 ( 0.10495595893276816 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09267619317851016) - present_state_Q (0.09159509441473207)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08925080746770311 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12033852402628906) - present_state_Q ( 0.10114632219496997)) * f1( 0.2688946394907635)
w2 ( 0.11826246255768723 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12033852402628906) - present_state_Q (0.10114632219496997)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.08768568388254991 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14537127564308777) - present_state_Q ( 0.14537127564308777)) * f1( 0.22628559349429625)
w2 ( 0.12794568182665816 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14537127564308777) - present_state_Q (0.14537127564308777)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08671141916429966 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16307552149065657) - present_state_Q ( 0.16307552149065657)) * f1( 0.18302227177883298)
w2 ( 0.13539816611883543 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16307552149065657) - present_state_Q (0.16307552149065657)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08604471346679059 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17751438425370145) - present_state_Q ( 0.17578266727866323)) * f1( 0.1588575694004747)
w2 ( 0.1412737940793744 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17751438425370145) - present_state_Q (0.17578266727866323)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07272940266041282 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0235431571848049) - present_state_Q ( -0.0235431571848049)) * f1( 0.6019883606291683)
w2 ( 0.1456975709087009 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0235431571848049) - present_state_Q (-0.0235431571848049)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06119298820703288 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010857995019480458) - present_state_Q ( -0.010857995019480458)) * f1( 0.5499496453721266)
w2 ( 0.14989301481905154 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.010857995019480458) - present_state_Q (-0.010857995019480458)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05120937839312959 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0004992680885715431) - present_state_Q ( -0.0004992680885715431)) * f1( 0.49806149275251516)
w2 ( 0.15390200164464582 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0004992680885715431) - present_state_Q (-0.0004992680885715431)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04277024418095152 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00833066402231751) - present_state_Q ( 0.00833066402231751)) * f1( 0.4383911113754816)
w2 ( 0.1577520496922441 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.00833066402231751) - present_state_Q (0.00833066402231751)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03468360926400676 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013167779528683247) - present_state_Q ( 0.013167779528683247)) * f1( 0.42979951977811254)
w2 ( 0.1615150296607278 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.013167779528683247) - present_state_Q (0.013167779528683247)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.027941739531656296 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01948754885021284) - present_state_Q ( 0.01948754885021284)) * f1( 0.3694960632379193)
w2 ( 0.165164253781424 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01948754885021284) - present_state_Q (0.01948754885021284)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.022560112431573714 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02457922344630746) - present_state_Q ( 0.02457922344630746)) * f1( 0.302544775367328)
w2 ( 0.16872182775939046 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02457922344630746) - present_state_Q (0.02457922344630746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.018166327504048577 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028071550210516997) - present_state_Q ( 0.028071550210516997)) * f1( 0.25145332757392547)
w2 ( 0.17221653985560115 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.028071550210516997) - present_state_Q (0.028071550210516997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.005828595336303087 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01069211792506888) - present_state_Q ( -0.01069211792506888)) * f1( 0.5885679382740412)
w2 ( 0.17221653985560115 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.01069211792506888) - present_state_Q (-0.01069211792506888)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0049931544239719475 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030782418293760203) - present_state_Q ( 0.030782418293760203)) * f1( 0.6280912408789777)
w2 ( 0.17566245632631347 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.030782418293760203) - present_state_Q (0.030782418293760203)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01717270172198039 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03891629045627462) - present_state_Q ( 0.03881653338126279)) * f1( 0.7378185818393975)
w2 ( 0.17896395823960076 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03891629045627462) - present_state_Q (0.03881653338126279)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02930700014230133 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049172274127595395) - present_state_Q ( 0.049172274127595395)) * f1( 0.7791134264301596)
w2 ( 0.18207885730530404 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.049172274127595395) - present_state_Q (0.049172274127595395)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.041404422322838516 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06082311844302177) - present_state_Q ( 0.06082311844302177)) * f1( 0.8328162849643463)
w2 ( 0.18498404117332964 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06082311844302177) - present_state_Q (0.06082311844302177)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05295217401475773 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07327065287292531) - present_state_Q ( 0.07244454304823014)) * f1( 0.8561340268720856)
w2 ( 0.18768169161811088 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07327065287292531) - present_state_Q (0.07244454304823014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06413681137547736 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0857583807543047) - present_state_Q ( 0.0857583807543047)) * f1( 0.9106716263857849)
w2 ( 0.1901380407645334 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0857583807543047) - present_state_Q (0.0857583807543047)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0748176069714995 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10062160384965586) - present_state_Q ( 0.10062160384965586)) * f1( 0.9759449270139727)
w2 ( 0.1923268518952396 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10062160384965586) - present_state_Q (0.10062160384965586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08467631476041171 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11458944543678926) - present_state_Q ( 0.11468378552868469)) * f1( 1.0187229749097813)
w2 ( 0.1942623550755395 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11458944543678926) - present_state_Q (0.11468378552868469)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09357776887713223 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12496284357714985) - present_state_Q ( 0.1248868677419406)) * f1( 1.0160385105358403)
w2 ( 0.19601454340785499 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12496284357714985) - present_state_Q (0.1248868677419406)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10161975280755717 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13443325684322038) - present_state_Q ( 0.1343494235422864)) * f1( 1.016764088334302)
w2 ( 0.1975964214506957 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13443325684322038) - present_state_Q (0.1343494235422864)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10888222751728706 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14304545059269752) - present_state_Q ( 0.14295445181177777)) * f1( 1.0178647818354707)
w2 ( 0.19902342331564554 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14304545059269752) - present_state_Q (0.14295445181177777)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11543902120897356 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1506759537593101) - present_state_Q ( 0.1506759537593101)) * f1( 1.0182678259275888)
w2 ( 0.20031125614797796 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1506759537593101) - present_state_Q (0.1506759537593101)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12134798812362078 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15772257719982746) - present_state_Q ( 0.15787135311557865)) * f1( 1.0205310184735459)
w2 ( 0.20146927424006603 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15772257719982746) - present_state_Q (0.15787135311557865)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12770641787672887 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1630681989573535) - present_state_Q ( 0.12288606137481795)) * f1( 0.680622792383386)
w2 ( 0.20333768941048438 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1630681989573535) - present_state_Q (0.12288606137481795)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13241610056814296 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17104929825519996) - present_state_Q ( 0.17093348418464482)) * f1( 1.020042285018829)
w2 ( 0.20426111832330188 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17104929825519996) - present_state_Q (0.17093348418464482)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13666896158917646 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17588332960128755) - present_state_Q ( 0.17588332960128755)) * f1( 1.0197483943211159)
w2 ( 0.2050952183904787 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17588332960128755) - present_state_Q (0.17588332960128755)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13619225527271003 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057070760699664914) - present_state_Q ( 0.054399221631472545)) * f1( 0.0979020971389048)
w2 ( 0.20412137547924858 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.057070760699664914) - present_state_Q (0.054399221631472545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11119699339058872 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29470760801361673) - present_state_Q ( 0.296017271702565)) * f1( 0.3749965152218186)
w2 ( 0.12413579417110417 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.29470760801361673) - present_state_Q (0.296017271702565)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.07716585751096242 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1923144150842654) - present_state_Q ( 0.19230587810823446)) * f1( 0.38978504527243646)
w2 ( 0.01936686177912718 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.1923144150842654) - present_state_Q (0.19230587810823446)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.042797761125799805 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05847727265957611) - present_state_Q ( 0.05847727265957611)) * f1( 0.45664027668736284)
w2 ( -0.07094868366810705 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.05847727265957611) - present_state_Q (0.05847727265957611)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.011181285972981077 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06335944077875055) - present_state_Q ( -0.06407058101555221)) * f1( 0.49226498844763)
w2 ( -0.1480205272355858 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06335944077875055) - present_state_Q (-0.06407058101555221)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01963809829381101 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14167301240582983) - present_state_Q ( -0.17127711785294703)) * f1( 0.5676909476328907)
w2 ( -0.21316734924210212 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.14167301240582983) - present_state_Q (-0.17127711785294703)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.047673510516633225 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22527782467670343) - present_state_Q ( -0.2679112945251239)) * f1( 0.6166826977548001)
w2 ( -0.2677213277952077 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.22527782467670343) - present_state_Q (-0.2679112945251239)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07372816319804423 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.300777826750882) - present_state_Q ( -0.35432209230992356)) * f1( 0.6933934295470214)
w2 ( -0.3128120106390274 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.300777826750882) - present_state_Q (-0.35432209230992356)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09038797466329884 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4232606915180262) - present_state_Q ( -0.48582309364583165)) * f1( 0.6494977858401805)
w2 ( -0.34872242720986335 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4232606915180262) - present_state_Q (-0.48582309364583165)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11004650891817092 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40114259305437) - present_state_Q ( -0.40114259305437)) * f1( 0.5799462377575693)
w2 ( -0.38261959383497 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.40114259305437) - present_state_Q (-0.40114259305437)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12213421347367955 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44142899073589825) - present_state_Q ( -0.5179529095028923)) * f1( 0.5344049300524206)
w2 ( -0.4097623925834537 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.44142899073589825) - present_state_Q (-0.5179529095028923)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.13531265971450845 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46737759165724213) - present_state_Q ( -0.46737759165724213)) * f1( 0.4717367675700858)
w2 ( -0.43769840933430193 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.46737759165724213) - present_state_Q (-0.46737759165724213)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.14851124597503482 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31724485066105557) - present_state_Q ( -0.404784532527916)) * f1( 0.403700623250829)
w2 ( -0.4638536055373571 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.31724485066105557) - present_state_Q (-0.404784532527916)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.15940034465064554 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.330518363094981) - present_state_Q ( -0.42328908420245237)) * f1( 0.35153027927152863)
w2 ( -0.48863462570592076 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.330518363094981) - present_state_Q (-0.42328908420245237)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.16814112095603223 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3402424090612364) - present_state_Q ( -0.4379693342024205)) * f1( 0.2952417307555258)
w2 ( -0.512319018242217 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3402424090612364) - present_state_Q (-0.4379693342024205)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17283037101298904 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4524704812421898) - present_state_Q ( -0.5534218418047695)) * f1( 0.2444543210420284)
w2 ( -0.531501538874162 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4524704812421898) - present_state_Q (-0.5534218418047695)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.17077390335225442 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4557223742376256) - present_state_Q ( -0.562022682012458)) * f1( 0.17659594757221353)
w2 ( -0.5198564944152925 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4557223742376256) - present_state_Q (-0.562022682012458)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1677715409707126 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22489736651344258) - present_state_Q ( -0.22489736651344258)) * f1( 0.09928196530328796)
w2 ( -0.5077601892208086 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22489736651344258) - present_state_Q (-0.22489736651344258)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16531599603068545 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21360693125366953) - present_state_Q ( -0.21360693125366953)) * f1( 0.06260212849317232)
w2 ( -0.4920703396956765 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21360693125366953) - present_state_Q (-0.21360693125366953)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16245059968410913 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11032476152285264) - present_state_Q ( -0.20873882946198793)) * f1( 0.07204804053871779)
w2 ( -0.4761620855632884 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11032476152285264) - present_state_Q (-0.20873882946198793)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17579819320938334 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33935025872554436) - present_state_Q ( -0.3408604346331182)) * f1( 0.33956897298262895)
w2 ( -0.4997465610376545 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33935025872554436) - present_state_Q (-0.3408604346331182)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17271133711962616 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3478454175702257) - present_state_Q ( -0.3478454175702257)) * f1( 0.27302601961594614)
w2 ( -0.4929629084888623 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3478454175702257) - present_state_Q (-0.3478454175702257)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17850685169775934 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42366438340081164) - present_state_Q ( -0.42601033856227744)) * f1( 0.1831959169494042)
w2 ( -0.5182713964710866 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42366438340081164) - present_state_Q (-0.42601033856227744)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18277660935275247 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4396639411182215) - present_state_Q ( -0.4396639411182215)) * f1( 0.1403129555147859)
w2 ( -0.5426155927105746 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4396639411182215) - present_state_Q (-0.4396639411182215)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1847825392231982 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4463852846551267) - present_state_Q ( -0.4463852846551267)) * f1( 0.06725592804351874)
w2 ( -0.5664758522154054 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4463852846551267) - present_state_Q (-0.4463852846551267)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18975625838736243 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17731376572266008) - present_state_Q ( -0.17731376572266008)) * f1( 0.5052695943850775)
w2 ( -0.4869704377802805 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.17731376572266008) - present_state_Q (-0.17731376572266008)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1941679133775682 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6739559911293398) - present_state_Q ( -0.6739559911293398)) * f1( 0.47213971520303644)
w2 ( -0.4981831907383118 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6739559911293398) - present_state_Q (-0.6739559911293398)) * f2(1.2000000000000002)
============================================================================
