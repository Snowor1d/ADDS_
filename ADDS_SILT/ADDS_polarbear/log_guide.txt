GUIDE learning . . .
w1 ( 0.9278724583989048 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5148153000446647) - present_state_Q ( 0.4951597366792507)) * f1( 0.29515973667925066)
w2 ( 0.9511264358665044 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5148153000446647) - present_state_Q (0.4951597366792507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8655643787039169 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4883147424090228) - present_state_Q ( 0.4883147424090228)) * f1( 0.32126123858670375)
w2 ( 0.912336770503142 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.4883147424090228) - present_state_Q (0.4883147424090228)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.767920878341903 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6519159061255677) - present_state_Q ( 0.6062990676004105)) * f1( 0.38425568869584864)
w2 ( 0.8361035461935064 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6519159061255677) - present_state_Q (0.6062990676004105)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.6544783040279821 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6692047016516978) - present_state_Q ( 0.6692047016516978)) * f1( 0.43593460292044284)
w2 ( 0.7320121769340452 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6692047016516978) - present_state_Q (0.6692047016516978)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5261219308588576 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6713289453415218) - present_state_Q ( 0.6216583605524848)) * f1( 0.5024666024143812)
w2 ( 0.6298311582933118 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6713289453415218) - present_state_Q (0.6216583605524848)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.3798151233003034 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.593098318247266) - present_state_Q ( 0.5878496065444965)) * f1( 0.5786217366296676)
w2 ( 0.5160468684309222 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.593098318247266) - present_state_Q (0.5878496065444965)) * f2(0.45)
============================================================================
