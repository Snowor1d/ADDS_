NOT GUIDE learning . . .
w3 ( 0.9358649589932977 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 1.224800047459389) - present_state_Q (1.224800047459389)) * f3(1.064800047459389)
w4 ( 0.9903628793165848 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 1.224800047459389) - present_state_Q (1.224800047459389)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.8841063497953531 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.34804961687029173) - present_state_Q (0.34804961687029173)) * f3(0.22374896266147035)
w4 ( 0.9579774541440191 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.34804961687029173) - present_state_Q (0.34804961687029173)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.8880058663196544 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1039900438581551) - present_state_Q (0.1039900438581551)) * f3(0.09595055481154582)
w4 ( 0.9587902720650745 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.1039900438581551) - present_state_Q (0.1039900438581551)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.89792314137345 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11767325332013057) - present_state_Q (0.11767325332013057)) * f3(0.11091981665284752)
w4 ( 0.9605784602090983 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11767325332013057) - present_state_Q (0.11767325332013057)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.910746570244874 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.15270191210623826) - present_state_Q (0.15270191210623826)) * f3(0.14866566719493546)
w4 ( 0.962303596767307 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.15270191210623826) - present_state_Q (0.15270191210623826)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.9226345960473796 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14358334449769225) - present_state_Q (0.14358334449769225)) * f3(0.13652236157081032)
w4 ( 0.9640451467472112 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14358334449769225) - present_state_Q (0.14358334449769225)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.9226345960473796 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.019280902934944225) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.9640451467472112 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.019280902934944225) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.6291179208745902 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 1.068602958780979) - present_state_Q (1.068602958780979)) * f3(0.9910269344099805)
w4 ( 0.9166572641407651 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 1.068602958780979) - present_state_Q (1.068602958780979)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.6072817963261704 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.792060125948896) - present_state_Q (0.792060125948896)) * f3(1.025872801062725)
w4 ( 0.913251598327101 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.792060125948896) - present_state_Q (0.792060125948896)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.38621292827227294 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.731577979273641) - present_state_Q (0.731577979273641)) * f3(1.0242160908447846)
w4 ( 0.8873505561509457 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.731577979273641) - present_state_Q (0.731577979273641)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.42356702622233 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3718899804892304) - present_state_Q (0.2325298005535489)) * f3(0.46422259344487016)
w4 ( 0.892178511335918 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3718899804892304) - present_state_Q (0.2325298005535489)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.45689727518899315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3622500869333046) - present_state_Q (0.22826125169482478)) * f3(0.4125215850087296)
w4 ( 0.897026293877909 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3622500869333046) - present_state_Q (0.22826125169482478)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.45689727518899315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4147220420658691) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.897026293877909 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4147220420658691) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.44996293474905075 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15959777736265154) - present_state_Q (0.0909796874496879)) * f3(0.1205930494371635)
w4 ( 0.8947262142390553 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15959777736265154) - present_state_Q (0.0909796874496879)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.4617707930631096 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07486397348689729) - present_state_Q (0.07486397348689729)) * f3(0.12660920445344828)
w4 ( 0.8965914590867788 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07486397348689729) - present_state_Q (0.07486397348689729)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.4749423224066835 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08370972997205448) - present_state_Q (0.08370972997205448)) * f3(0.1424470793269273)
w4 ( 0.8984407815728291 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08370972997205448) - present_state_Q (0.08370972997205448)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.48311522164083043 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05896058158698611) - present_state_Q (0.05896058158698611)) * f3(0.08630893483615282)
w4 ( 0.9003346525259726 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05896058158698611) - present_state_Q (0.05896058158698611)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.46983673147280836 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6904520907411824) - present_state_Q (0.6904520907411824)) * f3(1.0937180813552128)
w4 ( 0.8981493286559654 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6904520907411824) - present_state_Q (0.6904520907411824)) * f4(0.18)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.18089409880110902 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6811925385193566) - present_state_Q (0.6811925385193566)) * f3(1.105757861316448)
w4 ( 0.8511140095319518 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6811925385193566) - present_state_Q (0.6811925385193566)) * f4(0.18)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.03864439326564778 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.30033945546488927) - present_state_Q (0.3261581419392576)) * f3(0.9561263820644097)
w4 ( 0.809783773996882 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.30033945546488927) - present_state_Q (0.3261581419392576)) * f4(0.18)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.029507026115707616 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0690910963270851) - present_state_Q (0.0690910963270851)) * f3(0.7267019657804938)
w4 ( 0.8210375901565494 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0690910963270851) - present_state_Q (0.0690910963270851)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.037048767172767845 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.18834935511995535) - present_state_Q (0.01864567421471483)) * f3(0.07540313967456846)
w4 ( 0.823037968679144 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.18834935511995535) - present_state_Q (0.01864567421471483)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.037048767172767845 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.17475261013333124) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.823037968679144 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.17475261013333124) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.037048767172767845 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06482228184620288) - present_state_Q (0.032921518747165764)) * f3(0.0)
w4 ( 0.8209322115168939 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06482228184620288) - present_state_Q (0.032921518747165764)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05043495957327816 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.060384337655888035) - present_state_Q (0.060384337655888035)) * f3(0.3003718021973529)
w4 ( 0.823606136093552 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.060384337655888035) - present_state_Q (0.060384337655888035)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0776432727785227 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.063977280765493) - present_state_Q (0.063977280765493)) * f3(0.2887067368166316)
w4 ( 0.8292606587774184 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.063977280765493) - present_state_Q (0.063977280765493)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0776432727785227 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01658521317554837) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8292606587774184 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.01658521317554837) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.04849090063648198 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16174161980311919) - present_state_Q (0.16174161980311919)) * f3(0.5878825806903227)
w4 ( 0.799222714367899 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16174161980311919) - present_state_Q (0.16174161980311919)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.08849224690197166 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07950324276350423) - present_state_Q (0.006277830618669445)) * f3(0.2001741263057875)
w4 ( 0.7952260593552144 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07950324276350423) - present_state_Q (0.006277830618669445)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.0894016890692616 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02096428680711932) - present_state_Q (0.02096428680711932)) * f3(0.4820060449877387)
w4 ( 0.7950751164902031 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.02096428680711932) - present_state_Q (0.02096428680711932)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.04988916384077775 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.011994158029389468) - present_state_Q (0.011994158029389468)) * f3(0.3994370725183622)
w4 ( 0.8010103480368445 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.011994158029389468) - present_state_Q (0.011994158029389468)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.027082174578341823 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02044886708064204) - present_state_Q (0.02044886708064204)) * f3(0.23234598354517194)
w4 ( 0.8049367321153541 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02044886708064204) - present_state_Q (0.02044886708064204)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.014170701172253813 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.028608350627656987) - present_state_Q (0.028608350627656987)) * f3(0.1325269743969331)
w4 ( 0.8088337420530946 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.028608350627656987) - present_state_Q (0.028608350627656987)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.014170701172253813 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.031351467792942116) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.8088337420530946 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.031351467792942116) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.008587031615534119 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015374327720919127) - present_state_Q (0.015374327720919127)) * f3(0.05662014253139143)
w4 ( 0.8108060682631969 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015374327720919127) - present_state_Q (0.015374327720919127)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.008647318993890067 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.015853287670935887) - present_state_Q (0.015853287670935887)) * f3(0.04225368096603665)
w4 ( 0.8107775323453892 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.015853287670935887) - present_state_Q (0.015853287670935887)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18799628708948732 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2582413356564474) - present_state_Q (0.2582413356564474)) * f3(1.189918979387999)
w4 ( 0.7846123147356023 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2582413356564474) - present_state_Q (0.2582413356564474)) * f4(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18791710727515742 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.004596695560973869) - present_state_Q (-0.004596695560973869)) * f3(0.19139307859452942)
w4 ( 0.7846288628396219 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.004596695560973869) - present_state_Q (-0.004596695560973869)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.07616780451688936 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.009239998993012106) - present_state_Q (-0.05722318260621853)) * f3(1.0560846790109477)
w4 ( 0.8036755121247212 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.009239998993012106) - present_state_Q (-0.05722318260621853)) * f4(0.18)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.06538995431430608 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08382383044045293) - present_state_Q (0.00786847500265726)) * f3(0.10772314223679362)
w4 ( 0.805676539940804 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08382383044045293) - present_state_Q (0.00786847500265726)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.02537060693927358 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0658227420600592) - present_state_Q (0.0658227420600592)) * f3(0.9647583484649649)
w4 ( 0.8207286924551391 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0658227420600592) - present_state_Q (0.0658227420600592)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1163936808130983 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.13705769353494937) - present_state_Q (0.13705769353494937)) * f3(0.8732812992712835)
w4 ( 0.7980017655165987 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.13705769353494937) - present_state_Q (0.13705769353494937)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.13854464446609055 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01913898730112562) - present_state_Q (0.01913898730112562)) * f3(0.10980908267745083)
w4 ( 0.7899328651623146 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01913898730112562) - present_state_Q (0.01913898730112562)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.13854464446609055 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.023692075137118976) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7899328651623146 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.023692075137118976) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.05807439428842137 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015798657303246293) - present_state_Q (-0.014888962219243199)) * f3(0.7916647118435954)
w4 ( 0.8021304910977095 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015798657303246293) - present_state_Q (-0.014888962219243199)) * f4(0.12)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0031789553666877607 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.027688807899211257) - present_state_Q (0.027688807899211257)) * f3(0.6281878930570105)
w4 ( 0.8099311316808352 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.027688807899211257) - present_state_Q (0.027688807899211257)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05874913505343659 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06667381319084155) - present_state_Q (0.06667381319084155)) * f3(0.5911761694008467)
w4 ( 0.8174510802258611 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06667381319084155) - present_state_Q (0.06667381319084155)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0541038000317732 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.09641614891955089) - present_state_Q (0.01905267154729119)) * f3(0.0460202510269267)
w4 ( 0.8154322581125504 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.09641614891955089) - present_state_Q (0.01905267154729119)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0541038000317732 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09053622554949863) - present_state_Q (0.016308645162251008)) * f3(0.0)
w4 ( 0.8114177480673358 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09053622554949863) - present_state_Q (0.016308645162251008)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05355986097858005 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.04052547989210215) - present_state_Q (0.04052547989210215)) * f3(0.1491349954101231)
w4 ( 0.8112718563397242 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.04052547989210215) - present_state_Q (0.04052547989210215)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.05355986097858005 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.048676311380383454) - present_state_Q (0.048676311380383454)) * f3(0.0)
w4 ( 0.8140090042582702 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.048676311380383454) - present_state_Q (0.048676311380383454)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.06054430922446807 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.024101194020106333) - present_state_Q (0.024101194020106333)) * f3(0.1460237908023837)
w4 ( 0.814965622109034 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.024101194020106333) - present_state_Q (0.024101194020106333)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.06054430922446807 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02421270939751017) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.814965622109034 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.02421270939751017) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.06054430922446807 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01629931244218068) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.814965622109034 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01629931244218068) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.0929886517238881 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.21236576501738275) - present_state_Q (0.1896311805010201)) * f3(0.9784021276707382)
w4 ( 0.8202713084450455 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.21236576501738275) - present_state_Q (0.1896311805010201)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.07367826504693764 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.2224002971704398) - present_state_Q (0.2224002971704398)) * f3(0.9803012101939639)
w4 ( 0.7930687441657911 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.2224002971704398) - present_state_Q (0.2224002971704398)) * f4(0.16)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.04519066021337252 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.028156263634073544) - present_state_Q (0.0260946977522924)) * f3(0.29166575629821034)
w4 ( 0.7989290697374578 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.028156263634073544) - present_state_Q (0.0260946977522924)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.04519066021337252 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01600794703109922) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7989290697374578 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.01600794703109922) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.02662887297773524 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015914114373716485) - present_state_Q (0.007540189076031333)) * f3(0.18672867975097185)
w4 ( 0.8009171721821805 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015914114373716485) - present_state_Q (0.007540189076031333)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.004239301784100695 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02363816678234363) - present_state_Q (0.02363816678234363)) * f3(0.3153914967398622)
w4 ( 0.8048320747817641 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02363816678234363) - present_state_Q (0.02363816678234363)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.023392707718552164 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05043773753512267) - present_state_Q (0.05043773753512267)) * f3(0.5066431119086865)
w4 ( 0.8015597109990744 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05043773753512267) - present_state_Q (0.05043773753512267)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.002125165533417489 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03834291341409309) - present_state_Q (0.02595801547058297)) * f3(0.26095196173202245)
w4 ( 0.8054712161025577 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03834291341409309) - present_state_Q (0.02595801547058297)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.029973678411900483 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.049234342204103373) - present_state_Q (0.03282766416979072)) * f3(0.28647910768126117)
w4 ( 0.8093595991827601 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.049234342204103373) - present_state_Q (0.03282766416979072)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( 0.00759343303975114 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06065729325349461) - present_state_Q (0.06065729325349461)) * f3(0.4035446412785501)
w4 ( 0.8060320497991913 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06065729325349461) - present_state_Q (0.06065729325349461)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.07176611041561488 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.034486294267979126) - present_state_Q (0.05130456835590967)) * f3(0.38752503018774664)
w4 ( 0.7937449141656167 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.034486294267979126) - present_state_Q (0.05130456835590967)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1311739199198468 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.010533024033222833) - present_state_Q (0.010533024033222833)) * f3(0.295637765660287)
w4 ( 0.785706995279097 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.010533024033222833) - present_state_Q (0.010533024033222833)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.13420994066475944 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.007860325431979499) - present_state_Q (0.007860325431979499)) * f3(0.05987329248376108)
w4 ( 0.7846928466933195 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.007860325431979499) - present_state_Q (0.007860325431979499)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.12947723458943283 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.009288548502097656) - present_state_Q (0.009288548502097656)) * f3(0.047726035791703654)
w4 ( 0.7866761273060158 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.009288548502097656) - present_state_Q (0.009288548502097656)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.14916567496961522 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01882810630060902) - present_state_Q (0.01882810630060902)) * f3(0.09761514316944739)
w4 ( 0.7786083461233336 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01882810630060902) - present_state_Q (0.01882810630060902)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1657955165974021 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.003186921027491494) - present_state_Q (0.003186921027491494)) * f3(0.08303013342377884)
w4 ( 0.7746026096654841 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.003186921027491494) - present_state_Q (0.003186921027491494)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.17491183396305865 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.003927173398568219) - present_state_Q (-0.015745031136622803)) * f3(0.18840728610161916)
w4 ( 0.7736348851624371 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.003927173398568219) - present_state_Q (-0.015745031136622803)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.18602279817406953 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.008603948140120818)) * f3(0.22611016447847157)
w4 ( 0.7716693009549975 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.008603948140120818)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1939609514300913 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0014078764936220657) - present_state_Q (0.0014078764936220657)) * f3(0.1583617483111499)
w4 ( 0.7696642325996205 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0014078764936220657) - present_state_Q (0.0014078764936220657)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.1939609514300913 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7696642325996205 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20094823524377198 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.01240129743539573)) * f3(0.14329988527307286)
w4 ( 0.7686890351944913 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.01240129743539573)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20584004121334368 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.004463292133196633)) * f3(0.0987173279378241)
w4 ( 0.7676979617787577 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.004463292133196633)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.20997461470606546 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.0017262248018655845)) * f3(0.08297794703479446)
w4 ( 0.7667014142283615 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.0017262248018655845)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2208559424873216 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.003930183885238523) - present_state_Q (0.003930183885238523)) * f3(0.05431058614057925)
w4 ( 0.762694339897368 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.003930183885238523) - present_state_Q (0.003930183885238523)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2208559424873216 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.01525388679794736) - present_state_Q (0.01525388679794736)) * f3(0.0)
w4 ( 0.7626668829011317 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.01525388679794736) - present_state_Q (0.01525388679794736)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2208559424873216 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7626668829011317 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.2208559424873216 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7626668829011317 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.28121330704342595 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.02161976780916129)) * f3(0.30508475354742687)
w4 ( 0.7507966015079867 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.02161976780916129)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.28121330704342595 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7507966015079867 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.31933815818863287 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.024231660965707908)) * f3(0.1929621524547834)
w4 ( 0.7428935281518495 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.024231660965707908)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.31151287986379755 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.01848100907019097)) * f3(0.15092699998537337)
w4 ( 0.7449674521881303 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.01848100907019097)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.32990462606460685 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.00861709579467003)) * f3(0.12332008197860628)
w4 ( 0.739001920571309 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.00861709579467003)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3199070091444932 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.003313593898110999)) * f3(0.09964598288029324)
w4 ( 0.7430151749469014 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.003313593898110999)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.010150298763169384)) * f3(0.17108474555261427)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.010150298763169384)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.013935577031363083) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.013935577031363083) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.00937312045124664) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.00937312045124664) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7460760767394804 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.014921521534789609) - present_state_Q (0.014921521534789609)) * f3(0.0)
w4 ( 0.7450492180007178 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.014921521534789609) - present_state_Q (0.014921521534789609)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7450492180007178 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7450492180007178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7450492180007178 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3111791157387445 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7450492180007178 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.3946289131840673 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10740563735871062)) * f3(0.4409280672740718)
w4 ( 0.7374788405501527 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.10740563735871062)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4323786434371181 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.09097713181159399)) * f3(0.267914248273245)
w4 ( 0.7346607948137759 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.09097713181159399)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4323786434371181 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.10796541563431435) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7346607948137759 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.10796541563431435) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.48877702307996174 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.11464847194926046)) * f3(0.2991398622683048)
w4 ( 0.7308900917576744 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.11464847194926046)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.507084014301794 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.08199160996448455) - present_state_Q (-0.08199160996448455)) * f3(0.19765538729882803)
w4 ( 0.7290376766556105 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.08199160996448455) - present_state_Q (-0.08199160996448455)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.48392286411212576 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09372912540423269) - present_state_Q (-0.09372912540423269)) * f3(0.21359355823211507)
w4 ( 0.7312063890813381 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09372912540423269) - present_state_Q (-0.09372912540423269)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.48392286411212576 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7312063890813381 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.48392286411212576 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7312063890813381 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5818815382742093 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2228756475367866)) * f3(0.5512201440844938)
w4 ( 0.7205436429665588 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.2228756475367866)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.01420598478984618) - present_state_Q (-0.01420598478984618)) * f3(0.08887813135407208)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.01420598478984618) - present_state_Q (-0.01420598478984618)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.21587113072480485) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.21587113072480485) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1714888306480415) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1714888306480415) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.23456560870076631) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.23456560870076631) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.18810662315243346) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.18810662315243346) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1729159233218304) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1729159233218304) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.167858557059355) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.167858557059355) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1675113138188264) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1675113138188264) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1399862309535981) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1399862309535981) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.04888223600030109) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.04888223600030109) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.008372253109668965) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.008372253109668965) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4797755758201596 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.05627730022573338) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7349412333029178 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.05627730022573338) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4521515125257923 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.663519178915869)) * f3(1.689346991436399)
w4 ( 0.7382116168812352 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.663519178915869)) * f4(0.2)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4521515125257923 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7382116168812352 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4521515125257923 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.01818750412647386) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7382116168812352 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.01818750412647386) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4521515125257923 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7382116168812352 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5061703506236229 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.08312678576478502)) * f3(0.2818070475223517)
w4 ( 0.7267103775958239 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.08312678576478502)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5061703506236229 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7267103775958239 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5061703506236229 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7267103775958239 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5061703506236229 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7267103775958239 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5061703506236229 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7267103775958239 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5061703506236229 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7267103775958239 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4959172028014891 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.038921138624629245) - present_state_Q (-0.07615549268390219)) * f3(0.17916833754502845)
w4 ( 0.7278549043534668 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.038921138624629245) - present_state_Q (-0.07615549268390219)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.47066741368413534 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.018102342335550227) - present_state_Q (-0.08634377968074712)) * f3(0.2328170412372295)
w4 ( 0.7321930385352555 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.018102342335550227) - present_state_Q (-0.08634377968074712)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.046295102865985385)) * f3(0.1916994517887992)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.046295102865985385)) * f4(0.06)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.07034944571145195) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.07034944571145195) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.04532070934354516) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.04532070934354516) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04653735667070883) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04653735667070883) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05374374105985169) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05374374105985169) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.060951072589502955) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.060951072589502955) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06790449215961687) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06790449215961687) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.07349828958698769) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.07349828958698769) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.08052373840185481) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.08052373840185481) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.08767702423839127) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.08767702423839127) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.0948595343717633) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.0948595343717633) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1020532631579567) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1020532631579567) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10925246873379886) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10925246873379886) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1164547550381832) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1164547550381832) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.07547442497664707) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.07547442497664707) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4506099939212639 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7384708091524514 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4404634006272581 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015874070564725867) - present_state_Q (-0.029570600655668744)) * f3(0.09839998543499993)
w4 ( 0.7405331251678757 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.015874070564725867) - present_state_Q (-0.029570600655668744)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.42866005673834906 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.03556620479604265) - present_state_Q (-0.03556620479604265)) * f3(0.11437242510424052)
w4 ( 0.7425971443365086 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.03556620479604265) - present_state_Q (-0.03556620479604265)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4542496103100967 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.041025745252652734) - present_state_Q (-0.041025745252652734)) * f3(0.1303543151758836)
w4 ( 0.7386709906779634 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.041025745252652734) - present_state_Q (-0.041025745252652734)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.46081006859182094 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.05170215302325305)) * f3(0.14634150768215806)
w4 ( 0.7377743949840099 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.05170215302325305)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.46081006859182094 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7377743949840099 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4271086976077827 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1103537403727939)) * f3(0.30351922777981766)
w4 ( 0.7422158099455011 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.1103537403727939)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.38984326165637095 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.11327984005878419)) * f3(0.3347355679183413)
w4 ( 0.7466689293057363 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.11327984005878419)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4018034899875221 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.08156238413899873)) * f3(0.2858306203313266)
w4 ( 0.7449951788422923 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.08156238413899873)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.415091633825395 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.01194042753359255) - present_state_Q (-0.01194042753359255)) * f3(0.0667996465418255)
w4 ( 0.7410166716118527 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.01194042753359255) - present_state_Q (-0.01194042753359255)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4253524317019644 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.00653836629351171) - present_state_Q (-0.00653836629351171)) * f3(0.051455384751823575)
w4 ( 0.737028440671181 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.00653836629351171) - present_state_Q (-0.00653836629351171)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4253524317019644 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.014740568813423621)) * f3(0.0)
w4 ( 0.7329989595335542 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.014740568813423621)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4253524317019644 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.04493158165850494) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7329989595335542 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.04493158165850494) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4253524317019644 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.03827315647599092) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7329989595335542 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.03827315647599092) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.42500893654899036 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.031590361551861394)) * f3(0.10873416323840164)
w4 ( 0.733062140256658 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.031590361551861394)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.41545361945716924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.024960716272032432)) * f3(0.09322617872200531)
w4 ( 0.735112061689202 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.024960716272032432)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.41545361945716924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.01807816556977397) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.735112061689202 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.01807816556977397) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.41545361945716924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.735112061689202 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.41545361945716924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.735112061689202 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.41545361945716924 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.735112061689202 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.4223205596269048 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0004405931059517955) - present_state_Q (0.0004405931059517955)) * f3(0.03432789476347921)
w4 ( 0.7311112686216112 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0004405931059517955) - present_state_Q (0.0004405931059517955)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5667476754297234 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.014622225372432225) - present_state_Q (-0.24560065968498906)) * f3(0.8239149843886676)
w4 ( 0.7065701489727225 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.014622225372432225) - present_state_Q (-0.24560065968498906)) * f4(0.14)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.5667476754297234 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.7065701489727225 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6267271751873551 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.15609146917336147)) * f3(0.32528457217312445)
w4 ( 0.699194514849416 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.15609146917336147)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6864167341198256 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.17726853276534285)) * f3(0.32747313581538207)
w4 ( 0.6919035889804773 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.17726853276534285)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6864167341198256 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6919035889804773 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6864167341198256 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4918640464885222) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6919035889804773 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.4918640464885222) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.6864167341198256 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6919035889804773 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7597364185238532 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.22879196547196734)) * f3(0.413952979970333)
w4 ( 0.6777339247042531 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.22879196547196734)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7597364185238532 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6777339247042531 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.7597364185238532 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6777339247042531 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8335280768692042 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.253161974170799)) * f3(0.42242988333347614)
w4 ( 0.6602655444459611 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.253161974170799)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.8335280768692042 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6602655444459611 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9032521792927952 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.0421345709835803) - present_state_Q (-0.28527741505012405)) * f3(0.40562359923822316)
w4 ( 0.6465140561095752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.0421345709835803) - present_state_Q (-0.28527741505012405)) * f4(0.08)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9032521792927952 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.37688889477882326) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6465140561095752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.37688889477882326) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.978750247926611 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.348191927458373)) * f3(0.45706320176561077)
w4 ( 0.629995975384159 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.348191927458373)) * f4(0.1)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.978750247926611 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.629995975384159 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0196251991363918 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.19664416955560032)) * f3(0.22666048774028105)
w4 ( 0.6227825520623814 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.19664416955560032)) * f4(0.04)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0196251991363918 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2267938327979374) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6227825520623814 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2267938327979374) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0196251991363918 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2103367545218853) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6227825520623814 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.2103367545218853) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -1.0088445770718626 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.024911302082495257) - present_state_Q (-0.15478336420676098)) * f3(0.16402008835173718)
w4 ( 0.6240971010512114 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.024911302082495257) - present_state_Q (-0.15478336420676098)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9906611863999707 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.14739511231859445)) * f3(0.1584754063937742)
w4 ( 0.6263918912758486 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.14739511231859445)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9906611863999707 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6263918912758486 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9906611863999707 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6263918912758486 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9906611863999707 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6263918912758486 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715115005900399 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.15213096442391738)) * f3(0.16621101594562201)
w4 ( 0.6286961532046964 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.15213096442391738)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715115005900399 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6286961532046964 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715115005900399 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6286961532046964 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9715115005900399 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.1069130225610275) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.6286961532046964 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.1069130225610275) - present_state_Q (0.0)) * f4(0.0)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.976537228298141 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.11387666460151304)) * f3(0.1301586111835096)
w4 ( 0.6279239065338994 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.11387666460151304)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.9818207562210847 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.12504712763375997)) * f3(0.1409117868493861)
w4 ( 0.627174000789167 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (-0.12504712763375997)) * f4(0.02)
============================================================================
NOT GUIDE learning . . .
w3 ( -0.41545361945716924 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f3(0.0)
w4 ( 0.735112061689202 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0) - present_state_Q (0.0)) * f4(0.0)
============================================================================
