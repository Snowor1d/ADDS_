GUIDE learning . . .
w1 ( 0.9234467428016881 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7942696838342405) - present_state_Q ( 0.7859370018958507)) * f1( 0.5859370018958506)
w2 ( 0.9738697993297515 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.7942696838342405) - present_state_Q (0.7859370018958507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8364264643269479 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8025107909495082) - present_state_Q ( 0.8025107909495082)) * f1( 0.6581178999448487)
w2 ( 0.9474246050926604 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.8025107909495082) - present_state_Q (0.8025107909495082)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.7367545542010383 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8237503219410754) - present_state_Q ( 0.815053734111775)) * f1( 0.7479065282763655)
w2 ( 0.920771031054307 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.8237503219410754) - present_state_Q (0.815053734111775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.6333507078907455 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7792170223227075) - present_state_Q ( 0.7725917945987861)) * f1( 0.7986887695944362)
w2 ( 0.8948776292069767 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.7792170223227075) - present_state_Q (0.7725917945987861)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.5364067714150765 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6899615103704297) - present_state_Q ( 0.6842257968738987)) * f1( 0.7977417009845045)
w2 ( 0.8705730362902395 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.6899615103704297) - present_state_Q (0.6842257968738987)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.46091623415309135 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.556293155215417) - present_state_Q ( 0.545619272203412)) * f1( 0.6925801178186289)
w2 ( 0.8487732371566021 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.556293155215417) - present_state_Q (0.545619272203412)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.37042599266401544 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6877954404098461) - present_state_Q ( 0.6830069157173428)) * f1( 0.7452495603368372)
w2 ( 0.8002041422895478 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.6877954404098461) - present_state_Q (0.6830069157173428)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.2941796369986199 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6059906402610515) - present_state_Q ( 0.4465125391740112)) * f1( 0.7733574759585993)
w2 ( 0.7804858727865898 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.6059906402610515) - present_state_Q (0.4465125391740112)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19573537223009554 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7058714451828887) - present_state_Q ( 0.7032352783382094)) * f1( 0.7986404397778141)
w2 ( 0.7065269847573945 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.7058714451828887) - present_state_Q (0.7032352783382094)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10618474995088469 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5821975065027557) - present_state_Q ( 0.5801490669430724)) * f1( 0.7981841723782916)
w2 ( 0.6392112257798267 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.5821975065027557) - present_state_Q (0.5801490669430724)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.023192793460336328 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4687203229285595) - present_state_Q ( 0.46968591187164865)) * f1( 0.8114081960319649)
w2 ( 0.5778423930050991 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.4687203229285595) - present_state_Q (0.46968591187164865)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05203033962789186 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36548639902710944) - present_state_Q ( 0.36548639902710944)) * f1( 0.8097758148956317)
w2 ( 0.5221061274576352 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.36548639902710944) - present_state_Q (0.36548639902710944)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11232353904560516 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2762990273312369) - present_state_Q ( 0.2762990273312369)) * f1( 0.710444125633357)
w2 ( 0.4711859799817484 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2762990273312369) - present_state_Q (0.2762990273312369)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.16113890117647772 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21349289868436055) - present_state_Q ( 0.21349289868436055)) * f1( 0.6162438425002315)
w2 ( 0.4236573634527929 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.21349289868436055) - present_state_Q (0.21349289868436055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20358387904659458 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1657177650338541) - present_state_Q ( 0.16250318452225662)) * f1( 0.5690198510724599)
w2 ( 0.37890147897166065 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1657177650338541) - present_state_Q (0.16250318452225662)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23935485025591108 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12673093951648606) - present_state_Q ( 0.12512495581032795)) * f1( 0.5020826405870484)
w2 ( 0.33615436726013986 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.12673093951648606) - present_state_Q (0.12512495581032795)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.26903408009601476 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09561219725350668) - present_state_Q ( 0.09859340758735452)) * f1( 0.4307379301422086)
w2 ( 0.2948124359884196 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.09561219725350668) - present_state_Q (0.09859340758735452)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.29595082440135817 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0640937974833434) - present_state_Q ( 0.06731833781273772)) * f1( 0.40726856516174564)
w2 ( 0.2551578985045554 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.0640937974833434) - present_state_Q (0.06731833781273772)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.31548613931825475 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06533728038877194) - present_state_Q ( 0.06533728038877194)) * f1( 0.2965271642391092)
w2 ( 0.2156296853635617 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.06533728038877194) - present_state_Q (0.06533728038877194)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.32604644167368074 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.053815898927469824) - present_state_Q ( 0.05533373692964058)) * f1( 0.2346983434787371)
w2 ( 0.18863255654134808 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.053815898927469824) - present_state_Q (0.05533373692964058)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3280466904788663 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05053215189700362) - present_state_Q ( 0.01740608446579829)) * f1( 0.1780327301005679)
w2 ( 0.18413844177030417 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.05053215189700362) - present_state_Q (0.01740608446579829)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.32117567985623047 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053879963818770416) - present_state_Q ( -0.053879963818770416)) * f1( 0.2765083593448869)
w2 ( 0.18910828111904204 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.053879963818770416) - present_state_Q (-0.053879963818770416)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.31739372758576095 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04067796196817028) - present_state_Q ( -0.04710078645561204)) * f1( 0.2644111867917108)
w2 ( 0.19196894092421796 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.04067796196817028) - present_state_Q (-0.04710078645561204)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.31239419700802185 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02775383543233688) - present_state_Q ( -0.031103538789089044)) * f1( 0.21896250912883652)
w2 ( 0.19653550402913506 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02775383543233688) - present_state_Q (-0.031103538789089044)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.31080572199688106 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007268920288988373) - present_state_Q ( -0.007268920288988373)) * f1( 0.14909374610956483)
w2 ( 0.19866634459433685 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.007268920288988373) - present_state_Q (-0.007268920288988373)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.309906561639102 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009249259815989713) - present_state_Q ( 0.009249259815989713)) * f1( 0.09808059165391932)
w2 ( 0.20049985791764904 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.009249259815989713) - present_state_Q (0.009249259815989713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3110301166245189 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007475142586815621) - present_state_Q ( 0.007475142586815621)) * f1( 0.10527311465804666)
w2 ( 0.19836530535108637 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.007475142586815621) - present_state_Q (0.007475142586815621)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3155250223898209 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005119824073070836) - present_state_Q ( 0.005119824073070836)) * f1( 0.11109289792299994)
w2 ( 0.1902731485177711 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.005119824073070836) - present_state_Q (0.005119824073070836)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32253949403838683 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0018827695045276621) - present_state_Q ( 0.0012312861119857174)) * f1( 0.11670498685861581)
w2 ( 0.17825228833454043 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.0018827695045276621) - present_state_Q (0.0012312861119857174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.33276541211713667 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021118948274538613) - present_state_Q ( -0.021118948274538613)) * f1( 0.17600761144212101)
w2 ( 0.16663242940348214 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.021118948274538613) - present_state_Q (-0.021118948274538613)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.9612781860834186 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8588023988072355) - present_state_Q ( 0.8487540669528707)) * f1( 0.4487540669528707)
w2 ( 0.9654850469171141 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.8588023988072355) - present_state_Q (0.8487540669528707)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.9118129518605539 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.869941642092148) - present_state_Q ( 0.869941642092148)) * f1( 0.5032337468264605)
w2 ( 0.9261671478017968 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.869941642092148) - present_state_Q (0.869941642092148)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.8856265688526879 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8267459041641829) - present_state_Q ( 0.8171802461810622)) * f1( 0.48991779086799)
w2 ( 0.904786921571211 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.8267459041641829) - present_state_Q (0.8171802461810622)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.8650276650566008 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8077268124197166) - present_state_Q ( 0.7985652952328746)) * f1( 0.49304135847015346)
w2 ( 0.8880752170115749 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.8077268124197166) - present_state_Q (0.7985652952328746)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.8441936619361139 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7922077207104388) - present_state_Q ( 0.7919179993938636)) * f1( 0.504825371753469)
w2 ( 0.8715673279186621 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7922077207104388) - present_state_Q (0.7919179993938636)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.7828884335457651 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.9386906273276004) - present_state_Q ( 0.9386906273276004)) * f1( 0.49248205633634085)
w2 ( 0.7968780340429716 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.9386906273276004) - present_state_Q (0.9386906273276004)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.7069995737578656 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.0272207707705183) - present_state_Q ( 1.0272207707705183)) * f1( 0.49779550551165375)
w2 ( 0.6749181385474943 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.0272207707705183) - present_state_Q (1.0272207707705183)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.6301676132518659 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.0310445726465867) - present_state_Q ( 1.030546458798986)) * f1( 0.5030106572218217)
w2 ( 0.5221739383940616 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.0310445726465867) - present_state_Q (1.030546458798986)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5569719948638013 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.9449699393040216) - present_state_Q ( 0.9446763383205137)) * f1( 0.5047349396556724)
w2 ( 0.3481524170672482 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.9449699393040216) - present_state_Q (0.9446763383205137)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.49584513517456924 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6958374665398424) - present_state_Q ( 0.6955010532320097)) * f1( 0.4986213944548925)
w2 ( 0.20104234027788512 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.6958374665398424) - present_state_Q (0.6955010532320097)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.4435210242703952 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4906849889580962) - present_state_Q ( 0.4903997339631729)) * f1( 0.5024732682755763)
w2 ( 0.07608259206980152 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.4906849889580962) - present_state_Q (0.4903997339631729)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.44605089938943854 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2589840682162114) - present_state_Q ( 0.2589840682162114)) * f1( 0.37807668308012254)
w2 ( 0.08411231270245069 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2589840682162114) - present_state_Q (0.2589840682162114)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.40167677629452847 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3231262335793546) - present_state_Q ( 0.3231262335793546)) * f1( 0.4981302775996034)
w2 ( -0.02278532052411962 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.3231262335793546) - present_state_Q (0.3231262335793546)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.37466253583322245 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17268406355008564) - present_state_Q ( 0.1258043083245154)) * f1( 0.3812684775212508)
w2 ( -0.10780962876046046 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.17268406355008564) - present_state_Q (0.1258043083245154)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.34365378815767106 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05824315575538322) - present_state_Q ( 0.050765930353250055)) * f1( 0.4807992997356671)
w2 ( -0.1852026225337859 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.05824315575538322) - present_state_Q (0.050765930353250055)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.3564377608649036 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11283631669478483) - present_state_Q ( -0.11283631669478483)) * f1( 0.3183635219977891)
w2 ( -0.13701630033074913 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11283631669478483) - present_state_Q (-0.11283631669478483)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.3538919560266964 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030169077621721913) - present_state_Q ( -0.03363436325640343)) * f1( 0.36692295682461523)
w2 ( -0.14534220567144138 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.030169077621721913) - present_state_Q (-0.03363436325640343)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.32908477303917777 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022890399751389207) - present_state_Q ( -0.022890399751389207)) * f1( 0.42815397319432236)
w2 ( -0.21487004249829136 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.022890399751389207) - present_state_Q (-0.022890399751389207)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.3014422509239076 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006778646727666565) - present_state_Q ( -0.04975265522732483)) * f1( 0.5017472724309526)
w2 ( -0.26996256344283553 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.006778646727666565) - present_state_Q (-0.04975265522732483)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2737445031096796 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04645029291680447) - present_state_Q ( -0.10339392424140725)) * f1( 0.5525723042834989)
w2 ( -0.32008767394786286 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.04645029291680447) - present_state_Q (-0.10339392424140725)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2456493462334487 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09047868650214627) - present_state_Q ( -0.15186432373357195)) * f1( 0.6145268610083828)
w2 ( -0.36580602843952714 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09047868650214627) - present_state_Q (-0.15186432373357195)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2165849422298896 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24394972167379161) - present_state_Q ( -0.24885186150141925)) * f1( 0.7739298945470854)
w2 ( -0.41087120171944236 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.24394972167379161) - present_state_Q (-0.24885186150141925)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.19217740208717538 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3221191078780743) - present_state_Q ( -0.32241026909144144)) * f1( 0.787844118871257)
w2 ( -0.44804739872300625 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.3221191078780743) - present_state_Q (-0.32241026909144144)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.17249455136948463 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38507213457858536) - present_state_Q ( -0.3871624384505082)) * f1( 0.7831016466172865)
w2 ( -0.4782087717238883 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.38507213457858536) - present_state_Q (-0.3871624384505082)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.1563264895567295 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4385667861600526) - present_state_Q ( -0.4382246451349693)) * f1( 0.7862618259934778)
w2 ( -0.5028846157416127 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4385667861600526) - present_state_Q (-0.4382246451349693)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.14339832349233292 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48013801464828376) - present_state_Q ( -0.4818405743148484)) * f1( 0.7779933197498928)
w2 ( -0.5228254029996103 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.48013801464828376) - present_state_Q (-0.4818405743148484)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.1325237174788784 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5142549621648329) - present_state_Q ( -0.5139551176110196)) * f1( 0.7910508521013354)
w2 ( -0.5393218484322659 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5142549621648329) - present_state_Q (-0.5139551176110196)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.12363658430957129 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5423808871547652) - present_state_Q ( -0.5421304262573562)) * f1( 0.792732001938497)
w2 ( -0.5527747679272403 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5423808871547652) - present_state_Q (-0.5421304262573562)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.11648659870190223 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5656974501358788) - present_state_Q ( -0.5658677354924962)) * f1( 0.7882940681712707)
w2 ( -0.5636590090697713 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5656974501358788) - present_state_Q (-0.5658677354924962)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.11061139535117703 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5840425507671435) - present_state_Q ( -0.5841827290236543)) * f1( 0.7915767383339828)
w2 ( -0.5725655921961385 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5840425507671435) - present_state_Q (-0.5841827290236543)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.10587697616572983 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5999127155115295) - present_state_Q ( -0.5999127155115295)) * f1( 0.7880381116890891)
w2 ( -0.5797750189208932 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5999127155115295) - present_state_Q (-0.5999127155115295)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.10195229196566918 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.611564146756807) - present_state_Q ( -0.6117036461106776)) * f1( 0.7936227462981889)
w2 ( -0.5857093511486936 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.611564146756807) - present_state_Q (-0.6117036461106776)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09876268333937105 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6219853070474306) - present_state_Q ( -0.6219853070474306)) * f1( 0.7931740696740016)
w2 ( -0.5905349379875711 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6219853070474306) - present_state_Q (-0.6219853070474306)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.096163513558042 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6305015616262164) - present_state_Q ( -0.6302883035157284)) * f1( 0.7933525033956006)
w2 ( -0.5944663603051983 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6305015616262164) - present_state_Q (-0.6302883035157284)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09569402359160872 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6376830951245617) - present_state_Q ( -0.6559096776056718)) * f1( 0.5974194643573506)
w2 ( -0.5954093961340124 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6376830951245617) - present_state_Q (-0.6559096776056718)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09537465138649143 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6393590826603396) - present_state_Q ( -0.6584795488182049)) * f1( 0.585321051831305)
w2 ( -0.5960641592677519 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6393590826603396) - present_state_Q (-0.6584795488182049)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09347015305010398 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6397562189919173) - present_state_Q ( -0.6398833042887597)) * f1( 0.7905002611964586)
w2 ( -0.5989552373810038 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6397562189919173) - present_state_Q (-0.6398833042887597)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09195208713631331 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6454230234937546) - present_state_Q ( -0.645238944095406)) * f1( 0.7864258093425336)
w2 ( -0.6012716403714801 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6454230234937546) - present_state_Q (-0.645238944095406)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09068958779298725 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6489048524155909) - present_state_Q ( -0.6489048524155909)) * f1( 0.7897712634029604)
w2 ( -0.6031899163105963 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6489048524155909) - present_state_Q (-0.6489048524155909)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0896510390605627 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6522697781409807) - present_state_Q ( -0.6520966412438535)) * f1( 0.7909536262597168)
w2 ( -0.6047655566990257 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6522697781409807) - present_state_Q (-0.6520966412438535)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08887680121197285 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6545741432936025) - present_state_Q ( -0.6555634618518872)) * f1( 0.7825364538112183)
w2 ( -0.6059528309963225 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6545741432936025) - present_state_Q (-0.6555634618518872)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08822231350853894 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6572664271171957) - present_state_Q ( -0.657387713158297)) * f1( 0.7848581754300697)
w2 ( -0.6069535025427332 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6572664271171957) - present_state_Q (-0.657387713158297)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08763487173709467 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6584302739146737) - present_state_Q ( -0.6584302739146737)) * f1( 0.7924744472931937)
w2 ( -0.6078430329599485 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6584302739146737) - present_state_Q (-0.6584302739146737)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08719326369112328 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6603009368809578) - present_state_Q ( -0.6604206289037462)) * f1( 0.7872552247827277)
w2 ( -0.6085161687340704 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6603009368809578) - present_state_Q (-0.6604206289037462)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0867884750545296 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6610010464836356) - present_state_Q ( -0.6610010464836356)) * f1( 0.7938498120961591)
w2 ( -0.6091280557138378 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6610010464836356) - present_state_Q (-0.6610010464836356)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08650407377081852 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6626513861208116) - present_state_Q ( -0.6626513861208116)) * f1( 0.7869971294331316)
w2 ( -0.6095617060127901 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6626513861208116) - present_state_Q (-0.6626513861208116)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08628115479481967 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6633790864883785) - present_state_Q ( -0.6635009905392677)) * f1( 0.785778677385373)
w2 ( -0.6099021361859386 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6633790864883785) - present_state_Q (-0.6635009905392677)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08620719993726768 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6636869029281127) - present_state_Q ( -0.6654087764971109)) * f1( 0.770432281349189)
w2 ( -0.6100173258414227 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6636869029281127) - present_state_Q (-0.6654087764971109)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08601643032846627 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6641671108412956) - present_state_Q ( -0.6639989985804492)) * f1( 0.7890500152975283)
w2 ( -0.6103074513418643 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6641671108412956) - present_state_Q (-0.6639989985804492)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08586388553161688 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6643946085238867) - present_state_Q ( -0.6645059534651063)) * f1( 0.788953783434004)
w2 ( -0.6105394722283382 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6643946085238867) - present_state_Q (-0.6645059534651063)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08674385702373556 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6648782289207246) - present_state_Q ( -0.6811639648787311)) * f1( 0.5995931988928865)
w2 ( -0.6087783351899391 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6648782289207246) - present_state_Q (-0.6811639648787311)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0864199170145828 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6621040441866045) - present_state_Q ( -0.6621040441866045)) * f1( 0.7888738221842972)
w2 ( -0.6092710984177858 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6621040441866045) - present_state_Q (-0.6621040441866045)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08612679580537705 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6627400794322859) - present_state_Q ( -0.6625785028380704)) * f1( 0.7931830720423613)
w2 ( -0.6097145590304048 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6627400794322859) - present_state_Q (-0.6625785028380704)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08591894166046782 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6639302860449149) - present_state_Q ( -0.6637565671547574)) * f1( 0.7883830235036945)
w2 ( -0.610030934404373 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6639302860449149) - present_state_Q (-0.6637565671547574)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08681223451511377 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6646468293125729) - present_state_Q ( -0.6817172658259071)) * f1( 0.5856666118886013)
w2 ( -0.608200624457015 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6646468293125729) - present_state_Q (-0.6817172658259071)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08646074610659384 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6615385559238857) - present_state_Q ( -0.6616773348138825)) * f1( 0.785182122257993)
w2 ( -0.6087378069504357 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6615385559238857) - present_state_Q (-0.6616773348138825)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08611698295187144 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6620327086367108) - present_state_Q ( -0.6618714913265379)) * f1( 0.7935841419803832)
w2 ( -0.6092576204948916 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6620327086367108) - present_state_Q (-0.6618714913265379)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08585185390048305 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6631245871263294) - present_state_Q ( -0.6629620427197931)) * f1( 0.7913317389691024)
w2 ( -0.6096596704140324 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6631245871263294) - present_state_Q (-0.6629620427197931)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08564248318784816 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6639034222393055) - present_state_Q ( -0.6637411536613159)) * f1( 0.7903201591217028)
w2 ( -0.6099775730415461 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6639034222393055) - present_state_Q (-0.6637411536613159)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08550026236005971 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6646562492722818) - present_state_Q ( -0.6646562492722818)) * f1( 0.786021561634556)
w2 ( -0.6101946981201396 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6646562492722818) - present_state_Q (-0.6646562492722818)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08533066690022599 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6642951652860452) - present_state_Q ( -0.6642951652860452)) * f1( 0.7945995787944969)
w2 ( -0.6104508202692467 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6642951652860452) - present_state_Q (-0.6642951652860452)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08524047251724236 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6653931342320285) - present_state_Q ( -0.6653931342320285)) * f1( 0.7869134571464343)
w2 ( -0.6105883617721876 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6653931342320285) - present_state_Q (-0.6653931342320285)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08511658632741885 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6651328057962029) - present_state_Q ( -0.6649546241094104)) * f1( 0.7948267767228776)
w2 ( -0.6107754005486128 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6651328057962029) - present_state_Q (-0.6649546241094104)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08512214070495921 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6657157842798975) - present_state_Q ( -0.6666428994275845)) * f1( 0.778785711350804)
w2 ( -0.6107668420286614 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6657157842798975) - present_state_Q (-0.6666428994275845)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08511240586351665 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6655121598568683) - present_state_Q ( -0.6664265949169472)) * f1( 0.7811553488524122)
w2 ( -0.6107817965569101 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6655121598568683) - present_state_Q (-0.6664265949169472)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08593489221150369 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.680947013488961) - present_state_Q ( -0.6817779526878714)) * f1( 0.601089848904748)
w2 ( -0.6091398063962331 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.680947013488961) - present_state_Q (-0.6817779526878714)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0866308177500569 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6786488811289889) - present_state_Q ( -0.6794801675104953)) * f1( 0.599146619492614)
w2 ( -0.6077459728685215 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6786488811289889) - present_state_Q (-0.6794801675104953)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08731129199040291 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6604944214338065) - present_state_Q ( -0.6774113807478833)) * f1( 0.5989068098610726)
w2 ( -0.6063825402359811 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6604944214338065) - present_state_Q (-0.6774113807478833)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08672410296787518 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6584373532399588) - present_state_Q ( -0.6584373532399588)) * f1( 0.7928149207874214)
w2 ( -0.6072713060860656 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6584373532399588) - present_state_Q (-0.6584373532399588)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08625809516458781 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6603085861367306) - present_state_Q ( -0.6601384799897784)) * f1( 0.7908653415407118)
w2 ( -0.607978391520933 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6603085861367306) - present_state_Q (-0.6601384799897784)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08592693349699973 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6611206933374024) - present_state_Q ( -0.6618915727681882)) * f1( 0.7846509585887276)
w2 ( -0.6084848511087992 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6611206933374024) - present_state_Q (-0.6618915727681882)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08561005577671554 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6624137657008696) - present_state_Q ( -0.6622341278466288)) * f1( 0.7907612982174307)
w2 ( -0.6089657209556142 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6624137657008696) - present_state_Q (-0.6622341278466288)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08535125491059503 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6630318218065809) - present_state_Q ( -0.6630318218065809)) * f1( 0.7911108423617782)
w2 ( -0.6093582842005034 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6630318218065809) - present_state_Q (-0.6630318218065809)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08611272049265352 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6642028420292454) - present_state_Q ( -0.6788213011135725)) * f1( 0.614034790489366)
w2 ( -0.6078701621712257 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6642028420292454) - present_state_Q (-0.6788213011135725)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.06537105592542339 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1996289849150117) - present_state_Q ( -0.20056074939398236)) * f1( 0.49455313025607117)
w2 ( -0.6246462481351265 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1996289849150117) - present_state_Q (-0.20056074939398236)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04029277850184915 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20865410484435362) - present_state_Q ( -0.20996127429157657)) * f1( 0.6103194203867468)
w2 ( -0.6410824135828408 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.20865410484435362) - present_state_Q (-0.20996127429157657)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.013154862767713195 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2286925625827942) - present_state_Q ( -0.2286925625827942)) * f1( 0.6884708347692887)
w2 ( -0.6568494813298602 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2286925625827942) - present_state_Q (-0.2286925625827942)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.012489395651506145 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2536290497598539) - present_state_Q ( -0.25366398724460143)) * f1( 0.6899201799062462)
w2 ( -0.6717174380391155 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2536290497598539) - present_state_Q (-0.25366398724460143)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.02189060561281875 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27645293869121706) - present_state_Q ( -0.27645293869121706)) * f1( 0.6218045846465193)
w2 ( -0.6777651322462317 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.27645293869121706) - present_state_Q (-0.27645293869121706)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.018426810092019835 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2833575950946673) - present_state_Q ( -0.28458593618874256)) * f1( 0.6157839362085199)
w2 ( -0.6755151251790606 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2833575950946673) - present_state_Q (-0.28458593618874256)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.041126767989817084 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28244235581310373) - present_state_Q ( -0.2822971420657455)) * f1( 0.6561684813454266)
w2 ( -0.6893530089196832 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.28244235581310373) - present_state_Q (-0.2822971420657455)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.054704582343808 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4411209692857057) - present_state_Q ( -0.4411209692857057)) * f1( 0.6688870844581556)
w2 ( -0.7015324765782551 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4411209692857057) - present_state_Q (-0.4411209692857057)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06609537083399004 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4533814004512543) - present_state_Q ( -0.4533814004512543)) * f1( 0.5934039364432783)
w2 ( -0.7130498809538874 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4533814004512543) - present_state_Q (-0.4533814004512543)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07626700608932484 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46359135437868076) - present_state_Q ( -0.46487421961706366)) * f1( 0.5604672547760453)
w2 ( -0.7239389759031357 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.46359135437868076) - present_state_Q (-0.46487421961706366)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07501583793788377 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4717707750042064) - present_state_Q ( -0.47231822932844986)) * f1( 0.49765745022317)
w2 ( -0.722430506793454 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4717707750042064) - present_state_Q (-0.47231822932844986)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.061344193040202964 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4644279752689527) - present_state_Q ( -0.46559329682251194)) * f1( 0.42837610869652104)
w2 ( -0.703281476835717 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.4644279752689527) - present_state_Q (-0.46559329682251194)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.039784835705193825 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44402511562353436) - present_state_Q ( -0.44402511562353436)) * f1( 0.35954877599659973)
w2 ( -0.6673041205920461 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.44402511562353436) - present_state_Q (-0.44402511562353436)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.020276856525243882 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4119532891421197) - present_state_Q ( -0.4119532891421197)) * f1( 0.2908348515658553)
w2 ( -0.6270586429783717 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.4119532891421197) - present_state_Q (-0.4119532891421197)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0036167544595476393 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3825297751604069) - present_state_Q ( -0.5079415037560813)) * f1( 0.3104322095265311)
w2 ( -0.5654835608791684 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.3825297751604069) - present_state_Q (-0.5079415037560813)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.027737608056924407 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33784592548369063) - present_state_Q ( -0.33784592548369063)) * f1( 0.3993113328437371)
w2 ( -0.5292398809030491 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.33784592548369063) - present_state_Q (-0.33784592548369063)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05843602451329275 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5173176836298214) - present_state_Q ( -0.5181294527138486)) * f1( 0.40055466089214103)
w2 ( -0.45260011246796245 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.5173176836298214) - present_state_Q (-0.5181294527138486)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.09069882781310776 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4238118265268411) - present_state_Q ( -0.42498054423201376)) * f1( 0.4726462565855406)
w2 ( -0.3843401763100295 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.4238118265268411) - present_state_Q (-0.42498054423201376)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08922458041943407 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41144610694221445) - present_state_Q ( -0.4132585772660233)) * f1( 0.528668732134182)
w2 ( -0.3876865003214133 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.41144610694221445) - present_state_Q (-0.4132585772660233)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.08059564400948166 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4138847578477107) - present_state_Q ( -0.49142205791199334)) * f1( 0.5753912464104238)
w2 ( -0.40868179882360217 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4138847578477107) - present_state_Q (-0.49142205791199334)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.07252914430118737 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43801133085562505) - present_state_Q ( -0.5197476906203455)) * f1( 0.6502439229411977)
w2 ( -0.42604928076873255 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.43801133085562505) - present_state_Q (-0.5197476906203455)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06498004194129758 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5460365824690521) - present_state_Q ( -0.5460365824690521)) * f1( 0.6953399366983554)
w2 ( -0.44124867137763196 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5460365824690521) - present_state_Q (-0.5460365824690521)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05905514163205473 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5725734202391428) - present_state_Q ( -0.5723855622254008)) * f1( 0.6981001604194736)
w2 ( -0.4531307205494238 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5725734202391428) - present_state_Q (-0.5723855622254008)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05449590255852574 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5933446453724873) - present_state_Q ( -0.593491060308332)) * f1( 0.6924367181377735)
w2 ( -0.4623487971414722 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5933446453724873) - present_state_Q (-0.593491060308332)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.050924789447004445 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6093317076567827) - present_state_Q ( -0.609471569588925)) * f1( 0.693937427103312)
w2 ( -0.46955342130621763 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6093317076567827) - present_state_Q (-0.609471569588925)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04810884898748734 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6217062390876739) - present_state_Q ( -0.6218285142156703)) * f1( 0.6980151709812397)
w2 ( -0.4752013166632512 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6217062390876739) - present_state_Q (-0.6218285142156703)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04591493239670756 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6318604590443101) - present_state_Q ( -0.6317290817298338)) * f1( 0.6974343037690368)
w2 ( -0.47960529164769483 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6318604590443101) - present_state_Q (-0.6317290817298338)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04428159952913128 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6396746900349735) - present_state_Q ( -0.6400681359331772)) * f1( 0.6834219443573816)
w2 ( -0.48295119827753963 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6396746900349735) - present_state_Q (-0.6400681359331772)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04295842970550136 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6453365930552956) - present_state_Q ( -0.6454419217846086)) * f1( 0.6930588806702257)
w2 ( -0.48562404153046856 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6453365930552956) - present_state_Q (-0.6454419217846086)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04192676759382553 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6500068644725506) - present_state_Q ( -0.650110346170188)) * f1( 0.6928398495128545)
w2 ( -0.48770868916925797 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6500068644725506) - present_state_Q (-0.650110346170188)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.041107721447516725 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6535973783368126) - present_state_Q ( -0.6535973783368126)) * f1( 0.6963281019652969)
w2 ( -0.48935541949881955 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6535973783368126) - present_state_Q (-0.6535973783368126)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04046483781466751 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6565532855886382) - present_state_Q ( -0.6564350965929838)) * f1( 0.6972532092774287)
w2 ( -0.49064625197404277 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6565532855886382) - present_state_Q (-0.6564350965929838)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.039965590686709254 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6588304819959532) - present_state_Q ( -0.6587163000698806)) * f1( 0.6966159810867119)
w2 ( -0.4916495967122028 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6588304819959532) - present_state_Q (-0.6587163000698806)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0395952145567793 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6606156562545265) - present_state_Q ( -0.660700205990607)) * f1( 0.6908250055130192)
w2 ( -0.4924001870610812 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6606156562545265) - present_state_Q (-0.660700205990607)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03928665174670044 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6616481028739274) - present_state_Q ( -0.6617411956180859)) * f1( 0.697535461711472)
w2 ( -0.49301949311478416 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6616481028739274) - present_state_Q (-0.6617411956180859)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03904588071969307 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6627286923292555) - present_state_Q ( -0.6628213906827292)) * f1( 0.697588072780227)
w2 ( -0.49350270011181163 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6627286923292555) - present_state_Q (-0.6628213906827292)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03885727600108444 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6635720031286283) - present_state_Q ( -0.6636546423800672)) * f1( 0.697874840423969)
w2 ( -0.493881058222403 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6635720031286283) - present_state_Q (-0.6636546423800672)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03872422461544215 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6644284730433384) - present_state_Q ( -0.6645217469035003)) * f1( 0.6925790322284278)
w2 ( -0.49415001227851973 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6644284730433384) - present_state_Q (-0.6645217469035003)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038602694226855534 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6648553002602849) - present_state_Q ( -0.6647465891712409)) * f1( 0.6988759177864792)
w2 ( -0.49439346399819 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6648553002602849) - present_state_Q (-0.6647465891712409)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03851087098775881 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6653264108990925) - present_state_Q ( -0.6652166118575267)) * f1( 0.697729479234161)
w2 ( -0.4945777080907236 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6653264108990925) - present_state_Q (-0.6652166118575267)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03845330275512777 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6656329372226045) - present_state_Q ( -0.665732226166702)) * f1( 0.6927022026790962)
w2 ( -0.49469405754850176 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6656329372226045) - present_state_Q (-0.665732226166702)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03841199305798601 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6658976056687442) - present_state_Q ( -0.665992122970723)) * f1( 0.6912165065882439)
w2 ( -0.49477772681196297 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6658976056687442) - present_state_Q (-0.665992122970723)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03836831806719437 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6658645938187996) - present_state_Q ( -0.6659588333308603)) * f1( 0.6958760032455722)
w2 ( -0.4948655944591057 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6658645938187996) - present_state_Q (-0.6659588333308603)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03833251399561811 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6659880505134582) - present_state_Q ( -0.6660848150330948)) * f1( 0.6965907956362942)
w2 ( -0.4949375530616609 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6659880505134582) - present_state_Q (-0.6660848150330948)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038331368373524695 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6662798000831203) - present_state_Q ( -0.6666112832709147)) * f1( 0.6861353006593092)
w2 ( -0.4949398906048965 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6662798000831203) - present_state_Q (-0.6666112832709147)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0383081453199637 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664132268421774) - present_state_Q ( -0.6663067857319516)) * f1( 0.6941850041879113)
w2 ( -0.4949867257782138 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664132268421774) - present_state_Q (-0.6663067857319516)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03828073588818507 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6663493188270984) - present_state_Q ( -0.6662422478300404)) * f1( 0.6980021621021751)
w2 ( -0.4950417015455875 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6663493188270984) - present_state_Q (-0.6662422478300404)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038250529868733195 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665509404995066) - present_state_Q ( -0.6662241854356243)) * f1( 0.7009843490621168)
w2 ( -0.4951020287515932 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665509404995066) - present_state_Q (-0.6662241854356243)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038242141090671256 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664313583125216) - present_state_Q ( -0.6665225977834874)) * f1( 0.6959444107074455)
w2 ( -0.49511890407828024 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664313583125216) - present_state_Q (-0.6665225977834874)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038235038793100674 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.666450138461575) - present_state_Q ( -0.666542995939361)) * f1( 0.6961814639799523)
w2 ( -0.49513318658523175 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.666450138461575) - present_state_Q (-0.666542995939361)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03822930604234026 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664715600258792) - present_state_Q ( -0.6665648200016168)) * f1( 0.696262958219135)
w2 ( -0.4951447136253677 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664715600258792) - present_state_Q (-0.6665648200016168)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03822474071327888 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664897735489124) - present_state_Q ( -0.6665834121060084)) * f1( 0.6963031696161264)
w2 ( -0.4951538927602113 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664897735489124) - present_state_Q (-0.6665834121060084)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0382211352073141 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665047045252485) - present_state_Q ( -0.6665986912210672)) * f1( 0.6963228041984426)
w2 ( -0.4951611418526154 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665047045252485) - present_state_Q (-0.6665986912210672)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03822832359820738 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.666683688860493) - present_state_Q ( -0.666772230313959)) * f1( 0.6921136208073795)
w2 ( -0.49514660125270804 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.666683688860493) - present_state_Q (-0.666772230313959)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03825135129439396 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665594571265095) - present_state_Q ( -0.6669917692945555)) * f1( 0.6857081344907572)
w2 ( -0.4950995859512414 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665594571265095) - present_state_Q (-0.6669917692945555)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03824851022826045 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665147340047367) - present_state_Q ( -0.6666105088028101)) * f1( 0.6935418130657246)
w2 ( -0.49510532099491433 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665147340047367) - present_state_Q (-0.6666105088028101)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03827548239050882 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6667411460187633) - present_state_Q ( -0.6670697184315051)) * f1( 0.6817972989208646)
w2 ( -0.4950499364587663 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6667411460187633) - present_state_Q (-0.6670697184315051)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038267085013979275 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664292436426157) - present_state_Q ( -0.6665218557459033)) * f1( 0.693604721307255)
w2 ( -0.4950668860653365 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664292436426157) - present_state_Q (-0.6665218557459033)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03826250039462395 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664922945684556) - present_state_Q ( -0.6665830521181323)) * f1( 0.6927778367140934)
w2 ( -0.49507615089275636 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664922945684556) - present_state_Q (-0.6665830521181323)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038252398529100305 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664028462693156) - present_state_Q ( -0.6664950385574666)) * f1( 0.6955000958623012)
w2 ( -0.49509648534248146 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664028462693156) - present_state_Q (-0.6664950385574666)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03824549606560632 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6664552542200254) - present_state_Q ( -0.6665462222507031)) * f1( 0.6950899355642638)
w2 ( -0.4951103877864634 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6664552542200254) - present_state_Q (-0.6665462222507031)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03824341860559677 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6666333794188868) - present_state_Q ( -0.6666333794188868)) * f1( 0.6934454043076781)
w2 ( -0.49511458197968367 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6666333794188868) - present_state_Q (-0.6666333794188868)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.038240409307665334 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665161405685454) - present_state_Q ( -0.6666082707244679)) * f1( 0.6942931624633435)
w2 ( -0.4951206500462178 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665161405685454) - present_state_Q (-0.6666082707244679)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03823825590346581 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6667536034542976) - present_state_Q ( -0.6666443147980173)) * f1( 0.6936273891129792)
w2 ( -0.4951249964228555 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6667536034542976) - present_state_Q (-0.6666443147980173)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03823537716553307 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665159833958176) - present_state_Q ( -0.6666101608823416)) * f1( 0.6947187700380559)
w2 ( -0.4951307976668691 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665159833958176) - present_state_Q (-0.6666101608823416)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0382226202219499 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6665836227838364) - present_state_Q ( -0.6664757287130919)) * f1( 0.698499400304076)
w2 ( -0.49515636636600996 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6665836227838364) - present_state_Q (-0.6664757287130919)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.08194974897083959 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10795127395512404) - present_state_Q ( -0.1096692181339437)) * f1( 0.13824687261293828)
w2 ( -0.613892680356457 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10795127395512404) - present_state_Q (-0.1096692181339437)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.050813947072076565 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0754959850632102) - present_state_Q ( -0.07487721391840113)) * f1( 0.5845206697330474)
w2 ( -0.6245461280482154 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.0754959850632102) - present_state_Q (-0.07487721391840113)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04090638001195189 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4704500302823858) - present_state_Q ( -0.4710339983660432)) * f1( 0.5628947507651312)
w2 ( -0.638627008421191 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4704500302823858) - present_state_Q (-0.4710339983660432)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.038214379044060595 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.614439114781488) - present_state_Q ( -0.6149448214241502)) * f1( 0.5789362683796868)
w2 ( -0.6432769174265909 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.614439114781488) - present_state_Q (-0.6149448214241502)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9539454034300833 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6030251528695142) - present_state_Q ( 0.6030251528695142)) * f1( 0.4030251528695142)
w2 ( 0.9771455472483488 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.6030251528695142) - present_state_Q (0.6030251528695142)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.9058484695128359 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7797334252428567) - present_state_Q ( 0.6031894415963295)) * f1( 0.42744619417472285)
w2 ( 0.9546412252669079 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.7797334252428567) - present_state_Q (0.6031894415963295)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8277097239287351 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.1878043464165384) - present_state_Q ( 1.1878043464165384)) * f1( 0.4681703181891866)
w2 ( 0.8211193123249171 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.1878043464165384) - present_state_Q (1.1878043464165384)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.7475573408240512 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.2132369696674217) - present_state_Q ( 1.2132369696674217)) * f1( 0.4737381306592763)
w2 ( 0.6519279850548492 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.2132369696674217) - present_state_Q (1.2132369696674217)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.6672782733589766 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.0448884929333648) - present_state_Q ( 1.0422033365328722)) * f1( 0.5220674457531416)
w2 ( 0.4981565363308956 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.0448884929333648) - present_state_Q (1.0422033365328722)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5912609462651732 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.9470488459393083) - present_state_Q ( 0.9470488459393083)) * f1( 0.5234113207134814)
w2 ( 0.3238752609694503 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.9470488459393083) - present_state_Q (0.9470488459393083)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.5264947677900813 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6968781292175186) - present_state_Q ( 0.6999220318491459)) * f1( 0.5264540481694591)
w2 ( 0.176247154698163 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.6968781292175186) - present_state_Q (0.6999220318491459)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.46648785688918804 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5098630014878452) - present_state_Q ( 0.5098630014878452)) * f1( 0.5667034776098883)
w2 ( 0.04918195053747568 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.5098630014878452) - present_state_Q (0.5098630014878452)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.40870403476298106 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.35713769606053114) - present_state_Q ( 0.3529085065975005)) * f1( 0.6300060368395439)
w2 ( -0.06088141790149802 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.35713769606053114) - present_state_Q (0.3529085065975005)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.35160542352503205 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22839136645975805) - present_state_Q ( 0.21977082125217223)) * f1( 0.7164806261425565)
w2 ( -0.15651322005424162 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.22839136645975805) - present_state_Q (0.21977082125217223)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.29952803996322275 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11565031886471483) - present_state_Q ( 0.08434767485386649)) * f1( 0.7740595585539373)
w2 ( -0.23724713721032903 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.11565031886471483) - present_state_Q (0.08434767485386649)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.2505561395784123 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005299199555363343) - present_state_Q ( 0.005299199555363343)) * f1( 0.8097617064348072)
w2 ( -0.29772406517031175 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.005299199555363343) - present_state_Q (0.005299199555363343)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.21318278123928835 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15422823811539693) - present_state_Q ( -0.15422823811539693)) * f1( 0.8103598675754463)
w2 ( -0.3530674154538489 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.15422823811539693) - present_state_Q (-0.15422823811539693)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.18321325879629394 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25234404394846655) - present_state_Q ( -0.25234404394846655)) * f1( 0.8037086935451607)
w2 ( -0.3978142587074145 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.25234404394846655) - present_state_Q (-0.25234404394846655)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.1585189461405363 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33067371109842725) - present_state_Q ( -0.32872032186889494)) * f1( 0.8113866297487067)
w2 ( -0.4343359046163282 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.33067371109842725) - present_state_Q (-0.32872032186889494)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.13845277478464052 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39384761138183955) - present_state_Q ( -0.3924098886900058)) * f1( 0.8124782556616638)
w2 ( -0.4639728893101096 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.39384761138183955) - present_state_Q (-0.3924098886900058)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.12217389963404837 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4455170948746763) - present_state_Q ( -0.4442464410261233)) * f1( 0.8127032941089966)
w2 ( -0.48800952152547095 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4455170948746763) - present_state_Q (-0.4442464410261233)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.10901109855842664 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48781369433962035) - present_state_Q ( -0.4865092987323584)) * f1( 0.8111562894779558)
w2 ( -0.5074821700096634 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.48781369433962035) - present_state_Q (-0.4865092987323584)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.10591410156495093 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6235248475953887) - present_state_Q ( -0.6235248475953887)) * f1( 0.7976269532917094)
w2 ( -0.5129180392126444 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6235248475953887) - present_state_Q (-0.6235248475953887)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10342915975088122 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6324985413663469) - present_state_Q ( -0.6324985413663469)) * f1( 0.8080766608672033)
w2 ( -0.5172232230004847 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6324985413663469) - present_state_Q (-0.6324985413663469)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1017798191429839 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6409890800669139) - present_state_Q ( -0.6430537042247146)) * f1( 0.783713298756383)
w2 ( -0.5201695515299615 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6409890800669139) - present_state_Q (-0.6430537042247146)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10023921962686366 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.646613727011722) - present_state_Q ( -0.6456704759443003)) * f1( 0.8112305257848121)
w2 ( -0.5228282770759235 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.646613727011722) - present_state_Q (-0.6456704759443003)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09919638704900031 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6511067936208706) - present_state_Q ( -0.6520322156456075)) * f1( 0.7973662660005916)
w2 ( -0.5246592619962306 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6511067936208706) - present_state_Q (-0.6520322156456075)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09837728047344123 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.654297001119463) - present_state_Q ( -0.6551880090343554)) * f1( 0.7997766866365621)
w2 ( -0.5260930987470933 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.654297001119463) - present_state_Q (-0.6551880090343554)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10530529284723773 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7626095246753174) - present_state_Q ( -0.764570907907241)) * f1( 0.7845109126486166)
w2 ( -0.5119635058767398 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.7626095246753174) - present_state_Q (-0.764570907907241)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.11025580801686263 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7345614008416512) - present_state_Q ( -0.736659917547485)) * f1( 0.7832625466884329)
w2 ( -0.5018509014826086 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.7345614008416512) - present_state_Q (-0.736659917547485)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.11366622184991447 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7144794460844047) - present_state_Q ( -0.7134608375447636)) * f1( 0.8117541056315318)
w2 ( -0.4951288386127969 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.7144794460844047) - present_state_Q (-0.7134608375447636)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.1162940788024544 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7018046498128764) - present_state_Q ( -0.7040717116871312)) * f1( 0.7753792521556409)
w2 ( -0.4897062391398619 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.7018046498128764) - present_state_Q (-0.7040717116871312)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.11802542491695266 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6895597637318918) - present_state_Q ( -0.6906291098359534)) * f1( 0.7988443929775122)
w2 ( -0.4862385377858196 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6895597637318918) - present_state_Q (-0.6906291098359534)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.11914462998052625 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6831781388420924) - present_state_Q ( -0.6820937597822307)) * f1( 0.8124342762803118)
w2 ( -0.48403438644213614 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6831781388420924) - present_state_Q (-0.6820937597822307)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12005887928699864 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6781756380003849) - present_state_Q ( -0.6792601913946471)) * f1( 0.7989854593390405)
w2 ( -0.48220356602699876 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6781756380003849) - present_state_Q (-0.6792601913946471)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12058530394836663 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6750653516515206) - present_state_Q ( -0.6739861587524624)) * f1( 0.812430929473939)
w2 ( -0.4811668262530291 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6750653516515206) - present_state_Q (-0.6739861587524624)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12110220361694395 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6724615030458541) - present_state_Q ( -0.6737296395453451)) * f1( 0.7972553811421865)
w2 ( -0.4801294679745075 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6724615030458541) - present_state_Q (-0.6737296395453451)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12132170539027946 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6711066288074897) - present_state_Q ( -0.6698122405259812)) * f1( 0.8124947795703358)
w2 ( -0.4796972155512703 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6711066288074897) - present_state_Q (-0.6698122405259812)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12160689760849477 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6693905587032116) - present_state_Q ( -0.6705057050191949)) * f1( 0.7996082774369763)
w2 ( -0.4791265516874505 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6693905587032116) - present_state_Q (-0.6705057050191949)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.1218071731218266 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6682104020695677) - present_state_Q ( -0.6693246827498316)) * f1( 0.7999365320811704)
w2 ( -0.47872596888059055 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6682104020695677) - present_state_Q (-0.6693246827498316)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12209739353280741 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6695618523376863) - present_state_Q ( -0.6706657864204801)) * f1( 0.782349358794772)
w2 ( -0.4781324326907167 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6695618523376863) - present_state_Q (-0.6706657864204801)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.955308966528274 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.0337395677419046) - present_state_Q ( 1.0337395677419046)) * f1( 0.4337395677419044)
w2 ( 0.9381780633419371 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 1.0337395677419046) - present_state_Q (1.0337395677419046)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.8993364688968438 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.193247986980295) - present_state_Q ( 1.2024787930422707)) * f1( 0.4730787192452728)
w2 ( 0.8435257437943978 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 1.193247986980295) - present_state_Q (1.2024787930422707)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.8137588271966731 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.137398861187122) - present_state_Q ( 1.1462583982335353)) * f1( 0.5242062559481195)
w2 ( 0.7129242628252119 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.137398861187122) - present_state_Q (1.1462583982335353)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.739755497571022 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.0264750872975577) - present_state_Q ( 1.025555288752947)) * f1( 0.5593990053059773)
w2 ( 0.6070916404233566 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 1.0264750872975577) - present_state_Q (1.025555288752947)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.6419884242276621 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.064574514844906) - present_state_Q ( 1.0697336785634664)) * f1( 0.6253985805569395)
w2 ( 0.45076401771545904 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.064574514844906) - present_state_Q (1.0697336785634664)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5470604729125703 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8845190205619501) - present_state_Q ( 0.8866344648383895)) * f1( 0.6789381718950775)
w2 ( 0.3109457614372396 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.8845190205619501) - present_state_Q (0.8866344648383895)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.4594632871089686 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7050745038644686) - present_state_Q ( 0.653752623377333)) * f1( 0.7403130627795632)
w2 ( 0.21628614759796871 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.7050745038644686) - present_state_Q (0.653752623377333)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3709651791589526 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5739429275205301) - present_state_Q ( 0.5788614649209114)) * f1( 0.7891279401327936)
w2 ( 0.10413943038108288 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.5739429275205301) - present_state_Q (0.5788614649209114)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2951473019801656 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3977388223217705) - present_state_Q ( 0.3977388223217705)) * f1( 0.7914473067427307)
w2 ( 0.008342936372123536 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.3977388223217705) - present_state_Q (0.3977388223217705)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.23110494338833604 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24041556054082852) - present_state_Q ( 0.2399969037607806)) * f1( 0.7848757750264802)
w2 ( -0.07325259839854624 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.24041556054082852) - present_state_Q (0.2399969037607806)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.1774302423044042 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09549349135680248) - present_state_Q ( 0.09347007695479163)) * f1( 0.7848088075220337)
w2 ( -0.15532308573683962 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.09549349135680248) - present_state_Q (0.09347007695479163)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.13611765741665335 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07688477432626586) - present_state_Q ( -0.07884583550018442)) * f1( 0.7811886109786965)
w2 ( -0.22936105560738151 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07688477432626586) - present_state_Q (-0.07884583550018442)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10381678763929233 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21315092196410976) - present_state_Q ( -0.21333698201857793)) * f1( 0.7917304622858672)
w2 ( -0.28647799103227817 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.21315092196410976) - present_state_Q (-0.21333698201857793)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.07902822548015069 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31885722020735496) - present_state_Q ( -0.31885722020735496)) * f1( 0.79189473212634)
w2 ( -0.33030198128615146 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.31885722020735496) - present_state_Q (-0.31885722020735496)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06010894004270221 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40026920829237655) - present_state_Q ( -0.4001044191287213)) * f1( 0.7885581928894904)
w2 ( -0.36389113152422375 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.40026920829237655) - present_state_Q (-0.4001044191287213)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04550150256917161 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4619387021839014) - present_state_Q ( -0.4618239060032674)) * f1( 0.7922894347631727)
w2 ( -0.38970292651434096 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4619387021839014) - present_state_Q (-0.4618239060032674)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03427512606674517 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5095712553216389) - present_state_Q ( -0.5094785014747377)) * f1( 0.7935033703656647)
w2 ( -0.4095099338823806 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5095712553216389) - present_state_Q (-0.5094785014747377)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.025972814578861043 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5462146632197319) - present_state_Q ( -0.5468978910540625)) * f1( 0.7707051559731575)
w2 ( -0.4245912344198881 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5462146632197319) - present_state_Q (-0.5468978910540625)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.019360342081459616 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5739155692115677) - present_state_Q ( -0.5738658518295946)) * f1( 0.7916691622240998)
w2 ( -0.43628483313270683 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5739155692115677) - present_state_Q (-0.5738658518295946)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.014253594499657605 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5954245582498934) - present_state_Q ( -0.5953878181402165)) * f1( 0.7960059889815423)
w2 ( -0.445266482408575 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.5954245582498934) - present_state_Q (-0.5953878181402165)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.010386274561241974 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6121205224905927) - present_state_Q ( -0.6121399661645295)) * f1( 0.7880895733174925)
w2 ( -0.45213657446040917 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6121205224905927) - present_state_Q (-0.6121399661645295)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.007391076582742682 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6247660939671201) - present_state_Q ( -0.624746154321841)) * f1( 0.7938409363353083)
w2 ( -0.45741883817089113 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6247660939671201) - present_state_Q (-0.624746154321841)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.005097503066108831 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6345412612812085) - present_state_Q ( -0.6345262830531001)) * f1( 0.7928601903314246)
w2 ( -0.461468736201394 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6345412612812085) - present_state_Q (-0.6345262830531001)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0033737494892498937 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6420285522123184) - present_state_Q ( -0.6420837241971039)) * f1( 0.7793043836028823)
w2 ( -0.4645654145447719 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6420285522123184) - present_state_Q (-0.6420837241971039)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.002030261644661557 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6477384187620586) - present_state_Q ( -0.647731910347131)) * f1( 0.7883424729739364)
w2 ( -0.46695128495884236 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6477384187620586) - present_state_Q (-0.647731910347131)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0012681822540408527 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6521317110059768) - present_state_Q ( -0.65251348493322)) * f1( 0.6000773409490232)
w2 ( -0.4687292410222752 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6521317110059768) - present_state_Q (-0.65251348493322)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0004547004831141941 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6552195738689763) - present_state_Q ( -0.6552195738689763)) * f1( 0.7896054049158512)
w2 ( -0.47017157471478416 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6552195738689763) - present_state_Q (-0.6552195738689763)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0001704995086631969 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.657879983509643) - present_state_Q ( -0.6578806906557961)) * f1( 0.7906610136841252)
w2 ( -0.4712785977921077 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.657879983509643) - present_state_Q (-0.6578806906557961)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0005011269512546949 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6598823608001683) - present_state_Q ( -0.6598823608001683)) * f1( 0.5414906584854131)
w2 ( -0.47213342033128647 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.6598823608001683) - present_state_Q (-0.6598823608001683)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.004333197590415997 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4723670080025103) - present_state_Q ( -0.5667988419651777)) * f1( 0.4764013730174538)
w2 ( -0.48178596339149526 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4723670080025103) - present_state_Q (-0.5667988419651777)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.017155185175599576 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19440479733811333) - present_state_Q ( -0.2907619900164124)) * f1( 0.39010729288089563)
w2 ( -0.5015066727745392 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.19440479733811333) - present_state_Q (-0.2907619900164124)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.027566312959393977 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20629003928152598) - present_state_Q ( -0.30659137383643387)) * f1( 0.33152484881361816)
w2 ( -0.5203489305800423 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.20629003928152598) - present_state_Q (-0.30659137383643387)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.02492376144012535 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4241640786841792) - present_state_Q ( -0.5287833395355693)) * f1( 0.3059679750408098)
w2 ( -0.5117122374133272 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4241640786841792) - present_state_Q (-0.5287833395355693)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.012954921716495683 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31268935624279615) - present_state_Q ( -0.4155298678175108)) * f1( 0.24715683070741343)
w2 ( -0.47297136283786867 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.31268935624279615) - present_state_Q (-0.4155298678175108)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0024072828351770024 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28607404630711797) - present_state_Q ( -0.286233274958605)) * f1( 0.18915261006661144)
w2 ( -0.43951381061819506 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.28607404630711797) - present_state_Q (-0.286233274958605)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.005483285425476051 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2640119541398866) - present_state_Q ( -0.35191471626352555)) * f1( 0.12614544686318743)
w2 ( -0.3894727289502321 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2640119541398866) - present_state_Q (-0.35191471626352555)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.009363789857260683 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15524483671051495) - present_state_Q ( -0.15530526157971208)) * f1( 0.08823724516196536)
w2 ( -0.3718814978338857 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15524483671051495) - present_state_Q (-0.15530526157971208)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.01866767202785053 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.295966989095269) - present_state_Q ( -0.295966989095269)) * f1( 0.16427207309088585)
w2 ( -0.3265718746190263 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.295966989095269) - present_state_Q (-0.295966989095269)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0007826409273456909 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12170924919385501) - present_state_Q ( -0.1873952699488082)) * f1( 0.4578961324076699)
w2 ( -0.35205841391726095 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12170924919385501) - present_state_Q (-0.1873952699488082)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.00040277062931107373 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14116634579051396) - present_state_Q ( -0.14116634579051396)) * f1( 0.4382344592848872)
w2 ( -0.3509764254688024 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.14116634579051396) - present_state_Q (-0.14116634579051396)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.00014691755292675548 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21037815794409329) - present_state_Q ( -0.21037815794409329)) * f1( 0.5156715064935513)
w2 ( -0.35161600493982137 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21037815794409329) - present_state_Q (-0.21037815794409329)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01618977078780189 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2110362263532344) - present_state_Q ( -0.2813594273411987)) * f1( 0.4534746734772659)
w2 ( -0.32279554056335136 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2110362263532344) - present_state_Q (-0.2813594273411987)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.014414459936973437 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18507933441621738) - present_state_Q ( -0.18507933441621738)) * f1( 0.531075457119602)
w2 ( -0.3248012565048756 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18507933441621738) - present_state_Q (-0.18507933441621738)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.018305758245013824 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1866278419811691) - present_state_Q ( -0.1866278419811691)) * f1( 0.5725439564050107)
w2 ( -0.3207233530378925 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1866278419811691) - present_state_Q (-0.1866278419811691)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0027574533502160263 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18029005279302918) - present_state_Q ( -0.18045366447040329)) * f1( 0.6544578592146274)
w2 ( -0.3349778734864265 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18029005279302918) - present_state_Q (-0.18045366447040329)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.02637082649671979 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1990713586422618) - present_state_Q ( -0.19907811350223567)) * f1( 0.6921642353335633)
w2 ( -0.3602276148281459 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1990713586422618) - present_state_Q (-0.19907811350223567)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.038021553663306215 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23222241064875204) - present_state_Q ( -0.23222241064875204)) * f1( 0.6099862571188404)
w2 ( -0.3716876046531133 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23222241064875204) - present_state_Q (-0.23222241064875204)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0586195898554223 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24358018157631964) - present_state_Q ( -0.24358018157631964)) * f1( 0.5409463002639211)
w2 ( -0.39453427484799203 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.24358018157631964) - present_state_Q (-0.24358018157631964)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07696844739059408 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2661556360610351) - present_state_Q ( -0.2665968600090401)) * f1( 0.5096640077818847)
w2 ( -0.41613539706381586 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2661556360610351) - present_state_Q (-0.2665968600090401)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07427155318580336 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2859006713637632) - present_state_Q ( -0.2859006713637632)) * f1( 0.470575077884445)
w2 ( -0.41269676081017265 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2859006713637632) - present_state_Q (-0.2859006713637632)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0804604198079677 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2783713681339116) - present_state_Q ( -0.2783713681339116)) * f1( 0.4140658210132373)
w2 ( -0.42166470693094144 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2783713681339116) - present_state_Q (-0.2783713681339116)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.060925970552172824 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28140736891552737) - present_state_Q ( -0.28140736891552737)) * f1( 0.35307477670094495)
w2 ( -0.3884687090095029 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.28140736891552737) - present_state_Q (-0.28140736891552737)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.045784471522509515 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2513241809890567) - present_state_Q ( -0.2506360153044324)) * f1( 0.28813311859673923)
w2 ( -0.3569384931771713 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2513241809890567) - present_state_Q (-0.2506360153044324)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.030499188363514857 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2978699221657461) - present_state_Q ( -0.2978699221657461)) * f1( 0.26906781304557575)
w2 ( -0.3114918587812376 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2978699221657461) - present_state_Q (-0.2978699221657461)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.02043876989170371 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2549884001094047) - present_state_Q ( -0.2549884001094047)) * f1( 0.19000220646352886)
w2 ( -0.2691326939733604 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2549884001094047) - present_state_Q (-0.2549884001094047)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.013186083327609708 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21810833341019842) - present_state_Q ( -0.218291888275788)) * f1( 0.14608183921634071)
w2 ( -0.22941420957857897 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.21810833341019842) - present_state_Q (-0.218291888275788)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.006113460398129617 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04859597330971914) - present_state_Q ( -0.04859597330971914)) * f1( 0.20575718555656677)
w2 ( -0.22253948205900403 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04859597330971914) - present_state_Q (-0.04859597330971914)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0001565173920295383 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09002181083224173) - present_state_Q ( -0.09002181083224173)) * f1( 0.1645578679053693)
w2 ( -0.20729869686904334 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09002181083224173) - present_state_Q (-0.09002181083224173)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.01038971736866489 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12434033370108157) - present_state_Q ( -0.12434033370108157)) * f1( 0.24843514091464028)
w2 ( -0.18258431884918494 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.12434033370108157) - present_state_Q (-0.12434033370108157)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.022405995036467563 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14315621649108243) - present_state_Q ( -0.14315621649108243)) * f1( 0.28020382893626516)
w2 ( -0.14827707126182701 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.14315621649108243) - present_state_Q (-0.14315621649108243)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.03545444843837541 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11131593431396723) - present_state_Q ( -0.11131593431396723)) * f1( 0.3260610690845788)
w2 ( -0.11626232399122137 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11131593431396723) - present_state_Q (-0.11131593431396723)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04965714205371041 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07945575777471017) - present_state_Q ( -0.07945575777471017)) * f1( 0.3822962143051181)
w2 ( -0.08654150943144223 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07945575777471017) - present_state_Q (-0.07945575777471017)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06482572167159135 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030051271652678773) - present_state_Q ( -0.04735957353896722)) * f1( 0.4404932120847289)
w2 ( -0.05899315372154628 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.030051271652678773) - present_state_Q (-0.04735957353896722)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.07982816739316057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015004022954970643) - present_state_Q ( -0.01630049205484873)) * f1( 0.47657056683299565)
w2 ( -0.03380914654079815 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.015004022954970643) - present_state_Q (-0.01630049205484873)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.07363888727487172 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023867618735459988) - present_state_Q ( 0.016318792188808044)) * f1( 0.5432432039666495)
w2 ( -0.04292370896601912 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.023867618735459988) - present_state_Q (0.016318792188808044)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.03829618141212222 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00997461274783673) - present_state_Q ( 0.008501835978368982)) * f1( 0.5817687466035778)
w2 ( -0.09152405894230593 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.00997461274783673) - present_state_Q (0.008501835978368982)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.004548481625946947 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049545265176961374) - present_state_Q ( -0.049933498194775376)) * f1( 0.6080436247280396)
w2 ( -0.1359257412081396 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.049545265176961374) - present_state_Q (-0.049933498194775376)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.02642424139299819 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10594895529886843) - present_state_Q ( -0.10594895529886843)) * f1( 0.6137515543029306)
w2 ( -0.17629741642662108 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.10594895529886843) - present_state_Q (-0.10594895529886843)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0506655727707529 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1549465517469525) - present_state_Q ( -0.1549465517469525)) * f1( 0.5263582934623466)
w2 ( -0.21314126470084052 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1549465517469525) - present_state_Q (-0.1549465517469525)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0706372326233485 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19414749951783247) - present_state_Q ( -0.1943163371842305)) * f1( 0.469812618743328)
w2 ( -0.24714913772224475 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.19414749951783247) - present_state_Q (-0.1943163371842305)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0860658295645824 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2251455400260583) - present_state_Q ( -0.2251455400260583)) * f1( 0.38826874765188635)
w2 ( -0.27893865884036856 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2251455400260583) - present_state_Q (-0.2251455400260583)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.08529069544926605 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13857019643533539) - present_state_Q ( -0.13857019643533539)) * f1( 0.3136521548186732)
w2 ( -0.2779501317686965 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13857019643533539) - present_state_Q (-0.13857019643533539)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07665060476078425 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07561223316337697) - present_state_Q ( -0.07561223316337697)) * f1( 0.23475253313589853)
w2 ( -0.2705891115717557 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07561223316337697) - present_state_Q (-0.07561223316337697)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06646043467375246 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12391223059656026) - present_state_Q ( -0.17908980864022364)) * f1( 0.21834585323106004)
w2 ( -0.24258719643692161 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.12391223059656026) - present_state_Q (-0.17908980864022364)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.059567406141830426 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10732698383703673) - present_state_Q ( -0.15584442312442107)) * f1( 0.15486063720153165)
w2 ( -0.21588049295247858 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.10732698383703673) - present_state_Q (-0.15584442312442107)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05623414036171454 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04831125437914869) - present_state_Q ( -0.0914873529696444)) * f1( 0.0862074768947657)
w2 ( -0.2004142438512094 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04831125437914869) - present_state_Q (-0.0914873529696444)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.04394668315532009 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021629040289556443) - present_state_Q ( -0.021629040289556443)) * f1( 0.3846247164166126)
w2 ( -0.2004142438512094 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.021629040289556443) - present_state_Q (-0.021629040289556443)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.02675653869258487 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061350232925301146) - present_state_Q ( -0.061350232925301146)) * f1( 0.4839360476851978)
w2 ( -0.19330993965855398 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.061350232925301146) - present_state_Q (-0.061350232925301146)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.012819645104566372 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04895484821424523) - present_state_Q ( -0.04948368840390913)) * f1( 0.40445068760696573)
w2 ( -0.1864181755869043 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04895484821424523) - present_state_Q (-0.04948368840390913)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0015822970816183057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04155333478990335) - present_state_Q ( -0.04155333478990335)) * f1( 0.3330591165118617)
w2 ( -0.17967021556068602 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04155333478990335) - present_state_Q (-0.04155333478990335)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.009734207450827995 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036441243600803315) - present_state_Q ( -0.03647204101435075)) * f1( 0.3400106771753006)
w2 ( -0.17301365722760062 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.036441243600803315) - present_state_Q (-0.03647204101435075)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.022724970369866974 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030743510103221093) - present_state_Q ( -0.030743510103221093)) * f1( 0.39645973868892276)
w2 ( -0.16646027404574265 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.030743510103221093) - present_state_Q (-0.030743510103221093)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03867414528193856 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02174522222579185) - present_state_Q ( -0.021957994135773956)) * f1( 0.4987491947801789)
w2 ( -0.16006460460747876 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02174522222579185) - present_state_Q (-0.021957994135773956)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.055893431281286224 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010492283874327456) - present_state_Q ( -0.010492283874327456)) * f1( 0.5564605730852125)
w2 ( -0.15387574349774086 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.010492283874327456) - present_state_Q (-0.010492283874327456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07429956195914578 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003926325865118777) - present_state_Q ( 0.003926325865118777)) * f1( 0.62085067545827)
w2 ( -0.147946417363313 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.003926325865118777) - present_state_Q (0.003926325865118777)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09314190557155658 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020065740154630286) - present_state_Q ( 0.020065740154630286)) * f1( 0.6683084303322825)
w2 ( -0.14230760068609632 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.020065740154630286) - present_state_Q (0.020065740154630286)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11170225987231336 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03617937309972953) - present_state_Q ( 0.03617937309972953)) * f1( 0.6940044101555151)
w2 ( -0.13695882940189147 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03617937309972953) - present_state_Q (0.03617937309972953)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12943219394881883 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050081484353657876) - present_state_Q ( 0.050390496657296224)) * f1( 0.6963356213794356)
w2 ( -0.1318664763663301 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.050081484353657876) - present_state_Q (0.050390496657296224)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14632423332312924 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06339689166475651) - present_state_Q ( 0.06375495192658837)) * f1( 0.6963356213794356)
w2 ( -0.12701478162153235 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06339689166475651) - present_state_Q (0.06375495192658837)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15545461964622156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07608302933529155) - present_state_Q ( 0.07648781960962424)) * f1( 0.6963356213794356)
w2 ( -0.12439237195505425 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07608302933529155) - present_state_Q (0.07648781960962424)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1571901590180976 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08294006626927325) - present_state_Q ( 0.08337011477664466)) * f1( 0.6963356213794356)
w2 ( -0.12389389411804859 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.08294006626927325) - present_state_Q (0.08337011477664466)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14491696652345915 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08424347854036571) - present_state_Q ( 0.08467832823098956)) * f1( 0.6963356213794356)
w2 ( -0.12741897372558764 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.08424347854036571) - present_state_Q (0.08467832823098956)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1332237898893368 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07502615396552782) - present_state_Q ( 0.07542705118741827)) * f1( 0.6963356213794356)
w2 ( -0.13077746244140495 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.07502615396552782) - present_state_Q (0.07542705118741827)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14297328692667183 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06624442872209993) - present_state_Q ( 0.06661297802683373)) * f1( 0.6963356213794356)
w2 ( -0.12797723314449744 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06624442872209993) - present_state_Q (0.06661297802683373)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15922539103562167 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07356642572278208) - present_state_Q ( 0.07396194596384487)) * f1( 0.6963356213794356)
w2 ( -0.12330933921232877 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07356642572278208) - present_state_Q (0.07396194596384487)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17470944085437212 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08577196382231796) - present_state_Q ( 0.08621244376370747)) * f1( 0.6963356213794356)
w2 ( -0.11886204415995828 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08577196382231796) - present_state_Q (0.08621244376370747)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18953574532048315 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0974006833241697) - present_state_Q ( 0.09934108150980406)) * f1( 0.7046756588524379)
w2 ( -0.11465406442350602 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0974006833241697) - present_state_Q (0.09934108150980406)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2034801460813328 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10978197637942709) - present_state_Q ( 0.1060212856830798)) * f1( 0.6803576726370946)
w2 ( -0.11055492618440876 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10978197637942709) - present_state_Q (0.1060212856830798)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.21687222382537058 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11901658275536332) - present_state_Q ( 0.11957948872304146)) * f1( 0.6963356213794356)
w2 ( -0.10670848279335887 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11901658275536332) - present_state_Q (0.11957948872304146)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.229631408579243 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12907420446643675) - present_state_Q ( 0.1296741581787077)) * f1( 0.6963356213794356)
w2 ( -0.10304381754800016 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12907420446643675) - present_state_Q (0.1296741581787077)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19682365363790358 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11954816000064442) - present_state_Q ( 0.09434721155909645)) * f1( 0.6801051435174338)
w2 ( -0.1319873612815421 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11954816000064442) - present_state_Q (0.09434721155909645)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.205195820670676 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005157105797039388) - present_state_Q ( 0.005157105797039388)) * f1( 0.42855378917588055)
w2 ( -0.12026584499458222 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.005157105797039388) - present_state_Q (0.005157105797039388)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17337762733322903 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03171494010342428) - present_state_Q ( 0.03171494010342428)) * f1( 0.5062210661048715)
w2 ( -0.15797845176016712 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.03171494010342428) - present_state_Q (0.03171494010342428)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.13617664017134254 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03844457424772292) - present_state_Q ( 0.03844457424772292)) * f1( 0.5862114767348101)
w2 ( -0.18336245643308516 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.03844457424772292) - present_state_Q (0.03844457424772292)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09329843492004836 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05342430315284132) - present_state_Q ( 0.05342430315284132)) * f1( 0.6616171050049053)
w2 ( -0.1963240938898363 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.05342430315284132) - present_state_Q (0.05342430315284132)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.055999730180649965 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05462005941851651) - present_state_Q ( -0.05462005941851651)) * f1( 0.6771217220259083)
w2 ( -0.2293746106812364 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.05462005941851651) - present_state_Q (-0.05462005941851651)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.021305500100966428 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09955895215011981) - present_state_Q ( -0.09955895215011981)) * f1( 0.6797499583627501)
w2 ( -0.25999842726512995 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09955895215011981) - present_state_Q (-0.09955895215011981)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.010814516678876056 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14151986648700876) - present_state_Q ( -0.14151986648700876)) * f1( 0.679598685947411)
w2 ( -0.2883563544748315 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.14151986648700876) - present_state_Q (-0.14151986648700876)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.036498546129995785 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17934687431065877) - present_state_Q ( -0.17934687431065877)) * f1( 0.585607458364754)
w2 ( -0.31467162326205594 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.17934687431065877) - present_state_Q (-0.17934687431065877)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.058922303556323996 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2086578702746028) - present_state_Q ( -0.2086578702746028)) * f1( 0.5439914304162321)
w2 ( -0.3394040982672274 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2086578702746028) - present_state_Q (-0.2086578702746028)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07771125506645446 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2319382419367427) - present_state_Q ( -0.2319382419367427)) * f1( 0.48022194090491066)
w2 ( -0.3628794332026433 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2319382419367427) - present_state_Q (-0.2319382419367427)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09331723601385537 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25007365414722826) - present_state_Q ( -0.25007365414722826)) * f1( 0.41623306943095606)
w2 ( -0.385375455878693 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.25007365414722826) - present_state_Q (-0.25007365414722826)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.107758963221604 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2688674351295744) - present_state_Q ( -0.2688674351295744)) * f1( 0.40337844550785457)
w2 ( -0.40685661438169596 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2688674351295744) - present_state_Q (-0.2688674351295744)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12019350792393639 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28290975630608306) - present_state_Q ( -0.28290975630608306)) * f1( 0.3600237652368905)
w2 ( -0.4275794875411675 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.28290975630608306) - present_state_Q (-0.28290975630608306)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13643043514102607 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21952704345040344) - present_state_Q ( -0.21952704345040344)) * f1( 0.4034764378840353)
w2 ( -0.44367651397695296 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.21952704345040344) - present_state_Q (-0.21952704345040344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.15000893367170526 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22396604694346567) - present_state_Q ( -0.22396604694346567)) * f1( 0.34079962659814755)
w2 ( -0.4596137362869882 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.22396604694346567) - present_state_Q (-0.22396604694346567)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16235906558398383 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2311059016801315) - present_state_Q ( -0.2311059016801315)) * f1( 0.3150506173770003)
w2 ( -0.4752939238265035 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2311059016801315) - present_state_Q (-0.2311059016801315)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17312138316604478 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23683879316567433) - present_state_Q ( -0.2350829780062247)) * f1( 0.2769504019617799)
w2 ( -0.4908379598789172 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.23683879316567433) - present_state_Q (-0.2350829780062247)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18215299747976216 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23510252870030585) - present_state_Q ( -0.23676389173931914)) * f1( 0.23352810062160895)
w2 ( -0.5063078143241456 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.23510252870030585) - present_state_Q (-0.23676389173931914)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.20200261339283454 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3141761679770536) - present_state_Q ( -0.3178192202706217)) * f1( 0.632962927517969)
w2 ( -0.518851750185229 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.3141761679770536) - present_state_Q (-0.3178192202706217)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21957263768180885 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32193550329147336) - present_state_Q ( -0.32193550329147336)) * f1( 0.5663035804141707)
w2 ( -0.5312620720667359 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.32193550329147336) - present_state_Q (-0.32193550329147336)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23537860627023685 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3255507276723324) - present_state_Q ( -0.3255507276723324)) * f1( 0.5148451102065695)
w2 ( -0.5435422458705319 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.3255507276723324) - present_state_Q (-0.3255507276723324)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24926420839155644 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3231255282227121) - present_state_Q ( -0.3231255282227121)) * f1( 0.44910041549458357)
w2 ( -0.5559097268545142 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.3231255282227121) - present_state_Q (-0.3231255282227121)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.26229711672491207 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32938376227032473) - present_state_Q ( -0.32938376227032473)) * f1( 0.4293431143568233)
w2 ( -0.5680519114127826 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.32938376227032473) - present_state_Q (-0.32938376227032473)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2744860401645568 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3340073374858372) - present_state_Q ( -0.3340073374858372)) * f1( 0.407120651016222)
w2 ( -0.5800276472632924 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.3340073374858372) - present_state_Q (-0.3340073374858372)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2853770224355406 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33095149983899275) - present_state_Q ( -0.33095149983899275)) * f1( 0.3604570960124606)
w2 ( -0.5921133932690886 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.33095149983899275) - present_state_Q (-0.33095149983899275)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2799571138855035 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31936744857631144) - present_state_Q ( -0.31936744857631144)) * f1( 0.2891686603371006)
w2 ( -0.5846161651203414 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.31936744857631144) - present_state_Q (-0.31936744857631144)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.27166456793620225 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1784210787747404) - present_state_Q ( -0.2953443117988087)) * f1( 0.21966880890129273)
w2 ( -0.569516076963488 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1784210787747404) - present_state_Q (-0.2953443117988087)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2609537302303478 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2797951921553507) - present_state_Q ( -0.28047236352242566)) * f1( 0.1938638267666857)
w2 ( -0.5474163631912125 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2797951921553507) - present_state_Q (-0.28047236352242566)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.25423105594430034 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14212772246083893) - present_state_Q ( -0.25161099509908147)) * f1( 0.12509669738685364)
w2 ( -0.5259204342770926 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.14212772246083893) - present_state_Q (-0.25161099509908147)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.22181873306195588 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2594662175731708) - present_state_Q ( -0.2595965366915825)) * f1( 0.6073705246694735)
w2 ( -0.5152474359784073 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2594662175731708) - present_state_Q (-0.2595965366915825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19390044391899225 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2260565164733082) - present_state_Q ( -0.2260565164733082)) * f1( 0.5545385079954894)
w2 ( -0.5051784186818877 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2260565164733082) - present_state_Q (-0.2260565164733082)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1705161919046851 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1961800050083487) - present_state_Q ( -0.1961800050083487)) * f1( 0.49068645408424416)
w2 ( -0.49564717859173746 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1961800050083487) - present_state_Q (-0.1961800050083487)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14918772710881797 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17467882198348567) - present_state_Q ( -0.17808537345791103)) * f1( 0.46304070515308154)
w2 ( -0.4864348287665462 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.17467882198348567) - present_state_Q (-0.17808537345791103)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13218645745594518 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15499782114365968) - present_state_Q ( -0.15499782114365968)) * f1( 0.3868337999965369)
w2 ( -0.47764486798596034 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15499782114365968) - present_state_Q (-0.15499782114365968)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11884239580647601 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13718307068114566) - present_state_Q ( -0.13718307068114566)) * f1( 0.31511622208225054)
w2 ( -0.4691755727136997 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13718307068114566) - present_state_Q (-0.13718307068114566)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10840447050802143 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02950742740591444) - present_state_Q ( -0.12334254194865439)) * f1( 0.24829041189950926)
w2 ( -0.4607677367295384 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02950742740591444) - present_state_Q (-0.12334254194865439)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10008506941813 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1145280371503824) - present_state_Q ( -0.1145280371503824)) * f1( 0.20639822047577908)
w2 ( -0.45270623206083155 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1145280371503824) - present_state_Q (-0.1145280371503824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09380665299280984 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10641828345301503) - present_state_Q ( -0.10641828345301503)) * f1( 0.15863542017959226)
w2 ( -0.4447907029586773 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.10641828345301503) - present_state_Q (-0.10641828345301503)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08921637210469742 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09897439822310501) - present_state_Q ( -0.09999633854164788)) * f1( 0.11766967051642375)
w2 ( -0.43698872498429053 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09897439822310501) - present_state_Q (-0.09999633854164788)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08563713506771475 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09566827964398904) - present_state_Q ( -0.09566827964398904)) * f1( 0.09270198341426918)
w2 ( -0.4292666959506987 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09566827964398904) - present_state_Q (-0.09566827964398904)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06672927698966852 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21304092760206841) - present_state_Q ( -0.21304092760206841)) * f1( 0.4826673520675955)
w2 ( -0.41359722255702425 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21304092760206841) - present_state_Q (-0.21304092760206841)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06290673061398888 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19804164568854937) - present_state_Q ( -0.19804164568854937)) * f1( 0.4885824953683742)
w2 ( -0.41046772331223647 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.19804164568854937) - present_state_Q (-0.19804164568854937)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07848706889773623 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1926343158394889) - present_state_Q ( -0.27472786050193626)) * f1( 0.45221276383211617)
w2 ( -0.4311398575771572 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1926343158394889) - present_state_Q (-0.27472786050193626)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08402989554257016 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28997714002460223) - present_state_Q ( -0.28997714002460223)) * f1( 0.3987054927364025)
w2 ( -0.4394810920158287 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.28997714002460223) - present_state_Q (-0.28997714002460223)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06564970751048134 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2967554850907493) - present_state_Q ( -0.2967554850907493)) * f1( 0.39351268578574095)
w2 ( -0.41145629582092824 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2967554850907493) - present_state_Q (-0.2967554850907493)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07729709939960162 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26819548889096934) - present_state_Q ( -0.26819548889096934)) * f1( 0.3247799907563675)
w2 ( -0.4329737394208159 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.26819548889096934) - present_state_Q (-0.26819548889096934)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07336928985262627 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1937625385785487) - present_state_Q ( -0.36695203434687507)) * f1( 0.26615543105784834)
w2 ( -0.4211676769816943 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1937625385785487) - present_state_Q (-0.36695203434687507)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.06384682453376349 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18348746138144598) - present_state_Q ( -0.18348746138144598)) * f1( 0.2047231289677065)
w2 ( -0.4025621283719622 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.18348746138144598) - present_state_Q (-0.18348746138144598)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0532148535850539 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2537118574164279) - present_state_Q ( -0.25436941051292256)) * f1( 0.20098311205687833)
w2 ( -0.3708222348856854 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2537118574164279) - present_state_Q (-0.25436941051292256)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04476192618602083 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30449378954201123) - present_state_Q ( -0.30449378954201123)) * f1( 0.14725215058495975)
w2 ( -0.32489868203866057 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.30449378954201123) - present_state_Q (-0.30449378954201123)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.035034371867096355 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08093842137104479) - present_state_Q ( -0.08093842137104479)) * f1( 0.35652364236945133)
w2 ( -0.31944179045398174 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08093842137104479) - present_state_Q (-0.08093842137104479)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04443911574438045 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20692977764616427) - present_state_Q ( -0.20709041818997603)) * f1( 0.4402917219724482)
w2 ( -0.33225794402846015 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.20692977764616427) - present_state_Q (-0.20709041818997603)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.040874403919253365 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21561002585116273) - present_state_Q ( -0.21610942389316984)) * f1( 0.37702499690742486)
w2 ( -0.3265850387499769 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.21561002585116273) - present_state_Q (-0.21610942389316984)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.028603054183392353 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14235011934805786) - present_state_Q ( -0.14235011934805786)) * f1( 0.2866366900716655)
w2 ( -0.30946043445344684 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.14235011934805786) - present_state_Q (-0.14235011934805786)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.017270963962135392 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13054562114076437) - present_state_Q ( -0.19243770803145377)) * f1( 0.23638899944158798)
w2 ( -0.2806974456984042 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13054562114076437) - present_state_Q (-0.19243770803145377)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.009326020663488718 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05948885960748175) - present_state_Q ( -0.1156283487471626)) * f1( 0.19393071950957802)
w2 ( -0.2643102671869476 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05948885960748175) - present_state_Q (-0.1156283487471626)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0030498737762276905 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.107027161967949) - present_state_Q ( -0.15988921540533854)) * f1( 0.1397225183374713)
w2 ( -0.237359077234435 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.107027161967949) - present_state_Q (-0.15988921540533854)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0008375069031109584 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0003947332367530451) - present_state_Q ( -0.0003947332367530451)) * f1( 0.1294260896401032)
w2 ( -0.237359077234435 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0003947332367530451) - present_state_Q (-0.0003947332367530451)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.006986974861748304 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04731345593182209) - present_state_Q ( -0.04732148356937755)) * f1( 0.17949926973860053)
w2 ( -0.2305072744749111 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04731345593182209) - present_state_Q (-0.04732148356937755)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.012555484775177417 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04517062355124933) - present_state_Q ( -0.09119672709326448)) * f1( 0.14400834647459734)
w2 ( -0.2150400878853855 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04517062355124933) - present_state_Q (-0.09119672709326448)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.018454719305853934 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040935274214211816) - present_state_Q ( -0.08406674647746498)) * f1( 0.1552539556690813)
w2 ( -0.19984115912314376 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.040935274214211816) - present_state_Q (-0.08406674647746498)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.025753665950079843 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03573957626200248) - present_state_Q ( -0.035915168066171885)) * f1( 0.21962207559403066)
w2 ( -0.19319433491434432 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03573957626200248) - present_state_Q (-0.035915168066171885)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03453362150537643 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03184675703318179) - present_state_Q ( -0.03175709514045415)) * f1( 0.2672152328042983)
w2 ( -0.1866228865256016 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03184675703318179) - present_state_Q (-0.03175709514045415)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0452975299744663 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02582495112431952) - present_state_Q ( -0.02582495112431952)) * f1( 0.3329979793463152)
w2 ( -0.18015803740536385 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02582495112431952) - present_state_Q (-0.02582495112431952)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.058471601731393444 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01869490384172184) - present_state_Q ( -0.017336703639350932)) * f1( 0.4127135376313001)
w2 ( -0.17377391352489338 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.01869490384172184) - present_state_Q (-0.017336703639350932)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07238884302186958 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008287372220983809) - present_state_Q ( -0.008287372220983809)) * f1( 0.45265410387731003)
w2 ( -0.16762474082491569 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.008287372220983809) - present_state_Q (-0.008287372220983809)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08798310740115343 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03767658144086489) - present_state_Q ( 0.00415163327588175)) * f1( 0.5204749774696954)
w2 ( -0.1616324203275516 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03767658144086489) - present_state_Q (0.00415163327588175)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1039487240248057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0563491734494599) - present_state_Q ( 0.0563491734494599)) * f1( 0.6404544589740323)
w2 ( -0.1616324203275516 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0563491734494599) - present_state_Q (0.0563491734494599)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.1217937440608087 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03827853285652806) - present_state_Q ( 0.03726076634122577)) * f1( 0.669438235625963)
w2 ( -0.15630107858866304 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03827853285652806) - present_state_Q (0.03726076634122577)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14382704685123682 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09269359038175332) - present_state_Q ( 0.0925835267329167)) * f1( 1.0168317215769072)
w2 ( -0.15196736194255786 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09269359038175332) - present_state_Q (0.0925835267329167)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16374752031582948 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1162588389870582) - present_state_Q ( 0.1162588389870582)) * f1( 1.019643485604312)
w2 ( -0.14806002104432492 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1162588389870582) - present_state_Q (0.1162588389870582)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18171821747738345 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13649383252036335) - present_state_Q ( 0.13649383252036335)) * f1( 1.014402150388905)
w2 ( -0.14451691002969147 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13649383252036335) - present_state_Q (0.13649383252036335)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19795737991691895 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15615296980494475) - present_state_Q ( 0.15615296980494475)) * f1( 1.0183698386427056)
w2 ( -0.14132766348618048 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15615296980494475) - present_state_Q (0.15615296980494475)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.21263601708676164 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17287382994829498) - present_state_Q ( 0.17269352327051696)) * f1( 1.0151632439876397)
w2 ( -0.13843578629169423 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17287382994829498) - present_state_Q (0.17269352327051696)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.22589126989218042 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.188046324538238) - present_state_Q ( 0.18832202166551093)) * f1( 1.0158635488159646)
w2 ( -0.13582613407592797 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.188046324538238) - present_state_Q (0.18832202166551093)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.23788777393191657 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20149421846077173) - present_state_Q ( 0.2017895746337317)) * f1( 1.013561974122325)
w2 ( -0.13345893713168105 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20149421846077173) - present_state_Q (0.2017895746337317)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.24872250104206214 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2146901334876182) - present_state_Q ( 0.2146901334876182)) * f1( 1.0146882158939277)
w2 ( -0.13132335953445817 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2146901334876182) - present_state_Q (0.2146901334876182)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.25852898906768645 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22582979643199746) - present_state_Q ( 0.22582979643199746)) * f1( 1.0135571461476125)
w2 ( -0.12938829587023412 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22582979643199746) - present_state_Q (0.22582979643199746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.26737292494808584 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23611522327234363) - present_state_Q ( 0.23645422501130348)) * f1( 1.0147097435045018)
w2 ( -0.1276451499239155 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23611522327234363) - present_state_Q (0.23645422501130348)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2753993451714019 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24557236648682526) - present_state_Q ( 0.24532445768697017)) * f1( 1.0130176334209726)
w2 ( -0.12606049434468128 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.24557236648682526) - present_state_Q (0.24532445768697017)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.28256510981127597 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2546993432132342) - present_state_Q ( 0.2550574801192645)) * f1( 1.0176842607006482)
w2 ( -0.1246522452606401 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2546993432132342) - present_state_Q (0.2550574801192645)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2890248939926729 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2628615377104331) - present_state_Q ( 0.2628615377104331)) * f1( 1.018497955939345)
w2 ( -0.1233837529394279 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2628615377104331) - present_state_Q (0.2628615377104331)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2949489619787502 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26799471866482244) - present_state_Q ( 0.2683722830671958)) * f1( 1.0139231593750209)
w2 ( -0.12221520916344217 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26799471866482244) - present_state_Q (0.2683722830671958)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.30029569944185674 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2747721938036239) - present_state_Q ( 0.2747721938036239)) * f1( 1.0144644470993915)
w2 ( -0.12116110865190739 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2747721938036239) - present_state_Q (0.2747721938036239)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3051054035991821 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2802578437627256) - present_state_Q ( 0.28065271701615807)) * f1( 1.015282401024099)
w2 ( -0.1202136473047051 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2802578437627256) - present_state_Q (0.28065271701615807)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.30948938373948415 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28560732607211836) - present_state_Q ( 0.28532492532837506)) * f1( 1.0139697663163427)
w2 ( -0.11934893115912837 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28560732607211836) - present_state_Q (0.28532492532837506)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.31348012719235346 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2890845873763868) - present_state_Q ( 0.2894944990088778)) * f1( 1.0125203050728262)
w2 ( -0.11856065196455315 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2890845873763868) - present_state_Q (0.2894944990088778)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3170772167629165 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2938835968985154) - present_state_Q ( 0.2938835968985154)) * f1( 1.013128743234646)
w2 ( -0.11785055670872642 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2938835968985154) - present_state_Q (0.2938835968985154)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.320285458657456 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2977383933074233) - present_state_Q ( 0.2981548983060313)) * f1( 1.0146582366664816)
w2 ( -0.1172181778882322 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2977383933074233) - present_state_Q (0.2981548983060313)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3232130201938555 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30157698631815233) - present_state_Q ( 0.3012824248655559)) * f1( 1.01386451262683)
w2 ( -0.11664067241290701 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.30157698631815233) - present_state_Q (0.3012824248655559)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3258949081917775 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3034201576175004) - present_state_Q ( 0.3038479949034207)) * f1( 1.0122616013110166)
w2 ( -0.11611079199574043 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3034201576175004) - present_state_Q (0.3038479949034207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3282843973613155 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3071426811875983) - present_state_Q ( 0.3071426811875983)) * f1( 1.013715867546904)
w2 ( -0.1156393602571172 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3071426811875983) - present_state_Q (0.3071426811875983)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3304539402750872 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30988484151470064) - present_state_Q ( 0.3095816072094839)) * f1( 1.0134794158210374)
w2 ( -0.11521122271827748 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.30988484151470064) - present_state_Q (0.3095816072094839)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3323641325762472 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31276545236468906) - present_state_Q ( 0.312462170703216)) * f1( 1.015283446060834)
w2 ( -0.11483493522761243 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.31276545236468906) - present_state_Q (0.312462170703216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3340568216105379 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3143457167217992) - present_state_Q ( 0.314777322171591)) * f1( 1.01618759701645)
w2 ( -0.11450179023760065 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3143457167217992) - present_state_Q (0.314777322171591)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3356658084309967 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31569535234435586) - present_state_Q ( 0.31569535234435586)) * f1( 1.0135871758566564)
w2 ( -0.11418430657979906 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.31569535234435586) - present_state_Q (0.31569535234435586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.33713297851896307 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31758737206639276) - present_state_Q ( 0.3172789998613015)) * f1( 1.0132573906382227)
w2 ( -0.11389471183289229 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.31758737206639276) - present_state_Q (0.3172789998613015)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3384141487206779 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.319649310330963) - present_state_Q ( 0.3193399417031952)) * f1( 1.0147891362414732)
w2 ( -0.11364221204629427 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.319649310330963) - present_state_Q (0.3193399417031952)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.33956365349233114 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3202567966978287) - present_state_Q ( 0.3206984153646273)) * f1( 1.0148123507015236)
w2 ( -0.11341566676019116 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3202567966978287) - present_state_Q (0.3206984153646273)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.33814888188330605 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0019357270822639994) - present_state_Q ( 0.001878295184666319)) * f1( 0.13913315339508142)
w2 ( -0.11748305565924876 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.0019357270822639994) - present_state_Q (0.001878295184666319)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.3451566828056897 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011261402084104574) - present_state_Q ( 0.011261402084104574)) * f1( 0.24176106992974145)
w2 ( -0.1000911713717904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.011261402084104574) - present_state_Q (0.011261402084104574)) * f2(0.6000000000000001)
============================================================================
