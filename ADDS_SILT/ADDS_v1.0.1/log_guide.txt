GUIDE learning . . .
w1 ( -2.0192327898548244 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7858578643762691) - present_state_Q ( 0.7858578643762691)) * f1( 0.007071067811865476)
w2 ( 0.9120291098014308 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.7858578643762691) - present_state_Q (0.7858578643762691)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -2.0360194412876385 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01599011181999293) - present_state_Q ( -0.01313732264125193)) * f1( 0.007071067811865476)
w2 ( 0.9120291098014308 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.01599011181999293) - present_state_Q (-0.01313732264125193)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0555049887672214 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16800899042466483) - present_state_Q ( 0.16800899042466483)) * f1( 0.007071067811865476)
w2 ( 0.36089459948360225 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.16800899042466483) - present_state_Q (0.16800899042466483)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.074914689496138 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06184817508141187) - present_state_Q ( 0.05012940543551329)) * f1( 0.007071067811865476)
w2 ( -0.18809464076512972 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.06184817508141187) - present_state_Q (0.05012940543551329)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1042563144691995 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056294755334030444) - present_state_Q ( -0.05822871366980781)) * f1( 0.010727054705146327)
w2 ( -0.7351530044925861 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.056294755334030444) - present_state_Q (-0.05822871366980781)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1287839651564093 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1721107838065454) - present_state_Q ( -0.1721107838065454)) * f1( 0.009000768694514202)
w2 ( -1.2801653588742528 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.1721107838065454) - present_state_Q (-0.1721107838065454)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.147990074392229 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27108584754928344) - present_state_Q ( -0.27108584754928344)) * f1( 0.007071067811865476)
w2 ( -1.8233961621085502 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.27108584754928344) - present_state_Q (-0.27108584754928344)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1671166720819874 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3781184521264257) - present_state_Q ( -0.39423541481148716)) * f1( 0.007071067811865476)
w2 ( -2.3643780392067577 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.3781184521264257) - present_state_Q (-0.39423541481148716)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1864855472697595 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015323828944515972) - present_state_Q ( -0.015323828944515972)) * f1( 0.007071067811865476)
w2 ( -2.3643780392067577 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.015323828944515972) - present_state_Q (-0.015323828944515972)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.20585731281022 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030454219664824568) - present_state_Q ( -0.012749292044904435)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.030454219664824568) - present_state_Q (-0.012749292044904435)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.231463043197751 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007990089174306684) - present_state_Q ( -0.03795828631870516)) * f1( 0.009355961453696788)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007990089174306684) - present_state_Q (-0.03795828631870516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2501856745864264 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015778826498122997) - present_state_Q ( -0.015778826498122997)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.015778826498122997) - present_state_Q (-0.015778826498122997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2688974493544487 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014078621692759571) - present_state_Q ( -0.030962386186890574)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.014078621692759571) - present_state_Q (-0.030962386186890574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2853758585119794 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012006093195661972) - present_state_Q ( -0.02041407716817691)) * f1( 0.006224659813323905)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.012006093195661972) - present_state_Q (-0.02041407716817691)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3318531941101224 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015520950393432554) - present_state_Q ( -0.046517125763267116)) * f1( 0.017573741381987095)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.015520950393432554) - present_state_Q (-0.046517125763267116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.349283465262266 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016488692062867783) - present_state_Q ( -0.016488692062867783)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.016488692062867783) - present_state_Q (-0.016488692062867783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.366708434333865 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013110949912532763) - present_state_Q ( -0.02364919206260699)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.013110949912532763) - present_state_Q (-0.02364919206260699)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.391496903376299 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011800522617006575) - present_state_Q ( -0.04156646903738728)) * f1( 0.01006655536136712)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.011800522617006575) - present_state_Q (-0.04156646903738728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.410218814612604 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01691043677564011) - present_state_Q ( -0.01691043677564011)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.01691043677564011) - present_state_Q (-0.01691043677564011)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.428929103100938 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015079892859925398) - present_state_Q ( -0.03316443019603749)) * f1( 0.007071067811865476)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.015079892859925398) - present_state_Q (-0.03316443019603749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.445414244512148 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00933112383698095) - present_state_Q ( -0.00933112383698095)) * f1( 0.006224659813323905)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.00933112383698095) - present_state_Q (-0.00933112383698095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.456282976883784 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020231249422323492) - present_state_Q ( -0.02924693883849051)) * f1( 0.0041068672648141525)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.020231249422323492) - present_state_Q (-0.02924693883849051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4981492230740483 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02227880334475225) - present_state_Q ( -0.029012087242851495)) * f1( 0.015819348021772005)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.02227880334475225) - present_state_Q (-0.029012087242851495)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.52014041075164 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009425388172725667) - present_state_Q ( -0.04074261802440556)) * f1( 0.008313557131833833)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.009425388172725667) - present_state_Q (-0.04074261802440556)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.525253403233342 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016115105604483582) - present_state_Q ( -0.016115105604483582)) * f1( 0.0019310709627970368)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.016115105604483582) - present_state_Q (-0.016115105604483582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.542191986996791 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0031224048203579713) - present_state_Q ( -0.0031224048203579713)) * f1( 0.0063945268826022285)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.0031224048203579713) - present_state_Q (-0.0031224048203579713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.553714660774813 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022444390623824594) - present_state_Q ( -0.022444390623824594)) * f1( 0.004352810794838981)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.022444390623824594) - present_state_Q (-0.022444390623824594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5771011165510926 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00337602752411516) - present_state_Q ( -0.00337602752411516)) * f1( 0.008828755160360328)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.00337602752411516) - present_state_Q (-0.00337602752411516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.591989145442016 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010792049395742218) - present_state_Q ( -0.026392359741404726)) * f1( 0.005826294358847279)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.010792049395742218) - present_state_Q (-0.026392359741404726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.60909762810577 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01858595157485009) - present_state_Q ( -0.02657386537778201)) * f1( 0.006695092047419749)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.01858595157485009) - present_state_Q (-0.02657386537778201)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.635273884284366 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008267208937040613) - present_state_Q ( -0.04724236602868339)) * f1( 0.010252305810968327)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.008267208937040613) - present_state_Q (-0.04724236602868339)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6527175541490875 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02276578875241539) - present_state_Q ( -0.029303304084529632)) * f1( 0.00682687895563605)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.02276578875241539) - present_state_Q (-0.029303304084529632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6811184357757227 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003632535245891363) - present_state_Q ( -0.03766440185010664)) * f1( 0.011119642728325836)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.003632535245891363) - present_state_Q (-0.03766440185010664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6954348596823907 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024868342412809993) - present_state_Q ( -0.02688258763220729)) * f1( 0.005602400824565173)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.024868342412809993) - present_state_Q (-0.02688258763220729)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.721076287857067 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005734850743037077) - present_state_Q ( -0.005734850743037077)) * f1( 0.010026631898649939)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.005734850743037077) - present_state_Q (-0.005734850743037077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7378440777818374 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029178378889229327) - present_state_Q ( -0.04076746833249966)) * f1( 0.00656514258217802)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.029178378889229327) - present_state_Q (-0.04076746833249966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.760925838434751 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00825605300138218) - present_state_Q ( -0.04663530054930891)) * f1( 0.009040087749962785)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.00825605300138218) - present_state_Q (-0.04663530054930891)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.780074697744824 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027326196607825818) - present_state_Q ( -0.032865403693621405)) * f1( 0.007495145166326631)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.027326196607825818) - present_state_Q (-0.032865403693621405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8105110666764888 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010872400018072093) - present_state_Q ( -0.010872400018072093)) * f1( 0.011903761859917887)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.010872400018072093) - present_state_Q (-0.010872400018072093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.831970805349982 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002216820186882981) - present_state_Q ( -0.002216820186882981)) * f1( 0.008390416560920806)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.002216820186882981) - present_state_Q (-0.002216820186882981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.845133705328788 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027183910097046505) - present_state_Q ( -0.027782025815036207)) * f1( 0.0053421072671323835)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.027183910097046505) - present_state_Q (-0.027782025815036207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8693248547905243 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006255966118245213) - present_state_Q ( -0.006255966118245213)) * f1( 0.009810138495267021)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.006255966118245213) - present_state_Q (-0.006255966118245213)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8858419479947313 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028602945917223045) - present_state_Q ( -0.031937254846725306)) * f1( 0.006704484417316291)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.028602945917223045) - present_state_Q (-0.031937254846725306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9132864364315343 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009037400489548228) - present_state_Q ( -0.009037400489548228)) * f1( 0.011130581744135379)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.009037400489548228) - present_state_Q (-0.009037400489548228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9321135744626354 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0012675461397891115) - present_state_Q ( -0.0012675461397891115)) * f1( 0.00763350296017075)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.0012675461397891115) - present_state_Q (-0.0012675461397891115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.943618883741131 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026784460663962735) - present_state_Q ( -0.026784460663962735)) * f1( 0.00466919962767715)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.026784460663962735) - present_state_Q (-0.026784460663962735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9661463254987352 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004464667303797687) - present_state_Q ( -0.004464667303797687)) * f1( 0.009134864657782394)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.004464667303797687) - present_state_Q (-0.004464667303797687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9810659637202845 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02856013343489209) - present_state_Q ( -0.03120073543569245)) * f1( 0.006055878460683979)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.02856013343489209) - present_state_Q (-0.03120073543569245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.006992500027622 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019446292585827026) - present_state_Q ( -0.019446292585827026)) * f1( 0.01051894681239176)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.019446292585827026) - present_state_Q (-0.019446292585827026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0402415929782785 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029330627369642766) - present_state_Q ( -0.027239971954023723)) * f1( 0.013493589297743755)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.029330627369642766) - present_state_Q (-0.027239971954023723)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0672401587980858 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00899986019735781) - present_state_Q ( -0.00899986019735781)) * f1( 0.010949715195018162)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.00899986019735781) - present_state_Q (-0.00899986019735781)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0856279835428406 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03579800507608141) - present_state_Q ( -0.011869482912227267)) * f1( 0.007457544052172038)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.03579800507608141) - present_state_Q (-0.011869482912227267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.125361240887097 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025783083882057807) - present_state_Q ( -0.03593055151059575)) * f1( 0.01613100069256893)
w2 ( -2.912294310295374 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.025783083882057807) - present_state_Q (-0.03593055151059575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.145208756638219 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04155134087018063) - present_state_Q ( -0.0028010390597458805)) * f1( 0.008355862735096899)
w2 ( -3.387350360887413 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.04155134087018063) - present_state_Q (-0.0028010390597458805)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.1475288215134056 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06229409285631632) - present_state_Q ( -0.023208966172554055)) * f1( 0.0009775086550885043)
w2 ( -3.862039738441168 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.06229409285631632) - present_state_Q (-0.023208966172554055)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.1736251656750554 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09443155596072023) - present_state_Q ( -0.05178847136111881)) * f1( 0.011006887958130434)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.09443155596072023) - present_state_Q (-0.05178847136111881)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.1929895115746065 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022440918755931124) - present_state_Q ( -0.022440918755931124)) * f1( 0.007071067811865476)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.022440918755931124) - present_state_Q (-0.022440918755931124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2123600902725675 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03366229621359468) - present_state_Q ( -0.014748548458592477)) * f1( 0.007071067811865476)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03366229621359468) - present_state_Q (-0.014748548458592477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2657839808076345 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025156100866234005) - present_state_Q ( -0.05018881890831933)) * f1( 0.019527814441034363)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.025156100866234005) - present_state_Q (-0.05018881890831933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2832063372248936 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021264149495497525) - present_state_Q ( -0.0313544198926622)) * f1( 0.006364031352184345)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.021264149495497525) - present_state_Q (-0.0313544198926622)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3094924975049733 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030189737414347263) - present_state_Q ( -0.029646451153873675)) * f1( 0.009600886058883843)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.030189737414347263) - present_state_Q (-0.029646451153873675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3393754283103436 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01004138579528989) - present_state_Q ( -0.01004138579528989)) * f1( 0.010907580321262487)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.01004138579528989) - present_state_Q (-0.01004138579528989)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.359751792685267 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002810034138809068) - present_state_Q ( -0.002810034138809068)) * f1( 0.007435818342916557)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002810034138809068) - present_state_Q (-0.002810034138809068)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.37388050526925 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0344250166711071) - present_state_Q ( -0.900078814389737)) * f1( 0.005329803443760391)
w2 ( -4.33622180081736 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0344250166711071) - present_state_Q (-0.900078814389737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4006623373647007 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.060285949241683616) - present_state_Q ( -0.007311499796277522)) * f1( 0.00977288093059462)
w2 ( -4.884306491210102 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.060285949241683616) - present_state_Q (-0.007311499796277522)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.4190058624865656 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.02748117835532) - present_state_Q ( -0.003110832253413737)) * f1( 0.0066691226845325535)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -1.02748117835532) - present_state_Q (-0.003110832253413737)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.4370771956921278 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024176022302808114) - present_state_Q ( -0.024176022302808114)) * f1( 0.007071067811865476)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.024176022302808114) - present_state_Q (-0.024176022302808114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.455131993173125 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021504585205331623) - present_state_Q ( -0.047293924536575875)) * f1( 0.007071067811865476)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.021504585205331623) - present_state_Q (-0.047293924536575875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4722360833797246 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02122123925927199) - present_state_Q ( -0.036815346478768114)) * f1( 0.006695987714743703)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.02122123925927199) - present_state_Q (-0.036815346478768114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.499456111315757 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03418744053022556) - present_state_Q ( -0.03581875649100116)) * f1( 0.010655264849942137)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.03418744053022556) - present_state_Q (-0.03581875649100116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5254103344243126 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007889924742665871) - present_state_Q ( -0.0437035653911579)) * f1( 0.010163949650966824)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.007889924742665871) - present_state_Q (-0.0437035653911579)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.542702759505067 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03561456594865771) - present_state_Q ( -0.039469444449638055)) * f1( 0.0067700398727350295)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.03561456594865771) - present_state_Q (-0.039469444449638055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5713283965843954 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011185676798809217) - present_state_Q ( -0.011185676798809217)) * f1( 0.011195702260311005)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.011185676798809217) - present_state_Q (-0.011185676798809217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5908906853303706 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00048752392951697176) - present_state_Q ( -0.00048752392951697176)) * f1( 0.007648078416446394)
w2 ( -5.434409585411929 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.00048752392951697176) - present_state_Q (-0.00048752392951697176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6027328046522755 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.057672296997752254) - present_state_Q ( -0.010552414500122334)) * f1( 0.004630585471097952)
w2 ( -5.945883540306761 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.057672296997752254) - present_state_Q (-0.010552414500122334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.6214492653270245 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025475167969728537) - present_state_Q ( -0.025475167969728537)) * f1( 0.007071067811865476)
w2 ( -5.945883540306761 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.025475167969728537) - present_state_Q (-0.025475167969728537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.640148304725225 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022658136509304757) - present_state_Q ( -0.04983087039246004)) * f1( 0.007071067811865476)
w2 ( -5.945883540306761 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.022658136509304757) - present_state_Q (-0.04983087039246004)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6850962344171685 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031781275971210096) - present_state_Q ( -0.0455916459296662)) * f1( 0.01699381176422692)
w2 ( -5.945883540306761 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.031781275971210096) - present_state_Q (-0.0455916459296662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.707182777694462 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05958349670877363) - present_state_Q ( -1.2005811838508809)) * f1( 0.008730764054298356)
w2 ( -6.451831087163673 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.05958349670877363) - present_state_Q (-1.2005811838508809)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.7645791561861333 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12554550227684985) - present_state_Q ( -1.3707896465591105)) * f1( 0.02283636307866935)
w2 ( -6.954506388777556 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.12554550227684985) - present_state_Q (-1.3707896465591105)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.8288437129547632 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14638892009318405) - present_state_Q ( -1.485927677289764)) * f1( 0.025684544201182075)
w2 ( -6.954506388777556 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.14638892009318405) - present_state_Q (-1.485927677289764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.892018188778292 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14115711468335895) - present_state_Q ( -1.4788270451956425)) * f1( 0.02524223707133393)
w2 ( -7.455052165643522 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.14115711468335895) - present_state_Q (-1.4788270451956425)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.9518438432070173 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16447967738039054) - present_state_Q ( -1.5963751241543607)) * f1( 0.024014723455263176)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.16447967738039054) - present_state_Q (-1.5963751241543607)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -4.019699882704245 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0992889490366812) - present_state_Q ( -0.10407070461526503)) * f1( 0.026626562239989122)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.0992889490366812) - present_state_Q (-0.10407070461526503)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.086829573097869 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09726512071514311) - present_state_Q ( -0.09726512071514311)) * f1( 0.026334720890896623)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.09726512071514311) - present_state_Q (-0.09726512071514311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.1495472956822415 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07710680615638929) - present_state_Q ( -0.09445217264078219)) * f1( 0.024603153432299133)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.07710680615638929) - present_state_Q (-0.09445217264078219)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.210910250589188 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11261727026545611) - present_state_Q ( -0.11261727026545611)) * f1( 0.02408550774838788)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.11261727026545611) - present_state_Q (-0.11261727026545611)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.278951119924565 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1058325257639653) - present_state_Q ( -0.11130906479869866)) * f1( 0.02670598868733349)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.1058325257639653) - present_state_Q (-0.11130906479869866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.34631660001708 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10398581344739419) - present_state_Q ( -0.10398581344739419)) * f1( 0.02643349256449348)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.10398581344739419) - present_state_Q (-0.10398581344739419)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.4090459572106635 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09675520525215382) - present_state_Q ( -0.09675520525215382)) * f1( 0.024608043971193438)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.09675520525215382) - present_state_Q (-0.09675520525215382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.469492854257987 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08429044974032564) - present_state_Q ( -0.08463845824438967)) * f1( 0.023702552248432186)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.08429044974032564) - present_state_Q (-0.08463845824438967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.516452324649927 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09598749937594325) - present_state_Q ( -0.1111098861743803)) * f1( 0.019117616499885862)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.09598749937594325) - present_state_Q (-0.1111098861743803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.582881242044447 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11322614473261046) - present_state_Q ( -0.11322614473261046)) * f1( 0.027044232915257673)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11322614473261046) - present_state_Q (-0.11322614473261046)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.645550150328214 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1070467359259478) - present_state_Q ( -0.1070467359259478)) * f1( 0.025507700086810442)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1070467359259478) - present_state_Q (-0.1070467359259478)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.705914483892278 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12869242259967756) - present_state_Q ( -0.12869242259967756)) * f1( 0.02458918211087739)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.12869242259967756) - present_state_Q (-0.12869242259967756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.7729378933173425 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12072336673115903) - present_state_Q ( -0.1275349029814769)) * f1( 0.0273013303401913)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.12072336673115903) - present_state_Q (-0.1275349029814769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.839490344850812 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11972800196956358) - present_state_Q ( -0.11972800196956358)) * f1( 0.027100981842745335)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11972800196956358) - present_state_Q (-0.11972800196956358)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.901321193202056 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11151260729331496) - present_state_Q ( -0.11151260729331496)) * f1( 0.025170708026386518)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11151260729331496) - present_state_Q (-0.11151260729331496)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.96178208307223 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13584364716670558) - present_state_Q ( -0.13584364716670558)) * f1( 0.0246349725599696)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.13584364716670558) - present_state_Q (-0.13584364716670558)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.028970218939838 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1270074547771786) - present_state_Q ( -0.1270074547771786)) * f1( 0.02736714152648971)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1270074547771786) - present_state_Q (-0.1270074547771786)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.094770454704083 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12452786678675175) - present_state_Q ( -0.12452786678675175)) * f1( 0.026799384573895472)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.12452786678675175) - present_state_Q (-0.12452786678675175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.156478746539586 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11667912259656357) - present_state_Q ( -0.11667912259656357)) * f1( 0.02512557256836722)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11667912259656357) - present_state_Q (-0.11667912259656357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.215957929278516 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14069236021682482) - present_state_Q ( -0.14069236021682482)) * f1( 0.02423928290946052)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.14069236021682482) - present_state_Q (-0.14069236021682482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.281926018163226 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13102223146352487) - present_state_Q ( -0.1390665142694114)) * f1( 0.026882955762445315)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.13102223146352487) - present_state_Q (-0.1390665142694114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.347376052640926 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.129630526551625) - present_state_Q ( -0.129630526551625)) * f1( 0.026661740020715127)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.129630526551625) - present_state_Q (-0.129630526551625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.407746681298361 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11933279591032035) - present_state_Q ( -0.11933279591032035)) * f1( 0.02458331100484234)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11933279591032035) - present_state_Q (-0.11933279591032035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.464442783037926 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1457762909679152) - present_state_Q ( -0.1457762909679152)) * f1( 0.024003176209036203)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.1457762909679152) - present_state_Q (-0.1457762909679152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.527239568187663 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11694020204184102) - present_state_Q ( -0.13761188778229078)) * f1( 0.026580052867663677)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.11694020204184102) - present_state_Q (-0.13761188778229078)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.59086757173821 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13822462494308138) - present_state_Q ( -0.13822462494308138)) * f1( 0.02693015572818871)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.13822462494308138) - present_state_Q (-0.13822462494308138)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.651942342263533 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13234528063715) - present_state_Q ( -0.13234528063715)) * f1( 0.02584372792364012)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.13234528063715) - present_state_Q (-0.13234528063715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.709840406968277 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1240434547289613) - present_state_Q ( -0.1240434547289613)) * f1( 0.024491764678801355)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.1240434547289613) - present_state_Q (-0.1240434547289613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.764000053777018 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1476465037047435) - present_state_Q ( -0.1476465037047435)) * f1( 0.022930962648579464)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.1476465037047435) - present_state_Q (-0.1476465037047435)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.82389270436907 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13710727877686762) - present_state_Q ( -0.14356845124761783)) * f1( 0.02535504544308359)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.13710727877686762) - present_state_Q (-0.14356845124761783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.882759820376841 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1304686846674197) - present_state_Q ( -0.1304686846674197)) * f1( 0.024907781038888212)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.1304686846674197) - present_state_Q (-0.1304686846674197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.937214754424089 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15346562552827053) - present_state_Q ( -0.15346562552827053)) * f1( 0.02306109959773119)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.15346562552827053) - present_state_Q (-0.15346562552827053)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.997603595358934 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14474941283726436) - present_state_Q ( -0.14915145225564552)) * f1( 0.02557031974149356)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.14474941283726436) - present_state_Q (-0.14915145225564552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.056961685324398 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1366675843809064) - present_state_Q ( -0.1366675843809064)) * f1( 0.025121451459121633)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.1366675843809064) - present_state_Q (-0.1366675843809064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.110906478127996 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1623892034303488) - present_state_Q ( -0.1623892034303488)) * f1( 0.02377283344652767)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1623892034303488) - present_state_Q (-0.1623892034303488)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.170702311457027 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15211449789331644) - present_state_Q ( -0.15900536452799405)) * f1( 0.026348581843592934)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.15211449789331644) - present_state_Q (-0.15900536452799405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.229782175696904 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14701336529105324) - present_state_Q ( -0.14701336529105324)) * f1( 0.026019930937758923)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.14701336529105324) - present_state_Q (-0.14701336529105324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.285063617562435 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11489407155814557) - present_state_Q ( -0.14245747761071928)) * f1( 0.024345590196968834)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.11489407155814557) - present_state_Q (-0.14245747761071928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.340032195889148 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1718847115683414) - present_state_Q ( -0.1718847115683414)) * f1( 0.024233130087452644)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1718847115683414) - present_state_Q (-0.1718847115683414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.4010267545301955 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1621809695563923) - present_state_Q ( -0.1685879681348355)) * f1( 0.02688695148057226)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1621809695563923) - present_state_Q (-0.1685879681348355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.461379131121033 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15712409277588751) - present_state_Q ( -0.15712409277588751)) * f1( 0.026591027131399627)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.15712409277588751) - present_state_Q (-0.15712409277588751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.518396226519611 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1254527058872259) - present_state_Q ( -0.1533859231277509)) * f1( 0.025120882921879507)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1254527058872259) - present_state_Q (-0.1533859231277509)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.574506660566902 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14487237663296146) - present_state_Q ( -0.14487237663296146)) * f1( 0.024710038648994778)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.14487237663296146) - present_state_Q (-0.14487237663296146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.627073650905801 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1570943960965694) - present_state_Q ( -0.1570943960965694)) * f1( 0.023160788344094782)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1570943960965694) - present_state_Q (-0.1570943960965694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.71319109135597 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14554028750006853) - present_state_Q ( -0.23770931533703427)) * f1( 0.03808016529947151)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.14554028750006853) - present_state_Q (-0.23770931533703427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.765826011365712 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17530984473648578) - present_state_Q ( -0.17530984473648578)) * f1( 0.023207480819024195)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.17530984473648578) - present_state_Q (-0.17530984473648578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.828007928003134 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1795164695650358) - present_state_Q ( -0.18337680816791618)) * f1( 0.027426133029337205)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1795164695650358) - present_state_Q (-0.18337680816791618)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.889486088658767 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1723321115164764) - present_state_Q ( -0.1723321115164764)) * f1( 0.027103388094796836)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1723321115164764) - present_state_Q (-0.1723321115164764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9489351570304745 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16592972648281518) - present_state_Q ( -0.16592972648281518)) * f1( 0.026202182134547624)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.16592972648281518) - present_state_Q (-0.16592972648281518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.005101526405113 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15514694416873986) - present_state_Q ( -0.15514694416873986)) * f1( 0.024744748286949454)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.15514694416873986) - present_state_Q (-0.15514694416873986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.058217543029514 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1849125603135015) - present_state_Q ( -0.1849125603135015)) * f1( 0.02342853074142931)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1849125603135015) - present_state_Q (-0.1849125603135015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.121218575051917 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19123619491384633) - present_state_Q ( -0.1938206665694795)) * f1( 0.02779878265969636)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.19123619491384633) - present_state_Q (-0.1938206665694795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.183480034841463 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18295129835859414) - present_state_Q ( -0.18295129835859414)) * f1( 0.027460285176516136)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18295129835859414) - present_state_Q (-0.18295129835859414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.2443272281294515 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17822941867659312) - present_state_Q ( -0.17822941867659312)) * f1( 0.026831496852211433)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.17822941867659312) - present_state_Q (-0.17822941867659312)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.301574321351035 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16675808371253864) - present_state_Q ( -0.16675808371253864)) * f1( 0.02523249050373236)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.16675808371253864) - present_state_Q (-0.16675808371253864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.356615922620979 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19973184391356524) - present_state_Q ( -0.19973184391356524)) * f1( 0.024292162663592222)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.19973184391356524) - present_state_Q (-0.19973184391356524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.4176786399993535 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1857343907410009) - present_state_Q ( -0.19658331072759655)) * f1( 0.02694745340389785)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1857343907410009) - present_state_Q (-0.19658331072759655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.47826679922679 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18266473192134042) - present_state_Q ( -0.18266473192134042)) * f1( 0.026721975538116553)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18266473192134042) - present_state_Q (-0.18266473192134042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.534363411184522 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1681037705807728) - present_state_Q ( -0.1681037705807728)) * f1( 0.024726719079384896)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1681037705807728) - present_state_Q (-0.1681037705807728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.588954470190749 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20401858356275998) - present_state_Q ( -0.20401858356275998)) * f1( 0.024097422633589348)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.20401858356275998) - present_state_Q (-0.20401858356275998)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.649480753551911 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18851861969651237) - present_state_Q ( -0.18851861969651237)) * f1( 0.026700889937898018)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18851861969651237) - present_state_Q (-0.18851861969651237)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.708724934545056 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1547812742802718) - present_state_Q ( -0.19247638987748114)) * f1( 0.026143751976213783)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1547812742802718) - present_state_Q (-0.19247638987748114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.766719673891325 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15780207156342987) - present_state_Q ( -0.18488626480441017)) * f1( 0.025583478197528833)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.15780207156342987) - present_state_Q (-0.18488626480441017)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.825563158625776 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18412772043342876) - present_state_Q ( -0.18412772043342876)) * f1( 0.025954006487467783)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18412772043342876) - present_state_Q (-0.18412772043342876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.880185410139469 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1686541130397157) - present_state_Q ( -0.1686541130397157)) * f1( 0.024077364209871152)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1686541130397157) - present_state_Q (-0.1686541130397157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.932268479440391 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20348541794566097) - present_state_Q ( -0.20348541794566097)) * f1( 0.02298986633296022)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.20348541794566097) - present_state_Q (-0.20348541794566097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.019202287130826 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24295419944566107) - present_state_Q ( -0.3543145844324322)) * f1( 0.038623666026032656)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.24295419944566107) - present_state_Q (-0.3543145844324322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.087568577508495 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23029398669605522) - present_state_Q ( -0.23029398669605522)) * f1( 0.03020957870524921)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.23029398669605522) - present_state_Q (-0.23029398669605522)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.155575549337609 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22908301953020982) - present_state_Q ( -0.22939373003111055)) * f1( 0.030049769048462155)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22908301953020982) - present_state_Q (-0.22939373003111055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.219675593842508 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21827592649991667) - present_state_Q ( -0.22981847173198236)) * f1( 0.02832532637402161)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.21827592649991667) - present_state_Q (-0.22981847173198236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.283553186380532 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18852137276992106) - present_state_Q ( -0.18852137276992106)) * f1( 0.028179307559801598)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18852137276992106) - present_state_Q (-0.18852137276992106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.345527350124673 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21183419002271242) - present_state_Q ( -0.21183419002271242)) * f1( 0.02736494740453014)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.21183419002271242) - present_state_Q (-0.21183419002271242)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.405952686239562 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20554898969306634) - present_state_Q ( -0.20554898969306634)) * f1( 0.02667439363727365)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.20554898969306634) - present_state_Q (-0.20554898969306634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.46285560219863 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19202339484947867) - present_state_Q ( -0.19202339484947867)) * f1( 0.025105951671249575)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.19202339484947867) - present_state_Q (-0.19202339484947867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.5173479742743 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22946449168078187) - present_state_Q ( -0.22946449168078187)) * f1( 0.024078201271635555)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22946449168078187) - present_state_Q (-0.22946449168078187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.577764815916542 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2130641506186357) - present_state_Q ( -0.22516128764734877)) * f1( 0.026692868223442162)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2130641506186357) - present_state_Q (-0.22516128764734877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.637642693215222 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20829919837275215) - present_state_Q ( -0.20829919837275215)) * f1( 0.02643560980805567)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.20829919837275215) - present_state_Q (-0.20829919837275215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.693111135779333 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19110913501066634) - present_state_Q ( -0.19110913501066634)) * f1( 0.024472163862983582)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.19110913501066634) - present_state_Q (-0.19110913501066634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.746715642356886 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2313056793849426) - present_state_Q ( -0.2313056793849426)) * f1( 0.023687620139770202)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2313056793849426) - present_state_Q (-0.2313056793849426)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.806041084589479 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21231029602917928) - present_state_Q ( -0.21231029602917928)) * f1( 0.026195889141863545)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.21231029602917928) - present_state_Q (-0.21231029602917928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.864306014607852 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2058679657738617) - present_state_Q ( -0.2058679657738617)) * f1( 0.025721021590121615)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2058679657738617) - present_state_Q (-0.2058679657738617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.917872747739242 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1861671014014367) - present_state_Q ( -0.1861671014014367)) * f1( 0.02362851052825889)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1861671014014367) - present_state_Q (-0.1861671014014367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.969018270396642 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22589027847400642) - present_state_Q ( -0.22589027847400642)) * f1( 0.02259613821872311)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22589027847400642) - present_state_Q (-0.22589027847400642)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.025239639615842 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2027717049717203) - present_state_Q ( -0.2027717049717203)) * f1( 0.024815840048135918)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2027717049717203) - present_state_Q (-0.2027717049717203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.080117372267013 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24611549365226879) - present_state_Q ( -0.24611549365226879)) * f1( 0.024264545439480024)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.24611549365226879) - present_state_Q (-0.24611549365226879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.141017247036094 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22761395248778568) - present_state_Q ( -0.22761395248778568)) * f1( 0.02690746396681404)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22761395248778568) - present_state_Q (-0.22761395248778568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.200546311216122 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22070464799389633) - present_state_Q ( -0.22070464799389633)) * f1( 0.026294572737740985)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22070464799389633) - present_state_Q (-0.22070464799389633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.25613640456089 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20373138383969683) - present_state_Q ( -0.20373138383969683)) * f1( 0.024538133421443554)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.20373138383969683) - present_state_Q (-0.20373138383969683)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.309314177213956 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24472589562064762) - present_state_Q ( -0.24472589562064762)) * f1( 0.023511596960916693)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.24472589562064762) - present_state_Q (-0.24472589562064762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.368233227928892 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1950555478129) - present_state_Q ( -0.1950555478129)) * f1( 0.02599861427199905)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1950555478129) - present_state_Q (-0.1950555478129)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.428135943005989 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22847774679657287) - present_state_Q ( -0.22847774679657287)) * f1( 0.02646779684845456)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22847774679657287) - present_state_Q (-0.22847774679657287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.48458840372319 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2130646863886485) - present_state_Q ( -0.2130646863886485)) * f1( 0.024928035652482285)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2130646863886485) - present_state_Q (-0.2130646863886485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.538425599372086 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23353163379683872) - present_state_Q ( -0.23353163379683872)) * f1( 0.023792550821551663)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.23353163379683872) - present_state_Q (-0.23353163379683872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.625263131704672 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18440617642206553) - present_state_Q ( -0.31899815473053567)) * f1( 0.03853045775481276)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18440617642206553) - present_state_Q (-0.31899815473053567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.681045098856238 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26734078413861456) - present_state_Q ( -0.26734078413861456)) * f1( 0.024685208727946405)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.26734078413861456) - present_state_Q (-0.26734078413861456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.743111946907408 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3837747780771042) - present_state_Q ( -0.24846909336149325)) * f1( 0.02742941846257486)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.3837747780771042) - present_state_Q (-0.24846909336149325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.799618567373168 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37904809604091905) - present_state_Q ( -1.8325632158114151)) * f1( 0.026852581846057877)
w2 ( -7.953293626055707 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.37904809604091905) - present_state_Q (-1.8325632158114151)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.852687795695383 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36197637968762403) - present_state_Q ( -1.8159920926179602)) * f1( 0.025201293446089512)
w2 ( -8.374456360704544 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.36197637968762403) - present_state_Q (-1.8159920926179602)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -9.903592265276957 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41708298779152697) - present_state_Q ( -1.9446077493069436)) * f1( 0.024315448967134803)
w2 ( -8.793156995435808 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.41708298779152697) - present_state_Q (-1.9446077493069436)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -9.960280304759813 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0085078813662633) - present_state_Q ( -2.0236535861297447)) * f1( 0.026974871831981374)
w2 ( -9.213459563217766 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -2.0085078813662633) - present_state_Q (-2.0236535861297447)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.016365452360526 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.088354004269136) - present_state_Q ( -2.088354004269136)) * f1( 0.026760207805785634)
w2 ( -9.632627814882742 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -2.088354004269136) - present_state_Q (-2.088354004269136)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.068005483052977 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.1516166269463817) - present_state_Q ( -2.1516166269463817)) * f1( 0.024706403577380947)
w2 ( -10.050657339339528 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -2.1516166269463817) - present_state_Q (-2.1516166269463817)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.118190103136556 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.283259582452097) - present_state_Q ( -2.283259582452097)) * f1( 0.024146959519065175)
w2 ( -10.46631729059721 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -2.283259582452097) - present_state_Q (-2.283259582452097)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.17365696693659 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.345164219710063) - present_state_Q ( -2.345164219710063)) * f1( 0.026760315260864816)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -2.345164219710063) - present_state_Q (-2.345164219710063)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.23038014531049 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24358783942521303) - present_state_Q ( -0.24358783942521303)) * f1( 0.026133469342335052)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.24358783942521303) - present_state_Q (-0.24358783942521303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.283264503291372 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22394813442310366) - present_state_Q ( -0.22394813442310366)) * f1( 0.024345024768761672)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.22394813442310366) - present_state_Q (-0.22394813442310366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.333721967475617 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26900603627806163) - present_state_Q ( -0.26900603627806163)) * f1( 0.023271260146517325)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.26900603627806163) - present_state_Q (-0.26900603627806163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.389462869884586 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24588852919950588) - present_state_Q ( -0.26188837875242926)) * f1( 0.02570231398491832)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.24588852919950588) - present_state_Q (-0.26188837875242926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.44434917651561 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2967799428645422) - present_state_Q ( -0.2967799428645422)) * f1( 0.02534308350628142)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.2967799428645422) - present_state_Q (-0.2967799428645422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.505562787948822 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27741882439122156) - present_state_Q ( -0.27741882439122156)) * f1( 0.028241916586961888)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.27741882439122156) - present_state_Q (-0.27741882439122156)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.565484364397888 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2706587393370853) - present_state_Q ( -0.2706587393370853)) * f1( 0.027638056807894853)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.2706587393370853) - present_state_Q (-0.2706587393370853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.619873998683452 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25545197709118583) - present_state_Q ( -0.25545197709118583)) * f1( 0.02617278316092909)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.25545197709118583) - present_state_Q (-0.25545197709118583)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.672634885658278 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24417434001060875) - present_state_Q ( -0.24417434001060875)) * f1( 0.025376620770961027)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.24417434001060875) - present_state_Q (-0.24417434001060875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.721586577213541 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22155446050955538) - present_state_Q ( -0.22155446050955538)) * f1( 0.023521465063899818)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.22155446050955538) - present_state_Q (-0.22155446050955538)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.767525162080455 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2662415126682034) - present_state_Q ( -0.2662415126682034)) * f1( 0.022116396460455982)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2662415126682034) - present_state_Q (-0.2662415126682034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.81793597423286 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23821643585520988) - present_state_Q ( -0.2547275632237042)) * f1( 0.024259304397359824)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.23821643585520988) - present_state_Q (-0.2547275632237042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.867029342640528 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2875129215914008) - present_state_Q ( -0.2875129215914008)) * f1( 0.02365702047493399)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2875129215914008) - present_state_Q (-0.2875129215914008)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.921374955051592 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26341456277109343) - present_state_Q ( -0.26341456277109343)) * f1( 0.02616062093097668)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.26341456277109343) - present_state_Q (-0.26341456277109343)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.974684415468365 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31702150216733455) - present_state_Q ( -0.31702150216733455)) * f1( 0.02572158041331978)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.31702150216733455) - present_state_Q (-0.31702150216733455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.034209724717915 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26095941922434246) - present_state_Q ( -0.30002343401223336)) * f1( 0.028704922438449874)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.26095941922434246) - present_state_Q (-0.30002343401223336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.095385877533358 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2728623318167553) - present_state_Q ( -0.2728623318167553)) * f1( 0.029460731193635976)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2728623318167553) - present_state_Q (-0.2728623318167553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.15187899436833 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30764069455886406) - present_state_Q ( -0.33401657250683975)) * f1( 0.02728128623379669)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.30764069455886406) - present_state_Q (-0.33401657250683975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.208237567490347 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29616672311305675) - present_state_Q ( -0.29881925280509325)) * f1( 0.027171634495571664)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.29616672311305675) - present_state_Q (-0.29881925280509325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.263862575141763 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27971909702478387) - present_state_Q ( -0.27971909702478387)) * f1( 0.026795417431985786)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.27971909702478387) - present_state_Q (-0.27971909702478387)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.31842251567392 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2714700586473577) - present_state_Q ( -0.2714700586473577)) * f1( 0.02627296225160363)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2714700586473577) - present_state_Q (-0.2714700586473577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.369163583944134 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24906932909822163) - present_state_Q ( -0.24906932909822163)) * f1( 0.02441031262911621)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.24906932909822163) - present_state_Q (-0.24906932909822163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.417830595625485 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29977253148119687) - present_state_Q ( -0.29977253148119687)) * f1( 0.023464044030464076)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.29977253148119687) - present_state_Q (-0.29977253148119687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.471672433646303 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2739226891494783) - present_state_Q ( -0.2739226891494783)) * f1( 0.02592992115232963)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2739226891494783) - present_state_Q (-0.2739226891494783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.524664939665904 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3310349066408985) - present_state_Q ( -0.3310349066408985)) * f1( 0.025584219986419278)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.3310349066408985) - present_state_Q (-0.3310349066408985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.583854364807962 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3098123736906265) - present_state_Q ( -0.3098123736906265)) * f1( 0.028549699972125392)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.3098123736906265) - present_state_Q (-0.3098123736906265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.641617779591838 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30144537323007853) - present_state_Q ( -0.30144537323007853)) * f1( 0.02785175525325475)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.30144537323007853) - present_state_Q (-0.30144537323007853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.6966629022714 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28608906105630705) - present_state_Q ( -0.28608906105630705)) * f1( 0.026523402824285218)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.28608906105630705) - present_state_Q (-0.28608906105630705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.749934724805096 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2734311394886011) - present_state_Q ( -0.2734311394886011)) * f1( 0.025654858365934726)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2734311394886011) - present_state_Q (-0.2734311394886011)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.79978775098184 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25084208246192036) - present_state_Q ( -0.25084208246192036)) * f1( 0.02398493780267273)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.25084208246192036) - present_state_Q (-0.25084208246192036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.846625760547294 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2997120405248743) - present_state_Q ( -0.2997120405248743)) * f1( 0.022582159784837037)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2997120405248743) - present_state_Q (-0.2997120405248743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.898245984686461 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2722766343659254) - present_state_Q ( -0.2888039350494934)) * f1( 0.02487803122850293)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2722766343659254) - present_state_Q (-0.2888039350494934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.948864446696318 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22160270279227468) - present_state_Q ( -0.26955954321194164)) * f1( 0.024378581790883817)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.22160270279227468) - present_state_Q (-0.26955954321194164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.995862424323223 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2916209303558479) - present_state_Q ( -0.29534717433554214)) * f1( 0.022655401775932015)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2916209303558479) - present_state_Q (-0.29534717433554214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.04523207914468 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3213877742603097) - present_state_Q ( -0.3213877742603097)) * f1( 0.023825158861609626)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.3213877742603097) - present_state_Q (-0.3213877742603097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.099950347623048 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29533487698171756) - present_state_Q ( -0.29533487698171756)) * f1( 0.02637648450372566)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.29533487698171756) - present_state_Q (-0.29533487698171756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.154044870299062 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28799018437944335) - present_state_Q ( -0.28799018437944335)) * f1( 0.026067506888898286)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.28799018437944335) - present_state_Q (-0.28799018437944335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.203721190608185 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26036818981616106) - present_state_Q ( -0.26036818981616106)) * f1( 0.023909784673706518)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.26036818981616106) - present_state_Q (-0.26036818981616106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.251627253729904 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31622870800572644) - present_state_Q ( -0.31622870800572644)) * f1( 0.02311366923153528)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.31622870800572644) - present_state_Q (-0.31622870800572644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.304433208748879 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24638593554008323) - present_state_Q ( -0.29355350644567235)) * f1( 0.025458490698244855)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.24638593554008323) - present_state_Q (-0.29355350644567235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.355081246684176 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2856763965228685) - present_state_Q ( -0.2856763965228685)) * f1( 0.025527896636268737)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2856763965228685) - present_state_Q (-0.2856763965228685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.40244570940072 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2613999871259464) - present_state_Q ( -0.2613999871259464)) * f1( 0.02384663067852766)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2613999871259464) - present_state_Q (-0.2613999871259464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.446810075189173 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.312260531162683) - present_state_Q ( -0.312260531162683)) * f1( 0.02238776417308168)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.312260531162683) - present_state_Q (-0.312260531162683)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.49686358760544 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2891009922411541) - present_state_Q ( -0.3091502264947881)) * f1( 0.025257688112630045)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2891009922411541) - present_state_Q (-0.3091502264947881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.545999943912264 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34934637508147415) - present_state_Q ( -0.34934637508147415)) * f1( 0.024837707382635507)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.34934637508147415) - present_state_Q (-0.34934637508147415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.600700221051195 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3245887289777778) - present_state_Q ( -0.3245887289777778)) * f1( 0.02761907992844785)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.3245887289777778) - present_state_Q (-0.3245887289777778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.65421237992485 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31528357560738396) - present_state_Q ( -0.31528357560738396)) * f1( 0.02700775903574042)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.31528357560738396) - present_state_Q (-0.31528357560738396)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.704645092602588 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2944517230151954) - present_state_Q ( -0.2944517230151954)) * f1( 0.025429489929156928)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2944517230151954) - present_state_Q (-0.2944517230151954)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.753179907499629 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35111010129360287) - present_state_Q ( -0.35111010129360287)) * f1( 0.02453560572101985)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.35111010129360287) - present_state_Q (-0.35111010129360287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.807071370855544 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32591436892774756) - present_state_Q ( -0.3448319262626894)) * f1( 0.027238354745828315)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.32591436892774756) - present_state_Q (-0.3448319262626894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.860739672673024 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2764727976782225) - present_state_Q ( -0.2764727976782225)) * f1( 0.02703889765249118)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2764727976782225) - present_state_Q (-0.2764727976782225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.911955659603962 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3058300812944595) - present_state_Q ( -0.3058300812944595)) * f1( 0.025837778686535567)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.3058300812944595) - present_state_Q (-0.3058300812944595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.961453014491747 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3643694441092974) - present_state_Q ( -0.3643694441092974)) * f1( 0.025037298729517647)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.3643694441092974) - present_state_Q (-0.3643694441092974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.016548319917703 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33944347847892337) - present_state_Q ( -0.33944347847892337)) * f1( 0.02783732750594657)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.33944347847892337) - present_state_Q (-0.33944347847892337)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.068752941567267 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3348186566414145) - present_state_Q ( -0.3348186566414145)) * f1( 0.02764705489684165)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3348186566414145) - present_state_Q (-0.3348186566414145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.11741414252116 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3105526949300314) - present_state_Q ( -0.3105526949300314)) * f1( 0.025740722601105215)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3105526949300314) - present_state_Q (-0.3105526949300314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.165232406333722 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3751347099831885) - present_state_Q ( -0.3751347099831885)) * f1( 0.02537283902858524)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3751347099831885) - present_state_Q (-0.3751347099831885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.218712523848874 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3078282002415287) - present_state_Q ( -0.3078282002415287)) * f1( 0.02828615627525255)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3078282002415287) - present_state_Q (-0.3078282002415287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.293438016691786 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33638392446735665) - present_state_Q ( -0.519028199776302)) * f1( 0.03996342122769699)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.33638392446735665) - present_state_Q (-0.519028199776302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.341488897597596 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3135443771208162) - present_state_Q ( -0.3328998827420166)) * f1( 0.02544755579338465)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3135443771208162) - present_state_Q (-0.3328998827420166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.38868163730628 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3763823441811772) - present_state_Q ( -0.3763823441811772)) * f1( 0.025042421856860043)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3763823441811772) - present_state_Q (-0.3763823441811772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.441251807048435 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35037255674419876) - present_state_Q ( -0.35037255674419876)) * f1( 0.027861300477599146)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.35037255674419876) - present_state_Q (-0.35037255674419876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.492998455106111 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3424953162490411) - present_state_Q ( -0.3424953162490411)) * f1( 0.027414547385758103)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3424953162490411) - present_state_Q (-0.3424953162490411)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.541636034787588 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31968836377496207) - present_state_Q ( -0.31968836377496207)) * f1( 0.025739422342167525)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.31968836377496207) - present_state_Q (-0.31968836377496207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.589100934655711 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27747391470481764) - present_state_Q ( -0.2813724594013631)) * f1( 0.02507358969890784)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.27747391470481764) - present_state_Q (-0.2813724594013631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.627825036127073 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31692454203164483) - present_state_Q ( -0.31692454203164483)) * f1( 0.020490427743885972)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.31692454203164483) - present_state_Q (-0.31692454203164483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.682455457092264 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37268230428358706) - present_state_Q ( -0.37268230428358706)) * f1( 0.028984041873269747)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.37268230428358706) - present_state_Q (-0.37268230428358706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.735746023199855 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3624236225496823) - present_state_Q ( -0.3624236225496823)) * f1( 0.028259342326352552)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3624236225496823) - present_state_Q (-0.3624236225496823)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.786731512548275 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34539636595626966) - present_state_Q ( -0.34539636595626966)) * f1( 0.0270150344816141)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.34539636595626966) - present_state_Q (-0.34539636595626966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.836135880858777 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3311759518487688) - present_state_Q ( -0.3311759518487688)) * f1( 0.026159526344637345)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.3311759518487688) - present_state_Q (-0.3311759518487688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.88271839149332 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30717846474082716) - present_state_Q ( -0.30717846474082716)) * f1( 0.02463718266107665)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.30717846474082716) - present_state_Q (-0.30717846474082716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.92688674671644 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24504353790113423) - present_state_Q ( -0.2970200931373782)) * f1( 0.023355478850571956)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.24504353790113423) - present_state_Q (-0.2970200931373782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.967307394598576 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32361379298571596) - present_state_Q ( -0.32361379298571596)) * f1( 0.02139495196555872)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.32361379298571596) - present_state_Q (-0.32361379298571596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.007570191164968 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35087632266112795) - present_state_Q ( -0.35087632266112795)) * f1( 0.022424835225647335)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.35087632266112795) - present_state_Q (-0.35087632266112795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.051798427106974 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31251902722441927) - present_state_Q ( -0.31251902722441927)) * f1( 0.024586160764795063)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.31251902722441927) - present_state_Q (-0.31251902722441927)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.094780392530154 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3783397751585761) - present_state_Q ( -0.3783397751585761)) * f1( 0.023972309544290544)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3783397751585761) - present_state_Q (-0.3783397751585761)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.14241496737678 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30128506600473326) - present_state_Q ( -0.35423193349504684)) * f1( 0.026542925317351154)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.30128506600473326) - present_state_Q (-0.35423193349504684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.190718091767263 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.353172284814506) - present_state_Q ( -0.353172284814506)) * f1( 0.026906086385489105)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.353172284814506) - present_state_Q (-0.353172284814506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.237034335828222 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3348317835703658) - present_state_Q ( -0.3348317835703658)) * f1( 0.025775643320913786)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3348317835703658) - present_state_Q (-0.3348317835703658)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.280971371340176 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26106815311457215) - present_state_Q ( -0.32887696915015197)) * f1( 0.024453514740420533)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.26106815311457215) - present_state_Q (-0.32887696915015197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.322952903944472 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37759487833106603) - present_state_Q ( -0.37759487833106603)) * f1( 0.02341346321795649)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.37759487833106603) - present_state_Q (-0.37759487833106603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.369505127156746 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35268826566194056) - present_state_Q ( -0.36608368076468356)) * f1( 0.025949522069152665)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.35268826566194056) - present_state_Q (-0.36608368076468356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.415432839795313 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.334685096846959) - present_state_Q ( -0.334685096846959)) * f1( 0.025559232318907218)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.334685096846959) - present_state_Q (-0.334685096846959)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.458805396626385 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26013875982805656) - present_state_Q ( -0.26013875982805656)) * f1( 0.024047475952706836)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.26013875982805656) - present_state_Q (-0.26013875982805656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.495192601911091 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2744527897964469) - present_state_Q ( -0.29098782252114463)) * f1( 0.020207477409934657)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.2744527897964469) - present_state_Q (-0.29098782252114463)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.543878171801232 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31346047102395164) - present_state_Q ( -0.37880227460830906)) * f1( 0.027163907919557185)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.31346047102395164) - present_state_Q (-0.37880227460830906)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.592445265675883 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37047841419323885) - present_state_Q ( -0.3910361031433333)) * f1( 0.027107684179629263)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.37047841419323885) - present_state_Q (-0.3910361031433333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.640691204995672 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3623815853658885) - present_state_Q ( -0.3623815853658885)) * f1( 0.026886645949874884)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3623815853658885) - present_state_Q (-0.3623815853658885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.685575374158448 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33354295430372377) - present_state_Q ( -0.33354295430372377)) * f1( 0.024977061098659888)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.33354295430372377) - present_state_Q (-0.33354295430372377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.72916152286731 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4019448184391304) - present_state_Q ( -0.4019448184391304)) * f1( 0.02433811789844921)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4019448184391304) - present_state_Q (-0.4019448184391304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.777590231756612 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37110535405787665) - present_state_Q ( -0.37110535405787665)) * f1( 0.027000314186298842)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.37110535405787665) - present_state_Q (-0.37110535405787665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.8251521421486 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.360355896831619) - present_state_Q ( -0.360355896831619)) * f1( 0.026502755534663407)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.360355896831619) - present_state_Q (-0.360355896831619)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.86950182470602 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33140837885946167) - present_state_Q ( -0.33140837885946167)) * f1( 0.024676992842215095)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.33140837885946167) - present_state_Q (-0.33140837885946167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.912136802126598 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3979240729950503) - present_state_Q ( -0.3979240729950503)) * f1( 0.02380218275913244)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3979240729950503) - present_state_Q (-0.3979240729950503)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.959407590018778 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36502556663132446) - present_state_Q ( -0.36502556663132446)) * f1( 0.026346704393253973)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.36502556663132446) - present_state_Q (-0.36502556663132446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.006098129083085 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35500588088763096) - present_state_Q ( -0.35500588088763096)) * f1( 0.026010225681242206)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.35500588088763096) - present_state_Q (-0.35500588088763096)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.049010020720907 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32043807064112234) - present_state_Q ( -0.32043807064112234)) * f1( 0.02386386904675568)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.32043807064112234) - present_state_Q (-0.32043807064112234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.090278817511575 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3885124480179851) - present_state_Q ( -0.3885124480179851)) * f1( 0.02302858382779446)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3885124480179851) - present_state_Q (-0.3885124480179851)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.138435548926795 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3847641962208511) - present_state_Q ( -0.401558745960855)) * f1( 0.026892290814157893)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3847641962208511) - present_state_Q (-0.401558745960855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.186163611841717 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37166013013815485) - present_state_Q ( -0.37166013013815485)) * f1( 0.026610425878603683)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.37166013013815485) - present_state_Q (-0.37166013013815485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.231214101737748 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29574035212836103) - present_state_Q ( -0.29574035212836103)) * f1( 0.025022241122438674)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.29574035212836103) - present_state_Q (-0.29574035212836103)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.275265509798103 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4221139790997789) - present_state_Q ( -0.4221139790997789)) * f1( 0.024622872455048163)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4221139790997789) - present_state_Q (-0.4221139790997789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.324244496256654 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3417995158357786) - present_state_Q ( -0.3958455543083716)) * f1( 0.027349301145926232)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3417995158357786) - present_state_Q (-0.3958455543083716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.374354769535103 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4017758749490569) - present_state_Q ( -0.4017758749490569)) * f1( 0.02798089348383248)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4017758749490569) - present_state_Q (-0.4017758749490569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.422730104700754 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33209259057173307) - present_state_Q ( -0.3975608600980373)) * f1( 0.027016281210306227)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.33209259057173307) - present_state_Q (-0.3975608600980373)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.47129816707529 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3907004620914239) - present_state_Q ( -0.3907004620914239)) * f1( 0.02710465769346713)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3907004620914239) - present_state_Q (-0.3907004620914239)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.519111677563686 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3231510071248812) - present_state_Q ( -0.3993333757638964)) * f1( 0.0267064950315484)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3231510071248812) - present_state_Q (-0.3993333757638964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.566180639842282 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38337729882364935) - present_state_Q ( -0.4030046146667457)) * f1( 0.026287171090658066)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.38337729882364935) - present_state_Q (-0.4030046146667457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.612762236687475 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36940103798464996) - present_state_Q ( -0.36940103798464996)) * f1( 0.025968278535515546)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.36940103798464996) - present_state_Q (-0.36940103798464996)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.656109168697327 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33734778545354893) - present_state_Q ( -0.33734778545354893)) * f1( 0.024126219586254678)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.33734778545354893) - present_state_Q (-0.33734778545354893)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.697324048978773 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4048721020566949) - present_state_Q ( -0.4048721020566949)) * f1( 0.023017408811566532)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4048721020566949) - present_state_Q (-0.4048721020566949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.742798940075632 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3674045183487701) - present_state_Q ( -0.39212260437380286)) * f1( 0.025383748552430325)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3674045183487701) - present_state_Q (-0.39212260437380286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.787443120254977 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44281391972948003) - present_state_Q ( -0.44281391972948003)) * f1( 0.024980219759132342)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.44281391972948003) - present_state_Q (-0.44281391972948003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.83719366520279 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.411591762943061) - present_state_Q ( -0.411591762943061)) * f1( 0.02779373660542701)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.411591762943061) - present_state_Q (-0.411591762943061)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.885925512818794 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4000029771201675) - present_state_Q ( -0.4000029771201675)) * f1( 0.027208775176178743)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4000029771201675) - present_state_Q (-0.4000029771201675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.932033404808509 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32251245837267817) - present_state_Q ( -0.32251245837267817)) * f1( 0.025643869441208438)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.32251245837267817) - present_state_Q (-0.32251245837267817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.978003134043657 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46296725451893345) - present_state_Q ( -0.46296725451893345)) * f1( 0.025748049478314564)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.46296725451893345) - present_state_Q (-0.46296725451893345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.029412684176624 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4330428126177368) - present_state_Q ( -0.4330428126177368)) * f1( 0.028751569678477144)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4330428126177368) - present_state_Q (-0.4330428126177368)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.079622072978353 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4209680254556623) - present_state_Q ( -0.4209680254556623)) * f1( 0.028063305219304505)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4209680254556623) - present_state_Q (-0.4209680254556623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.127540821505406 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39956229775728974) - present_state_Q ( -0.39956229775728974)) * f1( 0.02675419972541626)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.39956229775728974) - present_state_Q (-0.39956229775728974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.174002458338133 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3823910312221168) - present_state_Q ( -0.3823910312221168)) * f1( 0.025918295522543926)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3823910312221168) - present_state_Q (-0.3823910312221168)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.217627651274853 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35209239349067095) - present_state_Q ( -0.35209239349067095)) * f1( 0.024299042398372918)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.35209239349067095) - present_state_Q (-0.35209239349067095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.258754703411494 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41948693381975655) - present_state_Q ( -0.41948693381975655)) * f1( 0.022985243626095508)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.41948693381975655) - present_state_Q (-0.41948693381975655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.30422733556461 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33110877861766613) - present_state_Q ( -0.3852226546695275)) * f1( 0.025377854900325927)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.33110877861766613) - present_state_Q (-0.3852226546695275)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.350579674263255 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38382795146397325) - present_state_Q ( -0.38382795146397325)) * f1( 0.02585918986047861)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.38382795146397325) - present_state_Q (-0.38382795146397325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.39316102898339 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3456309514277841) - present_state_Q ( -0.3456309514277841)) * f1( 0.023709949180421647)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3456309514277841) - present_state_Q (-0.3456309514277841)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.433953797737185 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4188482240927686) - present_state_Q ( -0.4188482240927686)) * f1( 0.02279768563834158)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4188482240927686) - present_state_Q (-0.4188482240927686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.478892304988044 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3761754577695943) - present_state_Q ( -0.3761754577695943)) * f1( 0.025060808025675396)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3761754577695943) - present_state_Q (-0.3761754577695943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.522562467785846 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45315315269342177) - present_state_Q ( -0.45315315269342177)) * f1( 0.02444794704538303)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.45315315269342177) - present_state_Q (-0.45315315269342177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.57111751128787 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41879498345866195) - present_state_Q ( -0.41879498345866195)) * f1( 0.02713568329547646)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.41879498345866195) - present_state_Q (-0.41879498345866195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.618792169583312 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.406683162056243) - present_state_Q ( -0.406683162056243)) * f1( 0.026627445997135353)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.406683162056243) - present_state_Q (-0.406683162056243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.663348049745483 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3750203681639457) - present_state_Q ( -0.3750203681639457)) * f1( 0.024845988371203737)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3750203681639457) - present_state_Q (-0.3750203681639457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.706200050789544 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.449550953979442) - present_state_Q ( -0.449550953979442)) * f1( 0.023985560765223023)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.449550953979442) - present_state_Q (-0.449550953979442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.75670838175182 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4561129070442149) - present_state_Q ( -0.4682829416463255)) * f1( 0.028299672521385617)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4561129070442149) - present_state_Q (-0.4682829416463255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.806810832773937 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44008336586208524) - present_state_Q ( -0.44008336586208524)) * f1( 0.028030488095597432)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.44008336586208524) - present_state_Q (-0.44008336586208524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.855108295646094 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42152263188029365) - present_state_Q ( -0.42152263188029365)) * f1( 0.02699543429204888)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.42152263188029365) - present_state_Q (-0.42152263188029365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.901481237900025 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4000913278044209) - present_state_Q ( -0.4000913278044209)) * f1( 0.02589182687556816)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4000913278044209) - present_state_Q (-0.4000913278044209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.945557376738133 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37270042402942577) - present_state_Q ( -0.37270042402942577)) * f1( 0.024575605682915173)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.37270042402942577) - present_state_Q (-0.37270042402942577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.986684488687846 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43978511382301755) - present_state_Q ( -0.43978511382301755)) * f1( 0.02300876875854039)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.43978511382301755) - present_state_Q (-0.43978511382301755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.032388625414455 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34881477691300444) - present_state_Q ( -0.34881477691300444)) * f1( 0.025452822989531852)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.34881477691300444) - present_state_Q (-0.34881477691300444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.097817339123186 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49261791578722064) - present_state_Q ( -0.7287617708595894)) * f1( 0.03719475319056876)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.49261791578722064) - present_state_Q (-0.7287617708595894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.14922976959617 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46134802525236135) - present_state_Q ( -0.4619083763298541)) * f1( 0.02879510764535809)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.46134802525236135) - present_state_Q (-0.4619083763298541)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.197453503844887 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3792147796463384) - present_state_Q ( -0.43627924087247627)) * f1( 0.02698286080040789)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3792147796463384) - present_state_Q (-0.43627924087247627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.265470123225867 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5216161757031217) - present_state_Q ( -0.7633476773962161)) * f1( 0.03873569332544129)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.5216161757031217) - present_state_Q (-0.7633476773962161)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.319220503247273 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4919185248022265) - present_state_Q ( -0.4919185248022265)) * f1( 0.030150062215372385)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4919185248022265) - present_state_Q (-0.4919185248022265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.370396992018634 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4688228056617149) - present_state_Q ( -0.4688228056617149)) * f1( 0.028672864842131458)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4688228056617149) - present_state_Q (-0.4688228056617149)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.420902705427388 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3966381681873213) - present_state_Q ( -0.47642843719840605)) * f1( 0.028320568543263768)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3966381681873213) - present_state_Q (-0.47642843719840605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.43955698433758 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12318438437398779) - present_state_Q ( -0.12318438437398779)) * f1( 0.007071067811865476)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.12318438437398779) - present_state_Q (-0.12318438437398779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.457484157141817 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11669201358334884) - present_state_Q ( -0.2373012161468692)) * f1( 0.007071067811865476)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.11669201358334884) - present_state_Q (-0.2373012161468692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.530832998860756 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5120836717032736) - present_state_Q ( -0.5120836717032736)) * f1( 0.029202160816425727)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.5120836717032736) - present_state_Q (-0.5120836717032736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.60558472007751 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46883836454644434) - present_state_Q ( -0.5271815383555526)) * f1( 0.02978371565065547)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.46883836454644434) - present_state_Q (-0.5271815383555526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.624238167918637 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12449028342321074) - present_state_Q ( -0.12449028342321074)) * f1( 0.007071067811865476)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.12449028342321074) - present_state_Q (-0.12449028342321074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.642922302541997 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21356397117900877) - present_state_Q ( -0.08999998858522575)) * f1( 0.007071067811865476)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.21356397117900877) - present_state_Q (-0.08999998858522575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.69405154791731 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19174244836921953) - present_state_Q ( -0.39165076791585274)) * f1( 0.01957510636591879)
w2 ( -10.880862958384249 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.19174244836921953) - present_state_Q (-0.39165076791585274)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.751678896475966 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07700405404392427) - present_state_Q ( -2.520397713482018)) * f1( 0.024032120063540444)
w2 ( -11.360449015763209 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.07700405404392427) - present_state_Q (-2.520397713482018)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -17.762774083771443 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09319455173377159) - present_state_Q ( -0.09319455173377159)) * f1( 0.004351974099057492)
w2 ( -11.360449015763209 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.09319455173377159) - present_state_Q (-0.09319455173377159)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.77612981789363 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15389806438838294) - present_state_Q ( -0.15389806438838294)) * f1( 0.005249900715152775)
w2 ( -11.360449015763209 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.15389806438838294) - present_state_Q (-0.15389806438838294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.79813335339585 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20244793748034706) - present_state_Q ( -0.20244793748034706)) * f1( 0.008664078238150224)
w2 ( -11.360449015763209 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.20244793748034706) - present_state_Q (-0.20244793748034706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.814379329804737 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.397941611019129) - present_state_Q ( -4.6700314141717705)) * f1( 0.007071067811865476)
w2 ( -12.279460222620783 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -2.397941611019129) - present_state_Q (-4.6700314141717705)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -17.83046401053207 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.015658877956931) - present_state_Q ( -5.159909681270877)) * f1( 0.007071067811865476)
w2 ( -13.189347167862143 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -5.015658877956931) - present_state_Q (-5.159909681270877)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -17.864141054553226 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.697546820122399) - present_state_Q ( -2.9370630011304746)) * f1( 0.014698960046262397)
w2 ( -13.647570660070617 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -2.697546820122399) - present_state_Q (-2.9370630011304746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -17.917045296387222 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.934188505843702) - present_state_Q ( -3.1866515638722523)) * f1( 0.02429950886770521)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -2.934188505843702) - present_state_Q (-3.1866515638722523)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -17.97570155919943 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22661171818769582) - present_state_Q ( -0.47993342369519604)) * f1( 0.02518065560771958)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.22661171818769582) - present_state_Q (-0.47993342369519604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.04031115743521 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20789464707676467) - present_state_Q ( -0.5418625734632497)) * f1( 0.028951027966836688)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.20789464707676467) - present_state_Q (-0.5418625734632497)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.09343788345584 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17729000815700624) - present_state_Q ( -0.42659707504511174)) * f1( 0.023686556033705897)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.17729000815700624) - present_state_Q (-0.42659707504511174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.150475051006666 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10819099917088196) - present_state_Q ( -0.25570316306438745)) * f1( 0.025245451504581348)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.10819099917088196) - present_state_Q (-0.25570316306438745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.164121293492972 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018323422420721124) - present_state_Q ( -0.018323422420721124)) * f1( 0.005979571149925517)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.018323422420721124) - present_state_Q (-0.018323422420721124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.192310709266973 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037279775201336275) - present_state_Q ( -0.27098151937403075)) * f1( 0.012489397180305966)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.037279775201336275) - present_state_Q (-0.27098151937403075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.22104033072998 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31227289722065393) - present_state_Q ( -0.28247511725692703)) * f1( 0.013255848875262446)
w2 ( -14.083005319446025 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.31227289722065393) - present_state_Q (-0.28247511725692703)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.257572427899742 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.024817378597981) - present_state_Q ( -3.2642737263537316)) * f1( 0.021319430841373437)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -3.024817378597981) - present_state_Q (-3.2642737263537316)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -18.282405660469827 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14435300623852523) - present_state_Q ( -0.22091782296862292)) * f1( 0.013747465510209396)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14435300623852523) - present_state_Q (-0.22091782296862292)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.301467961754046 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05623773430021797) - present_state_Q ( -0.05623773430021797)) * f1( 0.010462449361043867)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.05623773430021797) - present_state_Q (-0.05623773430021797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.317092433022943 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15504272429427277) - present_state_Q ( -0.15504272429427277)) * f1( 0.009074872961550842)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.15504272429427277) - present_state_Q (-0.15504272429427277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.359557258520194 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15657008429233174) - present_state_Q ( -0.5155218645352327)) * f1( 0.026634660139576178)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.15657008429233174) - present_state_Q (-0.5155218645352327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.364258724910766 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07059248939043422) - present_state_Q ( -0.07059248939043422)) * f1( 0.002870287140706808)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.07059248939043422) - present_state_Q (-0.07059248939043422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.36937391952307 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08789736656752845) - present_state_Q ( -0.08789736656752845)) * f1( 0.0031258440555094963)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.08789736656752845) - present_state_Q (-0.08789736656752845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.392317708588447 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07339958068674857) - present_state_Q ( -0.31274158213678854)) * f1( 0.014217324751406229)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.07339958068674857) - present_state_Q (-0.31274158213678854)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.419884701483287 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12446263523258089) - present_state_Q ( -0.24761721832816783)) * f1( 0.017008116752086046)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.12446263523258089) - present_state_Q (-0.24761721832816783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.434986780150595 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053684705567255374) - present_state_Q ( -0.053684705567255374)) * f1( 0.009211396152348962)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.053684705567255374) - present_state_Q (-0.053684705567255374)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.462873403468752 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07326014189650194) - present_state_Q ( -0.3295043006920891)) * f1( 0.017298179201805413)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.07326014189650194) - present_state_Q (-0.3295043006920891)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.4935554960691 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1549944306577056) - present_state_Q ( -0.4092130339515582)) * f1( 0.01911704877118411)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1549944306577056) - present_state_Q (-0.4092130339515582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.537426198793302 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1736852672748278) - present_state_Q ( -0.5381387230201639)) * f1( 0.02755257767429627)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1736852672748278) - present_state_Q (-0.5381387230201639)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.571953862411387 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13788172156562392) - present_state_Q ( -0.3918228118277146)) * f1( 0.021492107583844774)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.13788172156562392) - present_state_Q (-0.3918228118277146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.599404324768354 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04152166601871974) - present_state_Q ( -0.3040669192412098)) * f1( 0.017004143901057867)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.04152166601871974) - present_state_Q (-0.3040669192412098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.6427935217535 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1588646785721916) - present_state_Q ( -0.5296018601306375)) * f1( 0.02723810417973084)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1588646785721916) - present_state_Q (-0.5296018601306375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.65110800400103 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11387304574366153) - present_state_Q ( -0.11387304574366153)) * f1( 0.005389457794774772)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11387304574366153) - present_state_Q (-0.11387304574366153)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.69016722053145 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2556219416837648) - present_state_Q ( -0.5189755506473567)) * f1( 0.025976476331494833)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.2556219416837648) - present_state_Q (-0.5189755506473567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.701458111070732 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.133318338755373) - present_state_Q ( -0.19458946365136523)) * f1( 0.007821874750131878)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.133318338755373) - present_state_Q (-0.19458946365136523)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.731766841547273 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0930378555609738) - present_state_Q ( -0.3574489698994067)) * f1( 0.021242257275079095)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0930378555609738) - present_state_Q (-0.3574489698994067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.74093585700925 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03389740507308916) - present_state_Q ( -0.03389740507308916)) * f1( 0.006286275276299395)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03389740507308916) - present_state_Q (-0.03389740507308916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.764280817001232 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2236944211596473) - present_state_Q ( -0.4667057666965189)) * f1( 0.022195519206716873)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2236944211596473) - present_state_Q (-0.4667057666965189)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.77094763492669 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09750134716909292) - present_state_Q ( -0.09750134716909292)) * f1( 0.006130714101208698)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.09750134716909292) - present_state_Q (-0.09750134716909292)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.786915722650825 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07409981596524177) - present_state_Q ( -0.3325188814226779)) * f1( 0.015011695501054374)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.07409981596524177) - present_state_Q (-0.3325188814226779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.78937060449278 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07991972516543677) - present_state_Q ( -0.07991972516543677)) * f1( 0.0024605997912130074)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.07991972516543677) - present_state_Q (-0.07991972516543677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.812898827470395 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17053547138661698) - present_state_Q ( -0.432942175876751)) * f1( 0.024425110358906016)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.17053547138661698) - present_state_Q (-0.432942175876751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.837571908369284 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19885680627803598) - present_state_Q ( -0.5130406375336154)) * f1( 0.02582072223544476)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.19885680627803598) - present_state_Q (-0.5130406375336154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.865880553218254 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25919039252315595) - present_state_Q ( -0.5708070013741117)) * f1( 0.02978665334490308)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.25919039252315595) - present_state_Q (-0.5708070013741117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.896124419513367 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36626009633945655) - present_state_Q ( -0.6252341549247316)) * f1( 0.0319699846654085)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.36626009633945655) - present_state_Q (-0.6252341549247316)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.91381762432389 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09467800407746108) - present_state_Q ( -0.3612969661318036)) * f1( 0.018246322825058156)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.09467800407746108) - present_state_Q (-0.3612969661318036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.92081857642484 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12754201602463044) - present_state_Q ( -0.12754201602463044)) * f1( 0.007047534962452848)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.12754201602463044) - present_state_Q (-0.12754201602463044)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.92570634789864 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05085828376754797) - present_state_Q ( -0.05085828376754797)) * f1( 0.004886345996242082)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.05085828376754797) - present_state_Q (-0.05085828376754797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.94518737936671 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12939973124441792) - present_state_Q ( -0.36903582372021904)) * f1( 0.02009888346287128)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.12939973124441792) - present_state_Q (-0.36903582372021904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.967446836172808 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2319158127732782) - present_state_Q ( -0.49910131545756803)) * f1( 0.025705949193096718)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2319158127732782) - present_state_Q (-0.49910131545756803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.97468005740486 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10258937402726784) - present_state_Q ( -0.10258937402726784)) * f1( 0.008897690052074185)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.10258937402726784) - present_state_Q (-0.10258937402726784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.977851071337128 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054895215437030444) - present_state_Q ( -0.054895215437030444)) * f1( 0.0038802216198680823)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.054895215437030444) - present_state_Q (-0.054895215437030444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.990977673789857 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11183953662082519) - present_state_Q ( -0.3698072594841827)) * f1( 0.016694072443973113)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11183953662082519) - present_state_Q (-0.3698072594841827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -18.994455816555888 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08935526684610402) - present_state_Q ( -0.08935526684610402)) * f1( 0.004272254219279433)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08935526684610402) - present_state_Q (-0.08935526684610402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.010977888804156 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11193191335018361) - present_state_Q ( -0.29787450927079245)) * f1( 0.020821835631569276)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11193191335018361) - present_state_Q (-0.29787450927079245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.02413113279714 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11880908589824411) - present_state_Q ( -0.36780249080311966)) * f1( 0.016722208742871714)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11880908589824411) - present_state_Q (-0.36780249080311966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.039723873520884 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17575019570238634) - present_state_Q ( -0.4127532211317042)) * f1( 0.01992306458239128)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.17575019570238634) - present_state_Q (-0.4127532211317042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.056285264149203 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14762644897278385) - present_state_Q ( -0.4119492936111252)) * f1( 0.021166153920704815)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14762644897278385) - present_state_Q (-0.4119492936111252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.060801600013605 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11944183647838857) - present_state_Q ( -0.11944183647838857)) * f1( 0.005565994772621436)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11944183647838857) - present_state_Q (-0.11944183647838857)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.08035700200578 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1196734452326294) - present_state_Q ( -0.496984817975029)) * f1( 0.025276357268136207)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1196734452326294) - present_state_Q (-0.496984817975029)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.096945392200908 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1290626353487234) - present_state_Q ( -0.39569597616346736)) * f1( 0.021161723165131888)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1290626353487234) - present_state_Q (-0.39569597616346736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.108855846322577 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12165451578746561) - present_state_Q ( -0.39131171388558345)) * f1( 0.019799691912770537)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12165451578746561) - present_state_Q (-0.39131171388558345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.120883393715562 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09247301234864078) - present_state_Q ( -0.205053819166389)) * f1( 0.01940298092003125)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.09247301234864078) - present_state_Q (-0.205053819166389)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.123961996792055 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03657545177586635) - present_state_Q ( -0.03657545177586635)) * f1( 0.0048392752079103075)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03657545177586635) - present_state_Q (-0.03657545177586635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.129638407667038 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04737677483480848) - present_state_Q ( -0.04737677483480848)) * f1( 0.008936440942577463)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.04737677483480848) - present_state_Q (-0.04737677483480848)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.13915267655051 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06899069455228972) - present_state_Q ( -0.28026591834576464)) * f1( 0.01554300648944715)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.06899069455228972) - present_state_Q (-0.28026591834576464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.149934525262942 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12281997182206242) - present_state_Q ( -0.30927616140334546)) * f1( 0.017682041679504115)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12281997182206242) - present_state_Q (-0.30927616140334546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.159394311771266 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05798590997135637) - present_state_Q ( -0.3272729643274016)) * f1( 0.015576417805606578)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.05798590997135637) - present_state_Q (-0.3272729643274016)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.163714518670346 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03547927814844701) - present_state_Q ( -0.03547927814844701)) * f1( 0.006789906971203858)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03547927814844701) - present_state_Q (-0.03547927814844701)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.16803874855321 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02138451344433045) - present_state_Q ( -0.02138451344433045)) * f1( 0.006782707066643958)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.02138451344433045) - present_state_Q (-0.02138451344433045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.171305682870337 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09176066962327015) - present_state_Q ( -0.09176066962327015)) * f1( 0.005175721827290816)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.09176066962327015) - present_state_Q (-0.09176066962327015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.17434031127438 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06520364445667545) - present_state_Q ( -0.06520364445667545)) * f1( 0.004789549181717728)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.06520364445667545) - present_state_Q (-0.06520364445667545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.18425960669204 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10812946150988999) - present_state_Q ( -0.3766177000792661)) * f1( 0.016453140140792112)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10812946150988999) - present_state_Q (-0.3766177000792661)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.194754167548712 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11369117712929422) - present_state_Q ( -0.38234576851976554)) * f1( 0.025007236532446075)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11369117712929422) - present_state_Q (-0.38234576851976554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.19744215543619 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13354097365056286) - present_state_Q ( -0.13354097365056286)) * f1( 0.0060439543859409795)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13354097365056286) - present_state_Q (-0.13354097365056286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.205591980079895 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19190241789424653) - present_state_Q ( -0.19190241789424653)) * f1( 0.018543932158254065)
w2 ( -14.425717068568948 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19190241789424653) - present_state_Q (-0.19190241789424653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.210676369320804 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2701679082844612) - present_state_Q ( -0.2868571669437632)) * f1( 0.011802899717422537)
w2 ( -14.511871985795006 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2701679082844612) - present_state_Q (-0.2868571669437632)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.219498988792015 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30894597496140064) - present_state_Q ( -0.440744142257015)) * f1( 0.021219764798453743)
w2 ( -14.511871985795006 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.30894597496140064) - present_state_Q (-0.440744142257015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.22076311296136 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.936275113649405) - present_state_Q ( -2.936275113649405)) * f1( 0.006567088146459327)
w2 ( -14.55037075849768 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -2.936275113649405) - present_state_Q (-2.936275113649405)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.223622002386875 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.9761558092555975) - present_state_Q ( -3.2345810423588426)) * f1( 0.017532521770956277)
w2 ( -14.582983174017379 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -2.9761558092555975) - present_state_Q (-3.2345810423588426)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.227034792452343 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.00408680685115) - present_state_Q ( -3.3883216553707944)) * f1( 0.023064484244073688)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.00408680685115) - present_state_Q (-3.3883216553707944)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.229442993908624 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08481613592016087) - present_state_Q ( -0.18763144918488328)) * f1( 0.006930237364717901)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08481613592016087) - present_state_Q (-0.18763144918488328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.23525963637749 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1005315973134655) - present_state_Q ( -0.34366823433650445)) * f1( 0.017517612402365722)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1005315973134655) - present_state_Q (-0.34366823433650445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.241004658605704 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08781975458097203) - present_state_Q ( -0.34053117603512756)) * f1( 0.017292201210180186)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08781975458097203) - present_state_Q (-0.34053117603512756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.242405847441628 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09161428998721567) - present_state_Q ( -0.09161428998721567)) * f1( 0.003923122713460259)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09161428998721567) - present_state_Q (-0.09161428998721567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.25071153353003 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10762218293415357) - present_state_Q ( -0.3764252747695591)) * f1( 0.025257484192856713)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10762218293415357) - present_state_Q (-0.3764252747695591)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.25719049956911 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09568909257718546) - present_state_Q ( -0.3672423280586332)) * f1( 0.01965469828868214)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09568909257718546) - present_state_Q (-0.3672423280586332)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.259698159647225 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06194879140920774) - present_state_Q ( -0.06194879140920774)) * f1( 0.0069689841606202264)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06194879140920774) - present_state_Q (-0.06194879140920774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.265144250239004 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09541369404476953) - present_state_Q ( -0.36425305181283857)) * f1( 0.016506519555658454)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09541369404476953) - present_state_Q (-0.36425305181283857)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.266663302134372 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11605802163100665) - present_state_Q ( -0.15653945725215052)) * f1( 0.004328849560986511)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11605802163100665) - present_state_Q (-0.15653945725215052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.271987682285882 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10860855660264446) - present_state_Q ( -0.35984987514658096)) * f1( 0.016109686298421796)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10860855660264446) - present_state_Q (-0.35984987514658096)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.273169237357155 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06706054858340049) - present_state_Q ( -0.06706054858340049)) * f1( 0.004408463115297675)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06706054858340049) - present_state_Q (-0.06706054858340049)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.275396211760704 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043195598376336734) - present_state_Q ( -0.043195598376336734)) * f1( 0.008242937523265824)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.043195598376336734) - present_state_Q (-0.043195598376336734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.27935927781837 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.058826712206362694) - present_state_Q ( -0.3313905171047967)) * f1( 0.016409913141946248)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.058826712206362694) - present_state_Q (-0.3313905171047967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.283658734646135 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08892023462387515) - present_state_Q ( -0.3615367418980966)) * f1( 0.018005126600514183)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08892023462387515) - present_state_Q (-0.3615367418980966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.288437984662693 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16985377843565636) - present_state_Q ( -0.4423347260386981)) * f1( 0.020642903741002666)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.16985377843565636) - present_state_Q (-0.4423347260386981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.294098177971104 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13516078804519654) - present_state_Q ( -0.5141397682772764)) * f1( 0.02526953176413572)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.13516078804519654) - present_state_Q (-0.5141397682772764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.30027054981147 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17995425078228572) - present_state_Q ( -0.5579985685887298)) * f1( 0.028049241190741255)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.17995425078228572) - present_state_Q (-0.5579985685887298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.305143943614535 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14205472799758612) - present_state_Q ( -0.4100322519686477)) * f1( 0.020784500866335466)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14205472799758612) - present_state_Q (-0.4100322519686477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.31029889012614 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18990937030763527) - present_state_Q ( -0.46046202538986364)) * f1( 0.022421773252851628)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18990937030763527) - present_state_Q (-0.46046202538986364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.316586421845955 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2910805937565751) - present_state_Q ( -0.5676061111787298)) * f1( 0.028553035498252513)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2910805937565751) - present_state_Q (-0.5676061111787298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.317141832090478 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07883173716730077) - present_state_Q ( -0.07883173716730077)) * f1( 0.0020804973896261457)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07883173716730077) - present_state_Q (-0.07883173716730077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.322610996318637 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05965443835093895) - present_state_Q ( -0.32859511935371644)) * f1( 0.022619274222922522)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05965443835093895) - present_state_Q (-0.32859511935371644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.32813897474274 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13972664510715266) - present_state_Q ( -0.4840433298127082)) * f1( 0.02434716800653789)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.13972664510715266) - present_state_Q (-0.4840433298127082)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.32906315604643 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1013746045338756) - present_state_Q ( -0.1013746045338756)) * f1( 0.003488378859426912)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1013746045338756) - present_state_Q (-0.1013746045338756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.330098100207294 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052863788040445975) - present_state_Q ( -0.052863788040445975)) * f1( 0.003843126717171333)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.052863788040445975) - present_state_Q (-0.052863788040445975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.33073942500213 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1014157251137623) - present_state_Q ( -0.1014157251137623)) * f1( 0.0024207534812143807)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1014157251137623) - present_state_Q (-0.1014157251137623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.331498074717423 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09552601468943996) - present_state_Q ( -0.09552601468943996)) * f1( 0.0043573986188465445)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09552601468943996) - present_state_Q (-0.09552601468943996)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.334692668527786 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.127684806572185) - present_state_Q ( -0.39955989608989617)) * f1( 0.022180934980743718)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.127684806572185) - present_state_Q (-0.39955989608989617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.33568344682753 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10340396180719658) - present_state_Q ( -0.10340396180719658)) * f1( 0.0057139268195291314)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10340396180719658) - present_state_Q (-0.10340396180719658)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.338479977123257 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1357831383748912) - present_state_Q ( -0.4006251878683218)) * f1( 0.019420516225730967)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1357831383748912) - present_state_Q (-0.4006251878683218)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.33998894185055 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09085274618183042) - present_state_Q ( -0.09085274618183042)) * f1( 0.008646039299758997)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09085274618183042) - present_state_Q (-0.09085274618183042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.34229071497425 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051866522166785285) - present_state_Q ( -0.32338897455571997)) * f1( 0.015255329025032441)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.051866522166785285) - present_state_Q (-0.32338897455571997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.34527699552548 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1456658985212886) - present_state_Q ( -0.24192841464545328)) * f1( 0.018668072580786347)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1456658985212886) - present_state_Q (-0.24192841464545328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.346628738702723 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03568688747614975) - present_state_Q ( -0.03568688747614975)) * f1( 0.007530953839325671)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03568688747614975) - present_state_Q (-0.03568688747614975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.348003209457755 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025590947061105946) - present_state_Q ( -0.025590947061105946)) * f1( 0.007619006328289518)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.025590947061105946) - present_state_Q (-0.025590947061105946)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.348470500330112 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06577855117688527) - present_state_Q ( -0.06577855117688527)) * f1( 0.0026432964040307627)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06577855117688527) - present_state_Q (-0.06577855117688527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.35133301237653 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13971358320195335) - present_state_Q ( -0.4096910453002298)) * f1( 0.01999917859049563)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13971358320195335) - present_state_Q (-0.4096910453002298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.352866092210178 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07053425658664053) - present_state_Q ( -0.2843044638480047)) * f1( 0.009892219632009399)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07053425658664053) - present_state_Q (-0.2843044638480047)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.353891733438534 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12938978073163587) - present_state_Q ( -0.12938978073163587)) * f1( 0.005995855291769829)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12938978073163587) - present_state_Q (-0.12938978073163587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.3558054708729 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03269544485456834) - present_state_Q ( -0.2951743316091076)) * f1( 0.01246629144487598)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03269544485456834) - present_state_Q (-0.2951743316091076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.357016419654954 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017582603010770373) - present_state_Q ( -0.017582603010770373)) * f1( 0.006685854651055974)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.017582603010770373) - present_state_Q (-0.017582603010770373)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.360604279132236 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20702996599020845) - present_state_Q ( -0.5575667747847126)) * f1( 0.027809183936781747)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.20702996599020845) - present_state_Q (-0.5575667747847126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.363886658681572 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14906181158409892) - present_state_Q ( -0.42126962044930283)) * f1( 0.02310443037620481)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14906181158409892) - present_state_Q (-0.42126962044930283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.36400783397218 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12658428931110696) - present_state_Q ( -0.12658428931110696)) * f1( 0.0007073415436834492)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12658428931110696) - present_state_Q (-0.12658428931110696)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.365196918621486 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008960733951548932) - present_state_Q ( -0.008960733951548932)) * f1( 0.006537132319680996)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.008960733951548932) - present_state_Q (-0.008960733951548932)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.366271569338547 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09417354698849208) - present_state_Q ( -0.18951046598113944)) * f1( 0.0065251303277017725)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09417354698849208) - present_state_Q (-0.18951046598113944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.36861532293755 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1131825260432996) - present_state_Q ( -0.3413337958581914)) * f1( 0.015656138463174495)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1131825260432996) - present_state_Q (-0.3413337958581914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.370345342742382 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20986496177754796) - present_state_Q ( -0.20986496177754796)) * f1( 0.010560775492791061)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.20986496177754796) - present_state_Q (-0.20986496177754796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.371868630045324 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17334395571145209) - present_state_Q ( -0.17334395571145209)) * f1( 0.009115886131884668)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17334395571145209) - present_state_Q (-0.17334395571145209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.37471968617082 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19903125907797545) - present_state_Q ( -0.46884080090066754)) * f1( 0.020688358641954976)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.19903125907797545) - present_state_Q (-0.46884080090066754)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.37637006049849 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1500907675141793) - present_state_Q ( -0.1500907675141793)) * f1( 0.00975425746847264)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1500907675141793) - present_state_Q (-0.1500907675141793)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.379312442927787 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15636683910395802) - present_state_Q ( -0.41745859503090677)) * f1( 0.020645217861856323)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15636683910395802) - present_state_Q (-0.41745859503090677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.38257389482919 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11293884008017993) - present_state_Q ( -0.49092869395989486)) * f1( 0.024205526672388157)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11293884008017993) - present_state_Q (-0.49092869395989486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.385295807809516 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06829899069864111) - present_state_Q ( -0.3414136190031592)) * f1( 0.01823787441340804)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06829899069864111) - present_state_Q (-0.3414136190031592)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.388060849142203 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10659545621624407) - present_state_Q ( -0.37931037054517164)) * f1( 0.018959629002182166)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10659545621624407) - present_state_Q (-0.37931037054517164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.388885198673172 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04592049711615245) - present_state_Q ( -0.04592049711615245)) * f1( 0.0046163786704646135)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04592049711615245) - present_state_Q (-0.04592049711615245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.38937773762025 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1432435687663621) - present_state_Q ( -0.1432435687663621)) * f1( 0.002900503587631311)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1432435687663621) - present_state_Q (-0.1432435687663621)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.390704385937887 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034817324401410964) - present_state_Q ( -0.034817324401410964)) * f1( 0.00738792185824921)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.034817324401410964) - present_state_Q (-0.034817324401410964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.392059336881395 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024180355596123394) - present_state_Q ( -0.024180355596123394)) * f1( 0.007505521673260942)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.024180355596123394) - present_state_Q (-0.024180355596123394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.39337635132414 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0413281177391752) - present_state_Q ( -0.0413281177391752)) * f1( 0.007358283642675029)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0413281177391752) - present_state_Q (-0.0413281177391752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.395322123162682 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052967316229448655) - present_state_Q ( -0.28950270582155607)) * f1( 0.012611718103145276)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.052967316229448655) - present_state_Q (-0.28950270582155607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.39702652582883 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12035196984102844) - present_state_Q ( -0.3393641598117053)) * f1( 0.011364915509240672)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12035196984102844) - present_state_Q (-0.3393641598117053)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.400342237916497 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22952864892875752) - present_state_Q ( -0.5034276494144605)) * f1( 0.024623580090402778)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.22952864892875752) - present_state_Q (-0.5034276494144605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.403902124199806 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25194818443533995) - present_state_Q ( -0.5712358198673572)) * f1( 0.027790041879620787)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.25194818443533995) - present_state_Q (-0.5712358198673572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.406918938592177 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12175043798536053) - present_state_Q ( -0.39600913269216037)) * f1( 0.02090364151558226)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12175043798536053) - present_state_Q (-0.39600913269216037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.40757098464594 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11154545862165455) - present_state_Q ( -0.11154545862165455)) * f1( 0.0037763789845360945)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11154545862165455) - present_state_Q (-0.11154545862165455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.409776932245713 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08169839725116307) - present_state_Q ( -0.3394383450269277)) * f1( 0.01474794596890717)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08169839725116307) - present_state_Q (-0.3394383450269277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.412883071811116 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08322011189493064) - present_state_Q ( -0.46806387821821444)) * f1( 0.02271744542415758)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08322011189493064) - present_state_Q (-0.46806387821821444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.415331839540606 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.058010543644795) - present_state_Q ( -0.3293183267108808)) * f1( 0.01628692836135016)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.058010543644795) - present_state_Q (-0.3293183267108808)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.417634491869638 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09447390810065134) - present_state_Q ( -0.35534687588030694)) * f1( 0.015546539063940632)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09447390810065134) - present_state_Q (-0.35534687588030694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.421108464419568 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20973083229412431) - present_state_Q ( -0.5298446901857067)) * f1( 0.026354652991111345)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.20973083229412431) - present_state_Q (-0.5298446901857067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.424544033174207 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23967029762264414) - present_state_Q ( -0.5108740714999317)) * f1( 0.02563613443153933)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23967029762264414) - present_state_Q (-0.5108740714999317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.427575492257212 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1801368195312921) - present_state_Q ( -0.417651229839638)) * f1( 0.021237673859437606)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1801368195312921) - present_state_Q (-0.417651229839638)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.429351012538774 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06975047309608427) - present_state_Q ( -0.2598811784192152)) * f1( 0.011279386880575464)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06975047309608427) - present_state_Q (-0.2598811784192152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.431895215991815 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17407195846987297) - present_state_Q ( -0.17407195846987297)) * f1( 0.015231379040074223)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17407195846987297) - present_state_Q (-0.17407195846987297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.433742789029374 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049297061863015135) - present_state_Q ( -0.049297061863015135)) * f1( 0.010364094321586774)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.049297061863015135) - present_state_Q (-0.049297061863015135)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.434564225442017 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046309327512790134) - present_state_Q ( -0.046309327512790134)) * f1( 0.004600966790598176)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.046309327512790134) - present_state_Q (-0.046309327512790134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.43804098077428 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17941927960798698) - present_state_Q ( -0.5289088808231769)) * f1( 0.02641775760763181)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17941927960798698) - present_state_Q (-0.5289088808231769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.456686182562237 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13744770590487504) - present_state_Q ( -0.13744770590487504)) * f1( 0.007071067811865476)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.13744770590487504) - present_state_Q (-0.13744770590487504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.475243728873554 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0624638263823879) - present_state_Q ( -0.25391288183036803)) * f1( 0.007071067811865476)
w2 ( -14.61257663927203 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.0624638263823879) - present_state_Q (-0.25391288183036803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.514992612165916 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.0057488722206154) - present_state_Q ( -3.2754406412870853)) * f1( 0.017585186912245677)
w2 ( -15.064648982781568 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -3.0057488722206154) - present_state_Q (-3.2754406412870853)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.551296036864823 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.1278547256512) - present_state_Q ( -3.2912173702438836)) * f1( 0.016063425984182186)
w2 ( -15.516650003418832 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -3.1278547256512) - present_state_Q (-3.2912173702438836)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.57362818594459 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.198304657308281) - present_state_Q ( -3.198304657308281)) * f1( 0.009837947349516013)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -3.198304657308281) - present_state_Q (-3.198304657308281)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.62705564829976 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13810880977456835) - present_state_Q ( -0.3157903372904709)) * f1( 0.0219297675006323)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.13810880977456835) - present_state_Q (-0.3157903372904709)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.668061968114326 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2834551873312079) - present_state_Q ( -0.3960717277202587)) * f1( 0.01687694598393127)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.2834551873312079) - present_state_Q (-0.3960717277202587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.70072091942633 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20316807521688204) - present_state_Q ( -0.20316807521688204)) * f1( 0.013339922671402571)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.20316807521688204) - present_state_Q (-0.20316807521688204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.716430373426352 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2721836711527676) - present_state_Q ( -0.006974636716751688)) * f1( 0.006608471071810232)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.2721836711527676) - present_state_Q (-0.006974636716751688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.732225657009504 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2528365060547912) - present_state_Q ( -0.035942583698763235)) * f1( 0.006653225909848197)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.2528365060547912) - present_state_Q (-0.035942583698763235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.74806029743463 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26099529501967134) - present_state_Q ( -0.02077673314214648)) * f1( 0.006665316709581144)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.26099529501967134) - present_state_Q (-0.02077673314214648)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.763870960318027 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27377158591528494) - present_state_Q ( -0.007235308902137608)) * f1( 0.006651074957261689)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.27377158591528494) - present_state_Q (-0.007235308902137608)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.77956342062216 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07046144953179266) - present_state_Q ( -0.21442938712888363)) * f1( 0.006665144772535642)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.07046144953179266) - present_state_Q (-0.21442938712888363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.81458661203576 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07577123994413325) - present_state_Q ( -0.3551974665229812)) * f1( 0.01623187952186888)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.07577123994413325) - present_state_Q (-0.3551974665229812)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.83681838979551 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02933232811053537) - present_state_Q ( -0.3094779745094915)) * f1( 0.010283981994811371)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02933232811053537) - present_state_Q (-0.3094779745094915)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.84732953253907 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2771156820458493) - present_state_Q ( -0.06144178499868585)) * f1( 0.0050107543151371985)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.2771156820458493) - present_state_Q (-0.06144178499868585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.88021327899 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08373513339647749) - present_state_Q ( -0.3636128854798258)) * f1( 0.01665662705773998)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.08373513339647749) - present_state_Q (-0.3636128854798258)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.930115406011698 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28508930783527714) - present_state_Q ( -0.45631921181071355)) * f1( 0.025370218639060974)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.28508930783527714) - present_state_Q (-0.45631921181071355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.975207902091466 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31539855195533856) - present_state_Q ( -0.48371444881428266)) * f1( 0.022953436434857834)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.31539855195533856) - present_state_Q (-0.48371444881428266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.006209221815244 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31741854483234483) - present_state_Q ( -0.31741854483234483)) * f1( 0.016404389570950997)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.31741854483234483) - present_state_Q (-0.31741854483234483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.016545772254492 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31662133786917757) - present_state_Q ( -0.10016378325446097)) * f1( 0.00567884816321517)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.31662133786917757) - present_state_Q (-0.10016378325446097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.02377292603753 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11351100743394216) - present_state_Q ( -0.22072111667491356)) * f1( 0.004717332995831851)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11351100743394216) - present_state_Q (-0.22072111667491356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.060206942561685 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11236324071157679) - present_state_Q ( -0.48519144012017523)) * f1( 0.02576240273916054)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.11236324071157679) - present_state_Q (-0.48519144012017523)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.092182843730434 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29455634838856376) - present_state_Q ( -0.4690799794341403)) * f1( 0.02255532691132435)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.29455634838856376) - present_state_Q (-0.4690799794341403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.120160282202185 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27256793923156775) - present_state_Q ( -0.3830697384605336)) * f1( 0.019618885054382484)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.27256793923156775) - present_state_Q (-0.3830697384605336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.149980190315553 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36301471608975305) - present_state_Q ( -0.43118606140323135)) * f1( 0.020968347848432167)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.36301471608975305) - present_state_Q (-0.43118606140323135)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.157641983174244 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13145971640225654) - present_state_Q ( -0.2440559862923178)) * f1( 0.005687261659979855)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.13145971640225654) - present_state_Q (-0.2440559862923178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.176338736052873 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1539863962953089) - present_state_Q ( -0.38621950784181047)) * f1( 0.015055660849343257)
w2 ( -15.970650178178122 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1539863962953089) - present_state_Q (-0.38621950784181047)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.198507925323312 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5471075142622746) - present_state_Q ( -3.692378535453275)) * f1( 0.0234555535609978)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -3.5471075142622746) - present_state_Q (-3.692378535453275)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -20.209584949147768 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25018069223282696) - present_state_Q ( -0.1907597658615686)) * f1( 0.009459470844573376)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.25018069223282696) - present_state_Q (-0.1907597658615686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.2275573778882 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29172032444426615) - present_state_Q ( -0.38172599369295945)) * f1( 0.01559677444756448)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.29172032444426615) - present_state_Q (-0.38172599369295945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.232415424861397 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33057084578899304) - present_state_Q ( -0.0980034659272813)) * f1( 0.0041132320332156035)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.33057084578899304) - present_state_Q (-0.0980034659272813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.2561256219564 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35160439068026167) - present_state_Q ( -0.4749880444181448)) * f1( 0.022533113731957176)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.35160439068026167) - present_state_Q (-0.4749880444181448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.269652725708493 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29026004537547145) - present_state_Q ( -0.29026004537547145)) * f1( 0.012641003618911517)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.29026004537547145) - present_state_Q (-0.29026004537547145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.296569507650172 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4009046919262555) - present_state_Q ( -0.534323430118415)) * f1( 0.025713458897445633)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.4009046919262555) - present_state_Q (-0.534323430118415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.319219100508608 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18444646657705646) - present_state_Q ( -0.2226639764166227)) * f1( 0.0210537450090814)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.18444646657705646) - present_state_Q (-0.2226639764166227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.32898321923521 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06073358227992092) - present_state_Q ( -0.26460703710362904)) * f1( 0.00912221296484808)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06073358227992092) - present_state_Q (-0.26460703710362904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.342739847653355 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1915541829682659) - present_state_Q ( -0.1915541829682659)) * f1( 0.012749650555978327)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1915541829682659) - present_state_Q (-0.1915541829682659)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.353416096061405 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07398596502058248) - present_state_Q ( -0.276315176523614)) * f1( 0.009984062920661339)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.07398596502058248) - present_state_Q (-0.276315176523614)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.37680685240521 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33185540184809353) - present_state_Q ( -0.4831240116707188)) * f1( 0.022250912205627235)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.33185540184809353) - present_state_Q (-0.4831240116707188)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.396964290624037 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29229443614644185) - present_state_Q ( -0.45807703414450435)) * f1( 0.019136761115822434)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.29229443614644185) - present_state_Q (-0.45807703414450435)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.426974589650065 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31205936443137006) - present_state_Q ( -0.6086060472521796)) * f1( 0.028898273635235)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.31205936443137006) - present_state_Q (-0.6086060472521796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.4466646227792 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3619646634626072) - present_state_Q ( -0.40766685001590836)) * f1( 0.01859175041225639)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.3619646634626072) - present_state_Q (-0.40766685001590836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.465924057246685 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.333594967570933) - present_state_Q ( -0.3951360473953296)) * f1( 0.018168540055379018)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.333594967570933) - present_state_Q (-0.3951360473953296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.48656166570325 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28212460785868415) - present_state_Q ( -0.38781434289365296)) * f1( 0.019464658247540182)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.28212460785868415) - present_state_Q (-0.38781434289365296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.504182105503986 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40267887494680477) - present_state_Q ( -0.40267887494680477)) * f1( 0.018191134369600008)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.40267887494680477) - present_state_Q (-0.40267887494680477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.521432722821682 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3294091906895799) - present_state_Q ( -0.38633596567111184)) * f1( 0.017792772571319587)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.3294091906895799) - present_state_Q (-0.38633596567111184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.53831525987235 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38597059084249336) - present_state_Q ( -0.38597059084249336)) * f1( 0.01740231605592972)
w2 ( -16.159681651793 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.38597059084249336) - present_state_Q (-0.38597059084249336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.561041874376127 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6569227417496064) - present_state_Q ( -3.9952870293881544)) * f1( 0.03540470209622001)
w2 ( -16.28806355113514 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -3.6569227417496064) - present_state_Q (-3.9952870293881544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -20.574585721536454 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42643104854382213) - present_state_Q ( -3.68404375877085)) * f1( 0.021138186549312682)
w2 ( -16.41620933250321 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.42643104854382213) - present_state_Q (-3.68404375877085)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -20.57944525485855 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31737100706743937) - present_state_Q ( -0.0939951921828865)) * f1( 0.005356088591562327)
w2 ( -16.41620933250321 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.31737100706743937) - present_state_Q (-0.0939951921828865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.589468062344377 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.587120194877953) - present_state_Q ( -3.733730720001813)) * f1( 0.020679923964435774)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -3.587120194877953) - present_state_Q (-3.733730720001813)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -20.60823137315069 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43453925586164993) - present_state_Q ( -0.593703290276293)) * f1( 0.027765048929881884)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.43453925586164993) - present_state_Q (-0.593703290276293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.612873622892767 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14691233511948643) - present_state_Q ( -0.26861618071930327)) * f1( 0.006580818754103097)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14691233511948643) - present_state_Q (-0.26861618071930327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.624874759023225 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1784657522273311) - present_state_Q ( -0.2980980107797783)) * f1( 0.01707645115610274)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1784657522273311) - present_state_Q (-0.2980980107797783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.63486659424921 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07520159579650497) - present_state_Q ( -0.40650647368355397)) * f1( 0.01446173960183354)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07520159579650497) - present_state_Q (-0.40650647368355397)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.647640980334838 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3609061311911113) - present_state_Q ( -0.36398684163770045)) * f1( 0.01830077888514082)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.3609061311911113) - present_state_Q (-0.36398684163770045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.66080956550723 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2776697563864486) - present_state_Q ( -0.37869264515023454)) * f1( 0.018927962213809792)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2776697563864486) - present_state_Q (-0.37869264515023454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.66879691257742 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13189832467311802) - present_state_Q ( -0.13189832467311802)) * f1( 0.011109848262501954)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13189832467311802) - present_state_Q (-0.13189832467311802)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.684203032525883 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34092518781859044) - present_state_Q ( -0.4916041775510655)) * f1( 0.02248862983667619)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.34092518781859044) - present_state_Q (-0.4916041775510655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.699685944871778 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3314729683707565) - present_state_Q ( -0.4965280320953772)) * f1( 0.02262010439900453)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.3314729683707565) - present_state_Q (-0.4965280320953772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.715035390273613 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3266145542456391) - present_state_Q ( -0.4974206873041699)) * f1( 0.02242963010975306)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.3266145542456391) - present_state_Q (-0.4974206873041699)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.72409118145218 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19409236234162133) - present_state_Q ( -0.23922196720854086)) * f1( 0.01277564285016636)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.19409236234162133) - present_state_Q (-0.23922196720854086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.729244417595346 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2709135311444163) - present_state_Q ( -0.022301406544401596)) * f1( 0.007046748162982592)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2709135311444163) - present_state_Q (-0.022301406544401596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.734349281373035 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28150823780016115) - present_state_Q ( -0.014019521863783704)) * f1( 0.006971696316041267)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.28150823780016115) - present_state_Q (-0.014019521863783704)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.73942202727722 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2614689847194169) - present_state_Q ( -0.03835055946136965)) * f1( 0.006952839324580887)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2614689847194169) - present_state_Q (-0.03835055946136965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.74448116587999 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27066434162675435) - present_state_Q ( -0.023782890785673454)) * f1( 0.006919500656177507)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.27066434162675435) - present_state_Q (-0.023782890785673454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.74937311331818 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.129226347376397) - present_state_Q ( -0.19550509720545434)) * f1( 0.006865355867683195)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.129226347376397) - present_state_Q (-0.19550509720545434)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.75903199052313 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010564219043701629) - present_state_Q ( -0.41631710492654256)) * f1( 0.014012837874497135)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.010564219043701629) - present_state_Q (-0.41631710492654256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.768025367317957 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1953737226207924) - present_state_Q ( -0.21337754961352529)) * f1( 0.01264127120408842)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1953737226207924) - present_state_Q (-0.21337754961352529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.7762047837331 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22542171876655603) - present_state_Q ( -0.22542171876655603)) * f1( 0.011511778906136248)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.22542171876655603) - present_state_Q (-0.22542171876655603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.78517427338739 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2551489769633692) - present_state_Q ( -0.2551489769633692)) * f1( 0.012671448345925925)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2551489769633692) - present_state_Q (-0.2551489769633692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.794268356736776 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19196594798971267) - present_state_Q ( -0.21508117088457132)) * f1( 0.012786500751715635)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.19196594798971267) - present_state_Q (-0.21508117088457132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.802528875976297 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2286894555219053) - present_state_Q ( -0.2286894555219053)) * f1( 0.011630737818368968)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2286894555219053) - present_state_Q (-0.2286894555219053)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.80896185680567 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16620236180923054) - present_state_Q ( -0.2835555610784743)) * f1( 0.00913619610824947)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16620236180923054) - present_state_Q (-0.2835555610784743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.82554707333123 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27984688346998343) - present_state_Q ( -0.553224234276157)) * f1( 0.02445151831047518)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.27984688346998343) - present_state_Q (-0.553224234276157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.830347339577624 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18547376250745684) - present_state_Q ( -0.18547376250745684)) * f1( 0.007707934878626758)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18547376250745684) - present_state_Q (-0.18547376250745684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.835359109001637 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19417244461329283) - present_state_Q ( -0.19417244461329283)) * f1( 0.00805768130844289)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19417244461329283) - present_state_Q (-0.19417244461329283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.840588967148065 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20046508000867022) - present_state_Q ( -0.20046508000867022)) * f1( 0.008415976840556522)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.20046508000867022) - present_state_Q (-0.20046508000867022)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.85129546479366 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3702362522291141) - present_state_Q ( -0.39140950274377817)) * f1( 0.01772530026208325)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3702362522291141) - present_state_Q (-0.39140950274377817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.86961197910204 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37529774226017215) - present_state_Q ( -0.664570973381711)) * f1( 0.031757714311262036)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.37529774226017215) - present_state_Q (-0.664570973381711)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.888202580688276 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42530262574095395) - present_state_Q ( -0.7130560931982154)) * f1( 0.03247780112889957)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.42530262574095395) - present_state_Q (-0.7130560931982154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.89945123721203 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3574138878918356) - present_state_Q ( -0.41926119221222313)) * f1( 0.01871313866908365)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3574138878918356) - present_state_Q (-0.41926119221222313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.9099524402905 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34429158995003956) - present_state_Q ( -0.38302440641751784)) * f1( 0.01736877078104481)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.34429158995003956) - present_state_Q (-0.38302440641751784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.921805801895726 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4535063507694366) - present_state_Q ( -0.4535063507694366)) * f1( 0.019800268726488827)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.4535063507694366) - present_state_Q (-0.4535063507694366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.93255555461562 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34045077157164133) - present_state_Q ( -0.39520347333927097)) * f1( 0.017816888746273244)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.34045077157164133) - present_state_Q (-0.39520347333927097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.943301115136396 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4028648298388331) - present_state_Q ( -0.4028648298388331)) * f1( 0.017814132981806768)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.4028648298388331) - present_state_Q (-0.4028648298388331)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.956455771943464 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38193123197939177) - present_state_Q ( -0.4427024560893401)) * f1( 0.021960621313620136)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.38193123197939177) - present_state_Q (-0.4427024560893401)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.967106025328555 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3712873261402698) - present_state_Q ( -0.390855645581132)) * f1( 0.017630260857846008)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3712873261402698) - present_state_Q (-0.390855645581132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.978335716441908 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36585666552679524) - present_state_Q ( -0.4206441346584589)) * f1( 0.018683262263316593)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.36585666552679524) - present_state_Q (-0.4206441346584589)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.99058103635153 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4736546606637418) - present_state_Q ( -0.4736546606637418)) * f1( 0.020517157654683912)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.4736546606637418) - present_state_Q (-0.4736546606637418)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.001605243457302 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3309787946827285) - present_state_Q ( -0.4015450413530611)) * f1( 0.01829387583480797)
w2 ( -16.513142063039986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3309787946827285) - present_state_Q (-0.4015450413530611)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.010008108068956 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6837863835172397) - present_state_Q ( -3.9785702400303786)) * f1( 0.03017805164795178)
w2 ( -16.568830645654124 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.6837863835172397) - present_state_Q (-3.9785702400303786)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.014205740084993 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.628715947976567) - present_state_Q ( -3.628715947976567)) * f1( 0.013416209748746835)
w2 ( -16.631406173238254 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.628715947976567) - present_state_Q (-3.628715947976567)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.01768046339319 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5441544457975405) - present_state_Q ( -3.5441544457975405)) * f1( 0.010841970467764219)
w2 ( -16.69550380786161 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.5441544457975405) - present_state_Q (-3.5441544457975405)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.031600670223305 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3067454461760093) - present_state_Q ( -0.6173747577671345)) * f1( 0.02844107045787251)
w2 ( -16.69550380786161 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.3067454461760093) - present_state_Q (-0.6173747577671345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.04912218726446 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4209612197218191) - present_state_Q ( -0.7920091184682412)) * f1( 0.03703405534842228)
w2 ( -16.69550380786161 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.4209612197218191) - present_state_Q (-0.7920091184682412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.067542576125945 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.746561249266613) - present_state_Q ( -0.7684441334680696)) * f1( 0.036208465834449005)
w2 ( -16.797250117388817 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -3.746561249266613) - present_state_Q (-0.7684441334680696)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.073732260325585 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8151513292326498) - present_state_Q ( -4.169029055717574)) * f1( 0.03654772288475266)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -3.8151513292326498) - present_state_Q (-4.169029055717574)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.082299339642073 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43705047521614526) - present_state_Q ( -0.43705047521614526)) * f1( 0.020523682525772288)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.43705047521614526) - present_state_Q (-0.43705047521614526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.08996155533799 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41791277216795814) - present_state_Q ( -0.41791277216795814)) * f1( 0.018280520454542915)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.41791277216795814) - present_state_Q (-0.41791277216795814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.09764656852171 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3445134462150055) - present_state_Q ( -0.41198919190430267)) * f1( 0.018341108426843434)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3445134462150055) - present_state_Q (-0.41198919190430267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.108310684188144 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41639126642764146) - present_state_Q ( -0.7798572986892256)) * f1( 0.036572912444663846)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.41639126642764146) - present_state_Q (-0.7798572986892256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.119137783517807 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4308045658266694) - present_state_Q ( -0.8069236823358098)) * f1( 0.03746108509530143)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.4308045658266694) - present_state_Q (-0.8069236823358098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.13042450590222 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5202687382146125) - present_state_Q ( -0.8181964914538777)) * f1( 0.03908281047692076)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.5202687382146125) - present_state_Q (-0.8181964914538777)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.137555659892257 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47312682081008084) - present_state_Q ( -0.47312682081008084)) * f1( 0.02208981111690676)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.47312682081008084) - present_state_Q (-0.47312682081008084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.15016165701355 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5894361271259033) - present_state_Q ( -0.9736928362009134)) * f1( 0.04601871338901222)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.5894361271259033) - present_state_Q (-0.9736928362009134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.159146383419824 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6413478303317975) - present_state_Q ( -0.6413478303317975)) * f1( 0.029200997946542087)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.6413478303317975) - present_state_Q (-0.6413478303317975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.16823168648544 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6196817992703484) - present_state_Q ( -0.6196817992703484)) * f1( 0.029341926162316123)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.6196817992703484) - present_state_Q (-0.6196817992703484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.17906367563092 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49032937412943023) - present_state_Q ( -0.7867200951265898)) * f1( 0.03714187568241383)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.49032937412943023) - present_state_Q (-0.7867200951265898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.190102888297343 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5017486858032634) - present_state_Q ( -0.80001252761032)) * f1( 0.03801078980504201)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.5017486858032634) - present_state_Q (-0.80001252761032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.195873305306534 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44561411588045957) - present_state_Q ( -0.44561411588045957)) * f1( 0.017738666217746602)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.44561411588045957) - present_state_Q (-0.44561411588045957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.202907727288597 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46757667911347667) - present_state_Q ( -0.46757667911347667)) * f1( 0.02175650515218958)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.46757667911347667) - present_state_Q (-0.46757667911347667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.208771887011423 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36633427518939915) - present_state_Q ( -0.4015864439952928)) * f1( 0.017828984353695256)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.36633427518939915) - present_state_Q (-0.4015864439952928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.21382247312548 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15221870087110484) - present_state_Q ( -0.15221870087110484)) * f1( 0.014360200432380234)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15221870087110484) - present_state_Q (-0.15221870087110484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.216274395838443 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2642027505012982) - present_state_Q ( -0.2642027505012982)) * f1( 0.007177157719553101)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2642027505012982) - present_state_Q (-0.2642027505012982)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.22074113593434 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2715844722721503) - present_state_Q ( -0.2715844722721503)) * f1( 0.013100316197006983)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2715844722721503) - present_state_Q (-0.2715844722721503)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.225286058108438 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2781378303763959) - present_state_Q ( -0.2781378303763959)) * f1( 0.013352710766375363)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2781378303763959) - present_state_Q (-0.2781378303763959)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.22972864005065 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26919710315292394) - present_state_Q ( -0.26919710315292394)) * f1( 0.013021258232746276)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.26919710315292394) - present_state_Q (-0.26919710315292394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.23406945651358 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2601884549498323) - present_state_Q ( -0.2601884549498323)) * f1( 0.012692819232369493)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2601884549498323) - present_state_Q (-0.2601884549498323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.238363057802065 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25555389338104406) - present_state_Q ( -0.25555389338104406)) * f1( 0.01253946525392835)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.25555389338104406) - present_state_Q (-0.25555389338104406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.242283159431942 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2455858100668558) - present_state_Q ( -0.2455858100668558)) * f1( 0.01141874146290604)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2455858100668558) - present_state_Q (-0.2455858100668558)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.25077808630249 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2492568006311565) - present_state_Q ( -0.5983271177993916)) * f1( 0.027574954864852237)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2492568006311565) - present_state_Q (-0.5983271177993916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.259164488074326 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3457035092933111) - present_state_Q ( -0.6370373464874394)) * f1( 0.02748196451336387)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.3457035092933111) - present_state_Q (-0.6370373464874394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.264890610090152 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25713861172697744) - present_state_Q ( -0.34618894340058615)) * f1( 0.017177023279859063)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.25713861172697744) - present_state_Q (-0.34618894340058615)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.27465954761855 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44549644419001566) - present_state_Q ( -0.7369723084503873)) * f1( 0.032984821458786125)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.44549644419001566) - present_state_Q (-0.7369723084503873)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.285030724713064 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42271828802933276) - present_state_Q ( -0.7946157343808109)) * f1( 0.03574141861481908)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.42271828802933276) - present_state_Q (-0.7946157343808109)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.291581252948085 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4308656186543837) - present_state_Q ( -0.4308656186543837)) * f1( 0.020054950323141477)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.4308656186543837) - present_state_Q (-0.4308656186543837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.297605100214543 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3601634033473184) - present_state_Q ( -0.4191770291486208)) * f1( 0.01841643577839345)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.3601634033473184) - present_state_Q (-0.4191770291486208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.303726847065924 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43253434708156563) - present_state_Q ( -0.43253434708156563)) * f1( 0.018750824574056687)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.43253434708156563) - present_state_Q (-0.43253434708156563)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.309731407330816 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3626013851085099) - present_state_Q ( -0.41776370508455685)) * f1( 0.01834817486620098)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.3626013851085099) - present_state_Q (-0.41776370508455685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.313133353773086 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32106104663345725) - present_state_Q ( -0.32106104663345725)) * f1( 0.01010945362856189)
w2 ( -16.83112190863097 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.32106104663345725) - present_state_Q (-0.32106104663345725)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.313541836971513 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.833289530051805) - present_state_Q ( -3.833289530051805)) * f1( 0.02001305054737428)
w2 ( -16.83520407688873 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -3.833289530051805) - present_state_Q (-3.833289530051805)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.313262176593305 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7603407073565296) - present_state_Q ( -4.109985040202147)) * f1( 0.03500919479536265)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -3.7603407073565296) - present_state_Q (-4.109985040202147)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.32063160257242 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43719317422885723) - present_state_Q ( -0.8036119194180298)) * f1( 0.03720693696653837)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.43719317422885723) - present_state_Q (-0.8036119194180298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.324423158801235 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3465309596578653) - present_state_Q ( -0.3465309596578653)) * f1( 0.015611631771461498)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3465309596578653) - present_state_Q (-0.3465309596578653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.32874322867829 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3631228777888753) - present_state_Q ( -0.41706928100503304)) * f1( 0.01830697300464756)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3631228777888753) - present_state_Q (-0.41706928100503304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.333822603627496 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3812086825381865) - present_state_Q ( -0.44658375056257493)) * f1( 0.021780366245970077)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3812086825381865) - present_state_Q (-0.44658375056257493)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.336504058635747 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34194769049670537) - present_state_Q ( -0.34194769049670537)) * f1( 0.011022099259088224)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.34194769049670537) - present_state_Q (-0.34194769049670537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.340766694161776 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3557951791404626) - present_state_Q ( -0.40868376961264175)) * f1( 0.01800519564923654)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3557951791404626) - present_state_Q (-0.40868376961264175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.345856412650072 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37840801146679093) - present_state_Q ( -0.449306312240047)) * f1( 0.021852855509470973)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.37840801146679093) - present_state_Q (-0.449306312240047)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.3521878229707 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28655816493271197) - present_state_Q ( -0.6485076753136261)) * f1( 0.02985528678290987)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.28655816493271197) - present_state_Q (-0.6485076753136261)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.358060279247134 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2546450309224675) - present_state_Q ( -0.6140518995936657)) * f1( 0.027288817714324926)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2546450309224675) - present_state_Q (-0.6140518995936657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.364005707948245 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2857461523029487) - present_state_Q ( -0.6045714805165492)) * f1( 0.027467211683257373)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2857461523029487) - present_state_Q (-0.6045714805165492)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.370279920800854 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37323059965547106) - present_state_Q ( -0.6693800752053604)) * f1( 0.029756834445151172)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.37323059965547106) - present_state_Q (-0.6693800752053604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.374482362318393 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3444566045529932) - present_state_Q ( -0.40020195217939813)) * f1( 0.017696014687299404)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3444566045529932) - present_state_Q (-0.40020195217939813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.38146634356264 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4308492142749541) - present_state_Q ( -0.7823256411554745)) * f1( 0.03489703082105206)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4308492142749541) - present_state_Q (-0.7823256411554745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.386597614482877 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3931072176662246) - present_state_Q ( -0.45472412396391515)) * f1( 0.022068669340668867)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3931072176662246) - present_state_Q (-0.45472412396391515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.390784940601577 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4091852017986813) - present_state_Q ( -0.4091852017986813)) * f1( 0.01765102427121609)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4091852017986813) - present_state_Q (-0.4091852017986813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.39811319041995 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4330095897071255) - present_state_Q ( -0.7959921805769596)) * f1( 0.036865010102715985)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4330095897071255) - present_state_Q (-0.7959921805769596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.402444337988186 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36861301245161027) - present_state_Q ( -0.42068309749672167)) * f1( 0.01837778475732034)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.36861301245161027) - present_state_Q (-0.42068309749672167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.407602305261754 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39640420520072045) - present_state_Q ( -0.4594416342439555)) * f1( 0.022225427549066173)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.39640420520072045) - present_state_Q (-0.4594416342439555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.41187919548638 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42287012645500455) - present_state_Q ( -0.42287012645500455)) * f1( 0.018122657109593307)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.42287012645500455) - present_state_Q (-0.42287012645500455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.416022903682684 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3704533539359284) - present_state_Q ( -0.3921232380909216)) * f1( 0.017370587458340848)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3704533539359284) - present_state_Q (-0.3921232380909216)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.42106499089778 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3718202780204197) - present_state_Q ( -0.44473748222072257)) * f1( 0.02161206702710681)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3718202780204197) - present_state_Q (-0.44473748222072257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.428096162211236 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4295532596279549) - present_state_Q ( -0.7924984192831442)) * f1( 0.035314619635267094)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4295532596279549) - present_state_Q (-0.7924984192831442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.432253613110525 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3677008909041527) - present_state_Q ( -0.39491771186743485)) * f1( 0.017450653585418877)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3677008909041527) - present_state_Q (-0.39491771186743485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.43660299777785 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3544884647119108) - present_state_Q ( -0.4228596279399573)) * f1( 0.01848331544707092)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3544884647119108) - present_state_Q (-0.4228596279399573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.440892772359252 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4246938158877499) - present_state_Q ( -0.4246938158877499)) * f1( 0.01818990335469227)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4246938158877499) - present_state_Q (-0.4246938158877499)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.4447166283577 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3499250462192688) - present_state_Q ( -0.3499250462192688)) * f1( 0.01576445303987372)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3499250462192688) - present_state_Q (-0.3499250462192688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.448929249803083 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3828693134441612) - present_state_Q ( -0.4027588567903037)) * f1( 0.01772929265773294)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3828693134441612) - present_state_Q (-0.4027588567903037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.45408783000678 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3977188066405689) - present_state_Q ( -0.4608083313422972)) * f1( 0.02223990599241208)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3977188066405689) - present_state_Q (-0.4608083313422972)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.456506426057636 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32136173746272173) - present_state_Q ( -0.32136173746272173)) * f1( 0.009866479915396494)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.32136173746272173) - present_state_Q (-0.32136173746272173)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.45794225449604 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08732523663956036) - present_state_Q ( -0.2531853185651912)) * f1( 0.0057522897472112745)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08732523663956036) - present_state_Q (-0.2531853185651912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.46021549114617 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1115010805364852) - present_state_Q ( -0.24959382148177342)) * f1( 0.009085285782074385)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1115010805364852) - present_state_Q (-0.24959382148177342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.464087750327554 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.147228426918477) - present_state_Q ( -0.147228426918477)) * f1( 0.01484735678153503)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.147228426918477) - present_state_Q (-0.147228426918477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.46933038259237 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19622819087157198) - present_state_Q ( -0.21808600484539442)) * f1( 0.020623326638308646)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.19622819087157198) - present_state_Q (-0.21808600484539442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.471782761819963 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24397495395529978) - present_state_Q ( -0.291072612164095)) * f1( 0.009913102384054821)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.24397495395529978) - present_state_Q (-0.291072612164095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.475239510783446 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.442192048643378) - present_state_Q ( -0.6105791824135407)) * f1( 0.02741983688671991)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.442192048643378) - present_state_Q (-0.6105791824135407)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.476240882257983 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24617316629694663) - present_state_Q ( -0.24617316629694663)) * f1( 0.006237214537048123)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24617316629694663) - present_state_Q (-0.24617316629694663)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.478600509624663 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17794917305075525) - present_state_Q ( -0.1296457729881419)) * f1( 0.013757287048656159)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17794917305075525) - present_state_Q (-0.1296457729881419)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.480429727009412 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10488989248380969) - present_state_Q ( -0.10488989248380969)) * f1( 0.010557439227077563)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10488989248380969) - present_state_Q (-0.10488989248380969)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.482486474138643 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2248721679539463) - present_state_Q ( -0.08143710840281038)) * f1( 0.01163262846880483)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2248721679539463) - present_state_Q (-0.08143710840281038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.48525048348573 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1511989438961436) - present_state_Q ( -0.42778208260671935)) * f1( 0.019542303917397863)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1511989438961436) - present_state_Q (-0.42778208260671935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.48861547763371 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1884339598182128) - present_state_Q ( -0.22197448055905677)) * f1( 0.020721639762888478)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1884339598182128) - present_state_Q (-0.22197448055905677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.49036159114623 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24167022554203538) - present_state_Q ( -0.07311249152584195)) * f1( 0.009820169220949495)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24167022554203538) - present_state_Q (-0.07311249152584195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.491952484463116 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03848715724299399) - present_state_Q ( -0.34158770476145917)) * f1( 0.010682187051203548)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03848715724299399) - present_state_Q (-0.34158770476145917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.493402723827156 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25918974837581266) - present_state_Q ( -0.28514538500700726)) * f1( 0.00925010770196145)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.25918974837581266) - present_state_Q (-0.28514538500700726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.49488932888006 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2571116283884791) - present_state_Q ( -0.047209595378532136)) * f1( 0.008233593799873892)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2571116283884791) - present_state_Q (-0.047209595378532136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.496340825490456 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27149883162147276) - present_state_Q ( -0.03610487718885959)) * f1( 0.007983680634682992)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.27149883162147276) - present_state_Q (-0.03610487718885959)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.497756129633146 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27816967683688076) - present_state_Q ( -0.030655230625847348)) * f1( 0.007758508201264828)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.27816967683688076) - present_state_Q (-0.030655230625847348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.49886529991009 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0714955179961806) - present_state_Q ( -0.36820562229718845)) * f1( 0.0075660750419746835)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0714955179961806) - present_state_Q (-0.36820562229718845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.500470478333938 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06290963452379891) - present_state_Q ( -0.06290963452379891)) * f1( 0.009066674631932304)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06290963452379891) - present_state_Q (-0.06290963452379891)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.502882790379967 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10757489957894743) - present_state_Q ( -0.5059807032298742)) * f1( 0.018113017055686697)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10757489957894743) - present_state_Q (-0.5059807032298742)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.504907825874362 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15725062105557938) - present_state_Q ( -0.21088389865615118)) * f1( 0.012409251182532182)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15725062105557938) - present_state_Q (-0.21088389865615118)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.506372913074262 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25212659316933345) - present_state_Q ( -0.2801231042431497)) * f1( 0.009319157733524855)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.25212659316933345) - present_state_Q (-0.2801231042431497)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.508957574828116 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11887079942527352) - present_state_Q ( -0.39150243606970403)) * f1( 0.017857037291388794)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11887079942527352) - present_state_Q (-0.39150243606970403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.51223677948915 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29102224001631316) - present_state_Q ( -0.42582709590943896)) * f1( 0.022926537080963056)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.29102224001631316) - present_state_Q (-0.42582709590943896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.513307698758826 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2492374647062566) - present_state_Q ( -0.2492374647062566)) * f1( 0.006681882990630573)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2492374647062566) - present_state_Q (-0.2492374647062566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.515778396752687 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1043090806601892) - present_state_Q ( -0.4072779803071857)) * f1( 0.017275344158388616)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1043090806601892) - present_state_Q (-0.4072779803071857)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.517978416563693 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14071121904443656) - present_state_Q ( -0.2709977529831142)) * f1( 0.014011902376930332)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14071121904443656) - present_state_Q (-0.2709977529831142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.519927292741063 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003055979184049379) - present_state_Q ( -0.42778025722469104)) * f1( 0.013924922162761156)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003055979184049379) - present_state_Q (-0.42778025722469104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.520986328420783 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21560428035808082) - present_state_Q ( -0.21560428035808082)) * f1( 0.006485252591565609)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21560428035808082) - present_state_Q (-0.21560428035808082)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.52298203744181 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08910217438239723) - present_state_Q ( -0.08910217438239723)) * f1( 0.0114246646597332)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08910217438239723) - present_state_Q (-0.08910217438239723)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.524899958566735 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03671749450402629) - present_state_Q ( -0.41372784310960864)) * f1( 0.013535288351287221)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03671749450402629) - present_state_Q (-0.41372784310960864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.526005420111126 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24856076950954928) - present_state_Q ( -0.24856076950954928)) * f1( 0.006894785731119676)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24856076950954928) - present_state_Q (-0.24856076950954928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.52864892324986 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1301794771371063) - present_state_Q ( -0.4215764473114254)) * f1( 0.018636220474392673)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1301794771371063) - present_state_Q (-0.4215764473114254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.531406434792313 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32152479573177595) - present_state_Q ( -0.43595301655429014)) * f1( 0.019374970130365234)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.32152479573177595) - present_state_Q (-0.43595301655429014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.533454993605577 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21555654110439923) - present_state_Q ( -0.22374201325314064)) * f1( 0.012607694024440286)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21555654110439923) - present_state_Q (-0.22374201325314064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.53471994926104 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07899296051141866) - present_state_Q ( -0.2509545978452109)) * f1( 0.007985936058465554)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07899296051141866) - present_state_Q (-0.2509545978452109)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.537456896873053 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16680501665362657) - present_state_Q ( -0.1787548611140665)) * f1( 0.016438517377250116)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16680501665362657) - present_state_Q (-0.1787548611140665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.53867659572191 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2367722045460415) - present_state_Q ( -0.2367722045460415)) * f1( 0.007557277336831061)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2367722045460415) - present_state_Q (-0.2367722045460415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.541060296664696 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33788167232766486) - present_state_Q ( -0.4347654659940265)) * f1( 0.016715324947179526)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.33788167232766486) - present_state_Q (-0.4347654659940265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.54312799476632 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22563976539732727) - present_state_Q ( -0.23545482236021567)) * f1( 0.01280987663469366)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.22563976539732727) - present_state_Q (-0.23545482236021567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.544147948427483 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13357147302888364) - present_state_Q ( -0.19309036751478334)) * f1( 0.006191664371271682)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13357147302888364) - present_state_Q (-0.19309036751478334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.54638963093795 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18903231335034715) - present_state_Q ( -0.2782404748209846)) * f1( 0.014299205464245679)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18903231335034715) - present_state_Q (-0.2782404748209846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.54842087076996 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3032636656886233) - present_state_Q ( -0.435162190409721)) * f1( 0.014282391576143852)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3032636656886233) - present_state_Q (-0.435162190409721)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.551018640424058 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3266781605118034) - present_state_Q ( -0.3985079130286024)) * f1( 0.01777839866942204)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3266781605118034) - present_state_Q (-0.3985079130286024)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.55184120912685 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11931757385437518) - present_state_Q ( -0.1884187728447322)) * f1( 0.004983611274459034)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11931757385437518) - present_state_Q (-0.1884187728447322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.554174063576156 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13923255729628356) - present_state_Q ( -0.36325255451573163)) * f1( 0.015787008500320207)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13923255729628356) - present_state_Q (-0.36325255451573163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.55089210641604 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.9137006324142982) - present_state_Q ( -2.344107690061599)) * f1( 0.1007652932732137)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -1.9137006324142982) - present_state_Q (-2.344107690061599)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.531586605665588 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.1685835382770864) - present_state_Q ( -3.3249381912612455)) * f1( 0.15070115093104308)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -2.1685835382770864) - present_state_Q (-3.3249381912612455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.52604409846869 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.3388758037781745) - present_state_Q ( -2.5605132555214944)) * f1( 0.11094085357832899)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -2.3388758037781745) - present_state_Q (-2.5605132555214944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.492788962566717 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.5978753306876095) - present_state_Q ( -3.9205259561752803)) * f1( 0.18135499080115963)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -2.5978753306876095) - present_state_Q (-3.9205259561752803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.460288739942435 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.639425917661992) - present_state_Q ( -3.943196822094482)) * f1( 0.18548029090418458)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.639425917661992) - present_state_Q (-3.943196822094482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.428040997675403 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6464385709449956) - present_state_Q ( -3.9493738218414944)) * f1( 0.18346603732825081)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.6464385709449956) - present_state_Q (-3.9493738218414944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.042262707782 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.836135079436984) - present_state_Q ( -10.561340342830588)) * f1( 0.48521345487920586)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -7.836135079436984) - present_state_Q (-10.561340342830588)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.663525510067995 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-10.531091256060934) - present_state_Q ( -10.497541305541079)) * f1( 0.4972002426253949)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -10.531091256060934) - present_state_Q (-10.497541305541079)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.4880371953049 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.543443539276563) - present_state_Q ( -5.799097224514831)) * f1( 0.5134662821106892)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -5.543443539276563) - present_state_Q (-5.799097224514831)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.126707131614666 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-10.207088778743582) - present_state_Q ( -10.177371778513695)) * f1( 0.49297187209466276)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -10.207088778743582) - present_state_Q (-10.177371778513695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.7787934340197 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.946033348369376) - present_state_Q ( -9.92452843890191)) * f1( 0.4898198729814917)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -9.946033348369376) - present_state_Q (-9.92452843890191)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.41308303389657 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.765842354199497) - present_state_Q ( -10.022376996985674)) * f1( 0.5066112295452397)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -9.765842354199497) - present_state_Q (-10.022376996985674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.05947900210563 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.609599201900304) - present_state_Q ( -9.84298551724273)) * f1( 0.5012111662145367)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -9.609599201900304) - present_state_Q (-9.84298551724273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.07812590742249 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13477086848271505) - present_state_Q ( -0.13477086848271505)) * f1( 0.007071067811865476)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.13477086848271505) - present_state_Q (-0.13477086848271505)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.096678548758497 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1112436344099821) - present_state_Q ( -0.2657275452395856)) * f1( 0.007071067811865476)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.1112436344099821) - present_state_Q (-0.2657275452395856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.11976266369094 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1721033225859794) - present_state_Q ( -0.1721033225859794)) * f1( 0.008764863494234124)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.1721033225859794) - present_state_Q (-0.1721033225859794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.143563480897004 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05018141034460422) - present_state_Q ( -0.22762735885988414)) * f1( 0.009060285121581883)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.05018141034460422) - present_state_Q (-0.22762735885988414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.166380861576542 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0419391591819711) - present_state_Q ( -0.0419391591819711)) * f1( 0.008625221438543813)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.0419391591819711) - present_state_Q (-0.0419391591819711)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.188503594231275 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04847612381010054) - present_state_Q ( -0.04847612381010054)) * f1( 0.008364497041422945)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.04847612381010054) - present_state_Q (-0.04847612381010054)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.209978621571135 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03823592925127157) - present_state_Q ( -0.03823592925127157)) * f1( 0.008116774465292109)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.03823592925127157) - present_state_Q (-0.03823592925127157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.230826015399717 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029184605500653424) - present_state_Q ( -0.029184605500653424)) * f1( 0.007877126628460848)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.029184605500653424) - present_state_Q (-0.029184605500653424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.251123151689928 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04113932425828993) - present_state_Q ( -0.04113932425828993)) * f1( 0.00767233250268266)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.04113932425828993) - present_state_Q (-0.04113932425828993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.271761459480032 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03069190370387255) - present_state_Q ( -0.03069190370387255)) * f1( 0.007798523815048856)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.03069190370387255) - present_state_Q (-0.03069190370387255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.318201416726396 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33382889869971794) - present_state_Q ( -3.700550186159336)) * f1( 0.021194508820769457)
w2 ( -16.83360643729809 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.33382889869971794) - present_state_Q (-3.700550186159336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.374073153415804 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.491457892331468) - present_state_Q ( -3.877366490425272)) * f1( 0.025338354760051078)
w2 ( -17.274611681865085 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -3.491457892331468) - present_state_Q (-3.877366490425272)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.38065127776807 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5303858610725882) - present_state_Q ( -3.5303858610725882)) * f1( 0.002936513779346833)
w2 ( -17.722634394956618 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -3.5303858610725882) - present_state_Q (-3.5303858610725882)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.432267487496787 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.639739297626166) - present_state_Q ( -4.025897724556951)) * f1( 0.023551237861458806)
w2 ( -18.16096557765157 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -3.639739297626166) - present_state_Q (-4.025897724556951)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.470241242526765 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.726726015707356) - present_state_Q ( -3.9911838962609028)) * f1( 0.01804282770439817)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -3.726726015707356) - present_state_Q (-3.9911838962609028)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -19.47789391268627 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09827174935305177) - present_state_Q ( -0.09827174935305177)) * f1( 0.003234023120957306)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.09827174935305177) - present_state_Q (-0.09827174935305177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.50944281921574 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2564984807422771) - present_state_Q ( -0.2564984807422771)) * f1( 0.013413308672047946)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.2564984807422771) - present_state_Q (-0.2564984807422771)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.528561024600144 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18309029919505695) - present_state_Q ( -0.18309029919505695)) * f1( 0.008105513699930787)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.18309029919505695) - present_state_Q (-0.18309029919505695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.548724003670664 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19167555667005637) - present_state_Q ( -0.19167555667005637)) * f1( 0.008551265960115356)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.19167555667005637) - present_state_Q (-0.19167555667005637)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.616125217997283 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.232763634309626) - present_state_Q ( -0.5793753402085466)) * f1( 0.02905807237319369)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.232763634309626) - present_state_Q (-0.5793753402085466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.679693637733696 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.243125076542524) - present_state_Q ( -0.586860889032896)) * f1( 0.028537520675115214)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.243125076542524) - present_state_Q (-0.586860889032896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.699268863929245 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1963633179339951) - present_state_Q ( -0.1963633179339951)) * f1( 0.008638210936171889)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1963633179339951) - present_state_Q (-0.1963633179339951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.728977708483114 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2524031436379388) - present_state_Q ( -0.2524031436379388)) * f1( 0.013139245904308136)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2524031436379388) - present_state_Q (-0.2524031436379388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.789728249163737 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24715645592505103) - present_state_Q ( -0.5698555634782155)) * f1( 0.02725120418838767)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.24715645592505103) - present_state_Q (-0.5698555634782155)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.80767771056126 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18078813955118223) - present_state_Q ( -0.18078813955118223)) * f1( 0.007915892292990268)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.18078813955118223) - present_state_Q (-0.18078813955118223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.838690413435405 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23669644051563526) - present_state_Q ( -0.2432663549379598)) * f1( 0.013711311461403237)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.23669644051563526) - present_state_Q (-0.2432663549379598)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.89437617626233 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23404595293776495) - present_state_Q ( -0.5150762910750029)) * f1( 0.024919500665684748)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.23404595293776495) - present_state_Q (-0.5150762910750029)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.916742447557965 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11393305728259587) - present_state_Q ( -0.2983502745837128)) * f1( 0.009918095368824214)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.11393305728259587) - present_state_Q (-0.2983502745837128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.925916836144253 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09841388083213042) - present_state_Q ( -0.09841388083213042)) * f1( 0.004032811961670256)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.09841388083213042) - present_state_Q (-0.09841388083213042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.967669550843542 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10773036721392626) - present_state_Q ( -0.3885558090779866)) * f1( 0.018589687800948173)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.10773036721392626) - present_state_Q (-0.3885558090779866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -19.97881057070428 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11959228906744568) - present_state_Q ( -0.11959228906744568)) * f1( 0.004901396276689868)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.11959228906744568) - present_state_Q (-0.11959228906744568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.023267535347475 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13280498000888172) - present_state_Q ( -0.41471035900531245)) * f1( 0.019814570907054924)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.13280498000888172) - present_state_Q (-0.41471035900531245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.072315271136084 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12255286983691957) - present_state_Q ( -0.47097133579397127)) * f1( 0.021916646980487533)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.12255286983691957) - present_state_Q (-0.47097133579397127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.092036574589113 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2050044787730063) - present_state_Q ( -0.2050044787730063)) * f1( 0.008705659994423954)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2050044787730063) - present_state_Q (-0.2050044787730063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.150898584762775 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21819125961752964) - present_state_Q ( -0.5444032552021177)) * f1( 0.026377367119171078)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.21819125961752964) - present_state_Q (-0.5444032552021177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.17050429841904 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2044183796262894) - present_state_Q ( -0.2044183796262894)) * f1( 0.008654433172101217)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2044183796262894) - present_state_Q (-0.2044183796262894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.19895506362602 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24506820324911688) - present_state_Q ( -0.24506820324911688)) * f1( 0.012579166124673304)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.24506820324911688) - present_state_Q (-0.24506820324911688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.225471815596446 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15072173780691273) - present_state_Q ( -0.3015516856912233)) * f1( 0.011758334830217695)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.15072173780691273) - present_state_Q (-0.3015516856912233)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.28154355887608 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2573937861249785) - present_state_Q ( -0.5483295038065009)) * f1( 0.025126993633709786)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2573937861249785) - present_state_Q (-0.5483295038065009)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.296751925952968 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2170638645611072) - present_state_Q ( -0.2581214027273153)) * f1( 0.006728914539159282)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2170638645611072) - present_state_Q (-0.2581214027273153)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.319182924007798 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1797563237999252) - present_state_Q ( -0.1797563237999252)) * f1( 0.009891889745180045)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1797563237999252) - present_state_Q (-0.1797563237999252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.349645986222004 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07893314654768512) - present_state_Q ( -0.2511522980222774)) * f1( 0.013482409454555904)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.07893314654768512) - present_state_Q (-0.2511522980222774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.358457982234658 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13727273596832545) - present_state_Q ( -0.16758127157011923)) * f1( 0.003884661447406084)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.13727273596832545) - present_state_Q (-0.16758127157011923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.38569870184183 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23759422136293598) - present_state_Q ( -0.23759422136293598)) * f1( 0.01204057794846706)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.23759422136293598) - present_state_Q (-0.23759422136293598)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.413015505623612 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1431863572523976) - present_state_Q ( -0.2760991351516143)) * f1( 0.012099850040737065)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1431863572523976) - present_state_Q (-0.2760991351516143)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.439156415396216 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2224364454766562) - present_state_Q ( -0.2224364454766562)) * f1( 0.011547491770902078)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.2224364454766562) - present_state_Q (-0.2224364454766562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.457056421960324 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1875116011977962) - present_state_Q ( -0.1875116011977962)) * f1( 0.007896189395131486)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1875116011977962) - present_state_Q (-0.1875116011977962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.483578693272026 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11080379111086543) - present_state_Q ( -0.22217107641985148)) * f1( 0.01172159733201761)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.11080379111086543) - present_state_Q (-0.22217107641985148)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.524493528019597 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21352111186735762) - present_state_Q ( -0.48891448089063333)) * f1( 0.018289745279375977)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.21352111186735762) - present_state_Q (-0.48891448089063333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.552682110675455 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09836781199456988) - present_state_Q ( -0.2362543228878754)) * f1( 0.012466473097695233)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.09836781199456988) - present_state_Q (-0.2362543228878754)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.563620312790146 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017019300826933585) - present_state_Q ( -0.017019300826933585)) * f1( 0.00479270350132052)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.017019300826933585) - present_state_Q (-0.017019300826933585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.60692389467841 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09855533507293511) - present_state_Q ( -0.31291085377075584)) * f1( 0.019216250466242615)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.09855533507293511) - present_state_Q (-0.31291085377075584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.61734737315107 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11712884601938114) - present_state_Q ( -0.1874864800108214)) * f1( 0.00479270350132052)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11712884601938114) - present_state_Q (-0.1874864800108214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.663536429333167 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09284748157284549) - present_state_Q ( -0.35585635801821824)) * f1( 0.021405780743157212)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.09284748157284549) - present_state_Q (-0.35585635801821824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.706331109367813 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20310882390544358) - present_state_Q ( -0.4646338198005035)) * f1( 0.01992295099227811)
w2 ( -18.581894665398934 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.20310882390544358) - present_state_Q (-0.4646338198005035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -20.7575839607541 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32232800452124827) - present_state_Q ( -4.338452493687129)) * f1( 0.02909086519853857)
w2 ( -18.934258550326383 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.32232800452124827) - present_state_Q (-4.338452493687129)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -20.811020611148145 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.220120877122184) - present_state_Q ( -8.357056835467127)) * f1( 0.03713620081712948)
w2 ( -19.50983331800048 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -8.220120877122184) - present_state_Q (-8.357056835467127)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -20.851904881395814 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.49728014961085) - present_state_Q ( -12.551646039991763)) * f1( 0.03999441094358419)
w2 ( -20.123183072875083 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -8.49728014961085) - present_state_Q (-12.551646039991763)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -20.91291737249899 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.731595597317872) - present_state_Q ( -9.011275928144618)) * f1( 0.04425589306200441)
w2 ( -20.674634975722864 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -8.731595597317872) - present_state_Q (-9.011275928144618)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -20.94796866311249 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.842930840234638) - present_state_Q ( -8.842930840234638)) * f1( 0.026854578035577142)
w2 ( -21.196725333159367 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -8.842930840234638) - present_state_Q (-8.842930840234638)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -20.983734965147033 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.648352325529154) - present_state_Q ( -9.181899891829865)) * f1( 0.03142821855907235)
w2 ( -21.651937924573893 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -4.648352325529154) - present_state_Q (-9.181899891829865)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -21.024988947619043 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.051751843360668) - present_state_Q ( -9.411926124553142)) * f1( 0.035592532757189356)
w2 ( -22.115563064750813 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -9.051751843360668) - present_state_Q (-9.411926124553142)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -21.03686428975029 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.484499694535378) - present_state_Q ( -4.659505269881087)) * f1( 0.007931273149486743)
w2 ( -22.41501920268539 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -4.484499694535378) - present_state_Q (-4.659505269881087)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.053715794072303 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.662192353318786) - present_state_Q ( -4.662192353318786)) * f1( 0.011243414087867769)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -4.662192353318786) - present_state_Q (-4.662192353318786)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -21.058865865242364 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0782214328646489) - present_state_Q ( -0.0782214328646489)) * f1( 0.0028297178828138835)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.0782214328646489) - present_state_Q (-0.0782214328646489)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.070188001992584 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14543812270506806) - present_state_Q ( -0.14543812270506806)) * f1( 0.00624171976064693)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14543812270506806) - present_state_Q (-0.14543812270506806)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.095574432107895 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1527405452132245) - present_state_Q ( -0.19411801835098735)) * f1( 0.014032240959539834)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.1527405452132245) - present_state_Q (-0.19411801835098735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.10880126264282 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026947574018024233) - present_state_Q ( -0.026947574018024233)) * f1( 0.007249130629436244)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.026947574018024233) - present_state_Q (-0.026947574018024233)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.12183963202153 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01727453864766123) - present_state_Q ( -0.01727453864766123)) * f1( 0.007142434265890944)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.01727453864766123) - present_state_Q (-0.01727453864766123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.134738584695146 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03166585435323168) - present_state_Q ( -0.03166585435323168)) * f1( 0.007071078716131968)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.03166585435323168) - present_state_Q (-0.03166585435323168)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.1478777543741 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019545923191330087) - present_state_Q ( -0.019545923191330087)) * f1( 0.007198458902891999)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.019545923191330087) - present_state_Q (-0.019545923191330087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.16083488010484 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038655906191936755) - present_state_Q ( -0.038655906191936755)) * f1( 0.007105419051257393)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.038655906191936755) - present_state_Q (-0.038655906191936755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.174168704806537 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025703626590791895) - present_state_Q ( -0.025703626590791895)) * f1( 0.007307321760352111)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.025703626590791895) - present_state_Q (-0.025703626590791895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.18712309192968 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13006106075862622) - present_state_Q ( -0.27234507496510485)) * f1( 0.007576777692453623)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.13006106075862622) - present_state_Q (-0.27234507496510485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.199865241284662 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19211962605644228) - present_state_Q ( -0.19211962605644228)) * f1( 0.00783148980296287)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.19211962605644228) - present_state_Q (-0.19211962605644228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.21552868785993 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23366820266264224) - present_state_Q ( -0.23366820266264224)) * f1( 0.010224520992195032)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.23366820266264224) - present_state_Q (-0.23366820266264224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.230645158547095 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19442708988604718) - present_state_Q ( -0.19442708988604718)) * f1( 0.009844779481232717)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.19442708988604718) - present_state_Q (-0.19442708988604718)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.266898465440295 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19740169619865228) - present_state_Q ( -0.5351087393057513)) * f1( 0.024145651777137775)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.19740169619865228) - present_state_Q (-0.5351087393057513)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.305141598566436 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2694984375854222) - present_state_Q ( -0.5868645548429003)) * f1( 0.025546722464052495)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.2694984375854222) - present_state_Q (-0.5868645548429003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.3223766954882 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1469903308964492) - present_state_Q ( -0.27422144776487023)) * f1( 0.011286700253000772)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1469903308964492) - present_state_Q (-0.27422144776487023)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.342045940948186 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2862582541014142) - present_state_Q ( -0.2862582541014142)) * f1( 0.013698540128429556)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.2862582541014142) - present_state_Q (-0.2862582541014142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.381968400440865 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29350687893935534) - present_state_Q ( -0.6458379678170866)) * f1( 0.028516472930937176)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.29350687893935534) - present_state_Q (-0.6458379678170866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.399621791486798 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27322209408944936) - present_state_Q ( -0.27322209408944936)) * f1( 0.01311850802043136)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.27322209408944936) - present_state_Q (-0.27322209408944936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.416951644360413 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2657836839769056) - present_state_Q ( -0.2657836839769056)) * f1( 0.012871678347480858)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2657836839769056) - present_state_Q (-0.2657836839769056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.450107334538995 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3035668976227603) - present_state_Q ( -0.5895071588146743)) * f1( 0.02522571394786405)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.3035668976227603) - present_state_Q (-0.5895071588146743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.483154335842716 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27316831084592347) - present_state_Q ( -0.5622808979233793)) * f1( 0.025096838304071452)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.27316831084592347) - present_state_Q (-0.5622808979233793)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.49958612971247 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2503480249574067) - present_state_Q ( -0.2503480249574067)) * f1( 0.012192068506007734)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2503480249574067) - present_state_Q (-0.2503480249574067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.511039437255082 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2119750772621531) - present_state_Q ( -0.2119750772621531)) * f1( 0.00847640876762496)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2119750772621531) - present_state_Q (-0.2119750772621531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.52885225245792 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27942715027653064) - present_state_Q ( -0.27942715027653064)) * f1( 0.013242474159708114)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.27942715027653064) - present_state_Q (-0.27942715027653064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.562465366275475 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3145767662096592) - present_state_Q ( -0.6000190008234572)) * f1( 0.025592057759183266)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.3145767662096592) - present_state_Q (-0.6000190008234572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.597357036660078 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29303518358538516) - present_state_Q ( -0.5816980438246524)) * f1( 0.026532854823639082)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.29303518358538516) - present_state_Q (-0.5816980438246524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.607773259976394 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20506987070015742) - present_state_Q ( -0.29128819743928785)) * f1( 0.007754795088699499)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.20506987070015742) - present_state_Q (-0.29128819743928785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.640741439110627 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2879852173190652) - present_state_Q ( -0.5872317450945449)) * f1( 0.025081681948680642)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2879852173190652) - present_state_Q (-0.5872317450945449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.673765199544924 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2524393969625005) - present_state_Q ( -0.5427611704986416)) * f1( 0.02504600350380486)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2524393969625005) - present_state_Q (-0.5427611704986416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.69010278417588 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2005460718815135) - present_state_Q ( -0.2215063212552458)) * f1( 0.012100743011880447)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2005460718815135) - present_state_Q (-0.2215063212552458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.702042654258822 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22412226246515243) - present_state_Q ( -0.22412226246515243)) * f1( 0.008843661236577788)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.22412226246515243) - present_state_Q (-0.22412226246515243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.71768440495061 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2507285461536295) - present_state_Q ( -0.2507285461536295)) * f1( 0.011606166894875748)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2507285461536295) - present_state_Q (-0.2507285461536295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.746500910279682 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15961145085670755) - present_state_Q ( -0.391585139290897)) * f1( 0.021622431181733907)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.15961145085670755) - present_state_Q (-0.391585139290897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.756405534747206 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2510409933117902) - present_state_Q ( -0.2510409933117902)) * f1( 0.007349377027521573)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2510409933117902) - present_state_Q (-0.2510409933117902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.76659019335206 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2849890107397089) - present_state_Q ( -0.2849890107397089)) * f1( 0.007574338209258743)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2849890107397089) - present_state_Q (-0.2849890107397089)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.776830229806702 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19032249858595066) - present_state_Q ( -0.19032249858595066)) * f1( 0.007567572198275965)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.19032249858595066) - present_state_Q (-0.19032249858595066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.81295462848325 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28878979431804797) - present_state_Q ( -0.6194861357385562)) * f1( 0.027550321182920938)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.28878979431804797) - present_state_Q (-0.6194861357385562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.837650735977903 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29592392870250506) - present_state_Q ( -0.3940930886034609)) * f1( 0.01851524199341628)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.29592392870250506) - present_state_Q (-0.3940930886034609)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.849405644893437 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29865177332248255) - present_state_Q ( -0.29865177332248255)) * f1( 0.008750136455399208)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.29865177332248255) - present_state_Q (-0.29865177332248255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.87980208330299 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42023605771839745) - present_state_Q ( -0.48850852636151443)) * f1( 0.022929851093349045)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.42023605771839745) - present_state_Q (-0.48850852636151443)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.903129595504435 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35199450116730824) - present_state_Q ( -0.4041978527773499)) * f1( 0.01749507390249139)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.35199450116730824) - present_state_Q (-0.4041978527773499)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.926802114472956 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16078493203957156) - present_state_Q ( -0.32514516176778535)) * f1( 0.017674379022079887)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.16078493203957156) - present_state_Q (-0.32514516176778535)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.93678208034968 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11936074771724342) - present_state_Q ( -0.11936074771724342)) * f1( 0.007340728699910185)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.11936074771724342) - present_state_Q (-0.11936074771724342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.95990675217802 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14231913690027123) - present_state_Q ( -0.31169916356531147)) * f1( 0.017250405851478804)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.14231913690027123) - present_state_Q (-0.31169916356531147)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.971368789593395 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04916380941011044) - present_state_Q ( -0.2807822830963048)) * f1( 0.008536626885673285)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.04916380941011044) - present_state_Q (-0.2807822830963048)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -21.98505074527003 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08036386683615479) - present_state_Q ( -0.08036386683615479)) * f1( 0.010037801037732759)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.08036386683615479) - present_state_Q (-0.08036386683615479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.004875419982618 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08767298530222425) - present_state_Q ( -0.3965121628336361)) * f1( 0.014888962894800047)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.08767298530222425) - present_state_Q (-0.3965121628336361)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.038652064260532 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15036205193111313) - present_state_Q ( -0.5884628824568454)) * f1( 0.025726094966520674)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.15036205193111313) - present_state_Q (-0.5884628824568454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.04454906666214 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11082426391908423) - present_state_Q ( -0.11082426391908423)) * f1( 0.004335069533409021)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.11082426391908423) - present_state_Q (-0.11082426391908423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.051895773850845 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01810380961743212) - present_state_Q ( -0.01810380961743212)) * f1( 0.005367863094286824)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.01810380961743212) - present_state_Q (-0.01810380961743212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.064617961494033 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06861107206600292) - present_state_Q ( -0.06861107206600292)) * f1( 0.00932642730371174)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.06861107206600292) - present_state_Q (-0.06861107206600292)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.069605334275405 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09868257161897963) - present_state_Q ( -0.09868257161897963)) * f1( 0.0036634296686109406)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.09868257161897963) - present_state_Q (-0.09868257161897963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.077036661725085 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02412631482697995) - present_state_Q ( -0.02412631482697995)) * f1( 0.005431841963810757)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.02412631482697995) - present_state_Q (-0.02412631482697995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.089715921359627 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0738968214287224) - present_state_Q ( -0.0738968214287224)) * f1( 0.009298200144441458)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.0738968214287224) - present_state_Q (-0.0738968214287224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.095938029775358 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11672678257105619) - present_state_Q ( -0.11672678257105619)) * f1( 0.004575852024390057)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.11672678257105619) - present_state_Q (-0.11672678257105619)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.1079520795525 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2300935944103835) - present_state_Q ( -0.2300935944103835)) * f1( 0.00890214846050331)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2300935944103835) - present_state_Q (-0.2300935944103835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.139696675791676 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2840098908962718) - present_state_Q ( -0.5857187415336025)) * f1( 0.02414874962067015)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2840098908962718) - present_state_Q (-0.5857187415336025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.150403320266392 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20342269944166283) - present_state_Q ( -0.20342269944166283)) * f1( 0.007919304180758837)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.20342269944166283) - present_state_Q (-0.20342269944166283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.181359156158816 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2761940001834413) - present_state_Q ( -0.606707686400477)) * f1( 0.02535253689299708)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2761940001834413) - present_state_Q (-0.606707686400477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.197038674477884 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26559947520620547) - present_state_Q ( -0.26559947520620547)) * f1( 0.01249343906788823)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.26559947520620547) - present_state_Q (-0.26559947520620547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.212504180310788 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05113285473407973) - present_state_Q ( -0.30726753040858573)) * f1( 0.012385198854330003)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.05113285473407973) - present_state_Q (-0.30726753040858573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.22984972138183 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06731119310677336) - present_state_Q ( -0.26553549326208997)) * f1( 0.013842726271495007)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.06731119310677336) - present_state_Q (-0.26553549326208997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.233682825625287 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15567266627183143) - present_state_Q ( -0.15567266627183143)) * f1( 0.0030303288886463393)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15567266627183143) - present_state_Q (-0.15567266627183143)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.24248258797031 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20437925706567675) - present_state_Q ( -0.24373578595199505)) * f1( 0.007002866336163188)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.20437925706567675) - present_state_Q (-0.24373578595199505)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.25622569016788 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06832195984071437) - present_state_Q ( -0.25955695240777266)) * f1( 0.010962456731238378)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.06832195984071437) - present_state_Q (-0.25955695240777266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.26029396400417 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06137144693131114) - present_state_Q ( -0.06137144693131114)) * f1( 0.0031948103879347427)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.06137144693131114) - present_state_Q (-0.06137144693131114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.285820167311048 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07027634647999537) - present_state_Q ( -0.5006319128200722)) * f1( 0.020760374529187477)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.07027634647999537) - present_state_Q (-0.5006319128200722)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.320260147245975 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21019832502805225) - present_state_Q ( -0.6526246206113848)) * f1( 0.028327857502960883)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.21019832502805225) - present_state_Q (-0.6526246206113848)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.32421278775199 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09218217245736478) - present_state_Q ( -0.09218217245736478)) * f1( 0.003110777726238058)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09218217245736478) - present_state_Q (-0.09218217245736478)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.3383718200285 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08567426194586217) - present_state_Q ( -0.2985062934429036)) * f1( 0.011327857827297649)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.08567426194586217) - present_state_Q (-0.2985062934429036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.34186017668698 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.071985663330935) - present_state_Q ( -0.071985663330935)) * f1( 0.002741458735797988)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.071985663330935) - present_state_Q (-0.071985663330935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.357955166806743 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06412834155914678) - present_state_Q ( -0.34867254151238375)) * f1( 0.012930837719093926)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.06412834155914678) - present_state_Q (-0.34867254151238375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.387218363915558 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1236431882651348) - present_state_Q ( -0.5508525557980227)) * f1( 0.023886855415886542)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1236431882651348) - present_state_Q (-0.5508525557980227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.408017950694855 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.68222473487705) - present_state_Q ( -4.851881997842949)) * f1( 0.024744969386697346)
w2 ( -22.71477698426878 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -4.68222473487705) - present_state_Q (-4.851881997842949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.413429934666375 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.544005092565982) - present_state_Q ( -4.544005092565982)) * f1( 0.006220930879370543)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -4.544005092565982) - present_state_Q (-4.544005092565982)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -22.417535421856446 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09310769073422245) - present_state_Q ( -0.09310769073422245)) * f1( 0.0034816082962058933)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.09310769073422245) - present_state_Q (-0.09310769073422245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.423380576813763 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12660375695311185) - present_state_Q ( -0.12660375695311185)) * f1( 0.004969617304390967)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12660375695311185) - present_state_Q (-0.12660375695311185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.431210917804847 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04920447462810365) - present_state_Q ( -0.04920447462810365)) * f1( 0.006618248383337985)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.04920447462810365) - present_state_Q (-0.04920447462810365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.443204118620887 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05608604826446289) - present_state_Q ( -0.05608604826446289)) * f1( 0.01014203005265048)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.05608604826446289) - present_state_Q (-0.05608604826446289)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.468900116358977 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10113671453358501) - present_state_Q ( -0.5391461738343902)) * f1( 0.022646246486396123)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.10113671453358501) - present_state_Q (-0.5391461738343902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.48572023357945 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07385299685221229) - present_state_Q ( -0.3822677478013509)) * f1( 0.01462511836790882)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07385299685221229) - present_state_Q (-0.3822677478013509)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.489658190560192 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10094472341056406) - present_state_Q ( -0.10094472341056406)) * f1( 0.00362232339773801)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.10094472341056406) - present_state_Q (-0.10094472341056406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.50698417018228 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03897926388302606) - present_state_Q ( -0.3534672279942883)) * f1( 0.01632579963980869)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03897926388302606) - present_state_Q (-0.3534672279942883)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.516729058761783 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08833259936401697) - present_state_Q ( -0.2716486949339783)) * f1( 0.009107890432648636)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.08833259936401697) - present_state_Q (-0.2716486949339783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.533759885950936 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14767996525374874) - present_state_Q ( -0.4006734823330245)) * f1( 0.01610281932197431)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14767996525374874) - present_state_Q (-0.4006734823330245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.544155426721158 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25112254068545) - present_state_Q ( -0.25112254068545)) * f1( 0.009682703360401243)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.25112254068545) - present_state_Q (-0.25112254068545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.572154629008118 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2729029523292613) - present_state_Q ( -0.6243927690242105)) * f1( 0.027012947431191533)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2729029523292613) - present_state_Q (-0.6243927690242105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.58557507543347 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18854577993534272) - present_state_Q ( -0.35697275868098016)) * f1( 0.012632091741459038)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.18854577993534272) - present_state_Q (-0.35697275868098016)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.61268638028538 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30326640183674064) - present_state_Q ( -0.6379566478674275)) * f1( 0.026182918868715856)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.30326640183674064) - present_state_Q (-0.6379566478674275)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.620343392381685 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2656497663105339) - present_state_Q ( -0.2920074729016382)) * f1( 0.007158250623590866)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2656497663105339) - present_state_Q (-0.2920074729016382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.62904201187852 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2121326835091853) - present_state_Q ( -0.2121326835091853)) * f1( 0.008075747168301147)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2121326835091853) - present_state_Q (-0.2121326835091853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.654538703992447 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3057331824138632) - present_state_Q ( -0.6054660114248747)) * f1( 0.024545990845115575)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.3057331824138632) - present_state_Q (-0.6054660114248747)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.679825827664448 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.269008536631649) - present_state_Q ( -0.576958669091676)) * f1( 0.02428617118049493)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.269008536631649) - present_state_Q (-0.576958669091676)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.69167248136845 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2095950562181458) - present_state_Q ( -0.2095950562181458)) * f1( 0.01099603208911927)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2095950562181458) - present_state_Q (-0.2095950562181458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.70422191684575 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25104914496228703) - present_state_Q ( -0.25104914496228703)) * f1( 0.011688830444993109)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.25104914496228703) - present_state_Q (-0.25104914496228703)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.718214590095595 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28983543872083356) - present_state_Q ( -0.28983543872083356)) * f1( 0.013075608791444274)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.28983543872083356) - present_state_Q (-0.28983543872083356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.7235302051986 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14992857347484265) - present_state_Q ( -0.14992857347484265)) * f1( 0.005906083634813516)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.14992857347484265) - present_state_Q (-0.14992857347484265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.726821196180413 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11624196161686069) - present_state_Q ( -0.11624196161686069)) * f1( 0.003644284418536441)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.11624196161686069) - present_state_Q (-0.11624196161686069)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.730498072419753 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12729581010451255) - present_state_Q ( -0.12729581010451255)) * f1( 0.004076085508193094)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.12729581010451255) - present_state_Q (-0.12729581010451255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.73338055831727 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09112170092792816) - present_state_Q ( -0.09112170092792816)) * f1( 0.003183954508877516)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09112170092792816) - present_state_Q (-0.09112170092792816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.735605254946233 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08708241870642097) - present_state_Q ( -0.08708241870642097)) * f1( 0.0024563831011396096)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.08708241870642097) - present_state_Q (-0.08708241870642097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.737061179772798 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2013545380032157) - present_state_Q ( -0.2931549944177311)) * f1( 0.0020695100432732956)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2013545380032157) - present_state_Q (-0.2931549944177311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.743563353228392 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05688256146797465) - present_state_Q ( -0.05688256146797465)) * f1( 0.008959933750622588)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05688256146797465) - present_state_Q (-0.05688256146797465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.744610302489097 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1435929164706932) - present_state_Q ( -0.1837218600058386)) * f1( 0.0014665670235510912)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1435929164706932) - present_state_Q (-0.1837218600058386)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.75248388756321 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2124621235182215) - present_state_Q ( -0.2124621235182215)) * f1( 0.01106318854991823)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2124621235182215) - present_state_Q (-0.2124621235182215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.758637747291882 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23124039731223572) - present_state_Q ( -0.23124039731223572)) * f1( 0.00866738167297448)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.23124039731223572) - present_state_Q (-0.23124039731223572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.765595562905354 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15445564911518403) - present_state_Q ( -0.3446055389292002)) * f1( 0.009969676946450736)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.15445564911518403) - present_state_Q (-0.3446055389292002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.771021180698302 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.060038229589710605) - present_state_Q ( -0.27301326026114164)) * f1( 0.007705608213440399)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.060038229589710605) - present_state_Q (-0.27301326026114164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.784407389544356 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20280920994471732) - present_state_Q ( -0.4931610391518726)) * f1( 0.019584058300181843)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.20280920994471732) - present_state_Q (-0.4931610391518726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.791337624900343 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1821619481604853) - present_state_Q ( -0.34648595226453555)) * f1( 0.009928891399020956)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1821619481604853) - present_state_Q (-0.34648595226453555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.80878546901746 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28318579954376966) - present_state_Q ( -0.590858910175212)) * f1( 0.025865527449636896)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.28318579954376966) - present_state_Q (-0.590858910175212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.817212135241515 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2584202077195386) - present_state_Q ( -0.2584202077195386)) * f1( 0.011909539980825846)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2584202077195386) - present_state_Q (-0.2584202077195386)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.82412163636671 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21422214958528046) - present_state_Q ( -0.21422214958528046)) * f1( 0.009710713786556418)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.21422214958528046) - present_state_Q (-0.21422214958528046)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.82941874581717 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2211635527779866) - present_state_Q ( -0.30457683478987)) * f1( 0.007539642191362009)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2211635527779866) - present_state_Q (-0.30457683478987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.838913849884122 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20845932604687692) - present_state_Q ( -0.21362301105650658)) * f1( 0.013344515054834529)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.20845932604687692) - present_state_Q (-0.21362301105650658)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.846337067747314 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06662709371565352) - present_state_Q ( -0.3920219494603313)) * f1( 0.010722887647029215)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.06662709371565352) - present_state_Q (-0.3920219494603313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.848994397421883 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12669927641405304) - present_state_Q ( -0.12669927641405304)) * f1( 0.003693758060704116)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.12669927641405304) - present_state_Q (-0.12669927641405304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.861180951915184 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07681086067898726) - present_state_Q ( -0.38693964733200154)) * f1( 0.01758805970930455)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07681086067898726) - present_state_Q (-0.38693964733200154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.864005911063884 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11236357012960206) - present_state_Q ( -0.11236357012960206)) * f1( 0.003919737655632543)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11236357012960206) - present_state_Q (-0.11236357012960206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.86866461814658 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16335133741362662) - present_state_Q ( -0.16335133741362662)) * f1( 0.006505554895293792)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16335133741362662) - present_state_Q (-0.16335133741362662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.87330994559167 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16358740212350273) - present_state_Q ( -0.16358740212350273)) * f1( 0.006487063637913222)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16358740212350273) - present_state_Q (-0.16358740212350273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.877634788406986 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07632275085674547) - present_state_Q ( -0.07632275085674547)) * f1( 0.005973995685260555)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07632275085674547) - present_state_Q (-0.07632275085674547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.87924057465033 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17312698914683033) - present_state_Q ( -0.19849563254486063)) * f1( 0.0022531168401123303)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17312698914683033) - present_state_Q (-0.19849563254486063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.885525631186837 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07143280802510021) - present_state_Q ( -0.07143280802510021)) * f1( 0.008676405335635759)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07143280802510021) - present_state_Q (-0.07143280802510021)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.886415489983087 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1472483620787776) - present_state_Q ( -0.1472483620787776)) * f1( 0.0012401151176048674)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1472483620787776) - present_state_Q (-0.1472483620787776)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.891062302290177 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09555983155600156) - present_state_Q ( -0.09555983155600156)) * f1( 0.006434126288020124)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.09555983155600156) - present_state_Q (-0.09555983155600156)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.90187799590247 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.044117860054152844) - present_state_Q ( -0.5018031053793385)) * f1( 0.015880334640883505)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.044117860054152844) - present_state_Q (-0.5018031053793385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.908003671830528 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051547651671848306) - present_state_Q ( -0.051547651671848306)) * f1( 0.008435542489168842)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.051547651671848306) - present_state_Q (-0.051547651671848306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.91404015195505 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04388364125249303) - present_state_Q ( -0.04388364125249303)) * f1( 0.008304824396230474)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.04388364125249303) - present_state_Q (-0.04388364125249303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.919880345620225 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049763733989340656) - present_state_Q ( -0.049763733989340656)) * f1( 0.008040632996119524)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.049763733989340656) - present_state_Q (-0.049763733989340656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.925575744744766 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037310652463232605) - present_state_Q ( -0.037310652463232605)) * f1( 0.007829202567530492)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.037310652463232605) - present_state_Q (-0.037310652463232605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.931131353272203 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029155450873167066) - present_state_Q ( -0.029155450873167066)) * f1( 0.007629341238939942)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.029155450873167066) - present_state_Q (-0.029155450873167066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.936446901490108 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09854432189908074) - present_state_Q ( -0.24498069397277822)) * f1( 0.007515254363729077)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.09854432189908074) - present_state_Q (-0.24498069397277822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.94742788108457 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14726406967071506) - present_state_Q ( -0.4024150170370886)) * f1( 0.015867437225082392)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14726406967071506) - present_state_Q (-0.4024150170370886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.95445096975266 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07305864228694857) - present_state_Q ( -0.33915278016038136)) * f1( 0.010067080922689932)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07305864228694857) - present_state_Q (-0.33915278016038136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.965620811918797 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12528055995207957) - present_state_Q ( -0.44158285298240885)) * f1( 0.016237399474876173)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.12528055995207957) - present_state_Q (-0.44158285298240885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.970746365172772 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.342902895146992) - present_state_Q ( -0.188745677889521)) * f1( 0.007164915675835916)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.342902895146992) - present_state_Q (-0.188745677889521)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.977137759767928 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5225708833851015) - present_state_Q ( -5.1003248277647035)) * f1( 0.02827962813183715)
w2 ( -22.88876972189801 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.5225708833851015) - present_state_Q (-5.1003248277647035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -22.978827228188624 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.628301300161725) - present_state_Q ( -4.628301300161725)) * f1( 0.005375906906422318)
w2 ( -22.951623058092483 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.628301300161725) - present_state_Q (-4.628301300161725)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -22.983557616043452 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.756925629711968) - present_state_Q ( -4.873552827844311)) * f1( 0.01625407716402702)
w2 ( -23.009828612392404 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.756925629711968) - present_state_Q (-4.873552827844311)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -22.9862796446278 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.78670894385387) - present_state_Q ( -4.9445305729958315)) * f1( 0.00957692490261771)
w2 ( -23.06667417841758 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.78670894385387) - present_state_Q (-4.9445305729958315)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -22.988809552180786 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.844354651621026) - present_state_Q ( -4.844354651621026)) * f1( 0.008581139088591606)
w2 ( -23.125638554285782 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.844354651621026) - present_state_Q (-4.844354651621026)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -22.995680367319835 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.755904429989843) - present_state_Q ( -5.196355228243644)) * f1( 0.026555176320610694)
w2 ( -23.17738601817827 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.755904429989843) - present_state_Q (-5.196355228243644)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.000223557687917 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.878309259908897) - present_state_Q ( -4.971303973939012)) * f1( 0.016083997492016302)
w2 ( -23.23387931681669 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.878309259908897) - present_state_Q (-4.971303973939012)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.004604678105682 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.901791446877287) - present_state_Q ( -4.9756249473779715)) * f1( 0.015521070463109172)
w2 ( -23.29033316036027 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.901791446877287) - present_state_Q (-4.9756249473779715)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.005782720036898 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.881089926595972) - present_state_Q ( -4.894819811220023)) * f1( 0.004060215424064136)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.881089926595972) - present_state_Q (-4.894819811220023)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.01067017644389 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12849434093426754) - present_state_Q ( -0.12849434093426754)) * f1( 0.0077838433253864334)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12849434093426754) - present_state_Q (-0.12849434093426754)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.015885981556156 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10204396005976257) - present_state_Q ( -0.31079405055866255)) * f1( 0.008558875144626875)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10204396005976257) - present_state_Q (-0.31079405055866255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.018994637392428 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12966101200653612) - present_state_Q ( -0.12966101200653612)) * f1( 0.004951724382577747)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12966101200653612) - present_state_Q (-0.12966101200653612)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.021870296072212 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12298016473770272) - present_state_Q ( -0.12298016473770272)) * f1( 0.004576204304497208)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12298016473770272) - present_state_Q (-0.12298016473770272)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.035528359342273 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1110881028512794) - present_state_Q ( -0.5563232458422456)) * f1( 0.023349486387133026)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1110881028512794) - present_state_Q (-0.5563232458422456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.044487089779793 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12593735343751214) - present_state_Q ( -0.41699414134732277)) * f1( 0.01495559420087588)
w2 ( -23.348361703586445 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12593735343751214) - present_state_Q (-0.41699414134732277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.057061161918448 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4412090508725695) - present_state_Q ( -0.4949878445016384)) * f1( 0.021155102611292276)
w2 ( -23.467236779445866 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.4412090508725695) - present_state_Q (-0.4949878445016384)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.060381037330224 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.7879628085156165) - present_state_Q ( -5.113623457932324)) * f1( 0.018865141321660152)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -4.7879628085156165) - present_state_Q (-5.113623457932324)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.062527574872895 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1078552382191748) - present_state_Q ( -0.1078552382191748)) * f1( 0.003986857501585555)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1078552382191748) - present_state_Q (-0.1078552382191748)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.06503583343815 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05367476872582267) - present_state_Q ( -0.05367476872582267)) * f1( 0.004616883233102712)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.05367476872582267) - present_state_Q (-0.05367476872582267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.07380258557808 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08328074572749027) - present_state_Q ( -0.5445668460657441)) * f1( 0.017729003012270237)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08328074572749027) - present_state_Q (-0.5445668460657441)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.076593255851378 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13585568902470269) - present_state_Q ( -0.13585568902470269)) * f1( 0.0052076078566650735)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13585568902470269) - present_state_Q (-0.13585568902470269)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.078623425106343 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1035872121323231) - present_state_Q ( -0.1035872121323231)) * f1( 0.003768033328969118)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1035872121323231) - present_state_Q (-0.1035872121323231)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.081134068804804 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14187178875485537) - present_state_Q ( -0.14187178875485537)) * f1( 0.004689794983490408)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14187178875485537) - present_state_Q (-0.14187178875485537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.08335218933697 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11445526928434444) - present_state_Q ( -0.11445526928434444)) * f1( 0.0041243619734975066)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11445526928434444) - present_state_Q (-0.11445526928434444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.09252630904634 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15173781489914148) - present_state_Q ( -0.4771080570282365)) * f1( 0.018278163828898037)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15173781489914148) - present_state_Q (-0.4771080570282365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.094312752793318 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07715964613239784) - present_state_Q ( -0.07715964613239784)) * f1( 0.003301101347613344)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07715964613239784) - present_state_Q (-0.07715964613239784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.095852433766503 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12380312440436875) - present_state_Q ( -0.12380312440436875)) * f1( 0.0028673604017896426)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12380312440436875) - present_state_Q (-0.12380312440436875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.106728883444813 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08309661932887759) - present_state_Q ( -0.5446108205006712)) * f1( 0.021995721892433828)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08309661932887759) - present_state_Q (-0.5446108205006712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.115184686212192 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10263258731420064) - present_state_Q ( -0.42939181675239224)) * f1( 0.016704552846510585)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.10263258731420064) - present_state_Q (-0.42939181675239224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.128016250979446 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17538389284376082) - present_state_Q ( -0.6371864703990796)) * f1( 0.02639449239266766)
w2 ( -23.50243265055196 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17538389284376082) - present_state_Q (-0.6371864703990796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.129483479374084 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.7992533881113815) - present_state_Q ( -5.109810456112264)) * f1( 0.017236803744896355)
w2 ( -23.519457017903974 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -4.7992533881113815) - present_state_Q (-5.109810456112264)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.13006140779211 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.814762660870607) - present_state_Q ( -4.921424541975325)) * f1( 0.005550838271009989)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -4.814762660870607) - present_state_Q (-4.921424541975325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.13917127158503 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22825702548837487) - present_state_Q ( -0.5260110841633896)) * f1( 0.022413792625579642)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.22825702548837487) - present_state_Q (-0.5260110841633896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.142169480132804 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16456397506688486) - present_state_Q ( -0.16456397506688486)) * f1( 0.006784077441094049)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.16456397506688486) - present_state_Q (-0.16456397506688486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.152518862966645 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06255318143824314) - present_state_Q ( -0.41384886430311774)) * f1( 0.02487836782883322)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.06255318143824314) - present_state_Q (-0.41384886430311774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.158371209433867 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08934407574706375) - present_state_Q ( -0.3888060215232332)) * f1( 0.0139750364898823)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.08934407574706375) - present_state_Q (-0.3888060215232332)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.163762285276935 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07383192668097892) - present_state_Q ( -0.3462767756137273)) * f1( 0.012748800358818738)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.07383192668097892) - present_state_Q (-0.3462767756137273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.16584148785959 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13878922031078378) - present_state_Q ( -0.13878922031078378)) * f1( 0.004680068074155809)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13878922031078378) - present_state_Q (-0.13878922031078378)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.176072848898333 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15648079637988152) - present_state_Q ( -0.6196247400336128)) * f1( 0.02581324128908283)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15648079637988152) - present_state_Q (-0.6196247400336128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.183965649306373 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13646962007601854) - present_state_Q ( -0.4639985997802493)) * f1( 0.019170149810558614)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13646962007601854) - present_state_Q (-0.4639985997802493)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.194121501883618 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2732352842138354) - present_state_Q ( -0.6009596631063762)) * f1( 0.025428090777338902)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2732352842138354) - present_state_Q (-0.6009596631063762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.20473846522047 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19929391457872758) - present_state_Q ( -0.6585951798582121)) * f1( 0.027022596854831056)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19929391457872758) - present_state_Q (-0.6585951798582121)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.213176676371692 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13478694390993087) - present_state_Q ( -0.46218983940551017)) * f1( 0.020486688627157884)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13478694390993087) - present_state_Q (-0.46218983940551017)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.215342643010345 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12924128412313426) - present_state_Q ( -0.12924128412313426)) * f1( 0.00486595305494619)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12924128412313426) - present_state_Q (-0.12924128412313426)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.217477385846774 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034641825483250184) - present_state_Q ( -0.034641825483250184)) * f1( 0.0047057992946799646)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.034641825483250184) - present_state_Q (-0.034641825483250184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.22166359905169 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0790549079243957) - present_state_Q ( -0.0790549079243957)) * f1( 0.009310067887601546)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0790549079243957) - present_state_Q (-0.0790549079243957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.223862138003486 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16133596339299466) - present_state_Q ( -0.16133596339299466)) * f1( 0.006265666917796906)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16133596339299466) - present_state_Q (-0.16133596339299466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.225697182323568 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07424743003062204) - present_state_Q ( -0.07424743003062204)) * f1( 0.0051154678690327185)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07424743003062204) - present_state_Q (-0.07424743003062204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.230564034356735 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09571098315891316) - present_state_Q ( -0.4086301679609081)) * f1( 0.014951880800212078)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09571098315891316) - present_state_Q (-0.4086301679609081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.231871519241995 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18653607483240062) - present_state_Q ( -0.18653607483240062)) * f1( 0.0037504731223921314)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18653607483240062) - present_state_Q (-0.18653607483240062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.234769558844665 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049944112941157554) - present_state_Q ( -0.049944112941157554)) * f1( 0.008029769512118774)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.049944112941157554) - present_state_Q (-0.049944112941157554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.237600335382506 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037338445533099174) - present_state_Q ( -0.037338445533099174)) * f1( 0.00781882165856345)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.037338445533099174) - present_state_Q (-0.037338445533099174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.240364693462354 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0294351430890268) - present_state_Q ( -0.0294351430890268)) * f1( 0.007620397319296986)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0294351430890268) - present_state_Q (-0.0294351430890268)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.243073070001113 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04706671564460284) - present_state_Q ( -0.04706671564460284)) * f1( 0.0074988781747434875)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.04706671564460284) - present_state_Q (-0.04706671564460284)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.245748725200997 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03305377755141474) - present_state_Q ( -0.03305377755141474)) * f1( 0.007382501446381913)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03305377755141474) - present_state_Q (-0.03305377755141474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.248385971403838 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021638117355126267) - present_state_Q ( -0.021638117355126267)) * f1( 0.007255956803715941)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.021638117355126267) - present_state_Q (-0.021638117355126267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.2509772407609 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038752797389072925) - present_state_Q ( -0.038752797389072925)) * f1( 0.007159801786964506)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.038752797389072925) - present_state_Q (-0.038752797389072925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.253630630791783 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02514321370687665) - present_state_Q ( -0.02514321370687665)) * f1( 0.0073067156818551054)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02514321370687665) - present_state_Q (-0.02514321370687665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.256227922733974 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047468300006964746) - present_state_Q ( -0.047468300006964746)) * f1( 0.007192029921960566)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.047468300006964746) - present_state_Q (-0.047468300006964746)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.258772346576034 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11774120254522988) - present_state_Q ( -0.24000931446571344)) * f1( 0.007427166622144748)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11774120254522988) - present_state_Q (-0.24000931446571344)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.263711010792516 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1320055787609824) - present_state_Q ( -0.3896660900553807)) * f1( 0.015067912516374064)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1320055787609824) - present_state_Q (-0.3896660900553807)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.265856252078784 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16165201073224095) - present_state_Q ( -0.16165201073224095)) * f1( 0.006114268317067505)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16165201073224095) - present_state_Q (-0.16165201073224095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.26880632630847 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23342645967204145) - present_state_Q ( -0.23342645967204145)) * f1( 0.008565873773345977)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.23342645967204145) - present_state_Q (-0.23342645967204145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.272865498550782 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26250951025124253) - present_state_Q ( -0.26250951025124253)) * f1( 0.01187652833474598)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.26250951025124253) - present_state_Q (-0.26250951025124253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.27594436293646 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13770794849503057) - present_state_Q ( -0.33482286741908457)) * f1( 0.009237470021945377)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13770794849503057) - present_state_Q (-0.33482286741908457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.27747304313052 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11049768335730199) - present_state_Q ( -0.11049768335730199)) * f1( 0.004300543326119305)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11049768335730199) - present_state_Q (-0.11049768335730199)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.278941376751675 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11507733252566806) - present_state_Q ( -0.11507733252566806)) * f1( 0.004135569280810501)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11507733252566806) - present_state_Q (-0.11507733252566806)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.28053784045715 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1307463684653236) - present_state_Q ( -0.1307463684653236)) * f1( 0.004514378868832511)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1307463684653236) - present_state_Q (-0.1307463684653236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.281703171140745 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09493514889670249) - present_state_Q ( -0.09493514889670249)) * f1( 0.003265487167379925)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09493514889670249) - present_state_Q (-0.09493514889670249)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.282513057089293 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1964323717876849) - present_state_Q ( -0.20153439916756222)) * f1( 0.002332501354140109)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1964323717876849) - present_state_Q (-0.20153439916756222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.285629602560995 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059740187351775) - present_state_Q ( -0.059740187351775)) * f1( 0.008656342608876563)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.059740187351775) - present_state_Q (-0.059740187351775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.287858409494923 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16281159806205905) - present_state_Q ( -0.16281159806205905)) * f1( 0.006354333391039032)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16281159806205905) - present_state_Q (-0.16281159806205905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.293523559228177 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09459528721952994) - present_state_Q ( -0.42249490301180376)) * f1( 0.017479453794945813)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09459528721952994) - present_state_Q (-0.42249490301180376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.295833569332196 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16850897746818094) - present_state_Q ( -0.16850897746818094)) * f1( 0.006595485690394599)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16850897746818094) - present_state_Q (-0.16850897746818094)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.297430526745096 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12965543713859523) - present_state_Q ( -0.12965543713859523)) * f1( 0.00451452154075292)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12965543713859523) - present_state_Q (-0.12965543713859523)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.29849745116957 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15714361300433624) - present_state_Q ( -0.15714361300433624)) * f1( 0.003037386421566621)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15714361300433624) - present_state_Q (-0.15714361300433624)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.303733399868175 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1284288149526312) - present_state_Q ( -0.44766964005385385)) * f1( 0.01626453780823039)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1284288149526312) - present_state_Q (-0.44766964005385385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.307908884745597 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06632135780848077) - present_state_Q ( -0.3468334834471418)) * f1( 0.012600035149755877)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06632135780848077) - present_state_Q (-0.3468334834471418)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.310415944305777 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040952505050898695) - present_state_Q ( -0.040952505050898695)) * f1( 0.0069309173572941465)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.040952505050898695) - present_state_Q (-0.040952505050898695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.313014359687767 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02586887300131388) - present_state_Q ( -0.02586887300131388)) * f1( 0.007156617501821239)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02586887300131388) - present_state_Q (-0.02586887300131388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.31558418313407 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017968065703135146) - present_state_Q ( -0.017968065703135146)) * f1( 0.007064034330996233)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.017968065703135146) - present_state_Q (-0.017968065703135146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.318121349320613 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04216889090676295) - present_state_Q ( -0.04216889090676295)) * f1( 0.007016272412862336)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.04216889090676295) - present_state_Q (-0.04216889090676295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.320651930781573 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025993593295015462) - present_state_Q ( -0.025993593295015462)) * f1( 0.006970003184112251)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.025993593295015462) - present_state_Q (-0.025993593295015462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.323167216614596 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014043772861828167) - present_state_Q ( -0.014043772861828167)) * f1( 0.006907413058101337)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.014043772861828167) - present_state_Q (-0.014043772861828167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.32567147982058 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04607952507680251) - present_state_Q ( -0.04607952507680251)) * f1( 0.00693202955288918)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.04607952507680251) - present_state_Q (-0.04607952507680251)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.32817795138749 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02995948448966278) - present_state_Q ( -0.02995948448966278)) * f1( 0.006910390664878957)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02995948448966278) - present_state_Q (-0.02995948448966278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.330675856521644 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014220752470870074) - present_state_Q ( -0.014220752470870074)) * f1( 0.006859982697340981)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.014220752470870074) - present_state_Q (-0.014220752470870074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.333147210022076 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03770845228158042) - present_state_Q ( -0.03770845228158042)) * f1( 0.0068266956059064)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03770845228158042) - present_state_Q (-0.03770845228158042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.335705951001646 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02203758041328963) - present_state_Q ( -0.02203758041328963)) * f1( 0.007040658795025191)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02203758041328963) - present_state_Q (-0.02203758041328963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.33824101483 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017550478403014016) - present_state_Q ( -0.017550478403014016)) * f1( 0.006967765856483534)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.017550478403014016) - present_state_Q (-0.017550478403014016)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.343864622430615 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2190792988343117) - present_state_Q ( -0.2190792988343117)) * f1( 0.016267787528562453)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2190792988343117) - present_state_Q (-0.2190792988343117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.347254317330894 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06902035131385313) - present_state_Q ( -0.06902035131385313)) * f1( 0.009436919401614584)
w2 ( -23.540280122084248 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06902035131385313) - present_state_Q (-0.06902035131385313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.360217024447937 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.077208528186676) - present_state_Q ( -9.581202067332416)) * f1( 0.007071067811865476)
w2 ( -24.273561570484066 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -5.077208528186676) - present_state_Q (-9.581202067332416)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -23.37629813214296 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.031803365796265) - present_state_Q ( -5.1665771608075595)) * f1( 0.007071067811865476)
w2 ( -25.183246394495317 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -5.031803365796265) - present_state_Q (-5.1665771608075595)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -23.413697184950735 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-10.219882668712435) - present_state_Q ( -10.550079254310477)) * f1( 0.0209197072852856)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -10.219882668712435) - present_state_Q (-10.550079254310477)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -23.425814111341886 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12818623156423445) - present_state_Q ( -0.12818623156423445)) * f1( 0.004758618982138213)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.12818623156423445) - present_state_Q (-0.12818623156423445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.493907020187233 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20208726520935602) - present_state_Q ( -0.6692233028013849)) * f1( 0.027314224226703012)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.20208726520935602) - present_state_Q (-0.6692233028013849)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.546098422661608 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04294771890177812) - present_state_Q ( -0.2894128489477678)) * f1( 0.020634424394999976)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.04294771890177812) - present_state_Q (-0.2894128489477678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.58227263989816 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17209233880594585) - present_state_Q ( -0.4369909149178554)) * f1( 0.014378412017076571)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.17209233880594585) - present_state_Q (-0.4369909149178554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.63126502009475 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17601379046562518) - present_state_Q ( -0.5056471279710306)) * f1( 0.02026411152954474)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.17601379046562518) - present_state_Q (-0.5056471279710306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.65538332462966 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09278678093910855) - present_state_Q ( -0.3488365535060907)) * f1( 0.009914861336187256)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.09278678093910855) - present_state_Q (-0.3488365535060907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.671470977440485 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1609526154275777) - present_state_Q ( -0.21158113694068728)) * f1( 0.006574579001244394)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1609526154275777) - present_state_Q (-0.21158113694068728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.704411999270288 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0633504800180842) - present_state_Q ( -0.32979817467954)) * f1( 0.013532862958997144)
w2 ( -25.898343451978118 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.0633504800180842) - present_state_Q (-0.32979817467954)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.7337856135749 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.579727331239617) - present_state_Q ( -5.549258236836748)) * f1( 0.014930411615077422)
w2 ( -26.291817055545028 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -5.579727331239617) - present_state_Q (-5.549258236836748)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.773288766959833 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10347019808409019) - present_state_Q ( -0.43027095911637475)) * f1( 0.01762117076166115)
w2 ( -26.291817055545028 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.10347019808409019) - present_state_Q (-0.43027095911637475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.812004391995746 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14703545076185032) - present_state_Q ( -0.4827423543518923)) * f1( 0.018043879639848145)
w2 ( -26.291817055545028 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.14703545076185032) - present_state_Q (-0.4827423543518923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.853270906851023 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12835676343244026) - present_state_Q ( -0.4646608513762861)) * f1( 0.02007216854174222)
w2 ( -26.291817055545028 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.12835676343244026) - present_state_Q (-0.4646608513762861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.89028377176393 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4211912320442679) - present_state_Q ( -5.8669746833247975)) * f1( 0.02437295183858396)
w2 ( -26.595537878185095 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.4211912320442679) - present_state_Q (-5.8669746833247975)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.925721070199714 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.4259103677683855) - present_state_Q ( -5.899626486934999)) * f1( 0.022638055804904536)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -5.4259103677683855) - present_state_Q (-5.899626486934999)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -23.930376063747378 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10482321132403317) - present_state_Q ( -0.10482321132403317)) * f1( 0.0025610654203375295)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.10482321132403317) - present_state_Q (-0.10482321132403317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -23.9771015181099 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1588670267324866) - present_state_Q ( -0.4939363849949433)) * f1( 0.026261622607097097)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.1588670267324866) - present_state_Q (-0.4939363849949433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.016088929546964 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21680323584508626) - present_state_Q ( -0.2647894737660723)) * f1( 0.0216269492842393)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.21680323584508626) - present_state_Q (-0.2647894737660723)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.03255764809165 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0632859604272683) - present_state_Q ( -0.0632859604272683)) * f1( 0.009042095254146329)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.0632859604272683) - present_state_Q (-0.0632859604272683)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.074358703165046 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2404343269803334) - present_state_Q ( -0.5626470407984187)) * f1( 0.023574139881791446)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.2404343269803334) - present_state_Q (-0.5626470407984187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.120701112060658 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17480727571202892) - present_state_Q ( -0.6562853808733226)) * f1( 0.026283811916959922)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.17480727571202892) - present_state_Q (-0.6562853808733226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.1555285926253 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11552794242271504) - present_state_Q ( -0.4566457767188617)) * f1( 0.019538282336054615)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.11552794242271504) - present_state_Q (-0.4566457767188617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.169450368543266 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15307403181542587) - present_state_Q ( -0.15307403181542587)) * f1( 0.0076777696318090696)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.15307403181542587) - present_state_Q (-0.15307403181542587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.191666057787042 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06130854466565152) - present_state_Q ( -0.3886801693248311)) * f1( 0.012419467271047495)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.06130854466565152) - present_state_Q (-0.3886801693248311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.20615978825653 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0416868435866318) - present_state_Q ( -0.0416868435866318)) * f1( 0.007949250347228794)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.0416868435866318) - present_state_Q (-0.0416868435866318)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.220441662911064 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056946089458959674) - present_state_Q ( -0.056946089458959674)) * f1( 0.007838960088057482)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.056946089458959674) - present_state_Q (-0.056946089458959674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.253831654487865 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09666965028305105) - present_state_Q ( -0.43740240614065923)) * f1( 0.01871362569515394)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.09666965028305105) - present_state_Q (-0.43740240614065923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.29182263814079 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25597941911375977) - present_state_Q ( -0.5186366647392053)) * f1( 0.02137049515159817)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.25597941911375977) - present_state_Q (-0.5186366647392053)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.32300759069118 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10006243073859634) - present_state_Q ( -0.438671374249202)) * f1( 0.017478708769372832)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.10006243073859634) - present_state_Q (-0.438671374249202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.35615506601757 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11165248475338693) - present_state_Q ( -0.4355451096221119)) * f1( 0.018574212804946543)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.11165248475338693) - present_state_Q (-0.4355451096221119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.400442119307137 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2088750320881046) - present_state_Q ( -0.6841708384458949)) * f1( 0.026529448939099174)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.2088750320881046) - present_state_Q (-0.6841708384458949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.43011327247933 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08269800775730603) - present_state_Q ( -0.4112049789296864)) * f1( 0.0175010860368064)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08269800775730603) - present_state_Q (-0.4112049789296864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.474199945184026 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24975279940055248) - present_state_Q ( -0.6658486324983317)) * f1( 0.026374008795777032)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.24975279940055248) - present_state_Q (-0.6658486324983317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.496349581025715 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13965864773870307) - present_state_Q ( -0.13965864773870307)) * f1( 0.012854426563034475)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.13965864773870307) - present_state_Q (-0.13965864773870307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.527652401975843 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14551269186979154) - present_state_Q ( -0.4761134898457067)) * f1( 0.01852756974097173)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.14551269186979154) - present_state_Q (-0.4761134898457067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.546241575433772 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05054493940260069) - present_state_Q ( -0.3936264540455762)) * f1( 0.010955264784125314)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.05054493940260069) - present_state_Q (-0.3936264540455762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.56266092638322 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07548911643906177) - present_state_Q ( -0.07548911643906177)) * f1( 0.009497054655439399)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.07548911643906177) - present_state_Q (-0.07548911643906177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.570592332208097 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1387659821177938) - present_state_Q ( -0.1387659821177938)) * f1( 0.004860400274306906)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1387659821177938) - present_state_Q (-0.1387659821177938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.612774986888358 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20973484487266206) - present_state_Q ( -0.6930788501599936)) * f1( 0.026746627440406533)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.20973484487266206) - present_state_Q (-0.6930788501599936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.61825111517459 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08110543899747012) - present_state_Q ( -0.08110543899747012)) * f1( 0.0033451574383491656)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.08110543899747012) - present_state_Q (-0.08110543899747012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.65935991852157 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1823710131623305) - present_state_Q ( -0.6634478191419995)) * f1( 0.026021359550614243)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1823710131623305) - present_state_Q (-0.6634478191419995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.689728905891407 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10610655255331185) - present_state_Q ( -0.4532023846747106)) * f1( 0.018979764528855553)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.10610655255331185) - present_state_Q (-0.4532023846747106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.72976415296659 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25452277437218257) - present_state_Q ( -0.6873011346614215)) * f1( 0.026927224189665377)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.25452277437218257) - present_state_Q (-0.6873011346614215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.761084634211485 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1308875165524804) - present_state_Q ( -0.6102984841680357)) * f1( 0.02389864117352794)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.1308875165524804) - present_state_Q (-0.6102984841680357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.77536704765245 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13982891345848458) - present_state_Q ( -0.4367722627075908)) * f1( 0.011549321772388843)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.13982891345848458) - present_state_Q (-0.4367722627075908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.80598110109642 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1565115687608562) - present_state_Q ( -0.6510999257716967)) * f1( 0.025188888941937996)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1565115687608562) - present_state_Q (-0.6510999257716967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.811800267905564 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14986551503187917) - present_state_Q ( -0.20396229106323516)) * f1( 0.004618288959971242)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.14986551503187917) - present_state_Q (-0.20396229106323516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.831005821121302 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12766742417789756) - present_state_Q ( -0.44673655602164897)) * f1( 0.015544420032175417)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.12766742417789756) - present_state_Q (-0.44673655602164897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.85150249986786 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12312261068726581) - present_state_Q ( -0.4291056570833416)) * f1( 0.017887077831473717)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12312261068726581) - present_state_Q (-0.4291056570833416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.872979027048796 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14344231963061163) - present_state_Q ( -0.49260008045841375)) * f1( 0.018843243504623117)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.14344231963061163) - present_state_Q (-0.49260008045841375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.87880239110575 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11265151235985775) - present_state_Q ( -0.11265151235985775)) * f1( 0.005361808528813378)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11265151235985775) - present_state_Q (-0.11265151235985775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.902890141078245 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22108982287803955) - present_state_Q ( -0.5703750227100423)) * f1( 0.023130292495258064)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.22108982287803955) - present_state_Q (-0.5703750227100423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.923841180769564 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1828277221209911) - present_state_Q ( -0.4452296948853904)) * f1( 0.019886589982205615)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1828277221209911) - present_state_Q (-0.4452296948853904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.942600764317593 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13423923113273942) - present_state_Q ( -0.48289301950947167)) * f1( 0.017878635466128785)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13423923113273942) - present_state_Q (-0.48289301950947167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.949179933643215 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18451011291302946) - present_state_Q ( -0.25935727085420623)) * f1( 0.00613654024211585)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.18451011291302946) - present_state_Q (-0.25935727085420623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.960533113088214 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04862387173613958) - present_state_Q ( -0.04862387173613958)) * f1( 0.010398164702425008)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04862387173613958) - present_state_Q (-0.04862387173613958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.96902091019426 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04003751613428254) - present_state_Q ( -0.04003751613428254)) * f1( 0.007768316451310931)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04003751613428254) - present_state_Q (-0.04003751613428254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.97709839148232 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2615343294197462) - present_state_Q ( -0.33012185091036195)) * f1( 0.007578626851682728)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2615343294197462) - present_state_Q (-0.33012185091036195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -24.991433542716386 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13300525138550698) - present_state_Q ( -0.13300525138550698)) * f1( 0.013221257337150173)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13300525138550698) - present_state_Q (-0.13300525138550698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.00144434607474 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06784410263846642) - present_state_Q ( -0.06784410263846642)) * f1( 0.009183256682609773)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06784410263846642) - present_state_Q (-0.06784410263846642)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.011055774876095 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06908686135785042) - present_state_Q ( -0.06908686135785042)) * f1( 0.008817801294369713)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06908686135785042) - present_state_Q (-0.06908686135785042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.013502508953156 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1930519275381297) - present_state_Q ( -0.1930519275381297)) * f1( 0.0022679177785915497)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1930519275381297) - present_state_Q (-0.1930519275381297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.021940089122147 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03422972802328825) - present_state_Q ( -0.03422972802328825)) * f1( 0.007718663669210344)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03422972802328825) - present_state_Q (-0.03422972802328825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.030172530051708 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.050375446150428785) - present_state_Q ( -0.050375446150428785)) * f1( 0.007541027448511733)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.050375446150428785) - present_state_Q (-0.050375446150428785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.03855788224027 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0365926289466806) - present_state_Q ( -0.0365926289466806)) * f1( 0.007672378323186616)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.0365926289466806) - present_state_Q (-0.0365926289466806)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.046548852038626 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25721054395580767) - present_state_Q ( -0.3291822017726719)) * f1( 0.007497101377110005)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.25721054395580767) - present_state_Q (-0.3291822017726719)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.060806795965217 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13022130980205585) - present_state_Q ( -0.13022130980205585)) * f1( 0.013147011234467272)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13022130980205585) - present_state_Q (-0.13022130980205585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.070642003881957 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17715867699396565) - present_state_Q ( -0.17715867699396565)) * f1( 0.009104343824051165)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.17715867699396565) - present_state_Q (-0.17715867699396565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.084495527213022 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037405761080365516) - present_state_Q ( -0.4869988191166885)) * f1( 0.013220336974873704)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.037405761080365516) - present_state_Q (-0.4869988191166885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.09215452283262 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02066170659352062) - present_state_Q ( -0.02066170659352062)) * f1( 0.006998599745504095)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.02066170659352062) - present_state_Q (-0.02066170659352062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.099727532595953 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04742098060632293) - present_state_Q ( -0.04742098060632293)) * f1( 0.006935290337473088)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04742098060632293) - present_state_Q (-0.04742098060632293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.107592911705193 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031365524936880676) - present_state_Q ( -0.031365524936880676)) * f1( 0.0071935201544564824)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.031365524936880676) - present_state_Q (-0.031365524936880676)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.115358932184673 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019466419165154784) - present_state_Q ( -0.019466419165154784)) * f1( 0.00709569889284929)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.019466419165154784) - present_state_Q (-0.019466419165154784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.123044217760235 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037210381501619705) - present_state_Q ( -0.037210381501619705)) * f1( 0.007032193381007327)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.037210381501619705) - present_state_Q (-0.037210381501619705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.130884124065076 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022537682220872962) - present_state_Q ( -0.022537682220872962)) * f1( 0.007165016848371691)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.022537682220872962) - present_state_Q (-0.022537682220872962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.1386135492501 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04564351785310778) - present_state_Q ( -0.04564351785310778)) * f1( 0.007077497236482289)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04564351785310778) - present_state_Q (-0.04564351785310778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.14657796047548 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03011580876585454) - present_state_Q ( -0.03011580876585454)) * f1( 0.007283343753921121)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03011580876585454) - present_state_Q (-0.03011580876585454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.163495806251433 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1742886635160586) - present_state_Q ( -0.41370842878777886)) * f1( 0.01601169980039546)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1742886635160586) - present_state_Q (-0.41370842878777886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.18962710024983 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17097748860327033) - present_state_Q ( -0.6712686792037837)) * f1( 0.02535041000862059)
w2 ( -26.908615103024406 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.17097748860327033) - present_state_Q (-0.6712686792037837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.190609512591738 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.437148911878801) - present_state_Q ( -5.437148911878801)) * f1( 0.0019056520156646081)
w2 ( -27.01172021705699 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -5.437148911878801) - present_state_Q (-5.437148911878801)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.199850769950086 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.524054727244749) - present_state_Q ( -5.879806188602061)) * f1( 0.01957358963989839)
w2 ( -27.106145997185838 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -5.524054727244749) - present_state_Q (-5.879806188602061)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.209486342729118 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.5051194748383985) - present_state_Q ( -5.991624338182732)) * f1( 0.020912449397003947)
w2 ( -27.19829754381826 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -5.5051194748383985) - present_state_Q (-5.991624338182732)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.213683656305673 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07701682527176419) - present_state_Q ( -0.07701682527176419)) * f1( 0.004629803252799557)
w2 ( -27.19829754381826 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07701682527176419) - present_state_Q (-0.07701682527176419)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.21863559174171 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11256272323400773) - present_state_Q ( -0.11256272323400773)) * f1( 0.005481524103768671)
w2 ( -27.19829754381826 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.11256272323400773) - present_state_Q (-0.11256272323400773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.224817596115077 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.513735362061933) - present_state_Q ( -5.864232684517231)) * f1( 0.01617346314983527)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -5.513735362061933) - present_state_Q (-5.864232684517231)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.239391753199477 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1372743597870965) - present_state_Q ( -0.4858338177907553)) * f1( 0.01880645876925696)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1372743597870965) - present_state_Q (-0.4858338177907553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.26030400834035 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28202303213247093) - present_state_Q ( -0.7283533224587038)) * f1( 0.027803288494248145)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.28202303213247093) - present_state_Q (-0.7283533224587038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.265532911077372 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17022411261964276) - present_state_Q ( -0.17022411261964276)) * f1( 0.006480675287000739)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.17022411261964276) - present_state_Q (-0.17022411261964276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.27827547268389 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07551838957545004) - present_state_Q ( -0.43073676921056486)) * f1( 0.016339821940795227)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07551838957545004) - present_state_Q (-0.43073676921056486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.29944646272856 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23684354170118674) - present_state_Q ( -0.7353203700289617)) * f1( 0.028190327412567877)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.23684354170118674) - present_state_Q (-0.7353203700289617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.319139053470554 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2470914560530005) - present_state_Q ( -0.679463420884553)) * f1( 0.026024644523193096)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2470914560530005) - present_state_Q (-0.679463420884553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.32800579773297 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12331586914043681) - present_state_Q ( -0.4409243504052255)) * f1( 0.01137774052695513)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12331586914043681) - present_state_Q (-0.4409243504052255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.34488920673946 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1593502348098157) - present_state_Q ( -0.6655360178960171)) * f1( 0.025356033980692895)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1593502348098157) - present_state_Q (-0.6655360178960171)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.346435244739546 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2095588595889961) - present_state_Q ( -0.2095588595889961)) * f1( 0.0021715435049235823)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2095588595889961) - present_state_Q (-0.2095588595889961)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.352428908587484 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06573384744210521) - present_state_Q ( -0.06573384744210521)) * f1( 0.008268288643111223)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.06573384744210521) - present_state_Q (-0.06573384744210521)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.371158041297328 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24585458943119862) - present_state_Q ( -0.7440830483891083)) * f1( 0.028426399985445234)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.24585458943119862) - present_state_Q (-0.7440830483891083)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.37223729385577 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19165338655140857) - present_state_Q ( -0.19165338655140857)) * f1( 0.0017345380003410175)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19165338655140857) - present_state_Q (-0.19165338655140857)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.376878788040152 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13098772748328347) - present_state_Q ( -0.26328946824624644)) * f1( 0.007553986548010505)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.13098772748328347) - present_state_Q (-0.26328946824624644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.38582133969608 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14250304865126787) - present_state_Q ( -0.4207108088271453)) * f1( 0.014933721401574609)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.14250304865126787) - present_state_Q (-0.4207108088271453)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.388914707401206 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1424607674864929) - present_state_Q ( -0.1424607674864929)) * f1( 0.004936430363061985)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1424607674864929) - present_state_Q (-0.1424607674864929)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.39350344363824 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18942789913452956) - present_state_Q ( -0.18942789913452956)) * f1( 0.0073724875466388345)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18942789913452956) - present_state_Q (-0.18942789913452956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.40938261587017 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22888546603589296) - present_state_Q ( -0.7364926789026985)) * f1( 0.027951286450542977)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.22888546603589296) - present_state_Q (-0.7364926789026985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.414816521399267 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17110132381020782) - present_state_Q ( -0.17110132381020782)) * f1( 0.008707303475448501)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17110132381020782) - present_state_Q (-0.17110132381020782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.416102252898305 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2498636940016638) - present_state_Q ( -0.22142311003801132)) * f1( 0.0020743680741787413)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.2498636940016638) - present_state_Q (-0.22142311003801132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.42162822960915 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05770735593627582) - present_state_Q ( -0.05770735593627582)) * f1( 0.008712363115097571)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.05770735593627582) - present_state_Q (-0.05770735593627582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.426997824288904 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2933918736752429) - present_state_Q ( -0.06612432708137087)) * f1( 0.008445633111275481)
w2 ( -27.274743810348767 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.2933918736752429) - present_state_Q (-0.06612432708137087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.429539988270562 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.5150531016823185) - present_state_Q ( -5.910055197977687)) * f1( 0.02453658449098975)
w2 ( -27.29546522724029 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -5.5150531016823185) - present_state_Q (-5.910055197977687)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.431303799342153 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.624673426277989) - present_state_Q ( -5.654815048581903)) * f1( 0.013544095867688512)
w2 ( -27.321510687768917 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -5.624673426277989) - present_state_Q (-5.654815048581903)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.432926199139757 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.601476655485997) - present_state_Q ( -5.951779135882301)) * f1( 0.016175644735054846)
w2 ( -27.341570473009952 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -5.601476655485997) - present_state_Q (-5.951779135882301)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.43475570478087 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.712386409615923) - present_state_Q ( -6.043169531261289)) * f1( 0.019827959056936788)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -5.712386409615923) - present_state_Q (-6.043169531261289)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.44377921912721 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10854315445755153) - present_state_Q ( -0.4377591485986649)) * f1( 0.01785350155008388)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.10854315445755153) - present_state_Q (-0.4377591485986649)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.44467144835152 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18604155055356308) - present_state_Q ( -0.23322319189903254)) * f1( 0.0016941647376153036)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18604155055356308) - present_state_Q (-0.23322319189903254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.449551280567384 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17487642683846502) - present_state_Q ( -0.17487642683846502)) * f1( 0.009166216617840653)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17487642683846502) - present_state_Q (-0.17487642683846502)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.4564779260829 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06248515944224296) - present_state_Q ( -0.5089229988012897)) * f1( 0.013913315852597072)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06248515944224296) - present_state_Q (-0.5089229988012897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.465906999930848 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10437588938049969) - present_state_Q ( -0.46186930077963717)) * f1( 0.018746896961450626)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.10437588938049969) - present_state_Q (-0.46186930077963717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.468288358777947 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14120872812294508) - present_state_Q ( -0.14120872812294508)) * f1( 0.004447799580445922)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14120872812294508) - present_state_Q (-0.14120872812294508)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.47282206787881 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17760373674246016) - present_state_Q ( -0.17760373674246016)) * f1( 0.008519991500249078)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17760373674246016) - present_state_Q (-0.17760373674246016)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.484335746349416 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5865384871716283) - present_state_Q ( -0.5865384871716283)) * f1( 0.023244841036782735)
w2 ( -27.360024269851667 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.5865384871716283) - present_state_Q (-0.5865384871716283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.48270212161952 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48121357932420455) - present_state_Q ( -6.1619073217972105)) * f1( 0.025820609614028)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.48121357932420455) - present_state_Q (-6.1619073217972105)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.486443609402098 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14438809612970183) - present_state_Q ( -0.45146639137491834)) * f1( 0.011630213316721538)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14438809612970183) - present_state_Q (-0.45146639137491834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.48791923526895 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047203428989273495) - present_state_Q ( -0.047203428989273495)) * f1( 0.004085811347537854)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.047203428989273495) - present_state_Q (-0.047203428989273495)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.494949211877834 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23246835171584548) - present_state_Q ( -0.26605904501153405)) * f1( 0.020608171891090522)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.23246835171584548) - present_state_Q (-0.26605904501153405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.49840157803898 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08094768618892215) - present_state_Q ( -0.08094768618892215)) * f1( 0.009640206263413317)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08094768618892215) - present_state_Q (-0.08094768618892215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.50652435149488 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2186173733674147) - present_state_Q ( -0.7282906521744106)) * f1( 0.027556870068778747)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2186173733674147) - present_state_Q (-0.7282906521744106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.512772229984883 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12491122782267616) - present_state_Q ( -0.4856245580929681)) * f1( 0.01964163807381594)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12491122782267616) - present_state_Q (-0.4856245580929681)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.518188198225143 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1669042857104318) - present_state_Q ( -0.5277071548770583)) * f1( 0.017231556417099)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1669042857104318) - present_state_Q (-0.5277071548770583)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.52437512387939 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24594547655366314) - present_state_Q ( -0.5945577696916803)) * f1( 0.02006067923690164)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.24594547655366314) - present_state_Q (-0.5945577696916803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.527722753953295 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20232869791125502) - present_state_Q ( -0.3941653891160962)) * f1( 0.01020576461954746)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.20232869791125502) - present_state_Q (-0.3941653891160962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.534737912146447 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29994976716846716) - present_state_Q ( -0.6497916154865349)) * f1( 0.02311973805007039)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.29994976716846716) - present_state_Q (-0.6497916154865349)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.535296164414902 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18925905690474215) - present_state_Q ( -0.2232442380653835)) * f1( 0.002201107619808759)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18925905690474215) - present_state_Q (-0.2232442380653835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.537626523538055 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08342278821489013) - present_state_Q ( -0.08342278821489013)) * f1( 0.00874276598543782)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08342278821489013) - present_state_Q (-0.08342278821489013)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.542934182546045 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20004170464920482) - present_state_Q ( -0.6648817687951607)) * f1( 0.025326738046940803)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20004170464920482) - present_state_Q (-0.6648817687951607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.548268560912 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2972777691919184) - present_state_Q ( -0.6563887259799166)) * f1( 0.025234881492620587)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2972777691919184) - present_state_Q (-0.6563887259799166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.551324407228844 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06027269054838403) - present_state_Q ( -0.39620452297580366)) * f1( 0.013001529465871195)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06027269054838403) - present_state_Q (-0.39620452297580366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.55646324099492 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2949355828075742) - present_state_Q ( -0.6562669352142149)) * f1( 0.024311128592790342)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2949355828075742) - present_state_Q (-0.6562669352142149)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.560901947799042 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1245397386073565) - present_state_Q ( -0.4836077498327702)) * f1( 0.019558961758521783)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1245397386073565) - present_state_Q (-0.4836077498327702)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.56247605156532 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09535057509544036) - present_state_Q ( -0.09535057509544036)) * f1( 0.005929416834463411)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.09535057509544036) - present_state_Q (-0.09535057509544036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.56644174299638 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18350961602707652) - present_state_Q ( -0.542358442380949)) * f1( 0.017891325247287034)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18350961602707652) - present_state_Q (-0.542358442380949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.570852139944954 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17187558450290621) - present_state_Q ( -0.5319791191008325)) * f1( 0.019815238787343772)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.17187558450290621) - present_state_Q (-0.5319791191008325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.575558953770432 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24832766573548815) - present_state_Q ( -0.6059764013783048)) * f1( 0.02179677759295243)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.24832766573548815) - present_state_Q (-0.6059764013783048)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.58055836454088 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22860568142883927) - present_state_Q ( -0.5767095377919726)) * f1( 0.02286278150204741)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.22860568142883927) - present_state_Q (-0.5767095377919726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.586147543124685 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13568068367711436) - present_state_Q ( -0.4757909559975529)) * f1( 0.02453192203565675)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.13568068367711436) - present_state_Q (-0.4757909559975529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.59115427770501 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1442349816486438) - present_state_Q ( -0.45746869671886914)) * f1( 0.021792036212575822)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1442349816486438) - present_state_Q (-0.45746869671886914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.59305145180416 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027305297885214328) - present_state_Q ( -0.027305297885214328)) * f1( 0.006985236314834901)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.027305297885214328) - present_state_Q (-0.027305297885214328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.594965106420503 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01907998306054795) - present_state_Q ( -0.01907998306054795)) * f1( 0.0070267637516658625)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01907998306054795) - present_state_Q (-0.01907998306054795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.597061508101877 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19755415220246325) - present_state_Q ( -0.3619285946351149)) * f1( 0.008740912355063107)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.19755415220246325) - present_state_Q (-0.3619285946351149)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.598608851682837 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17356444402482113) - present_state_Q ( -0.17356444402482113)) * f1( 0.005987375267914022)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.17356444402482113) - present_state_Q (-0.17356444402482113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.6041756905632 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21527905955176052) - present_state_Q ( -0.7269805223043881)) * f1( 0.027354141176535153)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.21527905955176052) - present_state_Q (-0.7269805223043881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.609014453724672 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5382491172772119) - present_state_Q ( -0.5382491172772119)) * f1( 0.02144720580876205)
w2 ( -27.347370620272407 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.5382491172772119) - present_state_Q (-0.5382491172772119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.60457451258161 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.532838043550314) - present_state_Q ( -5.890910963328089)) * f1( 0.01709592688304831)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -5.532838043550314) - present_state_Q (-5.890910963328089)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -25.604832875012953 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17250964508431815) - present_state_Q ( -0.17250964508431815)) * f1( 0.0015454370682800333)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17250964508431815) - present_state_Q (-0.17250964508431815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.606054941403738 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014660517875429975) - present_state_Q ( -0.014660517875429975)) * f1( 0.006737454082650353)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014660517875429975) - present_state_Q (-0.014660517875429975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.607256594189053 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04341041227358854) - present_state_Q ( -0.04341041227358854)) * f1( 0.006720784254903236)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04341041227358854) - present_state_Q (-0.04341041227358854)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.609895042429134 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18479956539041736) - present_state_Q ( -0.5457253956450058)) * f1( 0.02029904951254768)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18479956539041736) - present_state_Q (-0.5457253956450058)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.612676913211615 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.298879334415456) - present_state_Q ( -0.6521667939452725)) * f1( 0.02309074722627586)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.298879334415456) - present_state_Q (-0.6521667939452725)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.61495826892077 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.275209888006104) - present_state_Q ( -0.43517096535375444)) * f1( 0.016072851856259167)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.275209888006104) - present_state_Q (-0.43517096535375444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.617482452686808 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1525043462393395) - present_state_Q ( -0.5118459613386619)) * f1( 0.018972563388999567)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1525043462393395) - present_state_Q (-0.5118459613386619)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.620420699344987 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23675271346040866) - present_state_Q ( -0.5977728005194836)) * f1( 0.023450873734233615)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23675271346040866) - present_state_Q (-0.5977728005194836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.620590020129566 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17408649146683255) - present_state_Q ( -0.17408649146683255)) * f1( 0.0010136804275358599)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17408649146683255) - present_state_Q (-0.17408649146683255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.62182352420855 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012976248635490595) - present_state_Q ( -0.012976248635490595)) * f1( 0.006794833445934917)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012976248635490595) - present_state_Q (-0.012976248635490595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.623039899403413 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04832612594491071) - present_state_Q ( -0.04832612594491071)) * f1( 0.006820001375898781)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04832612594491071) - present_state_Q (-0.04832612594491071)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.624141243800548 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18588261039647547) - present_state_Q ( -0.22842756055719285)) * f1( 0.006810213140834036)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18588261039647547) - present_state_Q (-0.22842756055719285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.62615647541809 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.254794144304743) - present_state_Q ( -0.3040836427618782)) * f1( 0.01301467467384473)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.254794144304743) - present_state_Q (-0.3040836427618782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.62857254286961 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16170332660118925) - present_state_Q ( -0.16170332660118925)) * f1( 0.014368512011712771)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16170332660118925) - present_state_Q (-0.16170332660118925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.629716699116138 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035886813937874455) - present_state_Q ( -0.035886813937874455)) * f1( 0.006375065834097691)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.035886813937874455) - present_state_Q (-0.035886813937874455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.630917451488042 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014788516930925899) - present_state_Q ( -0.014788516930925899)) * f1( 0.006620366839668866)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014788516930925899) - present_state_Q (-0.014788516930925899)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.632099794281796 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.045559413506731414) - present_state_Q ( -0.045559413506731414)) * f1( 0.006619945446925277)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.045559413506731414) - present_state_Q (-0.045559413506731414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.63334633383027 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027378291699359365) - present_state_Q ( -0.027378291699359365)) * f1( 0.006916021280822169)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.027378291699359365) - present_state_Q (-0.027378291699359365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.634591111897606 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01442911445445286) - present_state_Q ( -0.01442911445445286)) * f1( 0.006861879437570266)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01442911445445286) - present_state_Q (-0.01442911445445286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.635818303054325 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04949311775932909) - present_state_Q ( -0.04949311775932909)) * f1( 0.006884698833506453)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04949311775932909) - present_state_Q (-0.04949311775932909)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.637053628452303 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03139241035415841) - present_state_Q ( -0.03139241035415841)) * f1( 0.006867568505215059)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03139241035415841) - present_state_Q (-0.03139241035415841)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.640187147018125 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35081147573373855) - present_state_Q ( -0.7419197825311539)) * f1( 0.027972952567362822)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.35081147573373855) - present_state_Q (-0.7419197825311539)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.64236997599586 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3449361661855391) - present_state_Q ( -0.3721590430724287)) * f1( 0.014656064932996593)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3449361661855391) - present_state_Q (-0.3721590430724287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.6447064743435 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1639455498271588) - present_state_Q ( -0.52044815100922)) * f1( 0.01766086229597988)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1639455498271588) - present_state_Q (-0.52044815100922)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.645329062291967 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07714219295236184) - present_state_Q ( -0.07714219295236184)) * f1( 0.003542248739517017)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07714219295236184) - present_state_Q (-0.07714219295236184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.646244397772943 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027786457477284716) - present_state_Q ( -0.027786457477284716)) * f1( 0.005079477953840767)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.027786457477284716) - present_state_Q (-0.027786457477284716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.64792426818558 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0987281095058671) - present_state_Q ( -0.0987281095058671)) * f1( 0.00966454100995411)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0987281095058671) - present_state_Q (-0.0987281095058671)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.64930453461431 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04434696704767676) - present_state_Q ( -0.04434696704767676)) * f1( 0.0077234025166786355)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04434696704767676) - present_state_Q (-0.04434696704767676)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.650661920848044 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03144647902955662) - present_state_Q ( -0.03144647902955662)) * f1( 0.007546347831571244)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03144647902955662) - present_state_Q (-0.03144647902955662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.651979364287484 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.050753833330746574) - present_state_Q ( -0.050753833330746574)) * f1( 0.00739573339738017)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.050753833330746574) - present_state_Q (-0.050753833330746574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.670596603352717 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18138688559345065) - present_state_Q ( -0.18138688559345065)) * f1( 0.007071067811865476)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.18138688559345065) - present_state_Q (-0.18138688559345065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.688441080287152 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14968401390612848) - present_state_Q ( -0.3575500367984598)) * f1( 0.007071067811865476)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.14968401390612848) - present_state_Q (-0.3575500367984598)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.751112119541194 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1011418920512591) - present_state_Q ( -0.43665628091374753)) * f1( 0.024916979432367353)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.1011418920512591) - present_state_Q (-0.43665628091374753)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.77694892826954 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07430197375446623) - present_state_Q ( -0.4144884327201975)) * f1( 0.010264341030448064)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.07430197375446623) - present_state_Q (-0.4144884327201975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.8216657688934 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18643264703508083) - present_state_Q ( -0.5373030231130113)) * f1( 0.017844036774917162)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.18643264703508083) - present_state_Q (-0.5373030231130113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.878036272734033 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15881532321729047) - present_state_Q ( -0.6382274632353573)) * f1( 0.023446073857216594)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.15881532321729047) - present_state_Q (-0.6382274632353573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.932856559700447 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1343704981419675) - present_state_Q ( -0.6246963989761294)) * f1( 0.022790785838871636)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1343704981419675) - present_state_Q (-0.6246963989761294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -25.97707126170029 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19982169453015464) - present_state_Q ( -0.5566507838818789)) * f1( 0.01832483323456786)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.19982169453015464) - present_state_Q (-0.5566507838818789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.0294856266238 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2299052564944333) - present_state_Q ( -0.6184929291191191)) * f1( 0.021776291526257884)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.2299052564944333) - present_state_Q (-0.6184929291191191)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.095271169026553 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.424476650366547) - present_state_Q ( -0.7807479605845504)) * f1( 0.027494655581986473)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.424476650366547) - present_state_Q (-0.7807479605845504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.121776462006835 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24947235087301184) - present_state_Q ( -0.3288578878514241)) * f1( 0.010880190922934551)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.24947235087301184) - present_state_Q (-0.3288578878514241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.16632175769123 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16031134281828419) - present_state_Q ( -0.5270632584651567)) * f1( 0.019167167700208206)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.16031134281828419) - present_state_Q (-0.5270632584651567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.194933752913467 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.200650096484708) - present_state_Q ( -0.24464462593321223)) * f1( 0.012161412316795656)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.200650096484708) - present_state_Q (-0.24464462593321223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.213121017872293 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03761192088193286) - present_state_Q ( -0.03761192088193286)) * f1( 0.007668257630659521)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.03761192088193286) - present_state_Q (-0.03761192088193286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.230898756532472 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030842182399578828) - present_state_Q ( -0.030842182399578828)) * f1( 0.007493664901060724)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.030842182399578828) - present_state_Q (-0.030842182399578828)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.248334326245534 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04790072184880455) - present_state_Q ( -0.04790072184880455)) * f1( 0.007354193225709893)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.04790072184880455) - present_state_Q (-0.04790072184880455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.265535049869726 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03147830880643106) - present_state_Q ( -0.03147830880643106)) * f1( 0.007250616742547737)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.03147830880643106) - present_state_Q (-0.03147830880643106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.282486679028192 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021802914054523722) - present_state_Q ( -0.021802914054523722)) * f1( 0.007142994092746012)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.021802914054523722) - present_state_Q (-0.021802914054523722)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.299313184284014 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04965609363588859) - present_state_Q ( -0.04965609363588859)) * f1( 0.0070977673501772314)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.04965609363588859) - present_state_Q (-0.04965609363588859)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.31602081792623 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03201932872793333) - present_state_Q ( -0.03201932872793333)) * f1( 0.007042909201321598)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.03201932872793333) - present_state_Q (-0.03201932872793333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.33256350090232 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017463890458495612) - present_state_Q ( -0.017463890458495612)) * f1( 0.006969527512202493)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.017463890458495612) - present_state_Q (-0.017463890458495612)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.34898615494257 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039220723191597244) - present_state_Q ( -0.039220723191597244)) * f1( 0.006924671243025894)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.039220723191597244) - present_state_Q (-0.039220723191597244)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.365800651643273 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022702206838867357) - present_state_Q ( -0.022702206838867357)) * f1( 0.0070854515386023385)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.022702206838867357) - present_state_Q (-0.022702206838867357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.382417459995615 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049061778730731795) - present_state_Q ( -0.049061778730731795)) * f1( 0.007009154722511805)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.049061778730731795) - present_state_Q (-0.049061778730731795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.399599072494844 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03247680844985197) - present_state_Q ( -0.03247680844985197)) * f1( 0.007242835198889882)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.03247680844985197) - present_state_Q (-0.03247680844985197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.41653585210187 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021542218844170728) - present_state_Q ( -0.021542218844170728)) * f1( 0.007136666304869718)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.021542218844170728) - present_state_Q (-0.021542218844170728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.43339107132569 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05185475078068423) - present_state_Q ( -0.05185475078068423)) * f1( 0.0071104730024497565)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.05185475078068423) - present_state_Q (-0.05185475078068423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.439621097571624 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10075088740963697) - present_state_Q ( -0.10075088740963697)) * f1( 0.002738803490598578)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.10075088740963697) - present_state_Q (-0.10075088740963697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.455301763025556 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033215465204353166) - present_state_Q ( -0.033215465204353166)) * f1( 0.00687506130816985)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.033215465204353166) - present_state_Q (-0.033215465204353166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.471255677679764 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018228507728132868) - present_state_Q ( -0.018228507728132868)) * f1( 0.006990731024156198)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.018228507728132868) - present_state_Q (-0.018228507728132868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.48644630280615 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04077936617462307) - present_state_Q ( -0.04077936617462307)) * f1( 0.006940252490711424)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.04077936617462307) - present_state_Q (-0.04077936617462307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.50136956256707 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024073288436982483) - present_state_Q ( -0.024073288436982483)) * f1( 0.007109960317788804)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.024073288436982483) - present_state_Q (-0.024073288436982483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.51610419030138 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05114505109336949) - present_state_Q ( -0.05114505109336949)) * f1( 0.007028247941865879)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.05114505109336949) - present_state_Q (-0.05114505109336949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.529667228627794 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11417575453069267) - present_state_Q ( -0.11417575453069267)) * f1( 0.006783343453357697)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.11417575453069267) - present_state_Q (-0.11417575453069267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.546988178228165 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06359476010078632) - present_state_Q ( -0.06359476010078632)) * f1( 0.008643126247799332)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.06359476010078632) - present_state_Q (-0.06359476010078632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.57724608115723 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11499313421381797) - present_state_Q ( -0.4852712071329935)) * f1( 0.015419133740081819)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.11499313421381797) - present_state_Q (-0.4852712071329935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.618532170919416 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18553412719039106) - present_state_Q ( -0.5455512841783695)) * f1( 0.021096210928643236)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.18553412719039106) - present_state_Q (-0.5455512841783695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.652209286525363 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12326862829104902) - present_state_Q ( -0.4410326999921409)) * f1( 0.017122209803280303)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.12326862829104902) - present_state_Q (-0.4410326999921409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.69498728112058 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2315636457974371) - present_state_Q ( -0.30332152870736223)) * f1( 0.021586280488112265)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2315636457974371) - present_state_Q (-0.30332152870736223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.712212440909926 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18748644859284067) - present_state_Q ( -0.29053922737286136)) * f1( 0.00868834712004567)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.18748644859284067) - present_state_Q (-0.29053922737286136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.737870054343514 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06621142486213547) - present_state_Q ( -0.3679436488373381)) * f1( 0.013000374651235947)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.06621142486213547) - present_state_Q (-0.3679436488373381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.747568170531345 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17779497710750014) - present_state_Q ( -0.22750548861720016)) * f1( 0.005111229350529288)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.17779497710750014) - present_state_Q (-0.22750548861720016)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.763810648141543 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10520569483665512) - present_state_Q ( -0.10520569483665512)) * f1( 0.008508736415982482)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.10520569483665512) - present_state_Q (-0.10520569483665512)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.805817122410815 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11316002844339189) - present_state_Q ( -0.6437602196338971)) * f1( 0.022643268732195493)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.11316002844339189) - present_state_Q (-0.6437602196338971)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.81405671084194 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0555117380900763) - present_state_Q ( -0.0555117380900763)) * f1( 0.004306277198042147)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.0555117380900763) - present_state_Q (-0.0555117380900763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.82134655839151 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06945458537286325) - present_state_Q ( -0.06945458537286325)) * f1( 0.0038124119787775954)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.06945458537286325) - present_state_Q (-0.06945458537286325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.83343932436932 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08628793934278918) - present_state_Q ( -0.08628793934278918)) * f1( 0.006647048219221949)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.08628793934278918) - present_state_Q (-0.08628793934278918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.857975583506438 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09395452149181217) - present_state_Q ( -0.4571097439096594)) * f1( 0.014510671577840919)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09395452149181217) - present_state_Q (-0.4571097439096594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.8800713014479 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21200134038006854) - present_state_Q ( -0.3264786916005688)) * f1( 0.014513249486056175)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.21200134038006854) - present_state_Q (-0.3264786916005688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.894831844218533 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09335345919128916) - present_state_Q ( -0.09335345919128916)) * f1( 0.009556362617513999)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.09335345919128916) - present_state_Q (-0.09335345919128916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.923638973130384 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16311100419607755) - present_state_Q ( -0.536701855177396)) * f1( 0.02043655186810976)
w2 ( -27.295429111941964 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.16311100419607755) - present_state_Q (-0.536701855177396)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -26.933332800346538 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.657861540460922) - present_state_Q ( -5.657861540460922)) * f1( 0.010178100662123532)
w2 ( -27.485913123408434 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -5.657861540460922) - present_state_Q (-5.657861540460922)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -26.954661939645195 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7043865564398326) - present_state_Q ( -6.20156918112152)) * f1( 0.025137034437309327)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.7043865564398326) - present_state_Q (-6.20156918112152)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -26.98482792751471 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13117695288650338) - present_state_Q ( -0.5080610202064798)) * f1( 0.024536567705098777)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.13117695288650338) - present_state_Q (-0.5080610202064798)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.003680362766364 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14386476404870063) - present_state_Q ( -0.4660013579342335)) * f1( 0.015280439170739438)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.14386476404870063) - present_state_Q (-0.4660013579342335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.024545992253856 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15673782631702296) - present_state_Q ( -0.5300594827087802)) * f1( 0.016998675900947408)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15673782631702296) - present_state_Q (-0.5300594827087802)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.05644868657514 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20997117712016167) - present_state_Q ( -0.7484513998318749)) * f1( 0.026449392480719973)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.20997117712016167) - present_state_Q (-0.7484513998318749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.07695185756124 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25690271743877274) - present_state_Q ( -0.25865635685788335)) * f1( 0.01632902303123943)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.25690271743877274) - present_state_Q (-0.25865635685788335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.089025933143784 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08120160597351334) - present_state_Q ( -0.08120160597351334)) * f1( 0.009495064205016774)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.08120160597351334) - present_state_Q (-0.08120160597351334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.09345549043918 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09083049451632945) - present_state_Q ( -0.09083049451632945)) * f1( 0.0034857834979320117)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09083049451632945) - present_state_Q (-0.09083049451632945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.118424093001117 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.216389083902011) - present_state_Q ( -0.590405043566002)) * f1( 0.020431776905983905)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.216389083902011) - present_state_Q (-0.590405043566002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.13229303851863 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25746960811686415) - present_state_Q ( -0.25746960811686415)) * f1( 0.011044335843019037)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.25746960811686415) - present_state_Q (-0.25746960811686415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.157734663064304 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2946362173413099) - present_state_Q ( -0.7049385652246777)) * f1( 0.02100224112870433)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2946362173413099) - present_state_Q (-0.7049385652246777)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.18080762869713 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15907669238370503) - present_state_Q ( -0.5389975885080918)) * f1( 0.01881027276341609)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15907669238370503) - present_state_Q (-0.5389975885080918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.195000280441413 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09036798951291422) - present_state_Q ( -0.4394719302240803)) * f1( 0.011483836999543919)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09036798951291422) - present_state_Q (-0.4394719302240803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.22769562912718 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2228968812367342) - present_state_Q ( -0.7635016997217307)) * f1( 0.02713750726858921)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2228968812367342) - present_state_Q (-0.7635016997217307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.253644756486736 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2247489137867061) - present_state_Q ( -0.5848157246491539)) * f1( 0.021222980493275517)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2247489137867061) - present_state_Q (-0.5848157246491539)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.256762158664767 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10501511050034287) - present_state_Q ( -0.10501511050034287)) * f1( 0.0024556668020804507)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.10501511050034287) - present_state_Q (-0.10501511050034287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.2699886671116 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1488242068321134) - present_state_Q ( -0.45920721878025655)) * f1( 0.010714133439669265)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1488242068321134) - present_state_Q (-0.45920721878025655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.28246347821583 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07704401347006137) - present_state_Q ( -0.4164916456993695)) * f1( 0.012940806333614153)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.07704401347006137) - present_state_Q (-0.4164916456993695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.30626320613953 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.182930584201346) - present_state_Q ( -0.5551514576653886)) * f1( 0.025021183739835428)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.182930584201346) - present_state_Q (-0.5551514576653886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.309269244766707 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14872109691226984) - present_state_Q ( -0.14872109691226984)) * f1( 0.003716754009585393)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14872109691226984) - present_state_Q (-0.14872109691226984)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.32258216023654 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10835568327287194) - present_state_Q ( -0.4838661486734532)) * f1( 0.017181004470227825)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.10835568327287194) - present_state_Q (-0.4838661486734532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.324214528606774 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15135247031816043) - present_state_Q ( -0.15135247031816043)) * f1( 0.0020188991238505364)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.15135247031816043) - present_state_Q (-0.15135247031816043)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.335886191971014 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07370499589439156) - present_state_Q ( -0.4600616606767447)) * f1( 0.015023448769601133)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07370499589439156) - present_state_Q (-0.4600616606767447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.346783739179898 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1142739990297762) - present_state_Q ( -0.4633382686875116)) * f1( 0.014025618741066666)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1142739990297762) - present_state_Q (-0.4633382686875116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.366962608447214 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22133900450609395) - present_state_Q ( -0.7624271528276453)) * f1( 0.026972186795412845)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.22133900450609395) - present_state_Q (-0.7624271528276453)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.382377253853445 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19297170232393698) - present_state_Q ( -0.5784003263793778)) * f1( 0.02011685566765353)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.19297170232393698) - present_state_Q (-0.5784003263793778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.39701594177328 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20051142632646687) - present_state_Q ( -0.525036141035722)) * f1( 0.018970213169038887)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.20051142632646687) - present_state_Q (-0.525036141035722)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.40963946527698 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0543443288691094) - present_state_Q ( -0.4271254949719682)) * f1( 0.016184078844762844)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0543443288691094) - present_state_Q (-0.4271254949719682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.41439752534049 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015374847346122737) - present_state_Q ( -0.015374847346122737)) * f1( 0.005796985437441698)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.015374847346122737) - present_state_Q (-0.015374847346122737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.422638841663137 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11090533418107174) - present_state_Q ( -0.11090533418107174)) * f1( 0.010147104461282194)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11090533418107174) - present_state_Q (-0.11090533418107174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.425248893154755 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07682603343262913) - present_state_Q ( -0.07682603343262913)) * f1( 0.003201530468798706)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07682603343262913) - present_state_Q (-0.07682603343262913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.44071140617282 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2573267967999078) - present_state_Q ( -0.5454866267492631)) * f1( 0.020076228525088533)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2573267967999078) - present_state_Q (-0.5454866267492631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.454393745738134 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11862657889122945) - present_state_Q ( -0.5010751627693733)) * f1( 0.01769471828811537)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11862657889122945) - present_state_Q (-0.5010751627693733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.47039518148254 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20569026599196547) - present_state_Q ( -0.5859057597173827)) * f1( 0.02089964746954541)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.20569026599196547) - present_state_Q (-0.5859057597173827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.48985940121308 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3392538957103967) - present_state_Q ( -0.7277266822875137)) * f1( 0.025856266473217815)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.3392538957103967) - present_state_Q (-0.7277266822875137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.509254662677357 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21421852344597547) - present_state_Q ( -0.7521478372797826)) * f1( 0.0258916628853591)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.21421852344597547) - present_state_Q (-0.7521478372797826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.519558230986963 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18124517823717712) - present_state_Q ( -0.3172997173707467)) * f1( 0.013005483476691653)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.18124517823717712) - present_state_Q (-0.3172997173707467)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.52496333768314 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01980620503284388) - present_state_Q ( -0.01980620503284388)) * f1( 0.006588516499615599)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01980620503284388) - present_state_Q (-0.01980620503284388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.530372459353174 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013569439355794414) - present_state_Q ( -0.013569439355794414)) * f1( 0.006588902373352841)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.013569439355794414) - present_state_Q (-0.013569439355794414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.53574707564766 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04228796940283173) - present_state_Q ( -0.04228796940283173)) * f1( 0.006567548371686775)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.04228796940283173) - present_state_Q (-0.04228796940283173)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.541144647012576 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019403764656912317) - present_state_Q ( -0.019403764656912317)) * f1( 0.006579040898522515)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.019403764656912317) - present_state_Q (-0.019403764656912317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.54652364508894 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053784938438971926) - present_state_Q ( -0.053784938438971926)) * f1( 0.006581223965322088)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.053784938438971926) - present_state_Q (-0.053784938438971926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.563369748491507 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2662039597214266) - present_state_Q ( -0.6163622323037408)) * f1( 0.022073237128693492)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2662039597214266) - present_state_Q (-0.6163622323037408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.581329718965943 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2609169357299201) - present_state_Q ( -0.6497073357199186)) * f1( 0.02363763741032227)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2609169357299201) - present_state_Q (-0.6497073357199186)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.591713924993577 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1997592032279756) - present_state_Q ( -0.35360329681908936)) * f1( 0.014888795240502071)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1997592032279756) - present_state_Q (-0.35360329681908936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.5986750086167 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10667341272205036) - present_state_Q ( -0.10667341272205036)) * f1( 0.00965190835596849)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10667341272205036) - present_state_Q (-0.10667341272205036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.60940384065505 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11246640905014454) - present_state_Q ( -0.497668281177545)) * f1( 0.015727467259342473)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11246640905014454) - present_state_Q (-0.497668281177545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.620979512167647 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2157812363389728) - present_state_Q ( -0.3499946068737667)) * f1( 0.01658471834182139)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2157812363389728) - present_state_Q (-0.3499946068737667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.62695020730292 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05061534865250161) - present_state_Q ( -0.05061534865250161)) * f1( 0.008221171691413265)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05061534865250161) - present_state_Q (-0.05061534865250161)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.632645256702045 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05125976224635958) - present_state_Q ( -0.05125976224635958)) * f1( 0.007842255728728993)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05125976224635958) - present_state_Q (-0.05125976224635958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.638209689760377 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03707294796961128) - present_state_Q ( -0.03707294796961128)) * f1( 0.0076489446108332915)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.03707294796961128) - present_state_Q (-0.03707294796961128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.643636396454916 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.058559382844714715) - present_state_Q ( -0.058559382844714715)) * f1( 0.0074795061110798046)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.058559382844714715) - present_state_Q (-0.058559382844714715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.659343469479772 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07524484842805367) - present_state_Q ( -0.5687251684739371)) * f1( 0.02328030087609567)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07524484842805367) - present_state_Q (-0.5687251684739371)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.67334944882399 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07490391490200946) - present_state_Q ( -0.507837511552523)) * f1( 0.02057345713557684)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07490391490200946) - present_state_Q (-0.507837511552523)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.685567144153552 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11994303693895858) - present_state_Q ( -0.6657709211515858)) * f1( 0.01836043260075524)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11994303693895858) - present_state_Q (-0.6657709211515858)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.696069646235003 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1375709432073506) - present_state_Q ( -0.52608761835714)) * f1( 0.015454384412313312)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1375709432073506) - present_state_Q (-0.52608761835714)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.705924089825277 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09662988132636866) - present_state_Q ( -0.4693805433456965)) * f1( 0.014389367152160711)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.09662988132636866) - present_state_Q (-0.4693805433456965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.72064427744657 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10386859666384525) - present_state_Q ( -0.6540099837327988)) * f1( 0.022087410621459604)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10386859666384525) - present_state_Q (-0.6540099837327988)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.737180001240343 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24314862867362505) - present_state_Q ( -0.6351427139448445)) * f1( 0.024690097181977864)
w2 ( -27.655616032093647 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.24314862867362505) - present_state_Q (-0.6351427139448445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.737804165847738 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.61969639326265) - present_state_Q ( -5.61969639326265)) * f1( 0.002773558006644839)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -5.61969639326265) - present_state_Q (-5.61969639326265)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -27.744130996982875 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0872670354557391) - present_state_Q ( -0.4060245654042847)) * f1( 0.012445068919355184)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0872670354557391) - present_state_Q (-0.4060245654042847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.753940871702785 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22798188097493677) - present_state_Q ( -0.5371224520344018)) * f1( 0.019750978015090164)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.22798188097493677) - present_state_Q (-0.5371224520344018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.763654160711187 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2636283653615071) - present_state_Q ( -0.6322576593487065)) * f1( 0.019923842611978597)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2636283653615071) - present_state_Q (-0.6322576593487065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.768551587291803 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21299679260162757) - present_state_Q ( -0.21299679260162757)) * f1( 0.00925893424817312)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.21299679260162757) - present_state_Q (-0.21299679260162757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.77946100719877 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1189833332324542) - present_state_Q ( -0.6668364354146246)) * f1( 0.022604736973627852)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1189833332324542) - present_state_Q (-0.6668364354146246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.787914268874886 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12516846483085914) - present_state_Q ( -0.4800461525577684)) * f1( 0.016860749187305714)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12516846483085914) - present_state_Q (-0.4800461525577684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.79576484882802 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2563685383984591) - present_state_Q ( -0.2563685383984591)) * f1( 0.014952426707423663)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2563685383984591) - present_state_Q (-0.2563685383984591)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.800614904730825 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14272468160535876) - present_state_Q ( -0.3403926882007226)) * f1( 0.009408480403540646)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14272468160535876) - present_state_Q (-0.3403926882007226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.80822922301811 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20006748490933246) - present_state_Q ( -0.4412136842341381)) * f1( 0.01504836751761644)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.20006748490933246) - present_state_Q (-0.4412136842341381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.81662529119333 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15960517591753653) - present_state_Q ( -0.24329512237380677)) * f1( 0.015981038311351604)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15960517591753653) - present_state_Q (-0.24329512237380677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.81961391635628 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16671334641859137) - present_state_Q ( -0.16671334641859137)) * f1( 0.005606060215237898)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16671334641859137) - present_state_Q (-0.16671334641859137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.8244224607521 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14368980540495804) - present_state_Q ( -0.14368980540495804)) * f1( 0.008984939601020137)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14368980540495804) - present_state_Q (-0.14368980540495804)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.830549924464375 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15213339692403885) - present_state_Q ( -0.509729809139299)) * f1( 0.012287890882647031)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15213339692403885) - present_state_Q (-0.509729809139299)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.834167465315094 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1761640527365471) - present_state_Q ( -0.1761640527365471)) * f1( 0.0067966235793565435)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1761640527365471) - present_state_Q (-0.1761640527365471)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.839758687868642 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14419346856353457) - present_state_Q ( -0.4795552771440141)) * f1( 0.011146847527761412)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14419346856353457) - present_state_Q (-0.4795552771440141)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.84418982118661 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1822774291147958) - present_state_Q ( -0.1822774291147958)) * f1( 0.008333813208836683)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1822774291147958) - present_state_Q (-0.1822774291147958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.846021939892 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11578756204176088) - present_state_Q ( -0.11578756204176088)) * f1( 0.003407391839989058)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11578756204176088) - present_state_Q (-0.11578756204176088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.847981670792972 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0769753989987377) - present_state_Q ( -0.0769753989987377)) * f1( 0.003621201118578679)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0769753989987377) - present_state_Q (-0.0769753989987377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.852241609872856 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06316683738519785) - present_state_Q ( -0.06316683738519785)) * f1( 0.007853503182342594)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06316683738519785) - present_state_Q (-0.06316683738519785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.86026032239439 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11557715873466674) - present_state_Q ( -0.5077216182532399)) * f1( 0.01608587704783738)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11557715873466674) - present_state_Q (-0.5077216182532399)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.86874599945435 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1016137072662642) - present_state_Q ( -0.48967387771335724)) * f1( 0.016965955627778626)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1016137072662642) - present_state_Q (-0.48967387771335724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.87448854685608 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17765696232427333) - present_state_Q ( -0.49045581916591324)) * f1( 0.011465801615614977)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17765696232427333) - present_state_Q (-0.49045581916591324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.884943237728557 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26322212001195455) - present_state_Q ( -0.78939485027403)) * f1( 0.027479704303921017)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.26322212001195455) - present_state_Q (-0.78939485027403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.8933407627954 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21456011503839126) - present_state_Q ( -0.5973010346068521)) * f1( 0.02103724819817925)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21456011503839126) - present_state_Q (-0.5973010346068521)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.903468275517213 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07756003812809871) - present_state_Q ( -0.6275054556045405)) * f1( 0.025653321734790863)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.07756003812809871) - present_state_Q (-0.6275054556045405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.91227019791674 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11471150959698737) - present_state_Q ( -0.6665014310560533)) * f1( 0.022496604510045554)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11471150959698737) - present_state_Q (-0.6665014310560533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.91949133312155 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1288320081543273) - present_state_Q ( -0.5217937286001603)) * f1( 0.017791850646430497)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1288320081543273) - present_state_Q (-0.5217937286001603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.920964601470253 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07637303359595508) - present_state_Q ( -0.07637303359595508)) * f1( 0.0032747661793837545)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.07637303359595508) - present_state_Q (-0.07637303359595508)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.92897294353916 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21549213104768405) - present_state_Q ( -0.5985741218801269)) * f1( 0.020068209480770968)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21549213104768405) - present_state_Q (-0.5985741218801269)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.937868202305218 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32842880715657774) - present_state_Q ( -0.6915511596135787)) * f1( 0.02275655280223778)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.32842880715657774) - present_state_Q (-0.6915511596135787)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.947568055284325 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19246241338447032) - present_state_Q ( -0.720408173174885)) * f1( 0.025087399138021684)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19246241338447032) - present_state_Q (-0.720408173174885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.957767240667103 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21765111968402742) - present_state_Q ( -0.7736823649806216)) * f1( 0.026729743655595592)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21765111968402742) - present_state_Q (-0.7736823649806216)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.966735837699137 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2481135129220869) - present_state_Q ( -0.6209907324870163)) * f1( 0.022582921762298085)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2481135129220869) - present_state_Q (-0.6209907324870163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.967782907383516 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11353360076150844) - present_state_Q ( -0.11353360076150844)) * f1( 0.002344847669273657)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11353360076150844) - present_state_Q (-0.11353360076150844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.97380835681056 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2097468019807131) - present_state_Q ( -0.25759613853680835)) * f1( 0.013912487712095394)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2097468019807131) - present_state_Q (-0.25759613853680835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.977046613501322 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19390824640964302) - present_state_Q ( -0.26906330746020374)) * f1( 0.0074995863159871634)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19390824640964302) - present_state_Q (-0.26906330746020374)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.98258030619076 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09142980058419542) - present_state_Q ( -0.3713356235879006)) * f1( 0.013158560705181257)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.09142980058419542) - present_state_Q (-0.3713356235879006)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.985579582533457 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06487920687781518) - present_state_Q ( -0.06487920687781518)) * f1( 0.006651467446271513)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.06487920687781518) - present_state_Q (-0.06487920687781518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.994337183746776 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11287006238695209) - present_state_Q ( -0.6672094799848768)) * f1( 0.02238843045419987)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11287006238695209) - present_state_Q (-0.6672094799848768)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -27.995929853366988 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14790164283682442) - present_state_Q ( -0.14790164283682442)) * f1( 0.003591563165466046)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14790164283682442) - present_state_Q (-0.14790164283682442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.005545443479345 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1808728544610109) - present_state_Q ( -0.7374148256053032)) * f1( 0.02498685995098421)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1808728544610109) - present_state_Q (-0.7374148256053032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.015145439648432 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23855683035435446) - present_state_Q ( -0.7141547593738021)) * f1( 0.024759569694637166)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.23855683035435446) - present_state_Q (-0.7141547593738021)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.024496305932914 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22533864597196604) - present_state_Q ( -0.6200234232649322)) * f1( 0.02355324577996055)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.22533864597196604) - present_state_Q (-0.6200234232649322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.034118386986187 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11418585211839616) - present_state_Q ( -0.5105106623397375)) * f1( 0.023650227023028783)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11418585211839616) - present_state_Q (-0.5105106623397375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.035673058802974 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08358632233667168) - present_state_Q ( -0.08358632233667168)) * f1( 0.0034607028811554207)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.08358632233667168) - present_state_Q (-0.08358632233667168)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.037701339128905 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15322742912695553) - present_state_Q ( -0.15322742912695553)) * f1( 0.004578840041850952)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15322742912695553) - present_state_Q (-0.15322742912695553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.0435654648347 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2393914710728511) - present_state_Q ( -0.32140431117851903)) * f1( 0.013732926081180407)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2393914710728511) - present_state_Q (-0.32140431117851903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.048036213669494 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10111246091256088) - present_state_Q ( -0.10111246091256088)) * f1( 0.009986962857240917)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.10111246091256088) - present_state_Q (-0.10111246091256088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.0540889444649 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11102692145942819) - present_state_Q ( -0.505355935982474)) * f1( 0.014859405810779928)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11102692145942819) - present_state_Q (-0.505355935982474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.06380184964228 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17235999456863776) - present_state_Q ( -0.7325536960298605)) * f1( 0.025213468568960874)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.17235999456863776) - present_state_Q (-0.7325536960298605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.071306752845143 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16791032929241834) - present_state_Q ( -0.5551282697920076)) * f1( 0.018626059600256543)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.16791032929241834) - present_state_Q (-0.5551282697920076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.078959400729204 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13039050693251067) - present_state_Q ( -0.5087362574787723)) * f1( 0.018793851763860576)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13039050693251067) - present_state_Q (-0.5087362574787723)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.081663402298474 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10116048908831078) - present_state_Q ( -0.10116048908831078)) * f1( 0.00604038048061637)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.10116048908831078) - present_state_Q (-0.10116048908831078)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.09107732768818 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3151777334948669) - present_state_Q ( -0.7104243958547447)) * f1( 0.024208539457979548)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3151777334948669) - present_state_Q (-0.7104243958547447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.09676639479704 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13548944141075156) - present_state_Q ( -0.4274548372830041)) * f1( 0.013696449021892772)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13548944141075156) - present_state_Q (-0.4274548372830041)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.104652937986884 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1925845907933904) - present_state_Q ( -0.5852863831244097)) * f1( 0.01970867989974728)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1925845907933904) - present_state_Q (-0.5852863831244097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.10862544654516 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21706535749306682) - present_state_Q ( -0.21706535749306682)) * f1( 0.009085777523910002)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21706535749306682) - present_state_Q (-0.21706535749306682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.116230723105552 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1545283504184259) - present_state_Q ( -0.5520439152240032)) * f1( 0.018866995031582574)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1545283504184259) - present_state_Q (-0.5520439152240032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.12491313675399 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2538200898171351) - present_state_Q ( -0.6226852392183754)) * f1( 0.021868500640212504)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2538200898171351) - present_state_Q (-0.6226852392183754)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.126785860549955 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13385915410346585) - present_state_Q ( -0.13385915410346585)) * f1( 0.004211100092359193)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13385915410346585) - present_state_Q (-0.13385915410346585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.135627851011876 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2292526674816676) - present_state_Q ( -0.6185277859819487)) * f1( 0.02226089301787015)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2292526674816676) - present_state_Q (-0.6185277859819487)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.143242993739875 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11160848252789728) - present_state_Q ( -0.5089706226176866)) * f1( 0.018711452083752016)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11160848252789728) - present_state_Q (-0.5089706226176866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.1440401846641 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12832394667148456) - present_state_Q ( -0.12832394667148456)) * f1( 0.004657732530172807)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12832394667148456) - present_state_Q (-0.12832394667148456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.146637584608385 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23946389852087036) - present_state_Q ( -0.6349980257582765)) * f1( 0.021360498011396813)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23946389852087036) - present_state_Q (-0.6349980257582765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.148235266849433 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3017786798908161) - present_state_Q ( -0.3017786798908161)) * f1( 0.01027161915319058)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3017786798908161) - present_state_Q (-0.3017786798908161)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.149464557768454 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27564330423462435) - present_state_Q ( -0.27564330423462435)) * f1( 0.007785468954306401)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.27564330423462435) - present_state_Q (-0.27564330423462435)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.152280215919575 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45267312415538336) - present_state_Q ( -0.8290671188588279)) * f1( 0.026989690642908806)
w2 ( -27.7006242566123 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.45267312415538336) - present_state_Q (-0.8290671188588279)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.161250958937483 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-11.279316385110327) - present_state_Q ( -11.279316385110327)) * f1( 0.007071067811865476)
w2 ( -28.20808611423197 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -11.279316385110327) - present_state_Q (-11.279316385110327)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -28.17012022652487 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-11.525486686368387) - present_state_Q ( -11.44744134533302)) * f1( 0.007071067811865476)
w2 ( -28.709807654647765 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -11.525486686368387) - present_state_Q (-11.44744134533302)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -28.197478571095573 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.33869646721495) - present_state_Q ( -12.200952435612253)) * f1( 0.024273544993357207)
w2 ( -29.160641590575775 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -6.33869646721495) - present_state_Q (-12.200952435612253)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -28.22148506181296 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.531451831179011) - present_state_Q ( -12.363580149294167)) * f1( 0.02350356387049017)
w2 ( -29.56920074951302 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -6.531451831179011) - present_state_Q (-12.363580149294167)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -28.266671040062985 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15346005475160177) - present_state_Q ( -0.7140747128603961)) * f1( 0.02444449658696289)
w2 ( -29.56920074951302 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15346005475160177) - present_state_Q (-0.7140747128603961)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.269763497090082 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.38713231641011) - present_state_Q ( -6.015621739127084)) * f1( 0.0023984738599084512)
w2 ( -29.827069478356755 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -6.38713231641011) - present_state_Q (-6.015621739127084)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.28851768756212 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.112838418571676) - present_state_Q ( -6.500351145870839)) * f1( 0.019453089591311507)
w2 ( -30.01988399642092 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -6.112838418571676) - present_state_Q (-6.500351145870839)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.32144755665943 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1834671134527453) - present_state_Q ( -0.748456226794194)) * f1( 0.025384074291793377)
w2 ( -30.01988399642092 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.1834671134527453) - present_state_Q (-0.748456226794194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.338932419825788 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.278872665971536) - present_state_Q ( -6.679133504287895)) * f1( 0.022851512331037042)
w2 ( -30.172914245912196 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.278872665971536) - present_state_Q (-6.679133504287895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.359237486090933 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.4308854270207965) - present_state_Q ( -6.80772559466986)) * f1( 0.026936506440480474)
w2 ( -30.323676679117934 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.4308854270207965) - present_state_Q (-6.80772559466986)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.37156491322134 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24488569895811696) - present_state_Q ( -0.4335451169788613)) * f1( 0.009957385299469377)
w2 ( -30.323676679117934 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.24488569895811696) - present_state_Q (-0.4335451169788613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.404259024103858 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2359469666038388) - present_state_Q ( -0.8033768904433736)) * f1( 0.027223632758801863)
w2 ( -30.323676679117934 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2359469666038388) - present_state_Q (-0.8033768904433736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.417974139857417 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.265253513732551) - present_state_Q ( -6.666848872836137)) * f1( 0.020321947708902215)
w2 ( -30.458655037984094 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -6.265253513732551) - present_state_Q (-6.666848872836137)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.42292844871326 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.331190915395785) - present_state_Q ( -6.331190915395785)) * f1( 0.0069865891054953925)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -6.331190915395785) - present_state_Q (-6.331190915395785)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.445862629438853 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33070142264582497) - present_state_Q ( -0.7308939221267221)) * f1( 0.024526264055888026)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.33070142264582497) - present_state_Q (-0.7308939221267221)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.4685071385586 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15823318945209508) - present_state_Q ( -0.7224377432193656)) * f1( 0.02423927060883816)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.15823318945209508) - present_state_Q (-0.7224377432193656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.486180430977942 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14641071878700865) - present_state_Q ( -0.5485141764362933)) * f1( 0.0185744961036337)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.14641071878700865) - present_state_Q (-0.5485141764362933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.504036261987938 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1165993205623682) - present_state_Q ( -0.5158169401755585)) * f1( 0.01870791535286988)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1165993205623682) - present_state_Q (-0.5158169401755585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.519555632349817 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10364224615716677) - present_state_Q ( -0.4992525591341284)) * f1( 0.016233988272687364)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.10364224615716677) - present_state_Q (-0.4992525591341284)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.53645765999758 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14004952753279953) - present_state_Q ( -0.5432208700587822)) * f1( 0.019639917462715557)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.14004952753279953) - present_state_Q (-0.5432208700587822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.554838297826258 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2920355075226065) - present_state_Q ( -0.6258558822584146)) * f1( 0.021526725381137646)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2920355075226065) - present_state_Q (-0.6258558822584146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.55663159067047 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11070646551190648) - present_state_Q ( -0.11070646551190648)) * f1( 0.001984710939164061)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.11070646551190648) - present_state_Q (-0.11070646551190648)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.576896269657087 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28964919505780945) - present_state_Q ( -0.8028109219891861)) * f1( 0.027208912546996224)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.28964919505780945) - present_state_Q (-0.8028109219891861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.594796075461375 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28474612033130864) - present_state_Q ( -0.6706675583244617)) * f1( 0.023616194844075917)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.28474612033130864) - present_state_Q (-0.6706675583244617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.607931161920916 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13496709465052625) - present_state_Q ( -0.49670318628963034)) * f1( 0.01697379782820989)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13496709465052625) - present_state_Q (-0.49670318628963034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.612427657996403 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08539795523182234) - present_state_Q ( -0.08539795523182234)) * f1( 0.005520697493309686)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08539795523182234) - present_state_Q (-0.08539795523182234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.624708119189677 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15279416104092958) - present_state_Q ( -0.5369378712799712)) * f1( 0.015948657585997356)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.15279416104092958) - present_state_Q (-0.5369378712799712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.64284761350857 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14914069231911112) - present_state_Q ( -0.721429643612556)) * f1( 0.02413726844280284)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14914069231911112) - present_state_Q (-0.721429643612556)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.656370932488066 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13210937257903071) - present_state_Q ( -0.513570994484728)) * f1( 0.017514314219998356)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13210937257903071) - present_state_Q (-0.513570994484728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.673843847744404 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25368005749332334) - present_state_Q ( -0.6519165954129872)) * f1( 0.023005490363534324)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.25368005749332334) - present_state_Q (-0.6519165954129872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.6767059727563 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08769182201587375) - present_state_Q ( -0.08769182201587375)) * f1( 0.00351494413100999)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08769182201587375) - present_state_Q (-0.08769182201587375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.687948645410934 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11151586302280009) - present_state_Q ( -0.48336219305371525)) * f1( 0.01450771404282077)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11151586302280009) - present_state_Q (-0.48336219305371525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.702865535075563 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22673507726932077) - present_state_Q ( -0.6271215119914038)) * f1( 0.022251759432360768)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.22673507726932077) - present_state_Q (-0.6271215119914038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.70882318429712 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16351531463130928) - present_state_Q ( -0.5175751472847319)) * f1( 0.011963439637465502)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16351531463130928) - present_state_Q (-0.5175751472847319)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.7104218282886 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18388868198159639) - present_state_Q ( -0.18388868198159639)) * f1( 0.0030074552024374006)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18388868198159639) - present_state_Q (-0.18388868198159639)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.71835415600907 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15009409666080478) - present_state_Q ( -0.5461748598278214)) * f1( 0.016025105091824702)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15009409666080478) - present_state_Q (-0.5461748598278214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.730195365864915 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19855974998824674) - present_state_Q ( -0.7451768199062507)) * f1( 0.024898551410367495)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.19855974998824674) - present_state_Q (-0.7451768199062507)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.73970134007528 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2608515979992333) - present_state_Q ( -0.6606137232425253)) * f1( 0.019613798124268552)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2608515979992333) - present_state_Q (-0.6606137232425253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.742387743759803 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13827421845402918) - present_state_Q ( -0.13827421845402918)) * f1( 0.005015075336813401)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13827421845402918) - present_state_Q (-0.13827421845402918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.744395584262527 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07620454916102462) - present_state_Q ( -0.07620454916102462)) * f1( 0.003709622742763043)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07620454916102462) - present_state_Q (-0.07620454916102462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.752685689259273 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0933456516483869) - present_state_Q ( -0.4966841431276349)) * f1( 0.016600948206969925)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0933456516483869) - present_state_Q (-0.4966841431276349)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.760171717941365 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08160359791962857) - present_state_Q ( -0.4869492471920509)) * f1( 0.01496512971381376)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08160359791962857) - present_state_Q (-0.4869492471920509)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.764284392337746 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07307492486767266) - present_state_Q ( -0.4353963391935956)) * f1( 0.012748604187833742)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07307492486767266) - present_state_Q (-0.4353963391935956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.769458992553755 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1711301041196182) - present_state_Q ( -0.5628209995179384)) * f1( 0.016647359218019287)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1711301041196182) - present_state_Q (-0.5628209995179384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.772977142660718 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12065945139512112) - present_state_Q ( -0.5150974649383258)) * f1( 0.01572343378775768)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.12065945139512112) - present_state_Q (-0.5150974649383258)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.776194492502768 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09619460804731915) - present_state_Q ( -0.4817806978012849)) * f1( 0.014183403758369326)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.09619460804731915) - present_state_Q (-0.4817806978012849)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.7768627384444 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12958517659052565) - present_state_Q ( -0.12958517659052565)) * f1( 0.003906939196325859)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12958517659052565) - present_state_Q (-0.12958517659052565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.779505325059382 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21114481553613051) - present_state_Q ( -0.5971731528195371)) * f1( 0.021124202122186766)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21114481553613051) - present_state_Q (-0.5971731528195371)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.781327185416856 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14036990066185587) - present_state_Q ( -0.14036990066185587)) * f1( 0.010712404658271026)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14036990066185587) - present_state_Q (-0.14036990066185587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.78333369005866 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13478958393754817) - present_state_Q ( -0.5418167675547115)) * f1( 0.015450140619707899)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13478958393754817) - present_state_Q (-0.5418167675547115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.785753007827164 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10316637284913724) - present_state_Q ( -0.5052100389315549)) * f1( 0.018161122576519157)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10316637284913724) - present_state_Q (-0.5052100389315549)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.788572184516752 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1480990564574442) - present_state_Q ( -0.45356781831207993)) * f1( 0.0203070247350614)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1480990564574442) - present_state_Q (-0.45356781831207993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.791276634714176 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15095488186056086) - present_state_Q ( -0.7110928379622267)) * f1( 0.023911241190647636)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15095488186056086) - present_state_Q (-0.7110928379622267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.791936107725782 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12669024871426818) - present_state_Q ( -0.12669024871426818)) * f1( 0.00384978343555937)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12669024871426818) - present_state_Q (-0.12669024871426818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.79423942922131 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20307917435158188) - present_state_Q ( -0.6078853564351625)) * f1( 0.018583310203776633)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.20307917435158188) - present_state_Q (-0.6078853564351625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.795096164755172 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1676651124913265) - present_state_Q ( -0.1676651124913265)) * f1( 0.005111372753651726)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1676651124913265) - present_state_Q (-0.1676651124913265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.797902905106266 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.220455778557493) - present_state_Q ( -0.794193912931838)) * f1( 0.026607044985612387)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.220455778557493) - present_state_Q (-0.794193912931838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.800242350086865 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0813623364392823) - present_state_Q ( -0.4777870655260593)) * f1( 0.017234957545313265)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0813623364392823) - present_state_Q (-0.4777870655260593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.800483844744477 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21407983888676246) - present_state_Q ( -0.21407983888676246)) * f1( 0.0014776075499114436)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21407983888676246) - present_state_Q (-0.21407983888676246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.80181233748782 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04423126894569053) - present_state_Q ( -0.04423126894569053)) * f1( 0.007433265188690913)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04423126894569053) - present_state_Q (-0.04423126894569053)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.80312736603247 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02881657389089098) - present_state_Q ( -0.02881657389089098)) * f1( 0.0073012539693691125)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02881657389089098) - present_state_Q (-0.02881657389089098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.8044073427587 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05266601608575376) - present_state_Q ( -0.05266601608575376)) * f1( 0.0071923549960434445)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05266601608575376) - present_state_Q (-0.05266601608575376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.80573073956416 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03547927969930168) - present_state_Q ( -0.03547927969930168)) * f1( 0.007372260532467442)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03547927969930168) - present_state_Q (-0.03547927969930168)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.82497974098863 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20368727542979603) - present_state_Q ( -0.20368727542979603)) * f1( 0.007071067811865476)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.20368727542979603) - present_state_Q (-0.20368727542979603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.84426790907796 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4014855099144551) - present_state_Q ( -0.16807707023959462)) * f1( 0.007071067811865476)
w2 ( -30.60047843080239 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.4014855099144551) - present_state_Q (-0.16807707023959462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.90545049209475 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.627021175403768) - present_state_Q ( -7.0192703444330515)) * f1( 0.030385532909506775)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -6.627021175403768) - present_state_Q (-7.0192703444330515)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -28.94666612529075 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4648507700387682) - present_state_Q ( -0.5421828311473775)) * f1( 0.01772277049335694)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.4648507700387682) - present_state_Q (-0.5421828311473775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -28.995499134567684 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.437227019174149) - present_state_Q ( -0.6034451894821123)) * f1( 0.02191963004546747)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.437227019174149) - present_state_Q (-0.6034451894821123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.03708546998386 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.490370941586623) - present_state_Q ( -0.5788679256614918)) * f1( 0.01864180936668633)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.490370941586623) - present_state_Q (-0.5788679256614918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.112812395708303 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6031399670674268) - present_state_Q ( -1.007540489366113)) * f1( 0.034593187739499764)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.6031399670674268) - present_state_Q (-1.007540489366113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.15204465229226 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5806794693363867) - present_state_Q ( -0.5806794693363867)) * f1( 0.018331286224949436)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.5806794693363867) - present_state_Q (-0.5806794693363867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.188793920004763 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.538404880438059) - present_state_Q ( -0.538404880438059)) * f1( 0.017140636939440575)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.538404880438059) - present_state_Q (-0.538404880438059)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.224917347421748 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5159392562069128) - present_state_Q ( -0.5159392562069128)) * f1( 0.016832857375768055)
w2 ( -31.003187069805048 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.5159392562069128) - present_state_Q (-0.5159392562069128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.24309924708071 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.485053592942196) - present_state_Q ( -6.5806767003261735)) * f1( 0.011369199502128302)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -6.485053592942196) - present_state_Q (-6.5806767003261735)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -29.286462323680585 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4844058185475805) - present_state_Q ( -0.7392320343559902)) * f1( 0.022344513428862862)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.4844058185475805) - present_state_Q (-0.7392320343559902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.35459922003686 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.577621557786718) - present_state_Q ( -1.0929254190712445)) * f1( 0.03574447782359557)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.577621557786718) - present_state_Q (-1.0929254190712445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.38621850265589 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4913224782158641) - present_state_Q ( -0.4913224782158641)) * f1( 0.016086989687238415)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.4913224782158641) - present_state_Q (-0.4913224782158641)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.427036569556975 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38399938657418425) - present_state_Q ( -0.5815281862981473)) * f1( 0.02087426743671332)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.38399938657418425) - present_state_Q (-0.5815281862981473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.466402531431065 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6516904197254942) - present_state_Q ( -0.6516904197254942)) * f1( 0.020176438032163253)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.6516904197254942) - present_state_Q (-0.6516904197254942)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.506738515634286 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6158919057823472) - present_state_Q ( -0.6158919057823472)) * f1( 0.02063952608980196)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.6158919057823472) - present_state_Q (-0.6158919057823472)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.5428612218338 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5280427603075005) - present_state_Q ( -0.5838762842507125)) * f1( 0.01846168793825378)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.5280427603075005) - present_state_Q (-0.5838762842507125)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.564466883809317 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21094887764682696) - present_state_Q ( -0.21094887764682696)) * f1( 0.010853012356869807)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.21094887764682696) - present_state_Q (-0.21094887764682696)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.60062822912061 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32618317548110903) - present_state_Q ( -0.5007395772016817)) * f1( 0.01842216601896397)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.32618317548110903) - present_state_Q (-0.5007395772016817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.6410075415791 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5169247727265244) - present_state_Q ( -0.606211350060852)) * f1( 0.020661925264230992)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.5169247727265244) - present_state_Q (-0.606211350060852)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.677730336721616 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6409844500303951) - present_state_Q ( -0.6409844500303951)) * f1( 0.01973603597023556)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.6409844500303951) - present_state_Q (-0.6409844500303951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.711088269550892 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48022055895870425) - present_state_Q ( -0.5622241464127585)) * f1( 0.017867455882291057)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.48022055895870425) - present_state_Q (-0.5622241464127585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.7677584316662 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5128124553392305) - present_state_Q ( -0.9246126359873617)) * f1( 0.030949491696091364)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.5128124553392305) - present_state_Q (-0.9246126359873617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.79850021833393 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24076436222578498) - present_state_Q ( -0.5884017018423342)) * f1( 0.01736233620897857)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.24076436222578498) - present_state_Q (-0.5884017018423342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.83360764997404 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.340132897389778) - present_state_Q ( -0.5432004972295769)) * f1( 0.01976640945918209)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.340132897389778) - present_state_Q (-0.5432004972295769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.857455432171534 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43978373545866184) - present_state_Q ( -0.43978373545866184)) * f1( 0.013341760262077658)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.43978373545866184) - present_state_Q (-0.43978373545866184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.888248531232033 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.383485792456347) - present_state_Q ( -0.49866941126240355)) * f1( 0.01728975708122296)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.383485792456347) - present_state_Q (-0.49866941126240355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.93516110799861 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4421210868092654) - present_state_Q ( -0.8618556352464551)) * f1( 0.026879836897035388)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.4421210868092654) - present_state_Q (-0.8618556352464551)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -29.99716211142927 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5642442419952516) - present_state_Q ( -1.087982329688174)) * f1( 0.03596598952800148)
w2 ( -31.323031921776558 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.5642442419952516) - present_state_Q (-1.087982329688174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -30.027328583099706 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.745917054032376) - present_state_Q ( -6.980200077299193)) * f1( 0.025212817243688575)
w2 ( -31.562326653332097 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -6.745917054032376) - present_state_Q (-6.980200077299193)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -30.064112622635804 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.4470936992815355) - present_state_Q ( -7.4470936992815355)) * f1( 0.03776236372469917)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -7.4470936992815355) - present_state_Q (-7.4470936992815355)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -30.178982879474834 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.898596703309801) - present_state_Q ( -3.2080702320972985)) * f1( 0.09819594345826234)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -2.898596703309801) - present_state_Q (-3.2080702320972985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -30.297467011977133 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.0633642001553802) - present_state_Q ( -3.0633642001553802)) * f1( 0.09990863729973194)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -3.0633642001553802) - present_state_Q (-3.0633642001553802)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -30.415106508167394 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.036678209218253) - present_state_Q ( -3.036678209218253)) * f1( 0.09899593296389438)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -3.036678209218253) - present_state_Q (-3.036678209218253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -30.530866138480718 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.9641515480216274) - present_state_Q ( -2.9641515480216274)) * f1( 0.09688182364788511)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -2.9641515480216274) - present_state_Q (-2.9641515480216274)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -30.741235106971224 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.235910851358626) - present_state_Q ( -3.57699985463577)) * f1( 0.18513722370286315)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -3.235910851358626) - present_state_Q (-3.57699985463577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -30.873445907720438 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.2804423630078987) - present_state_Q ( -3.5652292966210584)) * f1( 0.11618748977211986)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -3.2804423630078987) - present_state_Q (-3.5652292966210584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.006716833528262 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.3310371808447696) - present_state_Q ( -3.625253745780131)) * f1( 0.11768760486027813)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -3.3310371808447696) - present_state_Q (-3.625253745780131)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.16971917714861 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.254319603549947) - present_state_Q ( -5.254319603549947)) * f1( 0.16485884684797764)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -5.254319603549947) - present_state_Q (-5.254319603549947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.33999765772633 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.3585170621093985) - present_state_Q ( -5.799110635666549)) * f1( 0.18205727630602922)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -5.3585170621093985) - present_state_Q (-5.799110635666549)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.4893231908406 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.4329824301991785) - present_state_Q ( -5.4329824301991785)) * f1( 0.16943636717175498)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -5.4329824301991785) - present_state_Q (-5.4329824301991785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.645426536617077 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.517564326935008) - present_state_Q ( -5.962890241073121)) * f1( 0.18826628988429317)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -5.517564326935008) - present_state_Q (-5.962890241073121)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.800146965405855 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.681179071104809) - present_state_Q ( -5.797461503801559)) * f1( 0.1825951244907095)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -5.681179071104809) - present_state_Q (-5.797461503801559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -31.95406095505067 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.451793715195051) - present_state_Q ( -5.493271549454962)) * f1( 0.17580794087525817)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -5.451793715195051) - present_state_Q (-5.493271549454962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.103057410003736 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.6416255340027766) - present_state_Q ( -5.6416255340027766)) * f1( 0.1727435900038726)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -5.6416255340027766) - present_state_Q (-5.6416255340027766)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.261132585717284 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.367745002989579) - present_state_Q ( -6.821608023032157)) * f1( 0.21026436380861976)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.367745002989579) - present_state_Q (-6.821608023032157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.425690068088336 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.623863283349774) - present_state_Q ( -7.210038204168003)) * f1( 0.2299860590947513)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.623863283349774) - present_state_Q (-7.210038204168003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.5853117839983 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.587197636713327) - present_state_Q ( -6.587197636713327)) * f1( 0.2053202337441279)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.587197636713327) - present_state_Q (-6.587197636713327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.741870828808885 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.73489228917399) - present_state_Q ( -6.898791134547623)) * f1( 0.20937472306344518)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.73489228917399) - present_state_Q (-6.898791134547623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.89784194633606 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.778123992789422) - present_state_Q ( -6.931993018739545)) * f1( 0.20939717028417446)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -6.778123992789422) - present_state_Q (-6.931993018739545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.054486061360286 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.453883272104672) - present_state_Q ( -7.632171428563576)) * f1( 0.2298190662151003)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -7.453883272104672) - present_state_Q (-7.632171428563576)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.2113777823427 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.425840111950547) - present_state_Q ( -7.425840111950547)) * f1( 0.22350831628069595)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -7.425840111950547) - present_state_Q (-7.425840111950547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.36687550577365 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.9597967389736475) - present_state_Q ( -7.9597967389736475)) * f1( 0.23780258613876057)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -7.9597967389736475) - present_state_Q (-7.9597967389736475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.4666743696799 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.37008028960166) - present_state_Q ( -9.024331835306784)) * f1( 0.2705749180870422)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -8.37008028960166) - present_state_Q (-9.024331835306784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.4924133444791 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.859337612943614) - present_state_Q ( -11.066722332967371)) * f1( 0.32938787800193114)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -8.859337612943614) - present_state_Q (-11.066722332967371)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.479372604975225 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-11.55215538279514) - present_state_Q ( -11.58395719047767)) * f1( 0.3431304639881873)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -11.55215538279514) - present_state_Q (-11.58395719047767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.44105473637515 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-11.415504315301568) - present_state_Q ( -11.415504315301568)) * f1( 0.3364813325843391)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -11.415504315301568) - present_state_Q (-11.415504315301568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.400245145232184 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-10.97094781620562) - present_state_Q ( -11.427319882166271)) * f1( 0.3414878162268642)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -10.97094781620562) - present_state_Q (-11.427319882166271)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.36616184906358 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.234974481021057) - present_state_Q ( -11.09148955400482)) * f1( 0.33000240451876395)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -9.234974481021057) - present_state_Q (-11.09148955400482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.39990385281304 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.713536165699296) - present_state_Q ( -8.713536165699296)) * f1( 0.26096107230693966)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -8.713536165699296) - present_state_Q (-8.713536165699296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.41241106980561 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.908204303400975) - present_state_Q ( -8.519019972202198)) * f1( 0.253461807419001)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -7.908204303400975) - present_state_Q (-8.519019972202198)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.43940810922034 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.842881661605274) - present_state_Q ( -7.842881661605274)) * f1( 0.23212043414446293)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -7.842881661605274) - present_state_Q (-7.842881661605274)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.4467090064123 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.2467970518935285) - present_state_Q ( -7.716057332778467)) * f1( 0.23048645890832994)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -7.2467970518935285) - present_state_Q (-7.716057332778467)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.47002440048146 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.883360090036665) - present_state_Q ( -6.148936112759296)) * f1( 0.13341853351421978)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -5.883360090036665) - present_state_Q (-6.148936112759296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.51442085591001 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8607143266368227) - present_state_Q ( -3.8607143266368227)) * f1( 0.11581195341006431)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -3.8607143266368227) - present_state_Q (-3.8607143266368227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.55776363953789 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.735103935404463) - present_state_Q ( -3.735103935404463)) * f1( 0.10982464358056421)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -3.735103935404463) - present_state_Q (-3.735103935404463)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.601097445921006 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7123006570542443) - present_state_Q ( -3.7123006570542443)) * f1( 0.1092338549844451)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -3.7123006570542443) - present_state_Q (-3.7123006570542443)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.65577636268303 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.002236229394266) - present_state_Q ( -4.671605535268889)) * f1( 0.18005699353496005)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.002236229394266) - present_state_Q (-4.671605535268889)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.67179237536237 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.0534535236583835) - present_state_Q ( -6.487183432301736)) * f1( 0.13060437075970666)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.0534535236583835) - present_state_Q (-6.487183432301736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.705332592929366 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6233645794941616) - present_state_Q ( -3.6233645794941616)) * f1( 0.1070343906574247)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.6233645794941616) - present_state_Q (-3.6233645794941616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.73865626719839 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.898966852934845) - present_state_Q ( -3.898966852934845)) * f1( 0.11548463116942866)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.898966852934845) - present_state_Q (-3.898966852934845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.770963456765095 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.872694290365452) - present_state_Q ( -4.193526467644083)) * f1( 0.12481704037330667)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.872694290365452) - present_state_Q (-4.193526467644083)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.793403461093696 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8426073478127782) - present_state_Q ( -3.8426073478127782)) * f1( 0.11093772385926984)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -3.8426073478127782) - present_state_Q (-3.8426073478127782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.788748654067064 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.880758074820823) - present_state_Q ( -6.1283213338642195)) * f1( 0.17962376926734794)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -3.880758074820823) - present_state_Q (-6.1283213338642195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.77988724681252 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8802853390968264) - present_state_Q ( -6.573138667503955)) * f1( 0.12587107339110723)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -3.8802853390968264) - present_state_Q (-6.573138667503955)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.793382254717905 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7543995793742133) - present_state_Q ( -3.7543995793742133)) * f1( 0.11353445837357203)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.7543995793742133) - present_state_Q (-3.7543995793742133)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.8058809026745 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.813618369474234) - present_state_Q ( -3.813618369474234)) * f1( 0.1100882668950734)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.813618369474234) - present_state_Q (-3.813618369474234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.81850955648564 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8528185309981877) - present_state_Q ( -3.8528185309981877)) * f1( 0.11480077149111917)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.8528185309981877) - present_state_Q (-3.8528185309981877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.82985877936207 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.003017339293191) - present_state_Q ( -4.003017339293191)) * f1( 0.11762429593557006)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.003017339293191) - present_state_Q (-4.003017339293191)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.84312789818592 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.719660149091509) - present_state_Q ( -3.719660149091509)) * f1( 0.10877288891902932)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.719660149091509) - present_state_Q (-3.719660149091509)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.821326297765836 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.804152054102007) - present_state_Q ( -6.157224335348789)) * f1( 0.18029430764946278)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.804152054102007) - present_state_Q (-6.157224335348789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.834854375312204 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7433594704125803) - present_state_Q ( -3.7433594704125803)) * f1( 0.11286916727698458)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.7433594704125803) - present_state_Q (-3.7433594704125803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.846827014377574 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.930696991770049) - present_state_Q ( -3.930696991770049)) * f1( 0.11624384763607125)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.930696991770049) - present_state_Q (-3.930696991770049)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.85924301369083 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8214186976706683) - present_state_Q ( -3.8214186976706683)) * f1( 0.11004073181126675)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.8214186976706683) - present_state_Q (-3.8214186976706683)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.868212320660604 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7476786860603277) - present_state_Q ( -4.212202175347688)) * f1( 0.12284165244157595)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.7476786860603277) - present_state_Q (-4.212202175347688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.836828054373726 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.037346482103989) - present_state_Q ( -6.592687338582903)) * f1( 0.1935667672764059)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.037346482103989) - present_state_Q (-6.592687338582903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.84881633794198 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.9287398048384627) - present_state_Q ( -3.9287398048384627)) * f1( 0.1161970182664241)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.9287398048384627) - present_state_Q (-3.9287398048384627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.86164082762036 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7786078443757654) - present_state_Q ( -3.7786078443757654)) * f1( 0.10990794554978789)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.7786078443757654) - present_state_Q (-3.7786078443757654)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.83900434313815 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.9907104923279726) - present_state_Q ( -6.210435705352483)) * f1( 0.18199772678037868)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.9907104923279726) - present_state_Q (-6.210435705352483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.851153930600546 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.9080649049033296) - present_state_Q ( -3.9080649049033296)) * f1( 0.1156742418536905)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.9080649049033296) - present_state_Q (-3.9080649049033296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.864432723205 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7174349920972203) - present_state_Q ( -3.7174349920972203)) * f1( 0.1086737844146743)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.7174349920972203) - present_state_Q (-3.7174349920972203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.841617183049244 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.916364575677177) - present_state_Q ( -6.211744686659077)) * f1( 0.18215680289397612)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.916364575677177) - present_state_Q (-6.211744686659077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.8505545058396 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.9541590311159904) - present_state_Q ( -4.251385564528662)) * f1( 0.12559182981052922)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.9541590311159904) - present_state_Q (-4.251385564528662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.862463636635624 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.9374627134831615) - present_state_Q ( -3.9374627134831615)) * f1( 0.11631489522288507)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.9374627134831615) - present_state_Q (-3.9374627134831615)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.87513371101903 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.799164461888567) - present_state_Q ( -3.799164461888567)) * f1( 0.11033399519217352)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.799164461888567) - present_state_Q (-3.799164461888567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.884336709507124 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.9283052748469416) - present_state_Q ( -4.2223790055889765)) * f1( 0.12469549656437927)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.9283052748469416) - present_state_Q (-4.2223790055889765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.89381937262751 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.880784080934481) - present_state_Q ( -4.191508825046989)) * f1( 0.12409331796519985)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.880784080934481) - present_state_Q (-4.191508825046989)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.908043573426866 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5762882134003178) - present_state_Q ( -3.5762882134003178)) * f1( 0.10544827429527003)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.5762882134003178) - present_state_Q (-3.5762882134003178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.91952860819382 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5341830507325636) - present_state_Q ( -3.91926681369325)) * f1( 0.11465111512506689)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.5341830507325636) - present_state_Q (-3.91926681369325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.93158111295837 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5396194326331885) - present_state_Q ( -3.8788101479011425)) * f1( 0.11558516507170913)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.5396194326331885) - present_state_Q (-3.8788101479011425)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.94469588081057 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.405762554665995) - present_state_Q ( -3.720386189753775)) * f1( 0.11041445950413699)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.405762554665995) - present_state_Q (-3.720386189753775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.93906088372382 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6326965400099755) - present_state_Q ( -3.6326965400099755)) * f1( 0.10654683161783884)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.6326965400099755) - present_state_Q (-3.6326965400099755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.93177412219497 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.780529718055675) - present_state_Q ( -3.780529718055675)) * f1( 0.11008439758316126)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.780529718055675) - present_state_Q (-3.780529718055675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.92704225404863 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5475684892400716) - present_state_Q ( -3.5475684892400716)) * f1( 0.10462718823132905)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.5475684892400716) - present_state_Q (-3.5475684892400716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.91967916866485 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.427803551978521) - present_state_Q ( -3.745687145209045)) * f1( 0.11116523397682347)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.427803551978521) - present_state_Q (-3.745687145209045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.90531071152128 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.564422725889253) - present_state_Q ( -4.250152780198923)) * f1( 0.12460085790171373)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.564422725889253) - present_state_Q (-4.250152780198923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.9017231779558 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.4529240350555406) - present_state_Q ( -3.4529240350555406)) * f1( 0.09773168381102249)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.4529240350555406) - present_state_Q (-3.4529240350555406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.888910054820506 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.457545945132718) - present_state_Q ( -4.1381917834729105)) * f1( 0.12181101257579056)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.457545945132718) - present_state_Q (-4.1381917834729105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.886897840924874 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.2766883529236446) - present_state_Q ( -3.2766883529236446)) * f1( 0.09652397805325975)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.2766883529236446) - present_state_Q (-3.2766883529236446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.87782440499173 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8893077242229714) - present_state_Q ( -3.9375409117451885)) * f1( 0.11228688382266656)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.8893077242229714) - present_state_Q (-3.9375409117451885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.856552844027824 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.263598311737901) - present_state_Q ( -4.739200831484542)) * f1( 0.13529037902897037)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.263598311737901) - present_state_Q (-4.739200831484542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.84487117194866 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.105020863376786) - present_state_Q ( -4.137332178681528)) * f1( 0.11844193964547682)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.105020863376786) - present_state_Q (-4.137332178681528)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.82327866960956 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.246694665678948) - present_state_Q ( -4.7441282939276475)) * f1( 0.1367560038553419)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.246694665678948) - present_state_Q (-4.7441282939276475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.811322862818045 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.113396160477079) - present_state_Q ( -4.152245861255677)) * f1( 0.11951569927024565)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.113396160477079) - present_state_Q (-4.152245861255677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.790329834570024 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.14513534137592) - present_state_Q ( -4.6915805972950135)) * f1( 0.13662752310448853)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.14513534137592) - present_state_Q (-4.6915805972950135)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.76758766428951 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.288520908863907) - present_state_Q ( -4.808388491086038)) * f1( 0.13875767642484924)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.288520908863907) - present_state_Q (-4.808388491086038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.752797413561154 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.290828244127003) - present_state_Q ( -4.348684233346338)) * f1( 0.1254421348718407)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.290828244127003) - present_state_Q (-4.348684233346338)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.74008425024082 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.195009810274594) - present_state_Q ( -4.195009810274594)) * f1( 0.12283758894475812)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.195009810274594) - present_state_Q (-4.195009810274594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.716775543245504 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.264109857713228) - present_state_Q ( -4.827114916782475)) * f1( 0.1404010256163778)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.264109857713228) - present_state_Q (-4.827114916782475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.691755489580075 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.391514004172692) - present_state_Q ( -4.928529788122089)) * f1( 0.14306766044154207)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.391514004172692) - present_state_Q (-4.928529788122089)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.67817103506967 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.275803893503452) - present_state_Q ( -4.275803893503452)) * f1( 0.12263971133050712)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.275803893503452) - present_state_Q (-4.275803893503452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.66503839999666 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.182232885260929) - present_state_Q ( -4.236796995639811)) * f1( 0.12182159087188518)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.182232885260929) - present_state_Q (-4.236796995639811)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.65353749395508 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.12940166413611) - present_state_Q ( -4.12940166413611)) * f1( 0.11784804875104234)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.12940166413611) - present_state_Q (-4.12940166413611)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.64202599048824 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.1289103915291445) - present_state_Q ( -4.1289103915291445)) * f1( 0.11801010458683439)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.1289103915291445) - present_state_Q (-4.1289103915291445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.6207472890919 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.218807188545829) - present_state_Q ( -4.717919359256881)) * f1( 0.1367976896775308)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.218807188545829) - present_state_Q (-4.717919359256881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.59898513204093 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.243506979395377) - present_state_Q ( -4.745472178018904)) * f1( 0.13768552271301396)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.243506979395377) - present_state_Q (-4.745472178018904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.57620729367045 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.285163917727894) - present_state_Q ( -4.803176137413769)) * f1( 0.13939004231062402)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.285163917727894) - present_state_Q (-4.803176137413769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.549628623947825 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.474179812162044) - present_state_Q ( -5.008121343641257)) * f1( 0.1460244818670657)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.474179812162044) - present_state_Q (-5.008121343641257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.5228998094023 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.480207301176865) - present_state_Q ( -5.015114895695027)) * f1( 0.14633558031343413)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.480207301176865) - present_state_Q (-5.015114895695027)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.50823068746675 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.346358306791257) - present_state_Q ( -4.346358306791257)) * f1( 0.12525178023951458)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.346358306791257) - present_state_Q (-4.346358306791257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.48276252965651 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.488955619852503) - present_state_Q ( -4.962811095271012)) * f1( 0.14361496463310147)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.488955619852503) - present_state_Q (-4.962811095271012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.45722369951209 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.492335908701333) - present_state_Q ( -4.965838125321998)) * f1( 0.14379544492958357)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.492335908701333) - present_state_Q (-4.965838125321998)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.44332422630922 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.291095565298943) - present_state_Q ( -4.291095565298943)) * f1( 0.12394371761534834)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.291095565298943) - present_state_Q (-4.291095565298943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.41965238796116 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.385915010850756) - present_state_Q ( -4.8588525987271)) * f1( 0.14092818066950188)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.385915010850756) - present_state_Q (-4.8588525987271)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.404455217979354 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.288509062678686) - present_state_Q ( -4.363870468919381)) * f1( 0.12722963084303812)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.288509062678686) - present_state_Q (-4.363870468919381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.390556952134396 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.289453061997984) - present_state_Q ( -4.289453061997984)) * f1( 0.12409653307204703)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.289453061997984) - present_state_Q (-4.289453061997984)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.37689466072161 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.272675052404971) - present_state_Q ( -4.272675052404971)) * f1( 0.12365678267245808)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.272675052404971) - present_state_Q (-4.272675052404971)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.35146663741434 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.474942546018359) - present_state_Q ( -4.946957814997235)) * f1( 0.14456678866928838)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.474942546018359) - present_state_Q (-4.946957814997235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.33631199793804 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.280671659896246) - present_state_Q ( -4.355020081961132)) * f1( 0.12773621448239664)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.280671659896246) - present_state_Q (-4.355020081961132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.311650676284316 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.431598716610022) - present_state_Q ( -4.903024606175162)) * f1( 0.14343706915244409)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.431598716610022) - present_state_Q (-4.903024606175162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.2857803125724 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.432551492476613) - present_state_Q ( -4.9587219215013985)) * f1( 0.1457555053482762)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.432551492476613) - present_state_Q (-4.9587219215013985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.25956400110072 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.4456266553056025) - present_state_Q ( -4.976383609853079)) * f1( 0.14635606666092277)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.4456266553056025) - present_state_Q (-4.976383609853079)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.23246767413845 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.487181616512917) - present_state_Q ( -5.02269708063646)) * f1( 0.14779058208078497)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.487181616512917) - present_state_Q (-5.02269708063646)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.20353352228754 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.6642639985443894) - present_state_Q ( -5.132768165599334)) * f1( 0.1502456212822825)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.6642639985443894) - present_state_Q (-5.132768165599334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.1730472139413 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.641493136857792) - present_state_Q ( -5.190408153708928)) * f1( 0.15352872729071373)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.641493136857792) - present_state_Q (-5.190408153708928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.15351602705141 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.516034288941065) - present_state_Q ( -4.624272062365607)) * f1( 0.13637983751435395)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.516034288941065) - present_state_Q (-4.624272062365607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.1369838504059 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.453104858180545) - present_state_Q ( -4.453104858180545)) * f1( 0.1304578638320007)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.453104858180545) - present_state_Q (-4.453104858180545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.1110184971222 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.499660093701574) - present_state_Q ( -4.967888959550205)) * f1( 0.14608852203063108)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.499660093701574) - present_state_Q (-4.967888959550205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.097827009354184 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.231825873642287) - present_state_Q ( -4.231825873642287)) * f1( 0.12350521679769409)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.231825873642287) - present_state_Q (-4.231825873642287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.072720229662266 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.383456983863645) - present_state_Q ( -4.908200424164475)) * f1( 0.1451843889357909)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.383456983863645) - present_state_Q (-4.908200424164475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.05636190204688 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.359289358035727) - present_state_Q ( -4.425746601165789)) * f1( 0.13094351903286555)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.359289358035727) - present_state_Q (-4.425746601165789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.03188384958881 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.421610577055696) - present_state_Q ( -4.889042742351288)) * f1( 0.14345439210066654)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.421610577055696) - present_state_Q (-4.889042742351288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.017367485266305 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.232922997782549) - present_state_Q ( -4.306569539574209)) * f1( 0.12703282071005462)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.232922997782549) - present_state_Q (-4.306569539574209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.99239423445892 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.375822456800046) - present_state_Q ( -4.897630486046412)) * f1( 0.14523583408082608)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.375822456800046) - present_state_Q (-4.897630486046412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.95377541808128 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.389370466362844) - present_state_Q ( -4.914315192331983)) * f1( 0.14582252709924282)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.389370466362844) - present_state_Q (-4.914315192331983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.91780064050592 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.306254658895088) - present_state_Q ( -4.77200427677651)) * f1( 0.14307816693039319)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.306254658895088) - present_state_Q (-4.77200427677651)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.87995325237587 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.416883797008181) - present_state_Q ( -4.882329770764515)) * f1( 0.14480903071756013)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.416883797008181) - present_state_Q (-4.882329770764515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.85383779729266 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.316328877610784) - present_state_Q ( -4.316328877610784)) * f1( 0.1269181308400893)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.316328877610784) - present_state_Q (-4.316328877610784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.816938057489594 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.380834755289698) - present_state_Q ( -4.845453456058601)) * f1( 0.14300365208140314)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.380834755289698) - present_state_Q (-4.845453456058601)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.793451720389534 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.160234118590115) - present_state_Q ( -4.160234118590115)) * f1( 0.12250484309135672)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.160234118590115) - present_state_Q (-4.160234118590115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.760586866867754 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.1748477731622895) - present_state_Q ( -4.63760848583674)) * f1( 0.13733233736397096)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.1748477731622895) - present_state_Q (-4.63760848583674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.728240329178114 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.147413954865961) - present_state_Q ( -4.608600692997287)) * f1( 0.1366663795937822)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -4.147413954865961) - present_state_Q (-4.608600692997287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.70840856013138 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.936839107921076) - present_state_Q ( -3.936839107921076)) * f1( 0.1155616211712164)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.936839107921076) - present_state_Q (-3.936839107921076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.67980203779266 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.960033225819432) - present_state_Q ( -4.41425995267299)) * f1( 0.13055053561287366)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.960033225819432) - present_state_Q (-4.41425995267299)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.66309130310881 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.744826595743121) - present_state_Q ( -3.724407110396512)) * f1( 0.10973041498899484)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.744826595743121) - present_state_Q (-3.724407110396512)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.63426034509205 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.969987896711844) - present_state_Q ( -4.423847078530902)) * f1( 0.13106090204908338)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.969987896711844) - present_state_Q (-4.423847078530902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.60530287648369 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.98621737179977) - present_state_Q ( -4.437976424214593)) * f1( 0.13089185151493957)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.98621737179977) - present_state_Q (-4.437976424214593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.587535981131815 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8024044997726) - present_state_Q ( -3.8024044997726)) * f1( 0.11138214634728233)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.8024044997726) - present_state_Q (-3.8024044997726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.558833921692056 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.960428073013382) - present_state_Q ( -4.413268940540569)) * f1( 0.13104816463643107)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.960428073013382) - present_state_Q (-4.413268940540569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.54209929726709 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.7371512932453417) - present_state_Q ( -3.7168775871194493)) * f1( 0.1103773878937762)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.7371512932453417) - present_state_Q (-3.7168775871194493)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.52465574019061 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.773853388226977) - present_state_Q ( -3.7632306401173525)) * f1( 0.11190297761395185)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.773853388226977) - present_state_Q (-3.7632306401173525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.496655692082086 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.922484884659477) - present_state_Q ( -4.37266934900614)) * f1( 0.13002798084064765)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.922484884659477) - present_state_Q (-4.37266934900614)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.46905640782914 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.900564810541436) - present_state_Q ( -4.343143357724299)) * f1( 0.12981469549134977)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -3.900564810541436) - present_state_Q (-4.343143357724299)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.488288924288106 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22959089964704513) - present_state_Q ( -0.22959089964704513)) * f1( 0.007071067811865476)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.22959089964704513) - present_state_Q (-0.22959089964704513)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.507360974227744 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18943764986335884) - present_state_Q ( -0.45250950259882433)) * f1( 0.007071067811865476)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.18943764986335884) - present_state_Q (-0.45250950259882433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.52695233327797 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16509383390187607) - present_state_Q ( -0.16509383390187607)) * f1( 0.007187660867388479)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.16509383390187607) - present_state_Q (-0.16509383390187607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.54439132739475 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23424146941746626) - present_state_Q ( -0.23424146941746626)) * f1( 0.0064126443944396505)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.23424146941746626) - present_state_Q (-0.23424146941746626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.56189705000277 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19373370312618565) - present_state_Q ( -0.273884051509859)) * f1( 0.0064475408862452125)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.19373370312618565) - present_state_Q (-0.273884051509859)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.58406784066052 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16384658164537738) - present_state_Q ( -0.16384658164537738)) * f1( 0.008415706680595155)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.16384658164537738) - present_state_Q (-0.16384658164537738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.639627378789235 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1846613431577999) - present_state_Q ( -0.7563658817461064)) * f1( 0.021573084364020122)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.1846613431577999) - present_state_Q (-0.7563658817461064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.69687608617387 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36790796838478007) - present_state_Q ( -0.8158055415508523)) * f1( 0.023084651234439986)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.36790796838478007) - present_state_Q (-0.8158055415508523)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.76041091740754 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4799778154692633) - present_state_Q ( -0.9175567146279758)) * f1( 0.025713313636307883)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.4799778154692633) - present_state_Q (-0.9175567146279758)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.77993070258466 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3051037867981212) - present_state_Q ( -0.3051037867981212)) * f1( 0.008003069800608201)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.3051037867981212) - present_state_Q (-0.3051037867981212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.808454310936284 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3138511466902734) - present_state_Q ( -0.3267234890516199)) * f1( 0.01170457307020283)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.3138511466902734) - present_state_Q (-0.3267234890516199)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.82793046235865 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24836527023995042) - present_state_Q ( -0.44273426068752464)) * f1( 0.00803237383745352)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.24836527023995042) - present_state_Q (-0.44273426068752464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.8536570556629 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2857990910507949) - present_state_Q ( -0.2857990910507949)) * f1( 0.010540339438600567)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.2857990910507949) - present_state_Q (-0.2857990910507949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.88130341336508 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3166719330843999) - present_state_Q ( -0.3166719330843999)) * f1( 0.011339787527810948)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.3166719330843999) - present_state_Q (-0.3166719330843999)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.921807390569995 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5718496288416629) - present_state_Q ( -0.605926053844818)) * f1( 0.017456573754554797)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.5718496288416629) - present_state_Q (-0.605926053844818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -32.972611071782445 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5848805698141925) - present_state_Q ( -0.6988035025732953)) * f1( 0.021982341253746224)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.5848805698141925) - present_state_Q (-0.6988035025732953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.050052138264626 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6163358000125286) - present_state_Q ( -1.1783425964590466)) * f1( 0.0342133678023234)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.6163358000125286) - present_state_Q (-1.1783425964590466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.13249093672009 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6869467048308984) - present_state_Q ( -1.2648671713097617)) * f1( 0.03654967409736608)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.6869467048308984) - present_state_Q (-1.2648671713097617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.189503385746335 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8823948960537045) - present_state_Q ( -0.8823948960537045)) * f1( 0.024834133953162026)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.8823948960537045) - present_state_Q (-0.8823948960537045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.23985783593633 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.743710612780199) - present_state_Q ( -0.743710612780199)) * f1( 0.021815359439241212)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.743710612780199) - present_state_Q (-0.743710612780199)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.28932408875892 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7306108518213181) - present_state_Q ( -0.7306108518213181)) * f1( 0.02141961975925263)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.7306108518213181) - present_state_Q (-0.7306108518213181)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.33883417241831 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6238685590493805) - present_state_Q ( -0.7179109479814462)) * f1( 0.0223195271132318)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.6238685590493805) - present_state_Q (-0.7179109479814462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.38022979028289 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6769881896649141) - present_state_Q ( -0.6769881896649141)) * f1( 0.018622648296063034)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.6769881896649141) - present_state_Q (-0.6769881896649141)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.42104008101604 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5516162362489133) - present_state_Q ( -0.6531840435577428)) * f1( 0.01835002614462684)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.5516162362489133) - present_state_Q (-0.6531840435577428)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.460671535041826 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6435030128990396) - present_state_Q ( -0.6435030128990396)) * f1( 0.01780486475007751)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.6435030128990396) - present_state_Q (-0.6435030128990396)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.5097313873313 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6143031758950441) - present_state_Q ( -0.7138386945289252)) * f1( 0.022113453313359285)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.6143031758950441) - present_state_Q (-0.7138386945289252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.58275317715191 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7305625124616622) - present_state_Q ( -1.1942339337610355)) * f1( 0.033624634458008874)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.7305625124616622) - present_state_Q (-1.1942339337610355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.62259097969933 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5506792306346279) - present_state_Q ( -0.6381990949971479)) * f1( 0.017900768607354192)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.5506792306346279) - present_state_Q (-0.6381990949971479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.67193550160613 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5231886456018205) - present_state_Q ( -0.7156911714828642)) * f1( 0.022252763707029406)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.5231886456018205) - present_state_Q (-0.7156911714828642)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.697366479459035 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5531467235340393) - present_state_Q ( -0.5531467235340393)) * f1( 0.011383556401513008)
w2 ( -31.75714517583914 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.5531467235340393) - present_state_Q (-0.5531467235340393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.72820602553084 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.940888875967968) - present_state_Q ( -7.027666699623062)) * f1( 0.01868570392972637)
w2 ( -32.08723224334043 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -6.940888875967968) - present_state_Q (-7.027666699623062)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -33.81383131006842 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.380946569350488) - present_state_Q ( -1.4512109561330102)) * f1( 0.04154288877460764)
w2 ( -32.499458196148616 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -1.380946569350488) - present_state_Q (-1.4512109561330102)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -33.85150200984635 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.386300590601874) - present_state_Q ( -7.386300590601874)) * f1( 0.024658854817208312)
w2 ( -32.80499306430993 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -7.386300590601874) - present_state_Q (-7.386300590601874)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -33.8857274079135 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.328090273084478) - present_state_Q ( -7.328090273084478)) * f1( 0.02232702837840521)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -7.328090273084478) - present_state_Q (-7.328090273084478)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -33.92282375446216 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6671331168239689) - present_state_Q ( -0.6671331168239689)) * f1( 0.01817514933828936)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.6671331168239689) - present_state_Q (-0.6671331168239689)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.959835635273436 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5473989169632129) - present_state_Q ( -0.653379424685284)) * f1( 0.018132184266900147)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.5473989169632129) - present_state_Q (-0.653379424685284)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -33.99604234462247 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5990956728223341) - present_state_Q ( -0.6377650230506482)) * f1( 0.01771968691797441)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.5990956728223341) - present_state_Q (-0.6377650230506482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.04115873187026 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6182952582292803) - present_state_Q ( -0.729048519970295)) * f1( 0.022177104807496052)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.6182952582292803) - present_state_Q (-0.729048519970295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.103854249963454 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6009994160076985) - present_state_Q ( -1.078105617800181)) * f1( 0.03135890554460332)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.6009994160076985) - present_state_Q (-1.078105617800181)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.172892323066016 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6770973048410737) - present_state_Q ( -1.2440234655729727)) * f1( 0.034806919435668306)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.6770973048410737) - present_state_Q (-1.2440234655729727)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.21126759411653 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38098448394647716) - present_state_Q ( -0.38098448394647716)) * f1( 0.018567472065004927)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.38098448394647716) - present_state_Q (-0.38098448394647716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.23413507779809 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5542710999700807) - present_state_Q ( -0.5550407846346856)) * f1( 0.011148733924676323)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.5542710999700807) - present_state_Q (-0.5550407846346856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.26789049004928 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42443317526131247) - present_state_Q ( -0.7268529724545365)) * f1( 0.01660661391681629)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.42443317526131247) - present_state_Q (-0.7268529724545365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.330572947048786 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.545319885594281) - present_state_Q ( -1.1209700124348736)) * f1( 0.03142850755799015)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.545319885594281) - present_state_Q (-1.1209700124348736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.36714953240381 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2712183960364427) - present_state_Q ( -0.5602802938141419)) * f1( 0.018695651297802224)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2712183960364427) - present_state_Q (-0.5602802938141419)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.38271309672335 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4412934668331137) - present_state_Q ( -0.4412934668331137)) * f1( 0.007900200106032833)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.4412934668331137) - present_state_Q (-0.4412934668331137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.43033133781964 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5467726417061007) - present_state_Q ( -1.013501585953126)) * f1( 0.024880792092510512)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.5467726417061007) - present_state_Q (-1.013501585953126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.46665183271439 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7000641933307987) - present_state_Q ( -0.7000641933307987)) * f1( 0.01865716070717829)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.7000641933307987) - present_state_Q (-0.7000641933307987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.509818402758526 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6010913928153989) - present_state_Q ( -0.7548179130759631)) * f1( 0.022247746599646595)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.6010913928153989) - present_state_Q (-0.7548179130759631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.56529955263315 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4828119790714185) - present_state_Q ( -0.9706035191037603)) * f1( 0.028934020648516662)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.4828119790714185) - present_state_Q (-0.9706035191037603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.61889489263759 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6213928097201123) - present_state_Q ( -1.1036899099982647)) * f1( 0.028125431080975365)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.6213928097201123) - present_state_Q (-1.1036899099982647)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.65337859805211 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6608957804348968) - present_state_Q ( -0.6608957804348968)) * f1( 0.017681618208111014)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.6608957804348968) - present_state_Q (-0.6608957804348968)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.689372526330345 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.599717703085177) - present_state_Q ( -0.6862136280849185)) * f1( 0.01848578600899195)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.599717703085177) - present_state_Q (-0.6862136280849185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.73622441850142 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8948935538733119) - present_state_Q ( -0.8948935538733119)) * f1( 0.024285689553957854)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.8948935538733119) - present_state_Q (-0.8948935538733119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.80894739838389 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9132988965540636) - present_state_Q ( -1.4041165772825162)) * f1( 0.03871416368891888)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.9132988965540636) - present_state_Q (-1.4041165772825162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.863484239905866 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5882685583190846) - present_state_Q ( -1.0107431424534192)) * f1( 0.028485516945632044)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.5882685583190846) - present_state_Q (-1.0107431424534192)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.898930201438915 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013900924542196) - present_state_Q ( -0.5031615654905378)) * f1( 0.018088727629381695)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.013900924542196) - present_state_Q (-0.5031615654905378)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.92729763651636 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49101225659518727) - present_state_Q ( -0.49101225659518727)) * f1( 0.014432337342651567)
w2 ( -33.11157571818656 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.49101225659518727) - present_state_Q (-0.49101225659518727)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -34.953772520935956 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.586280808416433) - present_state_Q ( -7.416170260320456)) * f1( 0.020781179436253093)
w2 ( -33.36637246348978 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.586280808416433) - present_state_Q (-7.416170260320456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -34.98229027835159 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.926488643369489) - present_state_Q ( -7.398396505171295)) * f1( 0.021295207211067504)
w2 ( -33.6342050995659 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -6.926488643369489) - present_state_Q (-7.398396505171295)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -34.99880007160965 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.859995072497868) - present_state_Q ( -7.257136083776956)) * f1( 0.013089790228257743)
w2 ( -33.88645961197848 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -6.859995072497868) - present_state_Q (-7.257136083776956)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.01972512676491 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-6.896396500518601) - present_state_Q ( -7.386916711563126)) * f1( 0.0167580286587153)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -6.896396500518601) - present_state_Q (-7.386916711563126)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.02688308837072 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07586306972627534) - present_state_Q ( -0.07586306972627534)) * f1( 0.003932499061258926)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.07586306972627534) - present_state_Q (-0.07586306972627534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.0585289687033 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2271399374907095) - present_state_Q ( -0.7184824192804596)) * f1( 0.018006624676909277)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.2271399374907095) - present_state_Q (-0.7184824192804596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.06263522180599 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15054916406485017) - present_state_Q ( -0.15054916406485017)) * f1( 0.0022642883419147757)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.15054916406485017) - present_state_Q (-0.15054916406485017)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.06777002156932 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13182165898342654) - present_state_Q ( -0.13182165898342654)) * f1( 0.0028288249694294514)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.13182165898342654) - present_state_Q (-0.13182165898342654)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.07451190776467 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0856765338850634) - present_state_Q ( -0.0856765338850634)) * f1( 0.0037057102814015063)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.0856765338850634) - present_state_Q (-0.0856765338850634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.080823001890266 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14899902324974634) - present_state_Q ( -0.14899902324974634)) * f1( 0.0034798238760031896)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14899902324974634) - present_state_Q (-0.14899902324974634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.08388619556609 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14564122967570603) - present_state_Q ( -0.14564122967570603)) * f1( 0.0016887085533641405)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14564122967570603) - present_state_Q (-0.14564122967570603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.090072150005696 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18062563841593868) - present_state_Q ( -0.18062563841593868)) * f1( 0.003416185639013085)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.18062563841593868) - present_state_Q (-0.18062563841593868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.096652398370566 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1607646071194293) - present_state_Q ( -0.1607646071194293)) * f1( 0.003630350301945483)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.1607646071194293) - present_state_Q (-0.1607646071194293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.10279541147366 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16580803798181007) - present_state_Q ( -0.16580803798181007)) * f1( 0.0033899747242620354)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.16580803798181007) - present_state_Q (-0.16580803798181007)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.10994437619855 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14262016593621846) - present_state_Q ( -0.14262016593621846)) * f1( 0.003940563293080882)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14262016593621846) - present_state_Q (-0.14262016593621846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.13545358597565 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10574770589934691) - present_state_Q ( -0.5883013554315161)) * f1( 0.014417995828026966)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.10574770589934691) - present_state_Q (-0.5883013554315161)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.141499572921795 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1507014641077037) - present_state_Q ( -0.1507014641077037)) * f1( 0.003510782184444078)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.1507014641077037) - present_state_Q (-0.1507014641077037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.14485885117098 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14661059607902552) - present_state_Q ( -0.14661059607902552)) * f1( 0.001950247901243411)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.14661059607902552) - present_state_Q (-0.14661059607902552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.175390721666105 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17391034434730837) - present_state_Q ( -0.6707826386791703)) * f1( 0.01827879624054347)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.17391034434730837) - present_state_Q (-0.6707826386791703)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.18284825062649 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17954431129176526) - present_state_Q ( -0.17954431129176526)) * f1( 0.0043369734327454714)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.17954431129176526) - present_state_Q (-0.17954431129176526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.21918977122245 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.130360667079366) - present_state_Q ( -0.8324539143833438)) * f1( 0.021975340102030165)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.130360667079366) - present_state_Q (-0.8324539143833438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.223928383049945 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13211962225731388) - present_state_Q ( -0.13211962225731388)) * f1( 0.002902778059995675)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.13211962225731388) - present_state_Q (-0.13211962225731388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.249812953855916 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15342814553074144) - present_state_Q ( -0.5566215264886409)) * f1( 0.016277524296692783)
w2 ( -34.13619131469139 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.15342814553074144) - present_state_Q (-0.5566215264886409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.26957300554967 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7951184254416942) - present_state_Q ( -7.622356688379972)) * f1( 0.022201143807641585)
w2 ( -34.31420062686878 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.7951184254416942) - present_state_Q (-7.622356688379972)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.27383309901048 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.049752177535334) - present_state_Q ( -7.049752177535334)) * f1( 0.004638090283187367)
w2 ( -34.497900951817584 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -7.049752177535334) - present_state_Q (-7.049752177535334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.29257801568401 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0141405199961726) - present_state_Q ( -7.792190604127718)) * f1( 0.023912331735314322)
w2 ( -34.65468128491946 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -1.0141405199961726) - present_state_Q (-7.792190604127718)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.316788178473715 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.14029230800342) - present_state_Q ( -7.7302096391073425)) * f1( 0.028437002435428887)
w2 ( -34.82495354089776 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -7.14029230800342) - present_state_Q (-7.7302096391073425)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.3220605441337 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.3797851842504) - present_state_Q ( -7.3797851842504)) * f1( 0.00593201355045498)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -7.3797851842504) - present_state_Q (-7.3797851842504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.340671870609775 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4484376785121697) - present_state_Q ( -0.4484376785121697)) * f1( 0.013094872883111595)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.4484376785121697) - present_state_Q (-0.4484376785121697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.38598056995286 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42642684144376675) - present_state_Q ( -0.919513488586062)) * f1( 0.03297719139289889)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.42642684144376675) - present_state_Q (-0.919513488586062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.403085742259364 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4890498210163535) - present_state_Q ( -0.4890498210163535)) * f1( 0.0120661781135631)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.4890498210163535) - present_state_Q (-0.4890498210163535)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.455212824785114 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9700315179328078) - present_state_Q ( -1.468872184461861)) * f1( 0.039357808026279216)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.9700315179328078) - present_state_Q (-1.468872184461861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.48634473971108 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6607828744625615) - present_state_Q ( -0.7679003988378792)) * f1( 0.02237379581352554)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.6607828744625615) - present_state_Q (-0.7679003988378792)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.514917219973654 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.513921250095997) - present_state_Q ( -0.7114577606391911)) * f1( 0.020472950463647534)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.513921250095997) - present_state_Q (-0.7114577606391911)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.56027165728992 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6913378700866211) - present_state_Q ( -1.2575430135930372)) * f1( 0.033776353472648575)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.6913378700866211) - present_state_Q (-1.2575430135930372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.58516238198886 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5841155134138455) - present_state_Q ( -0.6701568956212305)) * f1( 0.017773337345178324)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.5841155134138455) - present_state_Q (-0.6701568956212305)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.63507850293118 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7281894192573174) - present_state_Q ( -1.3584400077539303)) * f1( 0.03744461269124984)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.7281894192573174) - present_state_Q (-1.3584400077539303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.688927017393965 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8283870154548377) - present_state_Q ( -1.482922904509769)) * f1( 0.04074435015304986)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.8283870154548377) - present_state_Q (-1.482922904509769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.71359188394724 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5951303520533229) - present_state_Q ( -0.6652035508120224)) * f1( 0.017604451042838667)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.5951303520533229) - present_state_Q (-0.6652035508120224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.73860887297904 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6126566022823476) - present_state_Q ( -0.6784038367003038)) * f1( 0.017870378461240146)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.6126566022823476) - present_state_Q (-0.6784038367003038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.78602838394287 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6513720986231654) - present_state_Q ( -1.2725571069820234)) * f1( 0.03536432246150164)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.6513720986231654) - present_state_Q (-1.2725571069820234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.80411366400499 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3482359358393233) - present_state_Q ( -0.4227099323344904)) * f1( 0.012710700609041583)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.3482359358393233) - present_state_Q (-0.4227099323344904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.83535167833904 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.445595551616087) - present_state_Q ( -0.9886125708464796)) * f1( 0.02284779473087462)
w2 ( -35.00271327172569 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.445595551616087) - present_state_Q (-0.9886125708464796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.84654938660259 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5002478517507916) - present_state_Q ( -0.3248053374422715)) * f1( 0.0078079084122012)
w2 ( -35.28954317987511 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.5002478517507916) - present_state_Q (-0.3248053374422715)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.87070058356472 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6544007659336308) - present_state_Q ( -0.6544007659336308)) * f1( 0.01721726253141039)
w2 ( -35.57008948528307 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.6544007659336308) - present_state_Q (-0.6544007659336308)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.88267107428421 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.286120311994954) - present_state_Q ( -7.792115192864846)) * f1( 0.01802986890364721)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -7.286120311994954) - present_state_Q (-7.792115192864846)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -35.89344465281638 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13108286590848384) - present_state_Q ( -0.13108286590848384)) * f1( 0.008502368886679641)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.13108286590848384) - present_state_Q (-0.13108286590848384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.89787675380218 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15623223701411657) - present_state_Q ( -0.15623223701411657)) * f1( 0.003504015950189031)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15623223701411657) - present_state_Q (-0.15623223701411657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.9062882179732 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013246180797102979) - present_state_Q ( -0.013246180797102979)) * f1( 0.006583120901330246)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.013246180797102979) - present_state_Q (-0.013246180797102979)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.91470671666237 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062388564833649164) - present_state_Q ( -0.062388564833649164)) * f1( 0.006611511838769089)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.062388564833649164) - present_state_Q (-0.062388564833649164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.91865864660098 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.144024101846048) - present_state_Q ( -0.144024101846048)) * f1( 0.0031216813849369563)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.144024101846048) - present_state_Q (-0.144024101846048)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.92663422528895 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2697892963905682) - present_state_Q ( -0.2697892963905682)) * f1( 0.0068560455191582475)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.2697892963905682) - present_state_Q (-0.2697892963905682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.94166947865534 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20794302226717723) - present_state_Q ( -0.6969136569038094)) * f1( 0.013424806991703658)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.20794302226717723) - present_state_Q (-0.6969136569038094)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.964636544513105 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2224550940548326) - present_state_Q ( -0.7216657448533021)) * f1( 0.020549786311365618)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.2224550940548326) - present_state_Q (-0.7216657448533021)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -35.991394152025265 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27370619913627087) - present_state_Q ( -0.9002304822716566)) * f1( 0.02431876523150479)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.27370619913627087) - present_state_Q (-0.9002304822716566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.01553618018624 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13590232410583314) - present_state_Q ( -0.8184948805080602)) * f1( 0.02180690237980255)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.13590232410583314) - present_state_Q (-0.8184948805080602)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.02019254148148 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16738577940337934) - present_state_Q ( -0.16738577940337934)) * f1( 0.00397128418769109)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.16738577940337934) - present_state_Q (-0.16738577940337934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.02317907135789 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.129922846179056) - present_state_Q ( -0.129922846179056)) * f1( 0.002539826757616339)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.129922846179056) - present_state_Q (-0.129922846179056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.02838030947759 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18334044886458967) - present_state_Q ( -0.18334044886458967)) * f1( 0.0044414340799765585)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.18334044886458967) - present_state_Q (-0.18334044886458967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.03269256644639 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28148252984511124) - present_state_Q ( -0.3109381024106196)) * f1( 0.0037197286139696007)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.28148252984511124) - present_state_Q (-0.3109381024106196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.042859895286746 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10538450053557018) - present_state_Q ( -0.10538450053557018)) * f1( 0.008630365832149954)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.10538450053557018) - present_state_Q (-0.10538450053557018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.049559667342024 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04498104741348775) - present_state_Q ( -0.04498104741348775)) * f1( 0.005660866474808874)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.04498104741348775) - present_state_Q (-0.04498104741348775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.060812306588254 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1102535056701223) - present_state_Q ( -0.1102535056701223)) * f1( 0.00955516750212756)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.1102535056701223) - present_state_Q (-0.1102535056701223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.07042958915404 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20354824196714713) - present_state_Q ( -0.20354824196714713)) * f1( 0.008225150509060902)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.20354824196714713) - present_state_Q (-0.20354824196714713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.09812978330326 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12487904322019747) - present_state_Q ( -0.6315676127574043)) * f1( 0.02460786083049547)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12487904322019747) - present_state_Q (-0.6315676127574043)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.1142014805033 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1346808925378318) - present_state_Q ( -0.6419023841466727)) * f1( 0.014289395321834963)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.1346808925378318) - present_state_Q (-0.6419023841466727)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.11866030565014 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18014863130177164) - present_state_Q ( -0.18014863130177164)) * f1( 0.0038065400751454417)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.18014863130177164) - present_state_Q (-0.18014863130177164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.1266502768739 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15603864249520003) - present_state_Q ( -0.15603864249520003)) * f1( 0.006808499496553272)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.15603864249520003) - present_state_Q (-0.15603864249520003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.15183973071733 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18005995062481983) - present_state_Q ( -0.8739554927469732)) * f1( 0.02285841088803003)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.18005995062481983) - present_state_Q (-0.8739554927469732)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.15621729088829 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15932074482208392) - present_state_Q ( -0.15932074482208392)) * f1( 0.00373119244113467)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.15932074482208392) - present_state_Q (-0.15932074482208392)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.174859025281236 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08054724679803214) - present_state_Q ( -0.5738957742954063)) * f1( 0.01648269404429474)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08054724679803214) - present_state_Q (-0.5738957742954063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.19668045180872 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22137789189743495) - present_state_Q ( -0.732412620555049)) * f1( 0.0195437064584196)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.22137789189743495) - present_state_Q (-0.732412620555049)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.2036308513765 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20814243744253597) - present_state_Q ( -0.20814243744253597)) * f1( 0.005946410058110482)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.20814243744253597) - present_state_Q (-0.20814243744253597)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.22290578440561 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18047275246758326) - present_state_Q ( -0.6924182627620789)) * f1( 0.01720768253060403)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.18047275246758326) - present_state_Q (-0.6924182627620789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.243076858252195 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12811612174042225) - present_state_Q ( -0.6246660244470491)) * f1( 0.017907765444523962)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12811612174042225) - present_state_Q (-0.6246660244470491)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.24885085489722 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.207493392740564) - present_state_Q ( -0.207493392740564)) * f1( 0.00493969239754528)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.207493392740564) - present_state_Q (-0.207493392740564)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.25408651403765 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07395296703453225) - present_state_Q ( -0.07395296703453225)) * f1( 0.004433555170565291)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07395296703453225) - present_state_Q (-0.07395296703453225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.27006366895973 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18555356023080463) - present_state_Q ( -0.5784222579333413)) * f1( 0.014119261560821161)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.18555356023080463) - present_state_Q (-0.5784222579333413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.28966884675084 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1797252077063878) - present_state_Q ( -0.6651118020440167)) * f1( 0.01900634914195576)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1797252077063878) - present_state_Q (-0.6651118020440167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.29388040635828 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14484658610717607) - present_state_Q ( -0.14484658610717607)) * f1( 0.0038881276375241597)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14484658610717607) - present_state_Q (-0.14484658610717607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.300091008065706 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23424775883430246) - present_state_Q ( -0.23424775883430246)) * f1( 0.005776560222406122)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.23424775883430246) - present_state_Q (-0.23424775883430246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.319486761216574 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3069859469681115) - present_state_Q ( -0.8127367108527672)) * f1( 0.019052486682397164)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.3069859469681115) - present_state_Q (-0.8127367108527672)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.33028445067908 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22571274062002195) - present_state_Q ( -0.29039493294746194)) * f1( 0.01009659851211864)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.22571274062002195) - present_state_Q (-0.29039493294746194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.33685059222301 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22867945156927386) - present_state_Q ( -0.41948210784321766)) * f1( 0.00621464565575985)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.22867945156927386) - present_state_Q (-0.41948210784321766)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.35165310865009 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17763874072139452) - present_state_Q ( -0.6327764326528668)) * f1( 0.014305826138751903)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.17763874072139452) - present_state_Q (-0.6327764326528668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.36161507305112 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24877984898737673) - present_state_Q ( -0.24877984898737673)) * f1( 0.009277036089918452)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.24877984898737673) - present_state_Q (-0.24877984898737673)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.36419540910385 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16044372025782824) - present_state_Q ( -0.16044372025782824)) * f1( 0.0026052710040521164)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.16044372025782824) - present_state_Q (-0.16044372025782824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.36654231421687 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12448472581453021) - present_state_Q ( -0.12448472581453021)) * f1( 0.002361866719334081)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.12448472581453021) - present_state_Q (-0.12448472581453021)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.371590284133255 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21129574592328554) - present_state_Q ( -0.21129574592328554)) * f1( 0.005120411682617228)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.21129574592328554) - present_state_Q (-0.21129574592328554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.374930339508055 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1524675845304617) - present_state_Q ( -0.1524675845304617)) * f1( 0.003369889377410259)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1524675845304617) - present_state_Q (-0.1524675845304617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.37757557042763 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14861717217278864) - present_state_Q ( -0.14861717217278864)) * f1( 0.002667925826073885)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.14861717217278864) - present_state_Q (-0.14861717217278864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.379845795583066 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1421578182090222) - present_state_Q ( -0.1421578182090222)) * f1( 0.002288360945468089)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1421578182090222) - present_state_Q (-0.1421578182090222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.38577679328298 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039751687548688394) - present_state_Q ( -0.039751687548688394)) * f1( 0.005923348759108195)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.039751687548688394) - present_state_Q (-0.039751687548688394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.390145553749456 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18776616589217815) - present_state_Q ( -0.18776616589217815)) * f1( 0.004421956526988611)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.18776616589217815) - present_state_Q (-0.18776616589217815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.393659451465936 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26946721784673894) - present_state_Q ( -0.26946721784673894)) * f1( 0.0035833541471763392)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.26946721784673894) - present_state_Q (-0.26946721784673894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.40107447626418 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038982872396479) - present_state_Q ( -0.038982872396479)) * f1( 0.0074049502618429176)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.038982872396479) - present_state_Q (-0.038982872396479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.40834987910387 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06454787546506242) - present_state_Q ( -0.06454787546506242)) * f1( 0.007282250606180987)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.06454787546506242) - present_state_Q (-0.06454787546506242)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.42375652857996 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2066440194076044) - present_state_Q ( -0.7196631579638831)) * f1( 0.016478244608964538)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.2066440194076044) - present_state_Q (-0.7196631579638831)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.435466489853404 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13139094073001206) - present_state_Q ( -0.6213282807453353)) * f1( 0.012403962299695917)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.13139094073001206) - present_state_Q (-0.6213282807453353)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.44014122684102 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2016349980757574) - present_state_Q ( -0.2016349980757574)) * f1( 0.004737644269607325)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.2016349980757574) - present_state_Q (-0.2016349980757574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.44244232508961 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1687087887382824) - present_state_Q ( -0.1687087887382824)) * f1( 0.002325081038085797)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1687087887382824) - present_state_Q (-0.1687087887382824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.44510219903077 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13143013001477866) - present_state_Q ( -0.13143013001477866)) * f1( 0.002678515712687387)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.13143013001477866) - present_state_Q (-0.13143013001477866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.45084726548164 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23677354449539445) - present_state_Q ( -0.23677354449539445)) * f1( 0.005841097877843104)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.23677354449539445) - present_state_Q (-0.23677354449539445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.4661793494824 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21446499772185762) - present_state_Q ( -0.7022353820751044)) * f1( 0.016366616451861014)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.21446499772185762) - present_state_Q (-0.7022353820751044)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.4710125891026 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2216933837730343) - present_state_Q ( -0.2216933837730343)) * f1( 0.005408941117552499)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2216933837730343) - present_state_Q (-0.2216933837730343)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.47407833745275 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1107677109212348) - present_state_Q ( -0.1107677109212348)) * f1( 0.0033930104757308233)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1107677109212348) - present_state_Q (-0.1107677109212348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.48937985013256 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0953080779179182) - present_state_Q ( -0.5297193231113332)) * f1( 0.017761510354183132)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0953080779179182) - present_state_Q (-0.5297193231113332)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.504556745802525 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15513136571092165) - present_state_Q ( -0.8848761102444234)) * f1( 0.018361051833859523)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15513136571092165) - present_state_Q (-0.8848761102444234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.52564628878747 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2615131528878753) - present_state_Q ( -0.9751033866423415)) * f1( 0.025762246733516486)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2615131528878753) - present_state_Q (-0.9751033866423415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.52732003166695 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13969589780164127) - present_state_Q ( -0.13969589780164127)) * f1( 0.0018577644491021938)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.13969589780164127) - present_state_Q (-0.13969589780164127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.5399248875899 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2530947113309218) - present_state_Q ( -0.3422104535399589)) * f1( 0.01429402115116142)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2530947113309218) - present_state_Q (-0.3422104535399589)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.54623074432762 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038208661555001695) - present_state_Q ( -0.038208661555001695)) * f1( 0.006928915428547842)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.038208661555001695) - present_state_Q (-0.038208661555001695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.55249595986234 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021144404535709822) - present_state_Q ( -0.021144404535709822)) * f1( 0.006872660807275129)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.021144404535709822) - present_state_Q (-0.021144404535709822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.558740256460325 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0688834304269494) - present_state_Q ( -0.0688834304269494)) * f1( 0.006882149804127541)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0688834304269494) - present_state_Q (-0.0688834304269494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.57644347663456 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1114076936843095) - present_state_Q ( -0.84205770601291)) * f1( 0.02131825073640072)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1114076936843095) - present_state_Q (-0.84205770601291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.5915449344665 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16045176572619568) - present_state_Q ( -0.5844746347896764)) * f1( 0.017628003784686687)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.16045176572619568) - present_state_Q (-0.5844746347896764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.600734213399306 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.231234262155635) - present_state_Q ( -0.231234262155635)) * f1( 0.010293733038397495)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.231234262155635) - present_state_Q (-0.231234262155635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.609421474260074 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3543423941137885) - present_state_Q ( -0.3543423941137885)) * f1( 0.00985367559923457)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3543423941137885) - present_state_Q (-0.3543423941137885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.616930753000474 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36480047174720664) - present_state_Q ( -0.36480047174720664)) * f1( 0.00852663210139369)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.36480047174720664) - present_state_Q (-0.36480047174720664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.626394265317465 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34784062374674296) - present_state_Q ( -0.34784062374674296)) * f1( 0.010727032392025193)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.34784062374674296) - present_state_Q (-0.34784062374674296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.64850651274338 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3011184372965932) - present_state_Q ( -1.0315787664821374)) * f1( 0.02718594530416461)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3011184372965932) - present_state_Q (-1.0315787664821374)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.663255029532586 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.184330951884972) - present_state_Q ( -0.6132024899931081)) * f1( 0.017269110896753877)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.184330951884972) - present_state_Q (-0.6132024899931081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.67630409939909 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1476838115755045) - present_state_Q ( -0.6528990978398966)) * f1( 0.01535719164140676)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1476838115755045) - present_state_Q (-0.6528990978398966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.696772592676794 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27287594250148445) - present_state_Q ( -0.4574235137424201)) * f1( 0.023513391526356023)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.27287594250148445) - present_state_Q (-0.4574235137424201)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.70436454968912 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0743809759887427) - present_state_Q ( -0.0743809759887427)) * f1( 0.008372038810419256)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0743809759887427) - present_state_Q (-0.0743809759887427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.706316750933716 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2868467064319482) - present_state_Q ( -0.31920442934440124)) * f1( 0.0022072107376658323)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2868467064319482) - present_state_Q (-0.31920442934440124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.71417633649453 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10852181789158855) - present_state_Q ( -0.10852181789158855)) * f1( 0.008696634126774573)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10852181789158855) - present_state_Q (-0.10852181789158855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.726997000321276 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12350057688617738) - present_state_Q ( -0.6351993565483822)) * f1( 0.015061298263352141)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.12350057688617738) - present_state_Q (-0.6351993565483822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.730519896053686 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18539996431489264) - present_state_Q ( -0.18539996431489264)) * f1( 0.003928158981612019)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.18539996431489264) - present_state_Q (-0.18539996431489264)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.733375230178225 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19391755041682227) - present_state_Q ( -0.19391755041682227)) * f1( 0.003186526850881638)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.19391755041682227) - present_state_Q (-0.19391755041682227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.73436276735264 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26656159003778707) - present_state_Q ( -0.26656159003778707)) * f1( 0.001110182718015722)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.26656159003778707) - present_state_Q (-0.26656159003778707)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.74094412682284 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07307308822906201) - present_state_Q ( -0.07307308822906201)) * f1( 0.007256659328674866)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07307308822906201) - present_state_Q (-0.07307308822906201)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.7434674645067 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17958888789495478) - present_state_Q ( -0.17958888789495478)) * f1( 0.00281197497051385)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.17958888789495478) - present_state_Q (-0.17958888789495478)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.74545281641066 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1531524257220291) - present_state_Q ( -0.1531524257220291)) * f1( 0.0022065998871837527)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1531524257220291) - present_state_Q (-0.1531524257220291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.74723003138178 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1608286228912454) - present_state_Q ( -0.1608286228912454)) * f1( 0.001976786005390456)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1608286228912454) - present_state_Q (-0.1608286228912454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.75077677121403 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20518259629754432) - present_state_Q ( -0.20518259629754432)) * f1( 0.003962612823201356)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.20518259629754432) - present_state_Q (-0.20518259629754432)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.75381900742499 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15092209816182137) - present_state_Q ( -0.15092209816182137)) * f1( 0.0033805093891282833)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15092209816182137) - present_state_Q (-0.15092209816182137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.75803814409112 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2134476698785659) - present_state_Q ( -0.2134476698785659)) * f1( 0.0047177723847347825)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2134476698785659) - present_state_Q (-0.2134476698785659)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.76121997977433 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16056746518095855) - present_state_Q ( -0.16056746518095855)) * f1( 0.003539045033066726)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.16056746518095855) - present_state_Q (-0.16056746518095855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.78000456600458 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15574050787896035) - present_state_Q ( -0.8845999185434379)) * f1( 0.02272471941661298)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15574050787896035) - present_state_Q (-0.8845999185434379)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.78472335798937 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21909576026065908) - present_state_Q ( -0.21909576026065908)) * f1( 0.0052794799984842225)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.21909576026065908) - present_state_Q (-0.21909576026065908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.78733188142056 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13092980693458667) - present_state_Q ( -0.13092980693458667)) * f1( 0.0028927873297005563)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.13092980693458667) - present_state_Q (-0.13092980693458667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.79134871275474 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18847479050910573) - present_state_Q ( -0.18847479050910573)) * f1( 0.004480297919397113)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.18847479050910573) - present_state_Q (-0.18847479050910573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.79578895407982 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2095282229690515) - present_state_Q ( -0.2095282229690515)) * f1( 0.004963050518837725)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2095282229690515) - present_state_Q (-0.2095282229690515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.79871636049189 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15175898489392142) - present_state_Q ( -0.15175898489392142)) * f1( 0.0032531836776904413)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15175898489392142) - present_state_Q (-0.15175898489392142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.81414379774924 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1237313842902509) - present_state_Q ( -0.6441119666969196)) * f1( 0.018142597330168465)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1237313842902509) - present_state_Q (-0.6441119666969196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.82700407104259 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15940408133151013) - present_state_Q ( -0.6555105574900719)) * f1( 0.015137565060900861)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15940408133151013) - present_state_Q (-0.6555105574900719)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.83044383002031 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24088269252568953) - present_state_Q ( -0.24088269252568953)) * f1( 0.0038569333547372993)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.24088269252568953) - present_state_Q (-0.24088269252568953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.84458072014866 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22163518766669196) - present_state_Q ( -0.7309297028684023)) * f1( 0.016776891168774503)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.22163518766669196) - present_state_Q (-0.7309297028684023)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.852809597204356 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22451759143816993) - present_state_Q ( -0.22451759143816993)) * f1( 0.009211663293597228)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.22451759143816993) - present_state_Q (-0.22451759143816993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.86437356385947 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4799444070834533) - present_state_Q ( -0.4799444070834533)) * f1( 0.013286994149433946)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.4799444070834533) - present_state_Q (-0.4799444070834533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.88512794377303 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5299711050249437) - present_state_Q ( -1.0223041974574536)) * f1( 0.02541602013714027)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.5299711050249437) - present_state_Q (-1.0223041974574536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.89660923851737 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4764055857230008) - present_state_Q ( -0.4764055857230008)) * f1( 0.013187178309001084)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.4764055857230008) - present_state_Q (-0.4764055857230008)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.903534738963074 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3360176188406559) - present_state_Q ( -0.3360176188406559)) * f1( 0.007840701091753841)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3360176188406559) - present_state_Q (-0.3360176188406559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.92408972280331 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38484895538088615) - present_state_Q ( -0.9574159551243654)) * f1( 0.025017502287699857)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.38484895538088615) - present_state_Q (-0.9574159551243654)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.93082760054649 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32499739489811974) - present_state_Q ( -0.32499739489811974)) * f1( 0.007619728058279004)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.32499739489811974) - present_state_Q (-0.32499739489811974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.937750193422325 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3341080121095825) - present_state_Q ( -0.3341080121095825)) * f1( 0.007835884610018587)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3341080121095825) - present_state_Q (-0.3341080121095825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.945091163018844 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35521595853647486) - present_state_Q ( -0.35521595853647486)) * f1( 0.008327364101099015)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.35521595853647486) - present_state_Q (-0.35521595853647486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.95439809090349 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3486147117125542) - present_state_Q ( -0.3486147117125542)) * f1( 0.010550374761082941)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3486147117125542) - present_state_Q (-0.3486147117125542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.963047309479514 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25057834483878094) - present_state_Q ( -0.562535789443384)) * f1( 0.010059927758572777)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.25057834483878094) - present_state_Q (-0.562535789443384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.97047327586909 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19263551415469693) - present_state_Q ( -0.19263551415469693)) * f1( 0.008286243796153945)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.19263551415469693) - present_state_Q (-0.19263551415469693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.98860105765608 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22297050041849956) - present_state_Q ( -0.8678736641023856)) * f1( 0.02186811286274231)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.22297050041849956) - present_state_Q (-0.8678736641023856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -36.995680487030604 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3453806179468481) - present_state_Q ( -0.3453806179468481)) * f1( 0.008022625445510353)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3453806179468481) - present_state_Q (-0.3453806179468481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.00604466604194 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4154807316744451) - present_state_Q ( -0.4154807316744451)) * f1( 0.011829580320376034)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.4154807316744451) - present_state_Q (-0.4154807316744451)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.016265282130874 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36840511338213405) - present_state_Q ( -0.36840511338213405)) * f1( 0.011609576706071321)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.36840511338213405) - present_state_Q (-0.36840511338213405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.02310826832131 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3304352929744595) - present_state_Q ( -0.3304352929744595)) * f1( 0.007742878460196257)
w2 ( -35.702874596294855 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3304352929744595) - present_state_Q (-0.3304352929744595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.02591713525117 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.508755703194427) - present_state_Q ( -7.508755703194427)) * f1( 0.011815403937851375)
w2 ( -35.75042044313408 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -7.508755703194427) - present_state_Q (-7.508755703194427)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.02875769658884 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.445945918756991) - present_state_Q ( -7.574193436523484)) * f1( 0.012320410419685296)
w2 ( -35.79653191573785 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -7.445945918756991) - present_state_Q (-7.574193436523484)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.032817003748406 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.435435819828222) - present_state_Q ( -7.958386883499261)) * f1( 0.021138600693898352)
w2 ( -35.83493849920425 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -7.435435819828222) - present_state_Q (-7.958386883499261)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.03718590806999 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.634316845632854) - present_state_Q ( -8.18096464274245)) * f1( 0.025435513559255052)
w2 ( -35.869291289537394 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -7.634316845632854) - present_state_Q (-8.18096464274245)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.04155581173842 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.672484076026197) - present_state_Q ( -8.403999945292519)) * f1( 0.02916339167517698)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -7.672484076026197) - present_state_Q (-8.403999945292519)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.06484772734931 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5936737296341984) - present_state_Q ( -1.2682775036968885)) * f1( 0.03321369205636668)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.5936737296341984) - present_state_Q (-1.2682775036968885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.080598586500905 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8682637093689348) - present_state_Q ( -0.8682637093689348)) * f1( 0.021169889627892693)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.8682637093689348) - present_state_Q (-0.8682637093689348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.09470514069915 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7548483686575277) - present_state_Q ( -0.7548483686575277)) * f1( 0.018703273179616123)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.7548483686575277) - present_state_Q (-0.7548483686575277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.11940552433482 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7205471404290028) - present_state_Q ( -1.3811026168942018)) * f1( 0.035732369099174946)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.7205471404290028) - present_state_Q (-1.3811026168942018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.14451202000354 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8668616959379696) - present_state_Q ( -1.39152791819128)) * f1( 0.0362977775038052)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.8668616959379696) - present_state_Q (-1.39152791819128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.16089794818028 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8317129992596142) - present_state_Q ( -0.8317129992596142)) * f1( 0.021926507760522917)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.8317129992596142) - present_state_Q (-0.8317129992596142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.17750381438923 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6914001775105773) - present_state_Q ( -0.7932087261142442)) * f1( 0.02214828220641156)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6914001775105773) - present_state_Q (-0.7932087261142442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.191222633957786 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7303252325438423) - present_state_Q ( -0.7303252325438423)) * f1( 0.018136121233999016)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.7303252325438423) - present_state_Q (-0.7303252325438423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.208456742052284 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7346978025341072) - present_state_Q ( -0.8380097385259584)) * f1( 0.023110958423776796)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.7346978025341072) - present_state_Q (-0.8380097385259584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.22387696700425 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6165998530400068) - present_state_Q ( -0.7454777666237604)) * f1( 0.020457094043704188)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6165998530400068) - present_state_Q (-0.7454777666237604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.23920113883445 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.674178910668021) - present_state_Q ( -0.7579301603548562)) * f1( 0.020347737276440073)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.674178910668021) - present_state_Q (-0.7579301603548562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.255749262955284 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6470575074436823) - present_state_Q ( -0.8111716700542707)) * f1( 0.02213739804961123)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6470575074436823) - present_state_Q (-0.8111716700542707)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.279324217064904 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6897412908752311) - present_state_Q ( -1.3160602693197283)) * f1( 0.033801305593425666)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6897412908752311) - present_state_Q (-1.3160602693197283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.303663547334935 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6715962611125664) - present_state_Q ( -1.2950514297705342)) * f1( 0.03480147782462102)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6715962611125664) - present_state_Q (-1.2950514297705342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.32769002153648 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.718185778077887) - present_state_Q ( -1.350240860499527)) * f1( 0.03460415975244521)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.718185778077887) - present_state_Q (-1.350240860499527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.341579561656644 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6550948482753334) - present_state_Q ( -0.7349796538108933)) * f1( 0.01839141918902986)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6550948482753334) - present_state_Q (-0.7349796538108933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.360790151761414 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26404484128264405) - present_state_Q ( -0.26404484128264405)) * f1( 0.007071067811865476)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.26404484128264405) - present_state_Q (-0.26404484128264405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.379836118263306 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2832277462736591) - present_state_Q ( -0.49877606365888477)) * f1( 0.007071067811865476)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.2832277462736591) - present_state_Q (-0.49877606365888477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.43099779211969 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2375970373802623) - present_state_Q ( -0.6803016301056684)) * f1( 0.01912659191748863)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.2375970373802623) - present_state_Q (-0.6803016301056684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.48239219369755 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.281198159250658) - present_state_Q ( -0.9316712544168736)) * f1( 0.019392675096246276)
w2 ( -35.899259708280326 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.281198159250658) - present_state_Q (-0.9316712544168736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.528126236622526 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7625071748324407) - present_state_Q ( -8.20598859162356)) * f1( 0.026210505557716528)
w2 ( -36.248234609388355 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.7625071748324407) - present_state_Q (-8.20598859162356)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.562495747583306 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.5084847896460625) - present_state_Q ( -8.023975722670263)) * f1( 0.020857410510862366)
w2 ( -36.57780103320574 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -7.5084847896460625) - present_state_Q (-8.023975722670263)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.596283584968255 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.613127221198142) - present_state_Q ( -8.13736319465503)) * f1( 0.020633292371441368)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -7.613127221198142) - present_state_Q (-8.13736319465503)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -37.6389650324039 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.280823715254531) - present_state_Q ( -0.7322597929928375)) * f1( 0.019283420195057344)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.280823715254531) - present_state_Q (-0.7322597929928375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.69961998760282 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35056389909661423) - present_state_Q ( -1.099918799392193)) * f1( 0.027857788820883665)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.35056389909661423) - present_state_Q (-1.099918799392193)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.718470390941604 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18551208690169557) - present_state_Q ( -0.18551208690169557)) * f1( 0.008663883261331853)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.18551208690169557) - present_state_Q (-0.18551208690169557)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.74061196087985 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3994988299631409) - present_state_Q ( -0.5986279366554391)) * f1( 0.010363120679034783)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.3994988299631409) - present_state_Q (-0.5986279366554391)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.774982300285885 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2980897873732875) - present_state_Q ( -0.2980897873732875)) * f1( 0.015870949443358245)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.2980897873732875) - present_state_Q (-0.2980897873732875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.81395780476148 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17131965158533344) - present_state_Q ( -0.7021124244785151)) * f1( 0.019175275559135344)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.17131965158533344) - present_state_Q (-0.7021124244785151)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.85005339297715 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16419815113116656) - present_state_Q ( -0.6939840088607903)) * f1( 0.01775192935961535)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.16419815113116656) - present_state_Q (-0.6939840088607903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.897946323391494 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20652312240707724) - present_state_Q ( -0.9473276351165224)) * f1( 0.023846047867045545)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.20652312240707724) - present_state_Q (-0.9473276351165224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.90797913948991 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09080101540209395) - present_state_Q ( -0.09080101540209395)) * f1( 0.0047936986170451015)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.09080101540209395) - present_state_Q (-0.09080101540209395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.95035452711381 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27943382932915983) - present_state_Q ( -0.4013384552397593)) * f1( 0.02053319651018692)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.27943382932915983) - present_state_Q (-0.4013384552397593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -37.9714229633716 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15612179915734445) - present_state_Q ( -0.15612179915734445)) * f1( 0.010094894830755444)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.15612179915734445) - present_state_Q (-0.15612179915734445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.01951943553038 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20051779236989828) - present_state_Q ( -0.9599059500016976)) * f1( 0.02396311607012442)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.20051779236989828) - present_state_Q (-0.9599059500016976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.037392232394836 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02262421884043559) - present_state_Q ( -0.5364099367817177)) * f1( 0.008728337056712096)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.02262421884043559) - present_state_Q (-0.5364099367817177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.058263080525535 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16539888769069083) - present_state_Q ( -0.48870212821440084)) * f1( 0.010161701140244702)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.16539888769069083) - present_state_Q (-0.48870212821440084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.108486077097126 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24745984866695522) - present_state_Q ( -1.0016725389934824)) * f1( 0.025068918554659616)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.24745984866695522) - present_state_Q (-1.0016725389934824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.138561560599676 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19751343350475445) - present_state_Q ( -0.708884025757826)) * f1( 0.014799642141386293)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.19751343350475445) - present_state_Q (-0.708884025757826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.172334368891974 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06942284003574165) - present_state_Q ( -0.49682492962641733)) * f1( 0.01645767027680539)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.06942284003574165) - present_state_Q (-0.49682492962641733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.20888708133161 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17895518454403983) - present_state_Q ( -0.9285372598403224)) * f1( 0.018185198411275357)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.17895518454403983) - present_state_Q (-0.9285372598403224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.25001617382288 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23627403371328284) - present_state_Q ( -0.7572257342630582)) * f1( 0.02028331968025064)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.23627403371328284) - present_state_Q (-0.7572257342630582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.2669921797681 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06597815727430813) - present_state_Q ( -0.06597815727430813)) * f1( 0.008102519006819268)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.06597815727430813) - present_state_Q (-0.06597815727430813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.291018469818624 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21346390093736098) - present_state_Q ( -0.6841170727304815)) * f1( 0.0118076180771223)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.21346390093736098) - present_state_Q (-0.6841170727304815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.33175374106757 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27572680230014945) - present_state_Q ( -0.7787839808030051)) * f1( 0.020106566544185744)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.27572680230014945) - present_state_Q (-0.7787839808030051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.37908054687777 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21872581318984222) - present_state_Q ( -0.9307751515436764)) * f1( 0.023543338796741767)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.21872581318984222) - present_state_Q (-0.9307751515436764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.398748160727 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17234787056923467) - present_state_Q ( -0.5385638995133114)) * f1( 0.009598842812450641)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.17234787056923467) - present_state_Q (-0.5385638995133114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.44518124582264 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22156478126406656) - present_state_Q ( -0.7057085105350306)) * f1( 0.022842671225848158)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.22156478126406656) - present_state_Q (-0.7057085105350306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.46961205234687 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13261964016650457) - present_state_Q ( -0.6315817871174395)) * f1( 0.012542087245944822)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.13261964016650457) - present_state_Q (-0.6315817871174395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.475926427135136 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15928528440104547) - present_state_Q ( -0.15928528440104547)) * f1( 0.0033162852781226425)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15928528440104547) - present_state_Q (-0.15928528440104547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.52296779333863 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27174460832960107) - present_state_Q ( -1.040864202345937)) * f1( 0.025889333917070538)
w2 ( -36.90530899244653 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.27174460832960107) - present_state_Q (-1.040864202345937)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.54882535977427 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1191069557036872) - present_state_Q ( -8.325885745545863)) * f1( 0.02357140605797747)
w2 ( -37.12470673539015 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -1.1191069557036872) - present_state_Q (-8.325885745545863)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -38.57216148777585 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.621243402773867) - present_state_Q ( -8.164804495811032)) * f1( 0.01980796826594512)
w2 ( -37.360330376222606 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -7.621243402773867) - present_state_Q (-8.164804495811032)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -38.580594175057676 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.529347006889949) - present_state_Q ( -7.529347006889949)) * f1( 0.006796470955704017)
w2 ( -37.608479374041714 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -7.529347006889949) - present_state_Q (-7.529347006889949)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -38.62859145802365 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09042180099274164) - present_state_Q ( -0.8055452864894045)) * f1( 0.027468076795273272)
w2 ( -37.608479374041714 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.09042180099274164) - present_state_Q (-0.8055452864894045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.657611588393905 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49473852199373425) - present_state_Q ( -0.49473852199373425)) * f1( 0.01628050472401974)
w2 ( -37.608479374041714 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.49473852199373425) - present_state_Q (-0.49473852199373425)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.68929425036237 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15961706837717796) - present_state_Q ( -0.9321270803016688)) * f1( 0.018256502319420102)
w2 ( -37.608479374041714 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.15961706837717796) - present_state_Q (-0.9321270803016688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.708556805831236 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.80395047002518) - present_state_Q ( -8.348288593517278)) * f1( 0.0179982648304102)
w2 ( -37.82252840210487 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -7.80395047002518) - present_state_Q (-8.348288593517278)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -38.732237157763194 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.994039710309027) - present_state_Q ( -8.522399568737743)) * f1( 0.022451472063491087)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -7.994039710309027) - present_state_Q (-8.522399568737743)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -38.770612268256414 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4297243930917974) - present_state_Q ( -0.9678402154404042)) * f1( 0.024728712338001602)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.4297243930917974) - present_state_Q (-0.9678402154404042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.80701227680314 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19548822670032703) - present_state_Q ( -0.725975639802578)) * f1( 0.024555831373813137)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.19548822670032703) - present_state_Q (-0.725975639802578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.83303529458901 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1932484414445017) - present_state_Q ( -0.7134603954951904)) * f1( 0.017540858915392775)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1932484414445017) - present_state_Q (-0.7134603954951904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.84272511739106 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03929871532109171) - present_state_Q ( -0.03929871532109171)) * f1( 0.0062537481708062215)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.03929871532109171) - present_state_Q (-0.03929871532109171)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.85796957047652 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15706770855979724) - present_state_Q ( -0.15706770855979724)) * f1( 0.009906436743171222)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.15706770855979724) - present_state_Q (-0.15706770855979724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.880796964083025 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17598955089777865) - present_state_Q ( -0.7164712177900652)) * f1( 0.015391757323697497)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.17598955089777865) - present_state_Q (-0.7164712177900652)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.91521150949776 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1929156948882203) - present_state_Q ( -0.9478304628873087)) * f1( 0.023569581540782602)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1929156948882203) - present_state_Q (-0.9478304628873087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.920573811148316 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16885206808734665) - present_state_Q ( -0.16885206808734665)) * f1( 0.003487034857803364)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.16885206808734665) - present_state_Q (-0.16885206808734665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.94389069764057 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15162129557886755) - present_state_Q ( -0.6972625405250259)) * f1( 0.015704046960167223)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.15162129557886755) - present_state_Q (-0.6972625405250259)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.974181556509244 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23245916725491303) - present_state_Q ( -0.7790246642150923)) * f1( 0.020502794924990724)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.23245916725491303) - present_state_Q (-0.7790246642150923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -38.98024759169831 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0979165946518667) - present_state_Q ( -0.0979165946518667)) * f1( 0.003928354813869847)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0979165946518667) - present_state_Q (-0.0979165946518667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.01017357224947 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33341145008411427) - present_state_Q ( -0.8844290118631056)) * f1( 0.020387343336654137)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.33341145008411427) - present_state_Q (-0.8844290118631056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.03739530290211 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30143444252243645) - present_state_Q ( -0.8257216570526872)) * f1( 0.0184751821952829)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.30143444252243645) - present_state_Q (-0.8257216570526872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.05143476355249 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3160116701495431) - present_state_Q ( -0.3160116701495431)) * f1( 0.009795975752698952)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.3160116701495431) - present_state_Q (-0.3160116701495431)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.07552205402781 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1574026405207956) - present_state_Q ( -0.6491355675089158)) * f1( 0.017226271944540992)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.1574026405207956) - present_state_Q (-0.6491355675089158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.10920748746317 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24113004117226933) - present_state_Q ( -1.0190783941992372)) * f1( 0.024729950369390057)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.24113004117226933) - present_state_Q (-1.0190783941992372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.11404923200962 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16568055647131835) - present_state_Q ( -0.16568055647131835)) * f1( 0.003346713099768971)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.16568055647131835) - present_state_Q (-0.16568055647131835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.146581121350394 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4282383354828289) - present_state_Q ( -0.9813662199654593)) * f1( 0.023784561357612814)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.4282383354828289) - present_state_Q (-0.9813662199654593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.173926713233456 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3677463854043993) - present_state_Q ( -0.9135341779587342)) * f1( 0.019902877980930125)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.3677463854043993) - present_state_Q (-0.9135341779587342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.17554516755848 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2812180813378679) - present_state_Q ( -0.2812180813378679)) * f1( 0.0011268078241385563)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.2812180813378679) - present_state_Q (-0.2812180813378679)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.18599858623261 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06063507406972074) - present_state_Q ( -0.06063507406972074)) * f1( 0.007178705453667702)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.06063507406972074) - present_state_Q (-0.06063507406972074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.18825601069588 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19786703010081677) - present_state_Q ( -0.19786703010081677)) * f1( 0.0016691150791055185)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.19786703010081677) - present_state_Q (-0.19786703010081677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.21863829873077 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2799426762319443) - present_state_Q ( -1.0391076553847676)) * f1( 0.025795515073565704)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2799426762319443) - present_state_Q (-1.0391076553847676)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.24449687347195 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22457355874066878) - present_state_Q ( -0.9556747478722518)) * f1( 0.023631307847510354)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.22457355874066878) - present_state_Q (-0.9556747478722518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.26974588562579 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39331974026588784) - present_state_Q ( -0.9483093405508427)) * f1( 0.023023247000619765)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.39331974026588784) - present_state_Q (-0.9483093405508427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.283376664156094 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17490614860890255) - present_state_Q ( -0.17490614860890255)) * f1( 0.011632035730082622)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.17490614860890255) - present_state_Q (-0.17490614860890255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.30365010022108 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3181993323842453) - present_state_Q ( -0.8138359087086628)) * f1( 0.018274715381970823)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.3181993323842453) - present_state_Q (-0.8138359087086628)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.317783851467546 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20649970673439141) - present_state_Q ( -0.591548910858402)) * f1( 0.013601513116712871)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.20649970673439141) - present_state_Q (-0.591548910858402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.32491550855799 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09757071851905455) - present_state_Q ( -0.09757071851905455)) * f1( 0.0065582114592789625)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.09757071851905455) - present_state_Q (-0.09757071851905455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.3513975618452 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31041579177435147) - present_state_Q ( -1.0806295118605609)) * f1( 0.02671549586242974)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.31041579177435147) - present_state_Q (-1.0806295118605609)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.35630625292699 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20489738669867408) - present_state_Q ( -0.20489738669867408)) * f1( 0.004554446538894481)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.20489738669867408) - present_state_Q (-0.20489738669867408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.378711286395756 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1553253453860997) - present_state_Q ( -0.9360690610993289)) * f1( 0.022312058133555013)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1553253453860997) - present_state_Q (-0.9360690610993289)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.39726481001889 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1876257529485928) - present_state_Q ( -0.7445068539454649)) * f1( 0.018124936497029797)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1876257529485928) - present_state_Q (-0.7445068539454649)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.41553089050554 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12119783061555951) - present_state_Q ( -0.6741785510553603)) * f1( 0.017733803561626734)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12119783061555951) - present_state_Q (-0.6741785510553603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.42063636368802 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2131058164464424) - present_state_Q ( -0.2131058164464424)) * f1( 0.0047402767025928666)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2131058164464424) - present_state_Q (-0.2131058164464424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.43606238740547 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22014510273599794) - present_state_Q ( -0.3384604519142891)) * f1( 0.014490296820155391)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.22014510273599794) - present_state_Q (-0.3384604519142891)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.452045826372114 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1869949988526257) - present_state_Q ( -0.7390272425718711)) * f1( 0.015605963136456083)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1869949988526257) - present_state_Q (-0.7390272425718711)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.471135784603874 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32786867260510577) - present_state_Q ( -0.86603925224708)) * f1( 0.018846918575757904)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.32786867260510577) - present_state_Q (-0.86603925224708)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.497844364898995 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5462681379384637) - present_state_Q ( -1.0723793721532484)) * f1( 0.026857763322220497)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.5462681379384637) - present_state_Q (-1.0723793721532484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.51392912505879 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30170141106831416) - present_state_Q ( -0.36735719587937593)) * f1( 0.016562586358071948)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.30170141106831416) - present_state_Q (-0.36735719587937593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.521559884164766 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06525404265542176) - present_state_Q ( -0.06525404265542176)) * f1( 0.007638427258993168)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.06525404265542176) - present_state_Q (-0.06525404265542176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.5410854075356 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3762973692150322) - present_state_Q ( -0.8238373122175413)) * f1( 0.021080227806006106)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.3762973692150322) - present_state_Q (-0.8238373122175413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.557880324815855 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12200447919311752) - present_state_Q ( -0.6508674536985058)) * f1( 0.019766852786034458)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.12200447919311752) - present_state_Q (-0.6508674536985058)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.57432924107189 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.236807855073446) - present_state_Q ( -1.0249790434089205)) * f1( 0.020222732510153878)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.236807855073446) - present_state_Q (-1.0249790434089205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.59387336638605 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0171421571163342) - present_state_Q ( -0.5436955188248446)) * f1( 0.022743733874411184)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0171421571163342) - present_state_Q (-0.5436955188248446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.60639218532308 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25707553808216954) - present_state_Q ( -0.40830566530070306)) * f1( 0.014303013510469816)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.25707553808216954) - present_state_Q (-0.40830566530070306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.61228530360988 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06533137108027777) - present_state_Q ( -0.06533137108027777)) * f1( 0.006492811039306363)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06533137108027777) - present_state_Q (-0.06533137108027777)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.626310567385474 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20247015250406358) - present_state_Q ( -0.7563881499728282)) * f1( 0.016698668224522265)
w2 ( -38.03347538914419 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.20247015250406358) - present_state_Q (-0.7563881499728282)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.636921551379 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.729904070797367) - present_state_Q ( -0.7349251244057775)) * f1( 0.012522939043130454)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.729904070797367) - present_state_Q (-0.7349251244057775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -39.651642114545155 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3263442450237477) - present_state_Q ( -0.8868779854776951)) * f1( 0.019980644265251115)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.3263442450237477) - present_state_Q (-0.8868779854776951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.65333280482467 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14358490699537627) - present_state_Q ( -0.14358490699537627)) * f1( 0.0023550788508028644)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14358490699537627) - present_state_Q (-0.14358490699537627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.66084209735521 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19939121988234523) - present_state_Q ( -0.6886882875307717)) * f1( 0.011310216558382064)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.19939121988234523) - present_state_Q (-0.6886882875307717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.670500734411284 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16429681422617626) - present_state_Q ( -0.6871052195932865)) * f1( 0.014551701257696692)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16429681422617626) - present_state_Q (-0.6871052195932865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.68470045340864 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3457402868012653) - present_state_Q ( -0.8895040238500684)) * f1( 0.022004124198416863)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.3457402868012653) - present_state_Q (-0.8895040238500684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.70192819249819 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1810044067791866) - present_state_Q ( -0.7956826430176781)) * f1( 0.026380203579033874)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1810044067791866) - present_state_Q (-0.7956826430176781)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.71329447253336 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24128756364294585) - present_state_Q ( -0.7932333500405747)) * f1( 0.017382202175595357)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.24128756364294585) - present_state_Q (-0.7932333500405747)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.724686527642675 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10564337042807034) - present_state_Q ( -0.6533869777171509)) * f1( 0.017091547105540585)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10564337042807034) - present_state_Q (-0.6533869777171509)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.739845073923696 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37708189345075305) - present_state_Q ( -0.9370387438338298)) * f1( 0.023652678688685546)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.37708189345075305) - present_state_Q (-0.9370387438338298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.75348421249191 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17457870398652536) - present_state_Q ( -0.7245726708067752)) * f1( 0.02398087796538853)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17457870398652536) - present_state_Q (-0.7245726708067752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.764748873177325 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27975270791108436) - present_state_Q ( -0.7693377364039597)) * f1( 0.019925961549681025)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.27975270791108436) - present_state_Q (-0.7693377364039597)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.76889087506168 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24783936288444317) - present_state_Q ( -0.24783936288444317)) * f1( 0.006711428428808736)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.24783936288444317) - present_state_Q (-0.24783936288444317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.78110860484528 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17045583316870686) - present_state_Q ( -0.9586660929492402)) * f1( 0.022405518585207437)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17045583316870686) - present_state_Q (-0.9586660929492402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.794670208491645 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4436408895553028) - present_state_Q ( -1.003476976494071)) * f1( 0.02495002129235372)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.4436408895553028) - present_state_Q (-1.003476976494071)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.798788601446404 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2452177642061212) - present_state_Q ( -0.3233116219132367)) * f1( 0.006756081379508101)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.2452177642061212) - present_state_Q (-0.3233116219132367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.803894788555986 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12188062406323812) - present_state_Q ( -0.12188062406323812)) * f1( 0.008124495572380604)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12188062406323812) - present_state_Q (-0.12188062406323812)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.815258559976115 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.277927208108221) - present_state_Q ( -0.7779467321987517)) * f1( 0.02013258644586305)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.277927208108221) - present_state_Q (-0.7779467321987517)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.828374124571184 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22270839871045894) - present_state_Q ( -1.014498068640589)) * f1( 0.024277321869076097)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.22270839871045894) - present_state_Q (-1.014498068640589)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.842375581046944 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29444482522924953) - present_state_Q ( -1.090799035240877)) * f1( 0.026253061452542123)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.29444482522924953) - present_state_Q (-1.090799035240877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.85518883281815 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22288855772736213) - present_state_Q ( -0.9662879237658729)) * f1( 0.023507872239438062)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.22288855772736213) - present_state_Q (-0.9662879237658729)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.86974337037848 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08891974410930872) - present_state_Q ( -0.7307029510506285)) * f1( 0.025656664310990594)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08891974410930872) - present_state_Q (-0.7307029510506285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.877784346220906 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1662353128012295) - present_state_Q ( -0.7104204658971613)) * f1( 0.014104936633151892)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1662353128012295) - present_state_Q (-0.7104204658971613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.88620938308794 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15572775805631053) - present_state_Q ( -0.6843760097435116)) * f1( 0.014714120506481411)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.15572775805631053) - present_state_Q (-0.6843760097435116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.88910706016196 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10303590669915161) - present_state_Q ( -0.10303590669915161)) * f1( 0.004598109142178487)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10303590669915161) - present_state_Q (-0.10303590669915161)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.90256621014705 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27698458329889214) - present_state_Q ( -1.021898412795507)) * f1( 0.02492240982428006)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.27698458329889214) - present_state_Q (-1.021898412795507)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.90493835552141 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10900632559047953) - present_state_Q ( -0.10900632559047953)) * f1( 0.0037673941212412227)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10900632559047953) - present_state_Q (-0.10900632559047953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.91203665655984 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10651135286710692) - present_state_Q ( -0.5778165008521156)) * f1( 0.012180790056060967)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10651135286710692) - present_state_Q (-0.5778165008521156)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.91839064436587 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25371261054457767) - present_state_Q ( -0.6883007651221174)) * f1( 0.011085711970377428)
w2 ( -38.2029401442944 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.25371261054457767) - present_state_Q (-0.6883007651221174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.91436294645936 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5291018045178382) - present_state_Q ( -8.482245650510464)) * f1( 0.01979490211543433)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.5291018045178382) - present_state_Q (-8.482245650510464)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -39.922691819963305 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16110548220043613) - present_state_Q ( -0.7244983528020079)) * f1( 0.017451015442386635)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16110548220043613) - present_state_Q (-0.7244983528020079)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.93018029971926 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1645551115079827) - present_state_Q ( -0.7198640337714128)) * f1( 0.015673833961927862)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1645551115079827) - present_state_Q (-0.7198640337714128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.94225411902923 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25118376487937216) - present_state_Q ( -0.8157250576858623)) * f1( 0.025741024491127196)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.25118376487937216) - present_state_Q (-0.8157250576858623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.95269489978435 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3457749912776857) - present_state_Q ( -0.8856838683411202)) * f1( 0.022550296456421353)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.3457749912776857) - present_state_Q (-0.8856838683411202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.9624418715308 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24290580902425585) - present_state_Q ( -0.8077970768292103)) * f1( 0.020748846206539148)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.24290580902425585) - present_state_Q (-0.8077970768292103)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.97212520235713 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1106564009030958) - present_state_Q ( -0.741811200359018)) * f1( 0.02523777149255863)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1106564009030958) - present_state_Q (-0.741811200359018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.977900128380675 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17621567167248448) - present_state_Q ( -0.7061636641098533)) * f1( 0.01488749757520624)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.17621567167248448) - present_state_Q (-0.7061636641098533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.979210786147135 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2053923079923783) - present_state_Q ( -0.2053923079923783)) * f1( 0.0029905032283430834)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2053923079923783) - present_state_Q (-0.2053923079923783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.98194290445331 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14217425703652348) - present_state_Q ( -0.14217425703652348)) * f1( 0.006153933259460196)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14217425703652348) - present_state_Q (-0.14217425703652348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.99063179115453 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2607091855677289) - present_state_Q ( -0.6514602842617033)) * f1( 0.022040722429575294)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2607091855677289) - present_state_Q (-0.6514602842617033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -39.9966971948646 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20051756509461685) - present_state_Q ( -0.7498469510551921)) * f1( 0.015804413638768097)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.20051756509461685) - present_state_Q (-0.7498469510551921)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.0044053216864 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20025106750445984) - present_state_Q ( -0.9067781978882694)) * f1( 0.02094125573061642)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.20025106750445984) - present_state_Q (-0.9067781978882694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.00707741957089 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1297483123031827) - present_state_Q ( -0.1297483123031827)) * f1( 0.006003617835153442)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1297483123031827) - present_state_Q (-0.1297483123031827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.015687699552274 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21475664343408182) - present_state_Q ( -1.0105504664677794)) * f1( 0.024061065996505385)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21475664343408182) - present_state_Q (-1.0105504664677794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02241616584494 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4528763811509718) - present_state_Q ( -1.0148031218479716)) * f1( 0.025063632656669183)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.4528763811509718) - present_state_Q (-1.0148031218479716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.026394876752036 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13189415494054868) - present_state_Q ( -0.608715771979707)) * f1( 0.013008518709481076)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13189415494054868) - present_state_Q (-0.608715771979707)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02823523960566 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7469742987693012) - present_state_Q ( -0.19092687685978324)) * f1( 0.005201939859832966)
w2 ( -38.16224584954094 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.7469742987693012) - present_state_Q (-0.19092687685978324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02006384693495 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.981357973632646) - present_state_Q ( -8.438520213804447)) * f1( 0.020498610361755154)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -7.981357973632646) - present_state_Q (-8.438520213804447)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -40.02374457467077 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2009410848326404) - present_state_Q ( -0.7512684013376991)) * f1( 0.018317751782079842)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2009410848326404) - present_state_Q (-0.7512684013376991)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02645207796563 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26537969703864805) - present_state_Q ( -0.8213308276433826)) * f1( 0.026229355586318934)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.26537969703864805) - present_state_Q (-0.8213308276433826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02690208727194 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1620032600535684) - present_state_Q ( -0.1620032600535684)) * f1( 0.0026766646354623707)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1620032600535684) - present_state_Q (-0.1620032600535684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02877459381616 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35580635055716847) - present_state_Q ( -0.9038344323413284)) * f1( 0.019530081787662017)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.35580635055716847) - present_state_Q (-0.9038344323413284)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.02935146020198 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1793587894351556) - present_state_Q ( -0.1793587894351556)) * f1( 0.0034633908121010494)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1793587894351556) - present_state_Q (-0.1793587894351556)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.031391079611645 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24162405997290132) - present_state_Q ( -1.0382304596247574)) * f1( 0.02508860521108033)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24162405997290132) - present_state_Q (-1.0382304596247574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.033516447068735 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35705712046996746) - present_state_Q ( -0.9042754653405993)) * f1( 0.022174706743829375)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.35705712046996746) - present_state_Q (-0.9042754653405993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.03556054144059 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22750670301946652) - present_state_Q ( -1.0218617546689568)) * f1( 0.02468941384770971)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.22750670301946652) - present_state_Q (-1.0218617546689568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.037713320901965 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24969771154351503) - present_state_Q ( -0.8030398911948682)) * f1( 0.020522903473514046)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24969771154351503) - present_state_Q (-0.8030398911948682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.0378788774137 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2903297294976243) - present_state_Q ( -0.2903297294976243)) * f1( 0.0010573706418200977)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2903297294976243) - present_state_Q (-0.2903297294976243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.039155639685006 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0736940365688228) - present_state_Q ( -0.0736940365688228)) * f1( 0.007251406372053113)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0736940365688228) - present_state_Q (-0.0736940365688228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.04047755193846 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05023049175071776) - present_state_Q ( -0.05023049175071776)) * f1( 0.007418858366053724)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05023049175071776) - present_state_Q (-0.05023049175071776)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.041782597044794 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03938974928943543) - present_state_Q ( -0.03938974928943543)) * f1( 0.0072843099171248735)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03938974928943543) - present_state_Q (-0.03938974928943543)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.043048819633064 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.071551267212137) - present_state_Q ( -0.071551267212137)) * f1( 0.007183677724124094)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.071551267212137) - present_state_Q (-0.071551267212137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.045024332699676 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18490803503432462) - present_state_Q ( -0.9565113521765592)) * f1( 0.02222139568052345)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18490803503432462) - present_state_Q (-0.9565113521765592)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.047203247460736 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3648788980117116) - present_state_Q ( -0.9273671177507095)) * f1( 0.023275142893820017)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3648788980117116) - present_state_Q (-0.9273671177507095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.048158208706376 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17027706920581615) - present_state_Q ( -0.17027706920581615)) * f1( 0.005705399259397239)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17027706920581615) - present_state_Q (-0.17027706920581615)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.0499551610867 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28924487638910745) - present_state_Q ( -0.8535885442641333)) * f1( 0.017927028886579193)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.28924487638910745) - present_state_Q (-0.8535885442641333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.051961584762296 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41869764350716077) - present_state_Q ( -0.9557708191419354)) * f1( 0.021972951458336585)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.41869764350716077) - present_state_Q (-0.9557708191419354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.05235851657406 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16297798840799607) - present_state_Q ( -0.16297798840799607)) * f1( 0.002362191178894751)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16297798840799607) - present_state_Q (-0.16297798840799607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.05499024016866 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17230901548867247) - present_state_Q ( -0.7383253939936847)) * f1( 0.023796260091828638)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17230901548867247) - present_state_Q (-0.7383253939936847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.07418861992537 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2832315521918424) - present_state_Q ( -0.2832315521918424)) * f1( 0.007071067811865476)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.2832315521918424) - present_state_Q (-0.2832315521918424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.093189085164184 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23367066606774123) - present_state_Q ( -0.5581688589914329)) * f1( 0.007071067811865476)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.23367066606774123) - present_state_Q (-0.5581688589914329)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.13250318281265 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.134189258431137) - present_state_Q ( -0.672760073438583)) * f1( 0.014698960046262397)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.134189258431137) - present_state_Q (-0.672760073438583)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.20060964898622 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28247599089684017) - present_state_Q ( -1.0757230757144403)) * f1( 0.025838970249851996)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.28247599089684017) - present_state_Q (-1.0757230757144403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.21235118966659 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07614715988559938) - present_state_Q ( -0.07614715988559938)) * f1( 0.004295111801353639)
w2 ( -38.082519541010804 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07614715988559938) - present_state_Q (-0.07614715988559938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.22151989811133 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.837421135195832) - present_state_Q ( -7.837421135195832)) * f1( 0.004716821155214122)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -7.837421135195832) - present_state_Q (-7.837421135195832)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -40.242000678306354 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2707346039724603) - present_state_Q ( -0.2707346039724603)) * f1( 0.008386439865548935)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.2707346039724603) - present_state_Q (-0.2707346039724603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.304261765549725 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3045255672752407) - present_state_Q ( -1.1087844292513944)) * f1( 0.02639676680705527)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.3045255672752407) - present_state_Q (-1.1087844292513944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.342827442623665 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1432209795060818) - present_state_Q ( -0.6491234469858371)) * f1( 0.01668307559103648)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.1432209795060818) - present_state_Q (-0.6491234469858371)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.35204698904047 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09258147576238891) - present_state_Q ( -0.09258147576238891)) * f1( 0.0038953429455694404)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.09258147576238891) - present_state_Q (-0.09258147576238891)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.398250669184605 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25424240784204705) - present_state_Q ( -0.5613758361375252)) * f1( 0.019902085432036788)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.25424240784204705) - present_state_Q (-0.5613758361375252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.41319162887753 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04213985264493674) - present_state_Q ( -0.04213985264493674)) * f1( 0.006300607449012408)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.04213985264493674) - present_state_Q (-0.04213985264493674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.436353985893824 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3207111653630012) - present_state_Q ( -0.48128889757110654)) * f1( 0.009939974142681766)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.3207111653630012) - present_state_Q (-0.48128889757110654)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.46521412869356 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3022445075257278) - present_state_Q ( -0.4398663711838401)) * f1( 0.012364142759151939)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.3022445075257278) - present_state_Q (-0.4398663711838401)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.49432409125807 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21992047306423118) - present_state_Q ( -0.5981958460125641)) * f1( 0.012560800553300675)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.21992047306423118) - present_state_Q (-0.5981958460125641)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.528193863576305 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05220816926561366) - present_state_Q ( -0.8453167490883909)) * f1( 0.014782965045238405)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.05220816926561366) - present_state_Q (-0.8453167490883909)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.550158613054656 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4405944027013144) - present_state_Q ( -0.14050532308699465)) * f1( 0.009285456397771499)
w2 ( -38.47128596411779 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.4405944027013144) - present_state_Q (-0.14050532308699465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.557546568348116 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.91311247736459) - present_state_Q ( -7.91311247736459)) * f1( 0.00470087439685086)
w2 ( -38.78560856326705 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -7.91311247736459) - present_state_Q (-7.91311247736459)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -40.567842878820954 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.016542657907042) - present_state_Q ( -8.016542657907042)) * f1( 0.0065904642315602795)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -8.016542657907042) - present_state_Q (-8.016542657907042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -40.58543366663942 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22086390624266297) - present_state_Q ( -0.22086390624266297)) * f1( 0.008452184839649738)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.22086390624266297) - present_state_Q (-0.22086390624266297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.58704656243133 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28481694632073756) - present_state_Q ( -0.28481694632073756)) * f1( 0.0007771283419033013)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.28481694632073756) - present_state_Q (-0.28481694632073756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.60175046558662 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06482359997299297) - present_state_Q ( -0.06482359997299297)) * f1( 0.0070177135141678325)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.06482359997299297) - present_state_Q (-0.06482359997299297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.6107023020243 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21066134544471243) - present_state_Q ( -0.21066134544471243)) * f1( 0.0042993644973404075)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.21066134544471243) - present_state_Q (-0.21066134544471243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.6549069680538 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3644849527025058) - present_state_Q ( -0.9032800182450081)) * f1( 0.021944262818015182)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.3644849527025058) - present_state_Q (-0.9032800182450081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.65912890792876 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15811442408982151) - present_state_Q ( -0.15811442408982151)) * f1( 0.0020231070351665525)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.15811442408982151) - present_state_Q (-0.15811442408982151)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.67192133606028 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08597235077668032) - present_state_Q ( -0.08597235077668032)) * f1( 0.006110977593912298)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.08597235077668032) - present_state_Q (-0.08597235077668032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.71714211029253 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3367065627080703) - present_state_Q ( -0.8974802562540999)) * f1( 0.02244531630078087)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.3367065627080703) - present_state_Q (-0.8974802562540999)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.756124072858 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1827896130597168) - present_state_Q ( -0.7582235517244448)) * f1( 0.019230454677161203)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.1827896130597168) - present_state_Q (-0.7582235517244448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.763759014680424 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1459123193302962) - present_state_Q ( -0.1459123193302962)) * f1( 0.003656655655728865)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.1459123193302962) - present_state_Q (-0.1459123193302962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.782012869115924 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2857915831058377) - present_state_Q ( -0.2857915831058377)) * f1( 0.009200454016964623)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2857915831058377) - present_state_Q (-0.2857915831058377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.79317976100738 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15634213714935655) - present_state_Q ( -0.15634213714935655)) * f1( 0.005595568318890117)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.15634213714935655) - present_state_Q (-0.15634213714935655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.826607271828834 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15853056535435447) - present_state_Q ( -0.9463861640800655)) * f1( 0.017440276941521324)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.15853056535435447) - present_state_Q (-0.9463861640800655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.86695336502993 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2562192449885164) - present_state_Q ( -0.833517452044473)) * f1( 0.02091610816261283)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.2562192449885164) - present_state_Q (-0.833517452044473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.87611137207535 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18055970489902057) - present_state_Q ( -0.18055970489902057)) * f1( 0.0050574814173329095)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.18055970489902057) - present_state_Q (-0.18055970489902057)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.920458223052464 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49580268251472953) - present_state_Q ( -1.0737055010053194)) * f1( 0.025713954539040268)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.49580268251472953) - present_state_Q (-1.0737055010053194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -40.96372684639401 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08767462642052373) - present_state_Q ( -0.9003110379135879)) * f1( 0.024897357653951492)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.08767462642052373) - present_state_Q (-0.9003110379135879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.00182477756037 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15111109154997857) - present_state_Q ( -0.9693871112481981)) * f1( 0.022001489646232733)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.15111109154997857) - present_state_Q (-0.9693871112481981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.04666484172672 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29347742553436473) - present_state_Q ( -1.1129365404159872)) * f1( 0.026089893735776918)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.29347742553436473) - present_state_Q (-1.1129365404159872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.08447026344138 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3187612312024604) - present_state_Q ( -0.8772071098943449)) * f1( 0.021696063942997383)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3187612312024604) - present_state_Q (-0.8772071098943449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.11952805105726 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3111580265742957) - present_state_Q ( -0.8797689466227258)) * f1( 0.02012306726973015)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.3111580265742957) - present_state_Q (-0.8797689466227258)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.138093144239384 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41025513022960103) - present_state_Q ( -0.41025513022960103)) * f1( 0.010370914234855378)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.41025513022960103) - present_state_Q (-0.41025513022960103)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.165596805200856 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1616723884762091) - present_state_Q ( -0.6232806998859479)) * f1( 0.01557113769310518)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.1616723884762091) - present_state_Q (-0.6232806998859479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.17282093946766 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14306835706947976) - present_state_Q ( -0.14306835706947976)) * f1( 0.003982085850089857)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14306835706947976) - present_state_Q (-0.14306835706947976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.21012547798595 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36102800879412644) - present_state_Q ( -0.9241226502470176)) * f1( 0.022651632620689654)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.36102800879412644) - present_state_Q (-0.9241226502470176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.240503666485516 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15236802624533108) - present_state_Q ( -0.7245579824616514)) * f1( 0.01824789103327865)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.15236802624533108) - present_state_Q (-0.7245579824616514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.25459574911954 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32478570966902015) - present_state_Q ( -0.32478570966902015)) * f1( 0.00924829894554397)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.32478570966902015) - present_state_Q (-0.32478570966902015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.286814669226466 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14042634528743672) - present_state_Q ( -0.9528856086637406)) * f1( 0.02208144061369213)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.14042634528743672) - present_state_Q (-0.9528856086637406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.31473577502457 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25480171460122497) - present_state_Q ( -0.8288897468406544)) * f1( 0.018959920165150947)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.25480171460122497) - present_state_Q (-0.8288897468406544)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.34842139171342 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3200815916992549) - present_state_Q ( -1.0199824176513284)) * f1( 0.02316465141506287)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.3200815916992549) - present_state_Q (-1.0199824176513284)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.375070058447115 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4003387506133559) - present_state_Q ( -0.9401177537451655)) * f1( 0.02241425983358112)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.4003387506133559) - present_state_Q (-0.9401177537451655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.404299036168645 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33480616322890716) - present_state_Q ( -1.0577739105613968)) * f1( 0.024844119411201536)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.33480616322890716) - present_state_Q (-1.0577739105613968)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.40924529138063 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20563086707859987) - present_state_Q ( -0.20563086707859987)) * f1( 0.004230947371239852)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.20563086707859987) - present_state_Q (-0.20563086707859987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.432134104874144 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32815322459191254) - present_state_Q ( -0.835050749123106)) * f1( 0.020669920678386865)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.32815322459191254) - present_state_Q (-0.835050749123106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.43572060018942 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13878968809966966) - present_state_Q ( -0.13878968809966966)) * f1( 0.0030521251280913562)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.13878968809966966) - present_state_Q (-0.13878968809966966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.459918148050086 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16995926220998872) - present_state_Q ( -0.9793675261625182)) * f1( 0.022172423735497406)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.16995926220998872) - present_state_Q (-0.9793675261625182)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.48926930402973 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36648703525256565) - present_state_Q ( -1.1904333465723935)) * f1( 0.027374856737830657)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.36648703525256565) - present_state_Q (-1.1904333465723935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.51702293981024 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4965006589443064) - present_state_Q ( -1.0826671413061142)) * f1( 0.02559659263812042)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.4965006589443064) - present_state_Q (-1.0826671413061142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.53582774705072 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20466242571942175) - present_state_Q ( -0.7144977485432881)) * f1( 0.016817495996330853)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.20466242571942175) - present_state_Q (-0.7144977485432881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.55876763280099 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3254002522881788) - present_state_Q ( -0.8734766352906383)) * f1( 0.020788697133831736)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.3254002522881788) - present_state_Q (-0.8734766352906383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.580409402964726 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2841051060256804) - present_state_Q ( -0.870802207387531)) * f1( 0.021385538747363528)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2841051060256804) - present_state_Q (-0.870802207387531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.58566187074602 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2263773148186719) - present_state_Q ( -0.2263773148186719)) * f1( 0.004882171031076875)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2263773148186719) - present_state_Q (-0.2263773148186719)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.607171062379976 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40613028500670245) - present_state_Q ( -0.8753714390482261)) * f1( 0.021238509885386975)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.40613028500670245) - present_state_Q (-0.8753714390482261)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.61031801380727 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18489121731147423) - present_state_Q ( -0.18489121731147423)) * f1( 0.0029149762004413915)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.18489121731147423) - present_state_Q (-0.18489121731147423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.632428601191684 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32329045804194484) - present_state_Q ( -0.8907525323174519)) * f1( 0.02188347307815408)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.32329045804194484) - present_state_Q (-0.8907525323174519)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.650029095948426 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18821656781209822) - present_state_Q ( -0.7638596759958223)) * f1( 0.018917834973194096)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.18821656781209822) - present_state_Q (-0.7638596759958223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.654661362397526 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2171142139372471) - present_state_Q ( -0.2171142139372471)) * f1( 0.00577139391855337)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2171142139372471) - present_state_Q (-0.2171142139372471)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.67443394052129 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31410786947392166) - present_state_Q ( -0.8618203126406616)) * f1( 0.026751347366689524)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.31410786947392166) - present_state_Q (-0.8618203126406616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.68896786759507 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33017942405365336) - present_state_Q ( -0.9194349991166783)) * f1( 0.019813844807316464)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.33017942405365336) - present_state_Q (-0.9194349991166783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.70492759266305 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31770888935243174) - present_state_Q ( -0.8895162795614112)) * f1( 0.02167289581464031)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.31770888935243174) - present_state_Q (-0.8895162795614112)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.717925163804225 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17787034069688495) - present_state_Q ( -0.7635446412059912)) * f1( 0.01738596727973704)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.17787034069688495) - present_state_Q (-0.7635446412059912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.73515081858166 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25284306465892065) - present_state_Q ( -1.025097570759681)) * f1( 0.02385216246152873)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.25284306465892065) - present_state_Q (-1.025097570759681)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.74753173705804 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10356512988878953) - present_state_Q ( -0.6574601140638565)) * f1( 0.016345414339023433)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.10356512988878953) - present_state_Q (-0.6574601140638565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.76614329118206 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3065916430644617) - present_state_Q ( -1.1409849257693019)) * f1( 0.026171694355170954)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.3065916430644617) - present_state_Q (-1.1409849257693019)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.785506800883574 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4461411320399949) - present_state_Q ( -1.1820673990998547)) * f1( 0.027333367768919162)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.4461411320399949) - present_state_Q (-1.1820673990998547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.78912980568553 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17848607144383208) - present_state_Q ( -0.17848607144383208)) * f1( 0.0050689115643572135)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17848607144383208) - present_state_Q (-0.17848607144383208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.803405588654186 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3682475248043787) - present_state_Q ( -0.4484116627205265)) * f1( 0.0206998872690404)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.3682475248043787) - present_state_Q (-0.4484116627205265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.81017357362598 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13448473676158426) - present_state_Q ( -0.13448473676158426)) * f1( 0.009416848736099978)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13448473676158426) - present_state_Q (-0.13448473676158426)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.82297264574307 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20458910184598725) - present_state_Q ( -0.7921619349258187)) * f1( 0.019581120603137287)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.20458910184598725) - present_state_Q (-0.7921619349258187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.8275103093474 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13506308970125222) - present_state_Q ( -0.13506308970125222)) * f1( 0.006314078250273059)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13506308970125222) - present_state_Q (-0.13506308970125222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.839291729904424 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19990263720628143) - present_state_Q ( -1.0162706719093904)) * f1( 0.018665536132951112)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.19990263720628143) - present_state_Q (-1.0162706719093904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.84417390382832 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04653083814577445) - present_state_Q ( -0.04653083814577445)) * f1( 0.006718963775524999)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.04653083814577445) - present_state_Q (-0.04653083814577445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.849057395292085 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017435276752705682) - present_state_Q ( -0.017435276752705682)) * f1( 0.0066966437724917125)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.017435276752705682) - present_state_Q (-0.017435276752705682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.85391818278891 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05814162144406325) - present_state_Q ( -0.05814162144406325)) * f1( 0.006699165424795159)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05814162144406325) - present_state_Q (-0.05814162144406325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.858931700494956 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028954822845207867) - present_state_Q ( -0.028954822845207867)) * f1( 0.0068847343655245195)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.028954822845207867) - present_state_Q (-0.028954822845207867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.86364960304609 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19113922401037822) - present_state_Q ( -0.4324000564472438)) * f1( 0.006842645290476401)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.19113922401037822) - present_state_Q (-0.4324000564472438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.87401072810867 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2563652081123407) - present_state_Q ( -0.7314645720911278)) * f1( 0.015693181893380242)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2563652081123407) - present_state_Q (-0.7314645720911278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.88493205025782 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5038449260014004) - present_state_Q ( -0.7320208000309908)) * f1( 0.016481278793864212)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.5038449260014004) - present_state_Q (-0.7320208000309908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.90130680827062 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5478669216135846) - present_state_Q ( -1.139126295707001)) * f1( 0.026309910801566107)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.5478669216135846) - present_state_Q (-1.139126295707001)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.915065020669736 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2820314904409368) - present_state_Q ( -0.8734573419721279)) * f1( 0.021288039291252305)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2820314904409368) - present_state_Q (-0.8734573419721279)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.92375268341776 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33199685831918213) - present_state_Q ( -0.5097965663296409)) * f1( 0.014680006308619266)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.33199685831918213) - present_state_Q (-0.5097965663296409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.92961662852611 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13877190625796937) - present_state_Q ( -0.13877190625796937)) * f1( 0.009352793236396088)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.13877190625796937) - present_state_Q (-0.13877190625796937)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.93975483613913 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3241111459017645) - present_state_Q ( -0.9155203512769126)) * f1( 0.018394604857358065)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3241111459017645) - present_state_Q (-0.9155203512769126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.95112073402486 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3267348354089658) - present_state_Q ( -0.9141997779742874)) * f1( 0.02061618572591031)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3267348354089658) - present_state_Q (-0.9141997779742874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.96328146810337 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35152113523019357) - present_state_Q ( -1.0177033069993002)) * f1( 0.022469655999576885)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.35152113523019357) - present_state_Q (-1.0177033069993002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.97012616852043 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3728003514227759) - present_state_Q ( -0.5524492971633845)) * f1( 0.011641732994250016)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3728003514227759) - present_state_Q (-0.5524492971633845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.97655175716806 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.416464026677321) - present_state_Q ( -0.416464026677321)) * f1( 0.010674084404339888)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.416464026677321) - present_state_Q (-0.416464026677321)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -41.990218492838885 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3085284007279496) - present_state_Q ( -1.1416335568943687)) * f1( 0.025865157973309533)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3085284007279496) - present_state_Q (-1.1416335568943687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.00031397583768 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19131813483508672) - present_state_Q ( -0.7851398100732672)) * f1( 0.01793600567789889)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19131813483508672) - present_state_Q (-0.7851398100732672)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.01420643756069 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30483172335105646) - present_state_Q ( -1.1445282338360832)) * f1( 0.026308612145473278)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.30483172335105646) - present_state_Q (-1.1445282338360832)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.025826286289785 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21582916716790837) - present_state_Q ( -0.7913424295661208)) * f1( 0.024663554174823354)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.21582916716790837) - present_state_Q (-0.7913424295661208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.0343106437574 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15497425891935357) - present_state_Q ( -0.7187302778855301)) * f1( 0.017757612374813107)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15497425891935357) - present_state_Q (-0.7187302778855301)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.04490478092458 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4595487182705093) - present_state_Q ( -1.0534821240249244)) * f1( 0.023681584075251953)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.4595487182705093) - present_state_Q (-1.0534821240249244)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.05171035264892 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3189837148316235) - present_state_Q ( -0.6351287656665127)) * f1( 0.013951924533013964)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.3189837148316235) - present_state_Q (-0.6351287656665127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.06118254120082 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23438734632481584) - present_state_Q ( -0.8287154779465424)) * f1( 0.02025778343679398)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.23438734632481584) - present_state_Q (-0.8287154779465424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.06176754319356 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2874142932204711) - present_state_Q ( -0.2874142932204711)) * f1( 0.001120171880096939)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2874142932204711) - present_state_Q (-0.2874142932204711)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.06548517879692 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04509938406181013) - present_state_Q ( -0.04509938406181013)) * f1( 0.006833243286465753)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04509938406181013) - present_state_Q (-0.04509938406181013)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.068880560738606 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4334684449802195) - present_state_Q ( -0.5256391966504985)) * f1( 0.006792378930739527)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.4334684449802195) - present_state_Q (-0.5256391966504985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.07551202095295 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19347220920519867) - present_state_Q ( -0.19347220920519867)) * f1( 0.012495735979658845)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.19347220920519867) - present_state_Q (-0.19347220920519867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.08225885319553 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1467776026688024) - present_state_Q ( -0.7405734782192013)) * f1( 0.014188301696253023)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1467776026688024) - present_state_Q (-0.7405734782192013)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.087690061476295 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2290723110541794) - present_state_Q ( -0.7354291437674122)) * f1( 0.011389567733059838)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2290723110541794) - present_state_Q (-0.7354291437674122)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.088915197722486 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30164315407152326) - present_state_Q ( -0.30164315407152326)) * f1( 0.0028517356119716723)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.30164315407152326) - present_state_Q (-0.30164315407152326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.0955789470347 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27614645989128284) - present_state_Q ( -0.7717467606926647)) * f1( 0.017428610618420978)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.27614645989128284) - present_state_Q (-0.7717467606926647)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.10246448588231 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3225541045049648) - present_state_Q ( -1.0475749192692672)) * f1( 0.01938350741560919)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3225541045049648) - present_state_Q (-1.0475749192692672)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.10377193123603 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19589786483910474) - present_state_Q ( -0.19589786483910474)) * f1( 0.002977368561875011)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19589786483910474) - present_state_Q (-0.19589786483910474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.10582607529701 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.758958537279238) - present_state_Q ( -0.17504363323614322)) * f1( 0.004597006494158751)
w2 ( -39.09806941916654 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.758958537279238) - present_state_Q (-0.17504363323614322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.10301440903209 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8708738186573215) - present_state_Q ( -8.101416107298414)) * f1( 0.00815745961497871)
w2 ( -39.02913456940625 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.8708738186573215) - present_state_Q (-8.101416107298414)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.10262801979717 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.541534547101422) - present_state_Q ( -7.9997078422231045)) * f1( 0.001498813066814488)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -8.541534547101422) - present_state_Q (-7.9997078422231045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.09096852390423 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7731282886140615) - present_state_Q ( -8.568643329894934)) * f1( 0.02410350539492431)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.7731282886140615) - present_state_Q (-8.568643329894934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.094428236848366 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3819859461179467) - present_state_Q ( -0.3819859461179467)) * f1( 0.014434931348314231)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3819859461179467) - present_state_Q (-0.3819859461179467)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.09699675221021 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15746722675268526) - present_state_Q ( -0.15746722675268526)) * f1( 0.009883348037053768)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15746722675268526) - present_state_Q (-0.15746722675268526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.09988518173935 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1431565688340878) - present_state_Q ( -0.7368098478242368)) * f1( 0.014312919505103135)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1431565688340878) - present_state_Q (-0.7368098478242368)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.10312133661503 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19778999094183453) - present_state_Q ( -0.7878421342267778)) * f1( 0.016406456613586633)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.19778999094183453) - present_state_Q (-0.7878421342267778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.10755963000202 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3810731835136813) - present_state_Q ( -0.9755159844868931)) * f1( 0.024614205302319513)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3810731835136813) - present_state_Q (-0.9755159844868931)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.11155087019252 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28980814378739495) - present_state_Q ( -0.8654352523429971)) * f1( 0.020961324723733625)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.28980814378739495) - present_state_Q (-0.8654352523429971)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.11553248735799 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2934308859383829) - present_state_Q ( -0.8617620709856937)) * f1( 0.020866562583130396)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2934308859383829) - present_state_Q (-0.8617620709856937)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.11893848807562 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32133017611740333) - present_state_Q ( -0.9122805914238846)) * f1( 0.018307853610787406)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.32133017611740333) - present_state_Q (-0.9122805914238846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.12115868948417 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22469497768568109) - present_state_Q ( -0.653957066787912)) * f1( 0.010526950467011378)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.22469497768568109) - present_state_Q (-0.653957066787912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.12517556426672 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20537973787275188) - present_state_Q ( -1.0471484122470986)) * f1( 0.023436478094065332)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20537973787275188) - present_state_Q (-1.0471484122470986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.12871982750073 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3586874633458444) - present_state_Q ( -0.9291189267195659)) * f1( 0.019186164873075097)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3586874633458444) - present_state_Q (-0.9291189267195659)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.1290616808259 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3176957432238437) - present_state_Q ( -0.3176957432238437)) * f1( 0.0013926903102790473)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3176957432238437) - present_state_Q (-0.3176957432238437)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.130805724817314 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1911351498640271) - present_state_Q ( -0.4469386235960084)) * f1( 0.007541072800803662)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1911351498640271) - present_state_Q (-0.4469386235960084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.13398709634029 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2609359996640501) - present_state_Q ( -0.7270626594625483)) * f1( 0.015598149315362802)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2609359996640501) - present_state_Q (-0.7270626594625483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.13829418467065 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48805645859488545) - present_state_Q ( -1.0814607009944468)) * f1( 0.025218670206148496)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.48805645859488545) - present_state_Q (-1.0814607009944468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.14189251179586 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3848094668155248) - present_state_Q ( -0.9485173577661313)) * f1( 0.0196574541799978)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3848094668155248) - present_state_Q (-0.9485173577661313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.14550231967303 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14345463468248748) - present_state_Q ( -0.7197175188669552)) * f1( 0.017737047495770958)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14345463468248748) - present_state_Q (-0.7197175188669552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.14876282756209 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2281741956049665) - present_state_Q ( -0.8121136202237431)) * f1( 0.016709794382317223)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2281741956049665) - present_state_Q (-0.8121136202237431)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.15083718955666 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24610692123823036) - present_state_Q ( -0.8393988110917157)) * f1( 0.020492659136710233)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24610692123823036) - present_state_Q (-0.8393988110917157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.15127594423114 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16058690474650952) - present_state_Q ( -0.16058690474650952)) * f1( 0.0026077446456621483)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16058690474650952) - present_state_Q (-0.16058690474650952)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.15314873051368 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15901014168966596) - present_state_Q ( -0.7364787714154526)) * f1( 0.01692597838390302)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15901014168966596) - present_state_Q (-0.7364787714154526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.15489154302367 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3393964518192087) - present_state_Q ( -0.9284568440254544)) * f1( 0.018689331739349585)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3393964518192087) - present_state_Q (-0.9284568440254544)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.15681195936244 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5639378445513776) - present_state_Q ( -1.1586703093121675)) * f1( 0.026497346948449924)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.5639378445513776) - present_state_Q (-1.1586703093121675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.15888639339393 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28497282994881346) - present_state_Q ( -0.8667614355124328)) * f1( 0.02097993783215176)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.28497282994881346) - present_state_Q (-0.8667614355124328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.16078390704201 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24905533716833006) - present_state_Q ( -0.8322869043038664)) * f1( 0.018609403558077744)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.24905533716833006) - present_state_Q (-0.8322869043038664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.162609613507705 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3727783411141091) - present_state_Q ( -1.0838717237272681)) * f1( 0.023393278786077963)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3727783411141091) - present_state_Q (-1.0838717237272681)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.16443422246305 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5937831177262268) - present_state_Q ( -1.1766706647770269)) * f1( 0.025708054340664945)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.5937831177262268) - present_state_Q (-1.1766706647770269)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.166349314999074 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47943585698601804) - present_state_Q ( -0.5759889691018606)) * f1( 0.014742945256239148)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.47943585698601804) - present_state_Q (-0.5759889691018606)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.167938425511345 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4721399505065741) - present_state_Q ( -1.283153756309547)) * f1( 0.02688419337285651)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.4721399505065741) - present_state_Q (-1.283153756309547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.169632839509184 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6194734221765508) - present_state_Q ( -1.3321720849172476)) * f1( 0.030430752890744418)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.6194734221765508) - present_state_Q (-1.3321720849172476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.17228613854448 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13012949915927224) - present_state_Q ( -0.8048117655749388)) * f1( 0.025629903299560986)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13012949915927224) - present_state_Q (-0.8048117655749388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.173910312775526 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2192525590082654) - present_state_Q ( -0.8098192221241421)) * f1( 0.015629976755955448)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2192525590082654) - present_state_Q (-0.8098192221241421)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.17579360113337 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.232025724123738) - present_state_Q ( -1.0693097141486374)) * f1( 0.024116050616641366)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.232025724123738) - present_state_Q (-1.0693097141486374)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.17660582973464 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07758143130421943) - present_state_Q ( -0.07758143130421943)) * f1( 0.004622259396740563)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07758143130421943) - present_state_Q (-0.07758143130421943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.178275556852405 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2402183361076215) - present_state_Q ( -0.8364248169959774)) * f1( 0.016456487884540478)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2402183361076215) - present_state_Q (-0.8364248169959774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.179624805264375 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16761602987643393) - present_state_Q ( -0.16761602987643393)) * f1( 0.008049543349595897)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16761602987643393) - present_state_Q (-0.16761602987643393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.18183400309948 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3584134510483848) - present_state_Q ( -0.954585908495227)) * f1( 0.024322606232914233)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3584134510483848) - present_state_Q (-0.954585908495227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.183756389983714 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20492751778088947) - present_state_Q ( -1.0378124168594762)) * f1( 0.02374152989767001)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.20492751778088947) - present_state_Q (-1.0378124168594762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.18568107068639 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12035335126961663) - present_state_Q ( -0.6692083062342036)) * f1( 0.01645220962843428)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12035335126961663) - present_state_Q (-0.6692083062342036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.18759961741684 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3556538260305134) - present_state_Q ( -0.9329275990413914)) * f1( 0.02063680691313755)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3556538260305134) - present_state_Q (-0.9329275990413914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.18942042565951 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31463114110192014) - present_state_Q ( -0.9108521970850819)) * f1( 0.019214024777942987)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.31463114110192014) - present_state_Q (-0.9108521970850819)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.191337765809195 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35185063260668614) - present_state_Q ( -0.9284394383133635)) * f1( 0.02053310098162884)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.35185063260668614) - present_state_Q (-0.9284394383133635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.193888502192245 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19397569537254114) - present_state_Q ( -0.7906354105032257)) * f1( 0.024159352782388992)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.19397569537254114) - present_state_Q (-0.7906354105032257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.19636919201396 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1973073330553068) - present_state_Q ( -0.1973073330553068)) * f1( 0.015039424946268287)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1973073330553068) - present_state_Q (-0.1973073330553068)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.197978930739495 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3127616383762377) - present_state_Q ( -0.3127616383762377)) * f1( 0.010415319761829469)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3127616383762377) - present_state_Q (-0.3127616383762377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.20003204373815 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20929589496946666) - present_state_Q ( -0.7967184398413294)) * f1( 0.019530287798606598)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.20929589496946666) - present_state_Q (-0.7967184398413294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.20261939295673 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21196666693561858) - present_state_Q ( -0.7996449618241128)) * f1( 0.024674644955081277)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21196666693561858) - present_state_Q (-0.7996449618241128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.20447128067908 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2011508590790094) - present_state_Q ( -0.7805106727710726)) * f1( 0.017361899217087473)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2011508590790094) - present_state_Q (-0.7805106727710726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.20592835058627 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05826368663287455) - present_state_Q ( -0.6545122675929697)) * f1( 0.012365355312415057)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05826368663287455) - present_state_Q (-0.6545122675929697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.20753099862373 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1201898366198323) - present_state_Q ( -0.1201898366198323)) * f1( 0.009323881186935362)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1201898366198323) - present_state_Q (-0.1201898366198323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.20935908351641 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2086858591395714) - present_state_Q ( -0.7176077377730045)) * f1( 0.016173515212893667)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2086858591395714) - present_state_Q (-0.7176077377730045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.210914532251216 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26462890290230207) - present_state_Q ( -1.0393865388025454)) * f1( 0.019106105041826513)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.26462890290230207) - present_state_Q (-1.0393865388025454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.21300646914268 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4122736681898992) - present_state_Q ( -0.4336001373181856)) * f1( 0.0145813947342411)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.4122736681898992) - present_state_Q (-0.4336001373181856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.21486001715465 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13892311885727535) - present_state_Q ( -0.9538480750072326)) * f1( 0.020894966201214754)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13892311885727535) - present_state_Q (-0.9538480750072326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.21685494507809 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4988729461602831) - present_state_Q ( -1.0873360092208675)) * f1( 0.025265499620921818)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.4988729461602831) - present_state_Q (-1.0873360092208675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.219521660074 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15128635328731954) - present_state_Q ( -0.7641206533931372)) * f1( 0.0247366412089088)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15128635328731954) - present_state_Q (-0.7641206533931372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.22118210421752 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1671101440094095) - present_state_Q ( -0.7437215491343206)) * f1( 0.015094617967831592)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1671101440094095) - present_state_Q (-0.7437215491343206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.24037073611085 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2985488417560432) - present_state_Q ( -0.2985488417560432)) * f1( 0.007071067811865476)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.2985488417560432) - present_state_Q (-0.2985488417560432)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.258058851588935 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2463015697826929) - present_state_Q ( -0.5883402845847564)) * f1( 0.007071067811865476)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.2463015697826929) - present_state_Q (-0.5883402845847564)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.29329229230982 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14143493469648605) - present_state_Q ( -0.7090863916057705)) * f1( 0.014698960046262397)
w2 ( -38.97757520640436 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.14143493469648605) - present_state_Q (-0.7090863916057705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.30927722044257 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5456431844009464) - present_state_Q ( -0.08159592608119605)) * f1( 0.006487933641052014)
w2 ( -39.47033388789271 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.5456431844009464) - present_state_Q (-0.08159592608119605)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.323722266455015 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.16385676130341) - present_state_Q ( -8.16385676130341)) * f1( 0.00834130245090254)
w2 ( -39.81668377983041 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -8.16385676130341) - present_state_Q (-8.16385676130341)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.35704022673485 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06337337812633254) - present_state_Q ( -0.8515259106167251)) * f1( 0.013987519751767883)
w2 ( -39.81668377983041 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.06337337812633254) - present_state_Q (-0.8515259106167251)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.39001580235003 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6907531569520647) - present_state_Q ( -0.6907531569520647)) * f1( 0.01371508582083968)
w2 ( -40.29754953664644 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.6907531569520647) - present_state_Q (-0.6907531569520647)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.42885777132202 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.540189787880418) - present_state_Q ( -9.139674449117575)) * f1( 0.025114760725036673)
w2 ( -40.606865395931344 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -8.540189787880418) - present_state_Q (-9.139674449117575)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.43401511508514 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.269327811240489) - present_state_Q ( -8.269327811240489)) * f1( 0.0033498955220081996)
w2 ( -40.91477611907084 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -8.269327811240489) - present_state_Q (-8.269327811240489)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.48786565288636 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3262393379170324) - present_state_Q ( -1.1591623452924347)) * f1( 0.02589232622748036)
w2 ( -40.91477611907084 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.3262393379170324) - present_state_Q (-1.1591623452924347)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.5384799859142 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23357968648257177) - present_state_Q ( -1.0821078981784555)) * f1( 0.024257235716723488)
w2 ( -40.91477611907084 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.23357968648257177) - present_state_Q (-1.0821078981784555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.58193181884895 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31988432830911834) - present_state_Q ( -0.9030229369616157)) * f1( 0.02157503985337943)
w2 ( -40.91477611907084 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.31988432830911834) - present_state_Q (-0.9030229369616157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.624533347583686 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11776545810584368) - present_state_Q ( -0.9507029910976581)) * f1( 0.021224388144680986)
w2 ( -40.91477611907084 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.11776545810584368) - present_state_Q (-0.9507029910976581)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.64467881814056 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8119056373029282) - present_state_Q ( -8.994860861117095)) * f1( 0.01961547627686207)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.8119056373029282) - present_state_Q (-8.994860861117095)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -42.64652884776677 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29971742980599086) - present_state_Q ( -0.29971742980599086)) * f1( 0.0011438601525109514)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.29971742980599086) - present_state_Q (-0.29971742980599086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.658066222036105 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030660530214232814) - present_state_Q ( -0.030660530214232814)) * f1( 0.007028249200425317)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.030660530214232814) - present_state_Q (-0.030660530214232814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.669548054320835 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08180527888588617) - present_state_Q ( -0.08180527888588617)) * f1( 0.007014082305885052)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.08180527888588617) - present_state_Q (-0.08180527888588617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.70184885059955 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48889376792433986) - present_state_Q ( -0.9526747552897405)) * f1( 0.022084492676998548)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.48889376792433986) - present_state_Q (-0.9526747552897405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.7100250950643 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2569294455874769) - present_state_Q ( -0.2569294455874769)) * f1( 0.005344454788626207)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.2569294455874769) - present_state_Q (-0.2569294455874769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.73353034543085 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2660131739922789) - present_state_Q ( -0.8685557840633116)) * f1( 0.01967413218605196)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2660131739922789) - present_state_Q (-0.8685557840633116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.7541478413883 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15973472370483785) - present_state_Q ( -0.7586554699608267)) * f1( 0.01711484180493223)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15973472370483785) - present_state_Q (-0.7586554699608267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.772052700011955 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2236855911593916) - present_state_Q ( -0.7676313385577223)) * f1( 0.014866232406625737)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2236855911593916) - present_state_Q (-0.7676313385577223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.79859326745998 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1791731369515814) - present_state_Q ( -1.023572347596077)) * f1( 0.022523335831350244)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1791731369515814) - present_state_Q (-1.023572347596077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.805543231912566 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22186215709296944) - present_state_Q ( -0.22186215709296944)) * f1( 0.005520416443027991)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.22186215709296944) - present_state_Q (-0.22186215709296944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.826846351022404 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.309682695114999) - present_state_Q ( -0.9150123721734005)) * f1( 0.017893965519325816)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.309682695114999) - present_state_Q (-0.9150123721734005)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.84871572173212 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1932608584289109) - present_state_Q ( -0.4278268116852082)) * f1( 0.01907119951576223)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.1932608584289109) - present_state_Q (-0.4278268116852082)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.863641451078166 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10881709357604151) - present_state_Q ( -0.7145001166004288)) * f1( 0.013359817387896784)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.10881709357604151) - present_state_Q (-0.7145001166004288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.89016756004597 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31094420354146524) - present_state_Q ( -0.5463344172595137)) * f1( 0.023349452763828188)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.31094420354146524) - present_state_Q (-0.5463344172595137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.8987228088831 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09146486066850454) - present_state_Q ( -0.09146486066850454)) * f1( 0.007254264757145218)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.09146486066850454) - present_state_Q (-0.09146486066850454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.927279259568955 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3457383155806325) - present_state_Q ( -1.1979215617528876)) * f1( 0.026657437516562882)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.3457383155806325) - present_state_Q (-1.1979215617528876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.954241608465864 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2793868396897775) - present_state_Q ( -1.1326191309018643)) * f1( 0.025032252650497723)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.2793868396897775) - present_state_Q (-1.1326191309018643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.96460102647697 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2881962705620236) - present_state_Q ( -0.2881962705620236)) * f1( 0.0096791387841126)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.2881962705620236) - present_state_Q (-0.2881962705620236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -42.98870010071457 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43000494147892504) - present_state_Q ( -1.0649883883571534)) * f1( 0.02424400715349186)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.43000494147892504) - present_state_Q (-1.0649883883571534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.013124078205685 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5821762303120118) - present_state_Q ( -1.1671110745125972)) * f1( 0.02478757774803619)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.5821762303120118) - present_state_Q (-1.1671110745125972)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.03051840698204 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17152511019025285) - present_state_Q ( -0.7774785561727002)) * f1( 0.018727010893332022)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.17152511019025285) - present_state_Q (-0.7774785561727002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.05004300928632 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22931327494056766) - present_state_Q ( -1.0738128521115196)) * f1( 0.024151286024350082)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.22931327494056766) - present_state_Q (-1.0738128521115196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.06320241106998 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2351041967074084) - present_state_Q ( -0.8358406282613758)) * f1( 0.015811187287584325)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.2351041967074084) - present_state_Q (-0.8358406282613758)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.0815602414515 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3998360139433561) - present_state_Q ( -0.9808949325341356)) * f1( 0.022403277195578143)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3998360139433561) - present_state_Q (-0.9808949325341356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.100676751871326 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3680650521199443) - present_state_Q ( -1.22567992021081)) * f1( 0.027182004119972486)
w2 ( -41.12017995706623 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.3680650521199443) - present_state_Q (-1.22567992021081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.100993626463904 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.487370431167323) - present_state_Q ( -8.487370431167323)) * f1( 0.0054350381275673895)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -8.487370431167323) - present_state_Q (-8.487370431167323)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -43.11403843135079 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3038781116535827) - present_state_Q ( -0.8834279479169734)) * f1( 0.02020853161938724)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.3038781116535827) - present_state_Q (-0.8834279479169734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.129741453108295 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2836362825501479) - present_state_Q ( -1.1425093214409645)) * f1( 0.02535202020071116)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.2836362825501479) - present_state_Q (-1.1425093214409645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.133697901202574 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08294054317469862) - present_state_Q ( -0.08294054317469862)) * f1( 0.0054696243151669465)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.08294054317469862) - present_state_Q (-0.08294054317469862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.13699748442692 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26016608879456127) - present_state_Q ( -0.26016608879456127)) * f1( 0.00535605652412294)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.26016608879456127) - present_state_Q (-0.26016608879456127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.15019440356754 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25688102181564565) - present_state_Q ( -1.119094555014488)) * f1( 0.024894143953998346)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.25688102181564565) - present_state_Q (-1.119094555014488)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.16371974468387 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30670472126156706) - present_state_Q ( -1.1652972617611934)) * f1( 0.02571360587775465)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.30670472126156706) - present_state_Q (-1.1652972617611934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.17746166639505 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5389367587156716) - present_state_Q ( -1.1490174585961235)) * f1( 0.025930615380497297)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.5389367587156716) - present_state_Q (-1.1490174585961235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.18634550270462 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29087923892970535) - present_state_Q ( -0.33284057590693933)) * f1( 0.014585501101565368)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.29087923892970535) - present_state_Q (-0.33284057590693933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.19585419277764 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1929050784413003) - present_state_Q ( -0.8004571859892402)) * f1( 0.01693910733238156)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1929050784413003) - present_state_Q (-0.8004571859892402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.20392104012181 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2449445365954225) - present_state_Q ( -0.7620618357184468)) * f1( 0.01425980425606217)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.2449445365954225) - present_state_Q (-0.7620618357184468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.213152038310774 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4008822745286503) - present_state_Q ( -0.41994055650180956)) * f1( 0.01534722132250438)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.4008822745286503) - present_state_Q (-0.41994055650180956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.218962885484146 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14683246530852173) - present_state_Q ( -0.14683246530852173)) * f1( 0.009278840088527298)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.14683246530852173) - present_state_Q (-0.14683246530852173)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -43.222713916835545 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12475966249920735) - present_state_Q ( -0.12475966249920735)) * f1( 0.006986696330031954)
w2 ( -41.13184039385227 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12475966249920735) - present_state_Q (-0.12475966249920735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.018719731109728 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0353553390593273) - present_state_Q ( -1.0353553390593273)) * f1( 0.007071067811865476)
w2 ( -5.529473952387117 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -1.0353553390593273) - present_state_Q (-1.0353553390593273)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.037343753150242 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1273226080539975) - present_state_Q ( -1.1799051296853265)) * f1( 0.007071067811865476)
w2 ( -6.056240843499703 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -1.1273226080539975) - present_state_Q (-1.1799051296853265)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.1034424769992155 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.3144161165236077) - present_state_Q ( -1.339466736208689)) * f1( 0.025230935500300828)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -1.3144161165236077) - present_state_Q (-1.339466736208689)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.1725370318812125 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09088083096176196) - present_state_Q ( -0.1289534687907326)) * f1( 0.02619983381273695)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.09088083096176196) - present_state_Q (-0.1289534687907326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.279078637975795 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1288762609406993) - present_state_Q ( -0.22117527510603707)) * f1( 0.040535219665421424)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.1288762609406993) - present_state_Q (-0.22117527510603707)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.2833112091364995 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052875842781471324) - present_state_Q ( -0.036139228982508084)) * f1( 0.001784344297151793)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.052875842781471324) - present_state_Q (-0.036139228982508084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.299573757835913 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07216328526161506) - present_state_Q ( -0.0029633120085604276)) * f1( 0.006845745528876092)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.07216328526161506) - present_state_Q (-0.0029633120085604276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.315148043517929 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06756690428966436) - present_state_Q ( -0.007913999221939702)) * f1( 0.00681982742615669)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.06756690428966436) - present_state_Q (-0.007913999221939702)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.330508414070694 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07082894717277319) - present_state_Q ( -0.004398870714593953)) * f1( 0.007005199948601204)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.07082894717277319) - present_state_Q (-0.004398870714593953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.345723396730272 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0661167326268741) - present_state_Q ( -0.010075886911869749)) * f1( 0.006940840999037891)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.0661167326268741) - present_state_Q (-0.010075886911869749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.36148952227306 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05574686793933528) - present_state_Q ( -0.026185870811295756)) * f1( 0.007197894228098892)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.05574686793933528) - present_state_Q (-0.026185870811295756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.393319698156227 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07888417656328571) - present_state_Q ( -0.11420894763372039)) * f1( 0.015226415324700352)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.07888417656328571) - present_state_Q (-0.11420894763372039)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.412004569331114 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026586664121815883) - present_state_Q ( -0.06396141212761061)) * f1( 0.008918965791263207)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.026586664121815883) - present_state_Q (-0.06396141212761061)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.4436764496959915 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04753485872172706) - present_state_Q ( -0.09087415802823516)) * f1( 0.015136066610535794)
w2 ( -6.58019068949876 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.04753485872172706) - present_state_Q (-0.09087415802823516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.488977382400113 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4129156322241088) - present_state_Q ( -1.4465377893766083)) * f1( 0.02298880353244514)
w2 ( -6.974303698818151 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -1.4129156322241088) - present_state_Q (-1.4465377893766083)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.527125852295682 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.085575968311491) - present_state_Q ( -0.10642693978511185)) * f1( 0.01998767902041266)
w2 ( -6.974303698818151 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.085575968311491) - present_state_Q (-0.10642693978511185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.531822012739026 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08724691600654595) - present_state_Q ( -0.021960165926354493)) * f1( 0.002572236557865474)
w2 ( -7.339445888325093 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.08724691600654595) - present_state_Q (-0.021960165926354493)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.574918660328639 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5414667662393369) - present_state_Q ( -1.5776183166515045)) * f1( 0.025581392343551106)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -1.5414667662393369) - present_state_Q (-1.5776183166515045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.6129452704719816 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09182271931165997) - present_state_Q ( -0.13163631894433114)) * f1( 0.02206439926635915)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09182271931165997) - present_state_Q (-0.13163631894433114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.654779740596801 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03518231310376265) - present_state_Q ( -0.10662265399037542)) * f1( 0.02424663329016241)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03518231310376265) - present_state_Q (-0.10662265399037542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.690605205756103 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07178116114813948) - present_state_Q ( -0.12318916758660528)) * f1( 0.02077944791426758)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.07178116114813948) - present_state_Q (-0.12318916758660528)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.7196445908615425 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09544714941951782) - present_state_Q ( -0.1213190914790378)) * f1( 0.018834705953040334)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.09544714941951782) - present_state_Q (-0.1213190914790378)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.740870855969934 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0974424163534629) - present_state_Q ( -0.0974424163534629)) * f1( 0.014610008923431598)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0974424163534629) - present_state_Q (-0.0974424163534629)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.774842243507555 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051378236767826393) - present_state_Q ( -0.09914243341348926)) * f1( 0.02339261312426694)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.051378236767826393) - present_state_Q (-0.09914243341348926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.797665546336357 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06341633594938301) - present_state_Q ( -0.06384512046975417)) * f1( 0.015676667020755077)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.06341633594938301) - present_state_Q (-0.06384512046975417)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.813630077807585 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08381761999767404) - present_state_Q ( -0.08697600965691678)) * f1( 0.010981483696923443)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.08381761999767404) - present_state_Q (-0.08697600965691678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.844591298246975 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0530459006365871) - present_state_Q ( -0.0647754208383383)) * f1( 0.02126924154379621)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0530459006365871) - present_state_Q (-0.0647754208383383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.857660682885092 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0682216435453662) - present_state_Q ( -0.01668013598841438)) * f1( 0.008947700010201188)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0682216435453662) - present_state_Q (-0.01668013598841438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.885963386014774 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04959427713509779) - present_state_Q ( -0.06106678330461884)) * f1( 0.020739668941467237)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.04959427713509779) - present_state_Q (-0.06106678330461884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.899392954657938 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06484443417498756) - present_state_Q ( -0.020430223473832117)) * f1( 0.009810615945795936)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.06484443417498756) - present_state_Q (-0.020430223473832117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.903880786970089 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0983621911021332) - present_state_Q ( -0.03647921710503004)) * f1( 0.0035163938784361637)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.0983621911021332) - present_state_Q (-0.03647921710503004)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.935013209764997 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0541160752776771) - present_state_Q ( -0.10665221998821944)) * f1( 0.024536901563368782)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.0541160752776771) - present_state_Q (-0.10665221998821944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.955883736042289 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09084635297042185) - present_state_Q ( -0.10993068996105444)) * f1( 0.01644851504286173)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09084635297042185) - present_state_Q (-0.10993068996105444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.981036275019331 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08779464472373881) - present_state_Q ( -0.13532236559341457)) * f1( 0.02140790775135404)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08779464472373881) - present_state_Q (-0.13532236559341457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.005771614123573 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08084179299491392) - present_state_Q ( -0.12325675644749026)) * f1( 0.02103246502470071)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08084179299491392) - present_state_Q (-0.12325675644749026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.029883557369515 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0827966329202193) - present_state_Q ( -0.12984686434506548)) * f1( 0.02051354530370846)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.0827966329202193) - present_state_Q (-0.12984686434506548)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.031319145141565 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05575448131136656) - present_state_Q ( -0.042213106403394256)) * f1( 0.0012125832215975612)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.05575448131136656) - present_state_Q (-0.042213106403394256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.039633451841323 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07842154298037488) - present_state_Q ( -0.007088772257826045)) * f1( 0.007000650344533247)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07842154298037488) - present_state_Q (-0.007088772257826045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.047870744112296 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08254497934490333) - present_state_Q ( -0.0037891470518449303)) * f1( 0.006933637063370807)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08254497934490333) - present_state_Q (-0.0037891470518449303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.0560605117324595 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07719537612338023) - present_state_Q ( -0.00869931396119363)) * f1( 0.006896794841267937)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07719537612338023) - present_state_Q (-0.00869931396119363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.063747325324291 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07980779180131972) - present_state_Q ( -0.008179697307195732)) * f1( 0.0064728198527566096)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07980779180131972) - present_state_Q (-0.008179697307195732)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.071461796599734 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08307758031956237) - present_state_Q ( -0.002848040788886767)) * f1( 0.0064930156101888434)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08307758031956237) - present_state_Q (-0.002848040788886767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.079198260721708 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07753125595288575) - present_state_Q ( -0.010239067709444268)) * f1( 0.006515883824340246)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07753125595288575) - present_state_Q (-0.010239067709444268)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.087285229955689 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08048743922721456) - present_state_Q ( -0.005722693267805837)) * f1( 0.0068083305353627615)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08048743922721456) - present_state_Q (-0.005722693267805837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.095330336258781 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08327628928403623) - present_state_Q ( -0.003100160489159341)) * f1( 0.006771432555152749)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08327628928403623) - present_state_Q (-0.003100160489159341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.103375447081152 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07775906719733991) - present_state_Q ( -0.010868753899720826)) * f1( 0.006776181767612962)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07775906719733991) - present_state_Q (-0.010868753899720826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.1114130646836315 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08049982709503237) - present_state_Q ( -0.0063185009517233)) * f1( 0.006767120674780325)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08049982709503237) - present_state_Q (-0.0063185009517233)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.139611383129173 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11277436077265851) - present_state_Q ( -0.15438475324886058)) * f1( 0.02403412528134617)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.11277436077265851) - present_state_Q (-0.15438475324886058)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.1520149378799305 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09008836563633975) - present_state_Q ( -0.06669754469970392)) * f1( 0.010495445491894784)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.09008836563633975) - present_state_Q (-0.06669754469970392)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.165556469921792 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0852126361610613) - present_state_Q ( -0.0852126361610613)) * f1( 0.012439954654303365)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.0852126361610613) - present_state_Q (-0.0852126361610613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.175550693573866 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06381597352798846) - present_state_Q ( -0.04131617721725341)) * f1( 0.009146128416526232)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06381597352798846) - present_state_Q (-0.04131617721725341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.201174964016244 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12497660173786661) - present_state_Q ( -0.14536032397104554)) * f1( 0.02366188544597776)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12497660173786661) - present_state_Q (-0.14536032397104554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.2026301039261185 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06192382198686124) - present_state_Q ( -0.045730260199556456)) * f1( 0.0014538094061395438)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.06192382198686124) - present_state_Q (-0.045730260199556456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.210041874529644 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08320916452411416) - present_state_Q ( -0.006403862198774422)) * f1( 0.0073744508846979645)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.08320916452411416) - present_state_Q (-0.006403862198774422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.21733816531982 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07774387511443123) - present_state_Q ( -0.010110929080459083)) * f1( 0.007262626207643236)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.07774387511443123) - present_state_Q (-0.010110929080459083)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.224746756763655 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08184497888749079) - present_state_Q ( -0.006746457841559507)) * f1( 0.007371639066696069)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.08184497888749079) - present_state_Q (-0.006746457841559507)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.232027732209612 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07627199952980482) - present_state_Q ( -0.012062292605405665)) * f1( 0.0072488957263695235)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.07627199952980482) - present_state_Q (-0.012062292605405665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.239505978537154 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07979043686652042) - present_state_Q ( -0.00839255950651559)) * f1( 0.007442317641213529)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.07979043686652042) - present_state_Q (-0.00839255950651559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.246847442546152 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08367119131288056) - present_state_Q ( -0.006065845140608544)) * f1( 0.007304219068242617)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.08367119131288056) - present_state_Q (-0.006065845140608544)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.273428530009597 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12503856742662278) - present_state_Q ( -0.17414898171673343)) * f1( 0.026884765413700704)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.12503856742662278) - present_state_Q (-0.17414898171673343)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.29062062017342 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0923564869223825) - present_state_Q ( -0.12511232880101197)) * f1( 0.019061455079311413)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0923564869223825) - present_state_Q (-0.12511232880101197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.312411040735293 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11277802680257462) - present_state_Q ( -0.1596014849778695)) * f1( 0.02424700922971445)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.11277802680257462) - present_state_Q (-0.1596014849778695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.326608934851124 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10060730074004985) - present_state_Q ( -0.12153621799712833)) * f1( 0.015734010280288428)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10060730074004985) - present_state_Q (-0.12153621799712833)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.335226862341401 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07970658132268142) - present_state_Q ( -0.0439777455372588)) * f1( 0.009471118639278082)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07970658132268142) - present_state_Q (-0.0439777455372588)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.351803405311989 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10096792771811822) - present_state_Q ( -0.13882017563493906)) * f1( 0.018405192547371777)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10096792771811822) - present_state_Q (-0.13882017563493906)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.3688187938110365 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0881417935662162) - present_state_Q ( -0.1263621567218455)) * f1( 0.01886903641177294)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0881417935662162) - present_state_Q (-0.1263621567218455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.388571012603907 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0782959873703317) - present_state_Q ( -0.15274395788498962)) * f1( 0.021970691540437515)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0782959873703317) - present_state_Q (-0.15274395788498962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.410053639259566 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03944647079778434) - present_state_Q ( -0.12930700163521394)) * f1( 0.02384359533537935)
w2 ( -7.6763833545179985 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03944647079778434) - present_state_Q (-0.12930700163521394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.42796616902209 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13917309486292415) - present_state_Q ( -1.6916174508119997)) * f1( 0.02401957253268346)
w2 ( -7.825532801188213 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.13917309486292415) - present_state_Q (-1.6916174508119997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.447491457302305 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06109845047077933) - present_state_Q ( -0.14829805654895273)) * f1( 0.02171168958876298)
w2 ( -7.825532801188213 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06109845047077933) - present_state_Q (-0.14829805654895273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.468176185643867 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09599226291052562) - present_state_Q ( -0.17899446242953718)) * f1( 0.023070758720485588)
w2 ( -7.825532801188213 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09599226291052562) - present_state_Q (-0.17899446242953718)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.485268824476135 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.248553890476924) - present_state_Q ( -3.3077624581257306)) * f1( 0.027782674681540304)
w2 ( -8.071623417418547 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -3.248553890476924) - present_state_Q (-3.3077624581257306)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -6.48913659723156 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11218811382183966) - present_state_Q ( -0.020582761596265254)) * f1( 0.005299208798231519)
w2 ( -8.217598898011648 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11218811382183966) - present_state_Q (-0.020582761596265254)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.499021571270094 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7358735760974724) - present_state_Q ( -1.761119703737969)) * f1( 0.01727959358049566)
w2 ( -8.332011010686466 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -1.7358735760974724) - present_state_Q (-1.761119703737969)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.502819185782331 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.7536830937598515) - present_state_Q ( -1.702440587848623)) * f1( 0.007801903870487716)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -1.7536830937598515) - present_state_Q (-1.702440587848623)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.517302831533467 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11657501292391124) - present_state_Q ( -0.1828945837726041)) * f1( 0.02727685529814869)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11657501292391124) - present_state_Q (-0.1828945837726041)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.531389076196521 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12563228500409984) - present_state_Q ( -0.17817096784464262)) * f1( 0.026500340395956883)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12563228500409984) - present_state_Q (-0.17817096784464262)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.541242591446313 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12078176972258715) - present_state_Q ( -0.1392194570479859)) * f1( 0.018404155413927512)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12078176972258715) - present_state_Q (-0.1392194570479859)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5545588442115665 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0772788052563087) - present_state_Q ( -0.12361500983629829)) * f1( 0.02481960070701867)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0772788052563087) - present_state_Q (-0.12361500983629829)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.566735307963338 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12652911750130508) - present_state_Q ( -0.14714892310005825)) * f1( 0.022774186831565683)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12652911750130508) - present_state_Q (-0.14714892310005825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.576727030204497 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06399237493106766) - present_state_Q ( -0.07083882912913796)) * f1( 0.018446265889934428)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06399237493106766) - present_state_Q (-0.07083882912913796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.580942895667994 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07982707805172895) - present_state_Q ( -0.013206190085208604)) * f1( 0.00769897342797748)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07982707805172895) - present_state_Q (-0.013206190085208604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.585205957515226 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08420206032264312) - present_state_Q ( -0.009787111441562824)) * f1( 0.007779683905219086)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08420206032264312) - present_state_Q (-0.009787111441562824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.590300444339182 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07110078719730024) - present_state_Q ( -0.04880554920242432)) * f1( 0.009365884689015403)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07110078719730024) - present_state_Q (-0.04880554920242432)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.601928236080422 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08204069271079677) - present_state_Q ( -0.14883203802890763)) * f1( 0.02177295206407962)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08204069271079677) - present_state_Q (-0.14883203802890763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.612907267264661 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06412499556787181) - present_state_Q ( -0.06412499556787181)) * f1( 0.0202438496629289)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06412499556787181) - present_state_Q (-0.06412499556787181)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.618541520363884 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0822668366688304) - present_state_Q ( -0.015223303240149519)) * f1( 0.010292552260733109)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0822668366688304) - present_state_Q (-0.015223303240149519)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.623134153763777 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07643849912103699) - present_state_Q ( -0.017526070032106645)) * f1( 0.008394164991015715)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07643849912103699) - present_state_Q (-0.017526070032106645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.629192013629395 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11540177971052555) - present_state_Q ( -0.11540177971052555)) * f1( 0.011265738081426368)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11540177971052555) - present_state_Q (-0.11540177971052555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.640397122083999 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1356021647001206) - present_state_Q ( -0.1356021647001206)) * f1( 0.02090871391927104)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1356021647001206) - present_state_Q (-0.1356021647001206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.65256444338493 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1361584903275741) - present_state_Q ( -0.1904291015203611)) * f1( 0.027711114518308377)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1361584903275741) - present_state_Q (-0.1904291015203611)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.660755366415999 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11170763320763619) - present_state_Q ( -0.12556415458315315)) * f1( 0.018393371484327525)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11170763320763619) - present_state_Q (-0.12556415458315315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.662378979259878 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11153729384820395) - present_state_Q ( -0.03550745605392208)) * f1( 0.003573695249006129)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11153729384820395) - present_state_Q (-0.03550745605392208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.670705060930058 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11998742118532546) - present_state_Q ( -0.11999997806029089)) * f1( 0.018670081784344032)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11998742118532546) - present_state_Q (-0.11999997806029089)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.68200014865892 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10948452368846318) - present_state_Q ( -0.1759077835384006)) * f1( 0.02565533707348821)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.10948452368846318) - present_state_Q (-0.1759077835384006)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.693753717077418 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13235968083695657) - present_state_Q ( -0.18754587031204945)) * f1( 0.026753537728443643)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13235968083695657) - present_state_Q (-0.18754587031204945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.701682099802933 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1275914867656572) - present_state_Q ( -0.13639169377360294)) * f1( 0.017840831103200627)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1275914867656572) - present_state_Q (-0.13639169377360294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.707384050266486 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0533079720397158) - present_state_Q ( -0.0533079720397158)) * f1( 0.015811994690980852)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0533079720397158) - present_state_Q (-0.0533079720397158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.710686193160569 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0742422284775175) - present_state_Q ( -0.020767275791260392)) * f1( 0.009070012275660991)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0742422284775175) - present_state_Q (-0.020767275791260392)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.715883355382623 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0596093241903964) - present_state_Q ( -0.0596093241903964)) * f1( 0.014434875260698775)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0596093241903964) - present_state_Q (-0.0596093241903964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.719438317251674 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0763110948366073) - present_state_Q ( -0.024073274645605307)) * f1( 0.009772750313038732)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0763110948366073) - present_state_Q (-0.024073274645605307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.722353313002987 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08422154415493346) - present_state_Q ( -0.012254709584986518)) * f1( 0.00798577243805447)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08422154415493346) - present_state_Q (-0.012254709584986518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.725238275422079 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0864292547202056) - present_state_Q ( -0.010430196595730645)) * f1( 0.00789906869514799)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0864292547202056) - present_state_Q (-0.010430196595730645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.725789208074073 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0665811540013434) - present_state_Q ( -0.04465473244145638)) * f1( 0.001523566442209186)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0665811540013434) - present_state_Q (-0.04465473244145638)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.728219359484927 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0918044337983318) - present_state_Q ( -0.0033132093606684497)) * f1( 0.00663987365394185)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0918044337983318) - present_state_Q (-0.0033132093606684497)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.729657438489822 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11656706459629555) - present_state_Q ( -0.03290809957967413)) * f1( 0.003958577513374766)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11656706459629555) - present_state_Q (-0.03290809957967413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.738827409459961 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13587703035714915) - present_state_Q ( -0.18316211879973263)) * f1( 0.02631650236482825)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13587703035714915) - present_state_Q (-0.18316211879973263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.740810991783777 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09705128264475708) - present_state_Q ( -0.02950976139501763)) * f1( 0.005458002305479758)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09705128264475708) - present_state_Q (-0.02950976139501763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.742393336641413 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11747841754442956) - present_state_Q ( -0.04012492632141174)) * f1( 0.004364256254907486)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11747841754442956) - present_state_Q (-0.04012492632141174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.747899975940981 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06501091037402097) - present_state_Q ( -0.06988064324198091)) * f1( 0.015335882969174906)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06501091037402097) - present_state_Q (-0.06988064324198091)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.751450140356383 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08101368551391029) - present_state_Q ( -0.02183149543936385)) * f1( 0.00975229106125204)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08101368551391029) - present_state_Q (-0.02183149543936385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.756856221867088 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08880407075484717) - present_state_Q ( -0.11152812933220495)) * f1( 0.01522230426423222)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08880407075484717) - present_state_Q (-0.11152812933220495)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.760393535799668 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07154494525986131) - present_state_Q ( -0.026100360380503878)) * f1( 0.009730932933598972)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07154494525986131) - present_state_Q (-0.026100360380503878)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.765533988975405 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10429559069815157) - present_state_Q ( -0.12158616109175778)) * f1( 0.014509117395265968)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10429559069815157) - present_state_Q (-0.12158616109175778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7706448878849175 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06246654216956995) - present_state_Q ( -0.06246654216956995)) * f1( 0.014205428756578278)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06246654216956995) - present_state_Q (-0.06246654216956995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7731715701531945 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08823283241516092) - present_state_Q ( -0.007969365697176843)) * f1( 0.006913093195773644)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08823283241516092) - present_state_Q (-0.007969365697176843)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.775682110542395 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09262810537175589) - present_state_Q ( -0.0038377068975667464)) * f1( 0.006860348247615396)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09262810537175589) - present_state_Q (-0.0038377068975667464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.778178323167531 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0865166848638861) - present_state_Q ( -0.009901252264296796)) * f1( 0.006833660106297263)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0865166848638861) - present_state_Q (-0.009901252264296796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7812732847521495 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08031838689359684) - present_state_Q ( -0.015550329174972267)) * f1( 0.008487367952494667)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08031838689359684) - present_state_Q (-0.015550329174972267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.784084574807813 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0888385395238451) - present_state_Q ( -0.009064336290698409)) * f1( 0.007693967809413405)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0888385395238451) - present_state_Q (-0.009064336290698409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.786829581800884 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08277655709868847) - present_state_Q ( -0.013174053020847395)) * f1( 0.00752227230528849)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08277655709868847) - present_state_Q (-0.013174053020847395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.78962059952222 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08704076075364127) - present_state_Q ( -0.009479879545001796)) * f1( 0.007639731001266926)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08704076075364127) - present_state_Q (-0.009479879545001796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.796300095485742 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10824511100888924) - present_state_Q ( -0.12358142546207272)) * f1( 0.01886164173328596)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10824511100888924) - present_state_Q (-0.12358142546207272)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.801598108446474 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.106569893306082) - present_state_Q ( -0.116609351241162)) * f1( 0.01493190195295948)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.106569893306082) - present_state_Q (-0.116609351241162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.807279726798541 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06602435616984628) - present_state_Q ( -0.06638373608910872)) * f1( 0.015807355567901537)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06602435616984628) - present_state_Q (-0.06638373608910872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.811035506627066 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08461962420560101) - present_state_Q ( -0.015717155552058555)) * f1( 0.010298797903576289)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08461962420560101) - present_state_Q (-0.015717155552058555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.814096238936212 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07859553571504779) - present_state_Q ( -0.018097910600343433)) * f1( 0.008399765569524882)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07859553571504779) - present_state_Q (-0.018097910600343433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.821635108438129 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.050773087473291297) - present_state_Q ( -0.07486531561714649)) * f1( 0.021033143153501126)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.050773087473291297) - present_state_Q (-0.07486531561714649)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.825150897317537 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07090196669335731) - present_state_Q ( -0.025701063009923506)) * f1( 0.0096708276092939)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07090196669335731) - present_state_Q (-0.025701063009923506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.82852384138503 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0747362040242625) - present_state_Q ( -0.022099095478641178)) * f1( 0.009267746713808435)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0747362040242625) - present_state_Q (-0.022099095478641178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8345470820238425 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10001611336165725) - present_state_Q ( -0.11145810715829216)) * f1( 0.016954398062819766)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10001611336165725) - present_state_Q (-0.11145810715829216)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.837782177390915 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07987538213770413) - present_state_Q ( -0.01792999722842012)) * f1( 0.008877560478822709)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07987538213770413) - present_state_Q (-0.01792999722842012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.845300602200692 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05487233190358806) - present_state_Q ( -0.073475196708112)) * f1( 0.02096557427005184)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.05487233190358806) - present_state_Q (-0.073475196708112)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.848810732616504 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07383831180469864) - present_state_Q ( -0.023886107103833445)) * f1( 0.009649666086649149)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07383831180469864) - present_state_Q (-0.023886107103833445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.855686921964355 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09602209965222339) - present_state_Q ( -0.12927338605293937)) * f1( 0.01945505206265264)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09602209965222339) - present_state_Q (-0.12927338605293937)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.860643386449621 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04678972885177036) - present_state_Q ( -0.04678972885177036)) * f1( 0.01372237485653598)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.04678972885177036) - present_state_Q (-0.04678972885177036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.863812251973812 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08013258323744238) - present_state_Q ( -0.01696863250784297)) * f1( 0.008693461598832938)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08013258323744238) - present_state_Q (-0.01696863250784297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.866698184230005 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08919663759740919) - present_state_Q ( -0.01046311431751918)) * f1( 0.007901196651689465)
w2 ( -8.429361979764723 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08919663759740919) - present_state_Q (-0.01046311431751918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.869505811640423 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08308368106856562) - present_state_Q ( -0.014036201663138145)) * f1( 0.007695627382623625)
w2 ( -8.502328802892288 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08308368106856562) - present_state_Q (-0.014036201663138145)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.871025930292999 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0875421499380843) - present_state_Q ( -1.7108743451673523)) * f1( 0.007787697126254306)
w2 ( -8.541367780087509 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0875421499380843) - present_state_Q (-1.7108743451673523)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.875291789861253 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.787226770686502) - present_state_Q ( -1.787226770686502)) * f1( 0.02085418837571422)
w2 ( -8.582279078013842 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -1.787226770686502) - present_state_Q (-1.787226770686502)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.878294512216985 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8290975048662186) - present_state_Q ( -1.844880241604083)) * f1( 0.015073162082675411)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -1.8290975048662186) - present_state_Q (-1.844880241604083)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.881707208999207 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10399615855337536) - present_state_Q ( -0.10399615855337536)) * f1( 0.012892914782014231)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.10399615855337536) - present_state_Q (-0.10399615855337536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.885491417243481 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06489776194878288) - present_state_Q ( -0.06489776194878288)) * f1( 0.014108894181472351)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06489776194878288) - present_state_Q (-0.06489776194878288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.887506434176737 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0729063152119754) - present_state_Q ( -0.03506002270101075)) * f1( 0.0074278606697382255)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0729063152119754) - present_state_Q (-0.03506002270101075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.891404831496544 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10173346850467262) - present_state_Q ( -0.1452004023334484)) * f1( 0.014962043304773303)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.10173346850467262) - present_state_Q (-0.1452004023334484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.893888947853985 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08075607196903117) - present_state_Q ( -0.020303689830597178)) * f1( 0.009104918137194606)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08075607196903117) - present_state_Q (-0.020303689830597178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.899734499438638 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06554774532304831) - present_state_Q ( -0.07929561546147286)) * f1( 0.021911416515010083)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06554774532304831) - present_state_Q (-0.07929561546147286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.902068574847645 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08625123848063455) - present_state_Q ( -0.014643659342322608)) * f1( 0.008535553355392996)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08625123848063455) - present_state_Q (-0.014643659342322608)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9043142544940075 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08010835326465009) - present_state_Q ( -0.017659759448623726)) * f1( 0.008223213331836178)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08010835326465009) - present_state_Q (-0.017659759448623726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.906581118550917 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08460289783404734) - present_state_Q ( -0.013991312828411918)) * f1( 0.008288288424379939)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08460289783404734) - present_state_Q (-0.013991312828411918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.908776283823708 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09007197333617907) - present_state_Q ( -0.011226464921974417)) * f1( 0.008016430776548351)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.09007197333617907) - present_state_Q (-0.011226464921974417)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.912291974586954 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14295549288947) - present_state_Q ( -0.14295549288947)) * f1( 0.020700326441048297)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14295549288947) - present_state_Q (-0.14295549288947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9156562797965275 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09158016279796426) - present_state_Q ( -0.13284203176262752)) * f1( 0.019751103781012393)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09158016279796426) - present_state_Q (-0.13284203176262752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.91701105871624 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08461028858435617) - present_state_Q ( -0.013243030554652832)) * f1( 0.007434638858858403)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08461028858435617) - present_state_Q (-0.013243030554652832)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.918393865733773 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08882082159899421) - present_state_Q ( -0.009383251530319874)) * f1( 0.007570664045129865)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08882082159899421) - present_state_Q (-0.009383251530319874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.919749120125673 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09126195598691889) - present_state_Q ( -0.00764708291184327)) * f1( 0.007411781822281422)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09126195598691889) - present_state_Q (-0.00764708291184327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.921077016871668 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0862634698230894) - present_state_Q ( -0.01239279662984025)) * f1( 0.007283058294159807)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0862634698230894) - present_state_Q (-0.01239279662984025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9223913978752565 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08991422473185268) - present_state_Q ( -0.007964585845009444)) * f1( 0.007190026838111392)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08991422473185268) - present_state_Q (-0.007964585845009444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9236899025288015 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0934152203426118) - present_state_Q ( -0.005451664533670574)) * f1( 0.0070920715637151875)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0934152203426118) - present_state_Q (-0.005451664533670574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.941745464656184 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04895788080910943) - present_state_Q ( -0.04895788080910943)) * f1( 0.007071067811865476)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.04895788080910943) - present_state_Q (-0.04895788080910943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.959813065349611 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08411749286527755) - present_state_Q ( -0.03544873864209592)) * f1( 0.007071067811865476)
w2 ( -8.622121047990184 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.08411749286527755) - present_state_Q (-0.03544873864209592)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.973660695465452 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8237334322980816) - present_state_Q ( -1.7367944967187943)) * f1( 0.00576406697790157)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -1.8237334322980816) - present_state_Q (-1.7367944967187943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.986651492877024 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10704146956452701) - present_state_Q ( -0.033126960896478874)) * f1( 0.005474644268694535)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.10704146956452701) - present_state_Q (-0.033126960896478874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.039194370896491 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11103966802510648) - present_state_Q ( -0.16572097023065463)) * f1( 0.022266920951985677)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.11103966802510648) - present_state_Q (-0.16572097023065463)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.056534173577519 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03949550487475095) - present_state_Q ( -0.9732171983535965)) * f1( 0.007611125046968379)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.03949550487475095) - present_state_Q (-0.9732171983535965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.094093455069874 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10694392508473181) - present_state_Q ( -0.11694297871677915)) * f1( 0.017214683674814362)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.10694392508473181) - present_state_Q (-0.11694297871677915)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.112645058480552 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08352942155663115) - present_state_Q ( -0.017019118867690933)) * f1( 0.008464964833226793)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.08352942155663115) - present_state_Q (-0.017019118867690933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.1305795929529845 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08852714523196899) - present_state_Q ( -0.013352644505354543)) * f1( 0.008181845268442825)
w2 ( -9.102602283511244 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.08852714523196899) - present_state_Q (-0.013352644505354543)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.147953390167275 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09218790400761154) - present_state_Q ( -0.011097221295602283)) * f1( 0.007925085676054771)
w2 ( -9.541052993685495 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.09218790400761154) - present_state_Q (-0.011097221295602283)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.163433337699249 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08659352688557183) - present_state_Q ( -1.922947261011023)) * f1( 0.0077360569882329155)
w2 ( -9.941255514311194 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.08659352688557183) - present_state_Q (-1.922947261011023)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.2067346027145796 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021118130301629897) - present_state_Q ( -0.13615000754733966)) * f1( 0.023875459099753308)
w2 ( -9.941255514311194 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.021118130301629897) - present_state_Q (-0.13615000754733966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.227497561684775 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052464973433478994) - present_state_Q ( -0.1340816120576331)) * f1( 0.011445000531424797)
w2 ( -9.941255514311194 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.052464973433478994) - present_state_Q (-0.1340816120576331)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.239724833990181 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07860539746962664) - present_state_Q ( -0.07860539746962664)) * f1( 0.006718429110326655)
w2 ( -9.941255514311194 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.07860539746962664) - present_state_Q (-0.07860539746962664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.253461215547452 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0917971672474633) - present_state_Q ( -0.010918223383995443)) * f1( 0.007519118439668646)
w2 ( -10.306627643171465 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.0917971672474633) - present_state_Q (-0.010918223383995443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.2671906077494155 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09607098714293116) - present_state_Q ( -0.008483461637707176)) * f1( 0.007514115319619437)
w2 ( -10.672057014906454 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.09607098714293116) - present_state_Q (-0.008483461637707176)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.277044449283154 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.235550624098468) - present_state_Q ( -2.1390054378711314)) * f1( 0.0060250107324298354)
w2 ( -10.999154906390684 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -2.235550624098468) - present_state_Q (-2.1390054378711314)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.292931579978804 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.2794226996086824) - present_state_Q ( -2.22693879933795)) * f1( 0.009763868540749301)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -2.2794226996086824) - present_state_Q (-2.22693879933795)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.3053638122599995 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1182686126708775) - present_state_Q ( -0.0566906044808835)) * f1( 0.007181295149791577)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.1182686126708775) - present_state_Q (-0.0566906044808835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.333131771484841 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05998936691738689) - present_state_Q ( -0.09416846573405102)) * f1( 0.016079975031309524)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.05998936691738689) - present_state_Q (-0.09416846573405102)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.345529759978221 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06156929217054915) - present_state_Q ( -0.06364156767229101)) * f1( 0.007166739481206561)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.06156929217054915) - present_state_Q (-0.06364156767229101)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.365971466853828 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09178465005467153) - present_state_Q ( -0.09178465005467153)) * f1( 0.011833648685889769)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09178465005467153) - present_state_Q (-0.09178465005467153)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.380641730226554 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0854979020539168) - present_state_Q ( -0.019351481883074596)) * f1( 0.008457420370427033)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.0854979020539168) - present_state_Q (-0.019351481883074596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.395257992195845 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04891000970123997) - present_state_Q ( -0.07795602566614596)) * f1( 0.008456643522607497)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.04891000970123997) - present_state_Q (-0.07795602566614596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.640028601261952 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0874648661155414) - present_state_Q ( -0.10621613769227838)) * f1( 0.1418190690182638)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.0874648661155414) - present_state_Q (-0.10621613769227838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.653272783002183 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10097483103799708) - present_state_Q ( -0.009703611549810526)) * f1( 0.007630358167614671)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.10097483103799708) - present_state_Q (-0.009703611549810526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.666239543005722 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09429496113895026) - present_state_Q ( -0.013939731789727907)) * f1( 0.007472638757654333)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09429496113895026) - present_state_Q (-0.013939731789727907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.679382380377785 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09940960103273215) - present_state_Q ( -0.009884880872283964)) * f1( 0.007572118093071359)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09940960103273215) - present_state_Q (-0.009884880872283964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.692246441109524 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09258272606085284) - present_state_Q ( -0.016314577678954376)) * f1( 0.007414541919484358)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09258272606085284) - present_state_Q (-0.016314577678954376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.705457316995808 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09687904341024818) - present_state_Q ( -0.011951749879735721)) * f1( 0.007612335288519223)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09687904341024818) - present_state_Q (-0.011951749879735721)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7183877353532 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10251779157201824) - present_state_Q ( -0.0085147118335573)) * f1( 0.007449013515277561)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.10251779157201824) - present_state_Q (-0.0085147118335573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.731091788810266 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09575056574946537) - present_state_Q ( -0.013459083664238956)) * f1( 0.007320978904169948)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09575056574946537) - present_state_Q (-0.013459083664238956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.744012787275875 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10075017675049093) - present_state_Q ( -0.009186874312028265)) * f1( 0.007443950909543357)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.10075017675049093) - present_state_Q (-0.009186874312028265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7566905254728455 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09383797641332912) - present_state_Q ( -0.016149908108757636)) * f1( 0.007307027776212286)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09383797641332912) - present_state_Q (-0.016149908108757636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.769747962123867 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09804246923411253) - present_state_Q ( -0.011661974139282428)) * f1( 0.007523744881063737)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.09804246923411253) - present_state_Q (-0.011661974139282428)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.782549776558871 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10363550881103611) - present_state_Q ( -0.008067156897441636)) * f1( 0.007374689162931697)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.10363550881103611) - present_state_Q (-0.008067156897441636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.825542230882911 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12704781794735753) - present_state_Q ( -0.2177943899678287)) * f1( 0.026476086715633368)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.12704781794735753) - present_state_Q (-0.2177943899678287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.871781492258573 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11671713994079713) - present_state_Q ( -0.22978046612178565)) * f1( 0.028498420064827768)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.11671713994079713) - present_state_Q (-0.22978046612178565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.900958188456496 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09683785771404554) - present_state_Q ( -0.13376394589714422)) * f1( 0.017878720792128235)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.09683785771404554) - present_state_Q (-0.13376394589714422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.9536853064128765 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14808478093568234) - present_state_Q ( -0.2581875499189207)) * f1( 0.03254774153679524)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.14808478093568234) - present_state_Q (-0.2581875499189207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.960616966935601 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11614771456109388) - present_state_Q ( -0.0343416029096051)) * f1( 0.004470001194781526)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11614771456109388) - present_state_Q (-0.0343416029096051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.990631012545167 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10698436027322968) - present_state_Q ( -0.16498227443971636)) * f1( 0.019520683763786406)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.10698436027322968) - present_state_Q (-0.16498227443971636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.997890482797735 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12018133975500934) - present_state_Q ( -0.011621236845632228)) * f1( 0.004966568194865995)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.12018133975500934) - present_state_Q (-0.011621236845632228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.010771084085931 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09526353882028954) - present_state_Q ( -0.02130462653451169)) * f1( 0.008819612672872611)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.09526353882028954) - present_state_Q (-0.02130462653451169)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.023438286097068 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10052545642724293) - present_state_Q ( -0.0179109502544757)) * f1( 0.008671166444507213)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.10052545642724293) - present_state_Q (-0.0179109502544757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.03569572552218 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09331599225464172) - present_state_Q ( -0.020155612701752736)) * f1( 0.008392372567593951)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.09331599225464172) - present_state_Q (-0.020155612701752736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.047557180986004 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07985696964830911) - present_state_Q ( -0.046345683162528864)) * f1( 0.008136592010376668)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.07985696964830911) - present_state_Q (-0.046345683162528864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.06833779822792 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11284996780010556) - present_state_Q ( -0.16507722449271442)) * f1( 0.0143686365543834)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.11284996780010556) - present_state_Q (-0.16507722449271442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.082292961010351 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08828876897986068) - present_state_Q ( -0.0289378202810578)) * f1( 0.009560840710300337)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.08828876897986068) - present_state_Q (-0.0289378202810578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.093678191280514 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03654924404507614) - present_state_Q ( -0.08963029055091845)) * f1( 0.00783550911202979)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03654924404507614) - present_state_Q (-0.08963029055091845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.116126675962986 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06596212139301162) - present_state_Q ( -0.1372045669309898)) * f1( 0.015497031475999472)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.06596212139301162) - present_state_Q (-0.1372045669309898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.135168454177188 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1213677914315834) - present_state_Q ( -0.10366562390694165)) * f1( 0.013109886226225552)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.1213677914315834) - present_state_Q (-0.10366562390694165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.171720417903513 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1412460664729689) - present_state_Q ( -0.1412460664729689)) * f1( 0.025227119861648665)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.1412460664729689) - present_state_Q (-0.1412460664729689)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.197325123391604 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11467760096558707) - present_state_Q ( -0.18874631820727966)) * f1( 0.01773302174374309)
w2 ( -11.324581874796598 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.11467760096558707) - present_state_Q (-0.18874631820727966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.220575224868544 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.37365666848309) - present_state_Q ( -2.4274648117177353)) * f1( 0.018710583169167953)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -2.37365666848309) - present_state_Q (-2.4274648117177353)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.231733775610971 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0635711474023252) - present_state_Q ( -0.05664088453411114)) * f1( 0.008173280514672542)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.0635711474023252) - present_state_Q (-0.05664088453411114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.242276735492508 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04325716571600633) - present_state_Q ( -0.07366764456732688)) * f1( 0.007733175071497721)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.04325716571600633) - present_state_Q (-0.07366764456732688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.259719594622641 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02551337723101898) - present_state_Q ( -1.118715055219448)) * f1( 0.014942810720340162)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.02551337723101898) - present_state_Q (-1.118715055219448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.27863872164296 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09718424897616897) - present_state_Q ( -0.09718424897616897)) * f1( 0.01489486789524653)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09718424897616897) - present_state_Q (-0.09718424897616897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.293604823069572 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07722804552386131) - present_state_Q ( -0.07722804552386131)) * f1( 0.011766047002302502)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.07722804552386131) - present_state_Q (-0.07722804552386131)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.305431670734368 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12353214821500075) - present_state_Q ( -0.12353214821500075)) * f1( 0.00932859231095119)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.12353214821500075) - present_state_Q (-0.12353214821500075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.32039750748793 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07747798602274313) - present_state_Q ( -0.07747798602274313)) * f1( 0.011766047002302502)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.07747798602274313) - present_state_Q (-0.07747798602274313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.332270632226757 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06841239441936862) - present_state_Q ( -0.06841239441936862)) * f1( 0.00932859231095119)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.06841239441936862) - present_state_Q (-0.06841239441936862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.342729488578296 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055588357368910386) - present_state_Q ( -0.07461369439719372)) * f1( 0.008222250722733016)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.055588357368910386) - present_state_Q (-0.07461369439719372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.354144847673252 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04607347242329581) - present_state_Q ( -0.04607347242329581)) * f1( 0.00895478527889025)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.04607347242329581) - present_state_Q (-0.04607347242329581)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.36516646767364 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05243973617930479) - present_state_Q ( -0.05243973617930479)) * f1( 0.00864980416643571)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.05243973617930479) - present_state_Q (-0.05243973617930479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.375345643470059 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0672173421879319) - present_state_Q ( -0.13494839281162602)) * f1( 0.00866497374914061)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.0672173421879319) - present_state_Q (-0.13494839281162602)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.38987341194851 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13668716454332885) - present_state_Q ( -0.13668716454332885)) * f1( 0.013403003785151626)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13668716454332885) - present_state_Q (-0.13668716454332885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.413061089709261 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05957179425662126) - present_state_Q ( -0.18582171131732808)) * f1( 0.02150523218333336)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.05957179425662126) - present_state_Q (-0.18582171131732808)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.436912173747118 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1090905999156661) - present_state_Q ( -0.20432187776843924)) * f1( 0.022148333138458143)
w2 ( -11.573105411093975 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1090905999156661) - present_state_Q (-0.20432187776843924)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.451516985728722 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.4442464271442823) - present_state_Q ( -2.461249672925551)) * f1( 0.01670002760598027)
w2 ( -11.748013049885826 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -2.4442464271442823) - present_state_Q (-2.461249672925551)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.469664802368841 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.522787713893565) - present_state_Q ( -2.522787713893565)) * f1( 0.02087948359816391)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -2.522787713893565) - present_state_Q (-2.522787713893565)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.491389131692864 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1937540754873216) - present_state_Q ( -0.20896887310107404)) * f1( 0.022034808017124315)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1937540754873216) - present_state_Q (-0.20896887310107404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.503463405847546 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08143013787632734) - present_state_Q ( -0.08143013787632734)) * f1( 0.014818027366540892)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08143013787632734) - present_state_Q (-0.08143013787632734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.511485929706408 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09166503851342117) - present_state_Q ( -0.03236452567536867)) * f1( 0.009785406763423665)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.09166503851342117) - present_state_Q (-0.03236452567536867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.524082057312148 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09919600359984095) - present_state_Q ( -0.15557148518546973)) * f1( 0.015596981664691896)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.09919600359984095) - present_state_Q (-0.15557148518546973)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.544260514525645 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1530130827630457) - present_state_Q ( -0.22721811171640335)) * f1( 0.02519240472729399)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1530130827630457) - present_state_Q (-0.22721811171640335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.562726138236336 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16587802967407866) - present_state_Q ( -0.233037043408863)) * f1( 0.02603840127391222)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16587802967407866) - present_state_Q (-0.233037043408863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.582081066557791 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14885664040809682) - present_state_Q ( -0.24221440661836519)) * f1( 0.027334345090841136)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14885664040809682) - present_state_Q (-0.24221440661836519)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.584017910374799 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059148949922622894) - present_state_Q ( -0.0649939140227632)) * f1( 0.0026718555159672493)
w2 ( -11.921847010431815 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.059148949922622894) - present_state_Q (-0.0649939140227632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.596023914997215 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1616986598283676) - present_state_Q ( -2.612833487368109)) * f1( 0.025482478962869783)
w2 ( -12.016076497601492 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1616986598283676) - present_state_Q (-2.612833487368109)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.602126523497956 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.607614100076094) - present_state_Q ( -5.010829399596393)) * f1( 0.023856299960243493)
w2 ( -12.118399297212706 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -2.607614100076094) - present_state_Q (-5.010829399596393)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.612962490357239 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.5955020053798155) - present_state_Q ( -2.6702253931942095)) * f1( 0.027199083800775452)
w2 ( -12.198078208007292 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -2.5955020053798155) - present_state_Q (-2.6702253931942095)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.622115565823576 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.613223882189406) - present_state_Q ( -2.6326210133872507)) * f1( 0.022750043924940695)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -2.613223882189406) - present_state_Q (-2.6326210133872507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.625523871586923 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12570904827885512) - present_state_Q ( -0.04081594911717741)) * f1( 0.006250493755646616)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12570904827885512) - present_state_Q (-0.04081594911717741)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.630302656370496 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14152589055500453) - present_state_Q ( -0.08202168589068982)) * f1( 0.008827965761299048)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14152589055500453) - present_state_Q (-0.08202168589068982)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.643024829772388 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13422560006543066) - present_state_Q ( -0.21410060444911813)) * f1( 0.024093084061775096)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13422560006543066) - present_state_Q (-0.21410060444911813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.643938146418291 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08539136179970151) - present_state_Q ( -0.06586245034647435)) * f1( 0.0016839116209605963)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08539136179970151) - present_state_Q (-0.06586245034647435)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.648115316822663 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11429512025918719) - present_state_Q ( -0.01089857044400486)) * f1( 0.007620300952925623)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11429512025918719) - present_state_Q (-0.01089857044400486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.65214072777926 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041117831954742855) - present_state_Q ( -0.09231739693001358)) * f1( 0.0074642818253960784)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.041117831954742855) - present_state_Q (-0.09231739693001358)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.660325147906484 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07392746560225422) - present_state_Q ( -0.14629648875308549)) * f1( 0.015320318448208093)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07392746560225422) - present_state_Q (-0.14629648875308549)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.672314912288725 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17865606518847882) - present_state_Q ( -0.2515431577261582)) * f1( 0.027665013848557365)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.17865606518847882) - present_state_Q (-0.2515431577261582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.67450172423357 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13217157065207025) - present_state_Q ( -0.02368460396461579)) * f1( 0.004798672243075651)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13217157065207025) - present_state_Q (-0.02368460396461579)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.679156830078162 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10220700036679901) - present_state_Q ( -0.02139161719274179)) * f1( 0.010216596858257347)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.10220700036679901) - present_state_Q (-0.02139161719274179)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.685942577838816 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06900242691249191) - present_state_Q ( -1.3249222430943317)) * f1( 0.020882023759858447)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.06900242691249191) - present_state_Q (-1.3249222430943317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.753185200362699 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.136368940727738) - present_state_Q ( -0.1763663967477)) * f1( 0.15265564029246836)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.136368940727738) - present_state_Q (-0.1763663967477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.760159787149583 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15086685856712287) - present_state_Q ( -0.16156279608306442)) * f1( 0.015775645903926123)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15086685856712287) - present_state_Q (-0.16156279608306442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.767846125087996 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11806232999267133) - present_state_Q ( -0.19129119718152254)) * f1( 0.02212160615902385)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11806232999267133) - present_state_Q (-0.19129119718152254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.774707207086022 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12330716263908936) - present_state_Q ( -0.18680016546814288)) * f1( 0.019718021909461158)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12330716263908936) - present_state_Q (-0.18680016546814288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.782771601705447 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1571422645556866) - present_state_Q ( -0.21427040646384782)) * f1( 0.023337765081520034)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1571422645556866) - present_state_Q (-0.21427040646384782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.789825649077672 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12382829939620384) - present_state_Q ( -0.17268033954227724)) * f1( 0.020190351349354353)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12382829939620384) - present_state_Q (-0.17268033954227724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.791274941011013 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14666631857753118) - present_state_Q ( -0.04359771356357415)) * f1( 0.005344744397039335)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14666631857753118) - present_state_Q (-0.04359771356357415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.795365789762618 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0770785423079679) - present_state_Q ( -0.0770785423079679)) * f1( 0.01531475653777699)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0770785423079679) - present_state_Q (-0.0770785423079679)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.797983811166882 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020223866095701922) - present_state_Q ( -0.1592041338008585)) * f1( 0.010134132582894102)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.020223866095701922) - present_state_Q (-0.1592041338008585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.799635143833353 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021112055274272675) - present_state_Q ( -0.021112055274272675)) * f1( 0.0060676163392665)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021112055274272675) - present_state_Q (-0.021112055274272675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.800254250811205 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05339280997830692) - present_state_Q ( -0.05339280997830692)) * f1( 0.002299377487999589)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05339280997830692) - present_state_Q (-0.05339280997830692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.801905580502465 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02111750352790631) - present_state_Q ( -0.02111750352790631)) * f1( 0.006067616339266495)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02111750352790631) - present_state_Q (-0.02111750352790631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.80252468462943 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0534065861169377) - present_state_Q ( -0.0534065861169377)) * f1( 0.002299377487999589)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0534065861169377) - present_state_Q (-0.0534065861169377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.804176011345486 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021122951767559368) - present_state_Q ( -0.021122951767559368)) * f1( 0.006067616339266495)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021122951767559368) - present_state_Q (-0.021122951767559368)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.80479511262157 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05342036222021799) - present_state_Q ( -0.05342036222021799)) * f1( 0.002299377487999589)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05342036222021799) - present_state_Q (-0.05342036222021799)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.806446436362428 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021128399993231874) - present_state_Q ( -0.021128399993231874)) * f1( 0.006067616339266495)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021128399993231874) - present_state_Q (-0.021128399993231874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.807065534787636 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053434138288147866) - present_state_Q ( -0.053434138288147866)) * f1( 0.002299377487999589)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.053434138288147866) - present_state_Q (-0.053434138288147866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.808679551729146 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07721698820559324) - present_state_Q ( -0.08822235817185801)) * f1( 0.006067616339266495)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07721698820559324) - present_state_Q (-0.08822235817185801)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.811406563675577 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020254479460248276) - present_state_Q ( -0.020254479460248276)) * f1( 0.010017225127187078)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.020254479460248276) - present_state_Q (-0.020254479460248276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.813057878752709 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021144265141780885) - present_state_Q ( -0.021144265141780885)) * f1( 0.00606761633926649)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021144265141780885) - present_state_Q (-0.021144265141780885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.813676968876216 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05347425398402117) - present_state_Q ( -0.05347425398402117)) * f1( 0.0022993774879995817)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05347425398402117) - present_state_Q (-0.05347425398402117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.81532828097818 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021149713312761838) - present_state_Q ( -0.021149713312761838)) * f1( 0.006067616339266486)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021149713312761838) - present_state_Q (-0.021149713312761838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.815940954821704 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07136319435710706) - present_state_Q ( -0.08316757005564929)) * f1( 0.0022993774879995817)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07136319435710706) - present_state_Q (-0.08316757005564929)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.836811233816842 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0518543775293931) - present_state_Q ( -1.108444625575315)) * f1( 0.12013105645065807)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -1.0518543775293931) - present_state_Q (-1.108444625575315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.841025588330279 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17132476072931593) - present_state_Q ( -0.17132476072931593)) * f1( 0.016294542897877472)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.17132476072931593) - present_state_Q (-0.17132476072931593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.861525576687159 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0218206009379045) - present_state_Q ( -1.082812841160403)) * f1( 0.11648243760193902)
w2 ( -12.278544650151636 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -1.0218206009379045) - present_state_Q (-1.082812841160403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.857963252320987 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.9590967894953115) - present_state_Q ( -5.060444498268667)) * f1( 0.019530468299568556)
w2 ( -12.205585327076907 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.9590967894953115) - present_state_Q (-5.060444498268667)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.85694133002791 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.5669106245182083) - present_state_Q ( -4.9965703789224)) * f1( 0.005380442568499731)
w2 ( -12.129612224116121 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -3.5669106245182083) - present_state_Q (-4.9965703789224)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.854306762130994 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.051831795267753) - present_state_Q ( -5.051831795267753)) * f1( 0.014587079662655053)
w2 ( -12.057368349184518 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -5.051831795267753) - present_state_Q (-5.051831795267753)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.852913227766475 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.5500143584349373) - present_state_Q ( -2.624568183258127)) * f1( 0.025685742335750605)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -2.5500143584349373) - present_state_Q (-2.624568183258127)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.870960108388893 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06259954976609762) - present_state_Q ( -0.06259954976609762)) * f1( 0.007071067811865476)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.06259954976609762) - present_state_Q (-0.06259954976609762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.88897988227237 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06234667239787158) - present_state_Q ( -0.10090897987125669)) * f1( 0.007071067811865476)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.06234667239787158) - present_state_Q (-0.10090897987125669)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.917933802829259 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00833544418435241) - present_state_Q ( -0.12577660374075209)) * f1( 0.011375203883042076)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.00833544418435241) - present_state_Q (-0.12577660374075209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.935663572851155 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12580170614010697) - present_state_Q ( -0.10730145272579089)) * f1( 0.00695728137281673)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.12580170614010697) - present_state_Q (-0.10730145272579089)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.955260361986241 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11574080462043375) - present_state_Q ( -0.012067235255675796)) * f1( 0.00766158298899379)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.11574080462043375) - present_state_Q (-0.012067235255675796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.012472211087946 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03879047777931495) - present_state_Q ( -0.20354840780646688)) * f1( 0.024291943687403714)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.03879047777931495) - present_state_Q (-0.20354840780646688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.02234936894057 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08704219720814468) - present_state_Q ( -0.04071810284894566)) * f1( 0.00451168342764768)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.08704219720814468) - present_state_Q (-0.04071810284894566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.05561794142188 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14121998729280677) - present_state_Q ( -0.15774015335408498)) * f1( 0.015274268584919384)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.14121998729280677) - present_state_Q (-0.15774015335408498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.072388646533529 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11396680631263874) - present_state_Q ( -0.014196768517165382)) * f1( 0.007650305869397467)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11396680631263874) - present_state_Q (-0.014196768517165382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.088790814834981 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12070138310897008) - present_state_Q ( -0.0102732833612482)) * f1( 0.007480621460377099)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.12070138310897008) - present_state_Q (-0.0102732833612482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.104827780126108 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03101519710447509) - present_state_Q ( -0.10304765608164386)) * f1( 0.007348158697710916)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.03101519710447509) - present_state_Q (-0.10304765608164386)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.125131039646288 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10569133124867021) - present_state_Q ( -0.11721031103638432)) * f1( 0.009305835083141511)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.10569133124867021) - present_state_Q (-0.11721031103638432)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.144903444452126 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11364917910682207) - present_state_Q ( -0.02334216951803784)) * f1( 0.009023371105353464)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11364917910682207) - present_state_Q (-0.02334216951803784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.164013695246723 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10363415476448067) - present_state_Q ( -0.025738761418074175)) * f1( 0.008722541937610349)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.10363415476448067) - present_state_Q (-0.025738761418074175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.182475207765812 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10925632991349365) - present_state_Q ( -0.020902555517092514)) * f1( 0.008424360847006337)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.10925632991349365) - present_state_Q (-0.020902555517092514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.200318183790067 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11602032570868191) - present_state_Q ( -0.016381209608034207)) * f1( 0.008140179440728817)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11602032570868191) - present_state_Q (-0.016381209608034207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.217608620153712 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10794980431781698) - present_state_Q ( -0.02216396982558239)) * f1( 0.00789047638195791)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.10794980431781698) - present_state_Q (-0.02216396982558239)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.23519638041142 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11356531219027023) - present_state_Q ( -0.01723171146111181)) * f1( 0.008024148165531032)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11356531219027023) - present_state_Q (-0.01723171146111181)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.252288542985141 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12058106505936762) - present_state_Q ( -0.013189526415347104)) * f1( 0.007796351918726058)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.12058106505936762) - present_state_Q (-0.013189526415347104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.268963719759693 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11250587422509244) - present_state_Q ( -0.018341535600377656)) * f1( 0.007608217838041831)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11250587422509244) - present_state_Q (-0.018341535600377656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.285867426160392 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1186091293366916) - present_state_Q ( -0.013417864618591768)) * f1( 0.0077105399218777015)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.1186091293366916) - present_state_Q (-0.013417864618591768)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.302375656599553 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1213770908061324) - present_state_Q ( -0.011384140884729298)) * f1( 0.007529352025833019)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.1213770908061324) - present_state_Q (-0.011384140884729298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.318524258485903 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11579177738808462) - present_state_Q ( -0.016443821874404838)) * f1( 0.007367214384141476)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11579177738808462) - present_state_Q (-0.016443821874404838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.334437706897615 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12114747365660126) - present_state_Q ( -0.01069002407181737)) * f1( 0.007257851640876285)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.12114747365660126) - present_state_Q (-0.01069002407181737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.376292145055823 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11941889015893005) - present_state_Q ( -0.19030363629133132)) * f1( 0.019246913319266958)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.11941889015893005) - present_state_Q (-0.19030363629133132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.422338233473171 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1772035771681153) - present_state_Q ( -0.19323879593039212)) * f1( 0.02117169049545415)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.1772035771681153) - present_state_Q (-0.19323879593039212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.443852769968446 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11668119101445161) - present_state_Q ( -0.0728604862725871)) * f1( 0.010269613432992038)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.11668119101445161) - present_state_Q (-0.0728604862725871)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.451159744757762 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07828899540142745) - present_state_Q ( -0.08578951000941862)) * f1( 0.0034906592944313614)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.07828899540142745) - present_state_Q (-0.08578951000941862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.470230813186859 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10670922419475952) - present_state_Q ( -0.027816468206838356)) * f1( 0.009084164281153365)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.10670922419475952) - present_state_Q (-0.027816468206838356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.526611838371286 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17034116828122065) - present_state_Q ( -0.26632989164481485)) * f1( 0.027156395794809687)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.17034116828122065) - present_state_Q (-0.26632989164481485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.540682308848432 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016369140578482756) - present_state_Q ( -0.016369140578482756)) * f1( 0.0070062827496416525)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.016369140578482756) - present_state_Q (-0.016369140578482756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.555090764718578 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10942241379437682) - present_state_Q ( -0.10942241379437682)) * f1( 0.007204624451585231)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.10942241379437682) - present_state_Q (-0.10942241379437682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.574594747235608 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05650427505417712) - present_state_Q ( -0.11698719714594491)) * f1( 0.00975880159679005)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.05650427505417712) - present_state_Q (-0.11698719714594491)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.603224193518379 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05272506115356389) - present_state_Q ( -0.1502671867863935)) * f1( 0.01434888441730085)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.05272506115356389) - present_state_Q (-0.1502671867863935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.640953367723434 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06032262391747508) - present_state_Q ( -0.12252508492940448)) * f1( 0.018882632647201764)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.06032262391747508) - present_state_Q (-0.12252508492940448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.658756896977284 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1002658448156694) - present_state_Q ( -0.1002658448156694)) * f1( 0.008898587749095507)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.1002658448156694) - present_state_Q (-0.1002658448156694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.675781418904002 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10646349503951152) - present_state_Q ( -0.10646349503951152)) * f1( 0.008511596080412324)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.10646349503951152) - present_state_Q (-0.10646349503951152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.70124370113377 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021287617584269107) - present_state_Q ( -0.17854636829944737)) * f1( 0.01278165303297312)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.021287617584269107) - present_state_Q (-0.17854636829944737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.71650052898709 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11713224965748416) - present_state_Q ( -0.11713224965748416)) * f1( 0.007631481622031339)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.11713224965748416) - present_state_Q (-0.11713224965748416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.733212353835963 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11736972923551613) - present_state_Q ( -0.020781986654046535)) * f1( 0.008319168971599159)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.11736972923551613) - present_state_Q (-0.020781986654046535)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.749382023664474 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12474593994420216) - present_state_Q ( -0.01626537695229248)) * f1( 0.008047178680106088)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.12474593994420216) - present_state_Q (-0.01626537695229248)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.765079840564088 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11619762732961454) - present_state_Q ( -0.021688183520244184)) * f1( 0.007814792572770487)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.11619762732961454) - present_state_Q (-0.021688183520244184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.805777035644821 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13678890534054078) - present_state_Q ( -0.2542230691903755)) * f1( 0.020495307804890096)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.13678890534054078) - present_state_Q (-0.2542230691903755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.866227333753624 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19372290852721047) - present_state_Q ( -0.3269257149182054)) * f1( 0.03202442746379395)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.19372290852721047) - present_state_Q (-0.3269257149182054)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.902713035325045 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20655206992486524) - present_state_Q ( -0.20655206992486524)) * f1( 0.019205057451964245)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.20655206992486524) - present_state_Q (-0.20655206992486524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.921589636102143 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13874885249277497) - present_state_Q ( -0.1095709521272468)) * f1( 0.009889164128320797)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.13874885249277497) - present_state_Q (-0.1095709521272468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.965413027003915 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1305676482207981) - present_state_Q ( -0.249484491898012)) * f1( 0.02312893123919917)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.1305676482207981) - present_state_Q (-0.249484491898012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.970967314294537 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08197902270417917) - present_state_Q ( -0.0695394586589219)) * f1( 0.002904579053652479)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.08197902270417917) - present_state_Q (-0.0695394586589219)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.984346058890862 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12323425984099531) - present_state_Q ( -0.02365850596649484)) * f1( 0.006978080935580532)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.12323425984099531) - present_state_Q (-0.02365850596649484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.997736684983124 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12624066976257595) - present_state_Q ( -0.017497548538071894)) * f1( 0.006981925007012389)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.12624066976257595) - present_state_Q (-0.017497548538071894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.011048471249955 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13116298167895263) - present_state_Q ( -0.010472154586073704)) * f1( 0.006938098072897698)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.13116298167895263) - present_state_Q (-0.010472154586073704)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.05663141803034 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17283482122977586) - present_state_Q ( -0.24963483696861552)) * f1( 0.024052407831579147)
w2 ( -12.04651770413557 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.17283482122977586) - present_state_Q (-0.24963483696861552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.064643027577132 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1525167087172125) - present_state_Q ( -2.4229612895854693)) * f1( 0.004775594111539968)
w2 ( -12.382040755704425 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.1525167087172125) - present_state_Q (-2.4229612895854693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.098847591305814 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.676405664569697) - present_state_Q ( -2.676405664569697)) * f1( 0.020390083900398256)
w2 ( -12.7175426976853 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -2.676405664569697) - present_state_Q (-2.676405664569697)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.136183394935802 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20302508603490949) - present_state_Q ( -0.20302508603490949)) * f1( 0.02064163153211727)
w2 ( -12.7175426976853 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.20302508603490949) - present_state_Q (-0.20302508603490949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.169529462459598 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1611179324881074) - present_state_Q ( -2.7752841281902536)) * f1( 0.0214980958255382)
w2 ( -13.027766149979927 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.1611179324881074) - present_state_Q (-2.7752841281902536)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.201921721729555 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.396080191829681) - present_state_Q ( -5.472475155671428)) * f1( 0.024286645289063904)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -5.396080191829681) - present_state_Q (-5.472475155671428)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -10.20984321665272 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04626772256658801) - present_state_Q ( -0.11053145102545178)) * f1( 0.004848685912443551)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.04626772256658801) - present_state_Q (-0.11053145102545178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.238717467356691 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1320826694851949) - present_state_Q ( -0.18608963093709957)) * f1( 0.018802120503604698)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1320826694851949) - present_state_Q (-0.18608963093709957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.251026634889026 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1319806987028041) - present_state_Q ( -0.01599030191415327)) * f1( 0.007927588561378948)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1319806987028041) - present_state_Q (-0.01599030191415327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.263000205239182 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12291979657117551) - present_state_Q ( -0.022051810190005232)) * f1( 0.007714912757945327)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.12291979657117551) - present_state_Q (-0.022051810190005232)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.275159007233949 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12938041095340141) - present_state_Q ( -0.0165275205028803)) * f1( 0.007831149331748468)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.12938041095340141) - present_state_Q (-0.0165275205028803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.287009571645795 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13524101734907534) - present_state_Q ( -0.013132471683077022)) * f1( 0.0076306654016143225)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.13524101734907534) - present_state_Q (-0.013132471683077022)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.298655637732677 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12581220873726484) - present_state_Q ( -0.020682935842457364)) * f1( 0.007503090815002056)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.12581220873726484) - present_state_Q (-0.020682935842457364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.310123115710383 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1311540839630678) - present_state_Q ( -0.014495581003571855)) * f1( 0.007384835753752567)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1311540839630678) - present_state_Q (-0.014495581003571855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.321397019427108 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13841154180596885) - present_state_Q ( -0.009578927831739087)) * f1( 0.007257540546920578)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.13841154180596885) - present_state_Q (-0.009578927831739087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.337917570412099 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02715120286224641) - present_state_Q ( -0.1730379187332884)) * f1( 0.011436110030633563)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02715120286224641) - present_state_Q (-0.1730379187332884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.350584504834757 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12493430940850123) - present_state_Q ( -0.12915530081334375)) * f1( 0.008736049348727702)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.12493430940850123) - present_state_Q (-0.12915530081334375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.363494333402713 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03712656945828605) - present_state_Q ( -0.12939323269086853)) * f1( 0.008909108442995261)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03712656945828605) - present_state_Q (-0.12939323269086853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.377300496235504 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12505329117646574) - present_state_Q ( -0.1416153475120711)) * f1( 0.009529926618284102)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.12505329117646574) - present_state_Q (-0.1416153475120711)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.38917069045892 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13309388908817935) - present_state_Q ( -0.017986270018417926)) * f1( 0.00812381598998196)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.13309388908817935) - present_state_Q (-0.017986270018417926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.400595827848523 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03601447061081774) - present_state_Q ( -0.121398799232895)) * f1( 0.007880231918706597)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03601447061081774) - present_state_Q (-0.121398799232895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.414380159572957 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11619591566066721) - present_state_Q ( -0.13185653093802452)) * f1( 0.009509033264236341)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.11619591566066721) - present_state_Q (-0.13185653093802452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.42831951309486 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1253924119113034) - present_state_Q ( -0.031756523466626774)) * f1( 0.009549426243948595)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.1253924119113034) - present_state_Q (-0.031756523466626774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.441469574450583 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036931672407208224) - present_state_Q ( -0.1322969496435204)) * f1( 0.009076724814560585)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.036931672407208224) - present_state_Q (-0.1322969496435204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.452672711926471 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13366169475058912) - present_state_Q ( -0.014852883068994698)) * f1( 0.007665616853725494)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.13366169475058912) - present_state_Q (-0.014852883068994698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.463550406595019 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06670688657767118) - present_state_Q ( -0.10295713243049999)) * f1( 0.007491530663012109)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.06670688657767118) - present_state_Q (-0.10295713243049999)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.483373210157794 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10143538364444733) - present_state_Q ( -0.16127376684560898)) * f1( 0.013703839512615824)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.10143538364444733) - present_state_Q (-0.16127376684560898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.509414344691066 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13283124910347585) - present_state_Q ( -0.22110248963034532)) * f1( 0.018073507291736134)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.13283124910347585) - present_state_Q (-0.22110248963034532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.536061419461314 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22820151547415482) - present_state_Q ( -0.24779366188925966)) * f1( 0.021208617061413196)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.22820151547415482) - present_state_Q (-0.24779366188925966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.547372741323533 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005739360623069468) - present_state_Q ( -0.15086950771595528)) * f1( 0.008949576932977047)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.005739360623069468) - present_state_Q (-0.15086950771595528)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.560188026802264 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10905847767022796) - present_state_Q ( -0.13333936242391983)) * f1( 0.010117217794097074)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.10905847767022796) - present_state_Q (-0.13333936242391983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.571102738133082 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038094292037404325) - present_state_Q ( -0.12907182754797525)) * f1( 0.008618706074580677)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.038094292037404325) - present_state_Q (-0.12907182754797525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.583311768849734 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03954560155894092) - present_state_Q ( -0.03954560155894092)) * f1( 0.009572969550962823)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.03954560155894092) - present_state_Q (-0.03954560155894092)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.594471222709442 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1206261547080576) - present_state_Q ( -0.13697999638691297)) * f1( 0.009496755427182464)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.1206261547080576) - present_state_Q (-0.13697999638691297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.605330967341759 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12968011896901518) - present_state_Q ( -0.028453911984997388)) * f1( 0.009156430370010613)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12968011896901518) - present_state_Q (-0.028453911984997388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.615700650501449 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12038775613599328) - present_state_Q ( -0.030940842470155855)) * f1( 0.008745752469022287)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12038775613599328) - present_state_Q (-0.030940842470155855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.62605444756748 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12806523823767296) - present_state_Q ( -0.025207662514196278)) * f1( 0.008727568993473201)
w2 ( -13.561265262507302 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12806523823767296) - present_state_Q (-0.025207662514196278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.636006339879268 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13551998245868715) - present_state_Q ( -0.02092331053223534)) * f1( 0.008385234574205574)
w2 ( -13.798632320607322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.13551998245868715) - present_state_Q (-0.02092331053223534)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.64568350751079 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12500733575041792) - present_state_Q ( -0.02548781222616407)) * f1( 0.00815761785701683)
w2 ( -14.035887063380045 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12500733575041792) - present_state_Q (-0.02548781222616407)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.64781420024009 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.9269275128942396) - present_state_Q ( -5.70760104961364)) * f1( 0.003297869427057864)
w2 ( -14.165103381759307 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -2.9269275128942396) - present_state_Q (-5.70760104961364)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.65349166117023 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.9676042494375414) - present_state_Q ( -5.690672357912437)) * f1( 0.008759064102913995)
w2 ( -14.424375873132053 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -2.9676042494375414) - present_state_Q (-5.690672357912437)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -10.668001789337705 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.049557589854043) - present_state_Q ( -3.0917016841797107)) * f1( 0.015964531657092076)
w2 ( -14.787935004815774 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -3.049557589854043) - present_state_Q (-3.0917016841797107)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -10.672681264113297 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.105222940534763) - present_state_Q ( -2.9679655362574766)) * f1( 0.006331103675382741)
w2 ( -14.935759934418094 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -3.105222940534763) - present_state_Q (-2.9679655362574766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.693556411343174 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1001823373654741) - present_state_Q ( -0.15790100134637877)) * f1( 0.021084288710370668)
w2 ( -14.935759934418094 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1001823373654741) - present_state_Q (-0.15790100134637877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.702333744574084 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16052844836734362) - present_state_Q ( -3.180776001725915)) * f1( 0.012750400728248013)
w2 ( -15.073439265726712 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.16052844836734362) - present_state_Q (-3.180776001725915)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.709191623956414 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.0751413689895655) - present_state_Q ( -3.1733070298575248)) * f1( 0.009547512020913916)
w2 ( -15.217097202313942 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -3.0751413689895655) - present_state_Q (-3.1733070298575248)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.719773596795303 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.1486312023920697) - present_state_Q ( -3.2236970353824352)) * f1( 0.01482098956151498)
w2 ( -15.35989431845748 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -3.1486312023920697) - present_state_Q (-3.2236970353824352)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.733527502144195 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17542059133716337) - present_state_Q ( -3.348989849749009)) * f1( 0.020475524759612323)
w2 ( -15.494239157091576 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.17542059133716337) - present_state_Q (-3.348989849749009)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.735603428019258 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.273452368142085) - present_state_Q ( -3.1570733470281085)) * f1( 0.0032922753021337654)
w2 ( -15.620348044384025 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -3.273452368142085) - present_state_Q (-3.1570733470281085)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.73810979939943 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.1809126046986482) - present_state_Q ( -3.219636634672558)) * f1( 0.004020727162068274)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -3.1809126046986482) - present_state_Q (-3.219636634672558)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.740953916504152 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06870909582467712) - present_state_Q ( -0.08441883922565872)) * f1( 0.0034922392359946243)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06870909582467712) - present_state_Q (-0.08441883922565872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.747704945003909 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13791985343806484) - present_state_Q ( -0.019472850438982697)) * f1( 0.008216954194465713)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13791985343806484) - present_state_Q (-0.019472850438982697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.75424070131282 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12842564133063392) - present_state_Q ( -0.023805862176571387)) * f1( 0.007960055455961026)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12842564133063392) - present_state_Q (-0.023805862176571387)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.760825364717252 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13576377855561747) - present_state_Q ( -0.018292847715018892)) * f1( 0.008013523769332828)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13576377855561747) - present_state_Q (-0.018292847715018892)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.767214191972444 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12609040064768662) - present_state_Q ( -0.026197148457915962)) * f1( 0.007783595288146531)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12609040064768662) - present_state_Q (-0.026197148457915962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.7737514023793 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13205634144479306) - present_state_Q ( -0.020382237017336713)) * f1( 0.00795815620893365)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13205634144479306) - present_state_Q (-0.020382237017336713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.780117619538649 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13962213437426535) - present_state_Q ( -0.01514452509405784)) * f1( 0.0077443441814542486)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13962213437426535) - present_state_Q (-0.01514452509405784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.79886371244671 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1756696310037262) - present_state_Q ( -0.2667074585073233)) * f1( 0.023513400148319378)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1756696310037262) - present_state_Q (-0.2667074585073233)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.820700070136981 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22764177968445637) - present_state_Q ( -0.3093888914467101)) * f1( 0.027518933483732556)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.22764177968445637) - present_state_Q (-0.3093888914467101)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.827538515883228 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05716814779492005) - present_state_Q ( -0.15954766522209826)) * f1( 0.00847619559719104)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.05716814779492005) - present_state_Q (-0.15954766522209826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.84460658546561 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17260776709905545) - present_state_Q ( -0.2254722165341164)) * f1( 0.0212992940799634)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.17260776709905545) - present_state_Q (-0.2254722165341164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.86255859436715 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10829960992865809) - present_state_Q ( -0.12305256683946224)) * f1( 0.022137194720540862)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.10829960992865809) - present_state_Q (-0.12305256683946224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.869664633802648 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13580136775380494) - present_state_Q ( -0.024171735265727634)) * f1( 0.008654225270549775)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13580136775380494) - present_state_Q (-0.024171735265727634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.876528566589094 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12713852812341436) - present_state_Q ( -0.026601980437108323)) * f1( 0.008362728502218801)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12713852812341436) - present_state_Q (-0.026601980437108323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.883187856620737 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13383803138773503) - present_state_Q ( -0.020786109262851085)) * f1( 0.008106994142378891)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13383803138773503) - present_state_Q (-0.020786109262851085)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.889652733542835 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1421384932698202) - present_state_Q ( -0.016188776951352037)) * f1( 0.007865119671762354)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1421384932698202) - present_state_Q (-0.016188776951352037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.895950084342688 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13248743717204023) - present_state_Q ( -0.021545523511953657)) * f1( 0.00766720561395322)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13248743717204023) - present_state_Q (-0.021545523511953657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.914756157266618 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11823328508729619) - present_state_Q ( -0.27586800944207296)) * f1( 0.023632813913458207)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11823328508729619) - present_state_Q (-0.27586800944207296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.924308254490667 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09834586490536577) - present_state_Q ( -0.09834586490536577)) * f1( 0.013230735630867324)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.09834586490536577) - present_state_Q (-0.09834586490536577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.93109748368743 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12589546889963787) - present_state_Q ( -0.034701589137503236)) * f1( 0.009318151286982919)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.12589546889963787) - present_state_Q (-0.034701589137503236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.93776005740693 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13549328009809533) - present_state_Q ( -0.029103739474466546)) * f1( 0.009136095125964779)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13549328009809533) - present_state_Q (-0.029103739474466546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.952086773504144 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17846386075332318) - present_state_Q ( -0.21366027772433593)) * f1( 0.0201435085346342)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17846386075332318) - present_state_Q (-0.21366027772433593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.96327458177068 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1647303612548097) - present_state_Q ( -0.20934590796749172)) * f1( 0.015723670301355294)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1647303612548097) - present_state_Q (-0.20934590796749172)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.965969123137702 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06092453076704597) - present_state_Q ( -0.1274725556181342)) * f1( 0.0037493142429671908)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.06092453076704597) - present_state_Q (-0.1274725556181342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.984355216954684 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16420512857497094) - present_state_Q ( -0.28592493919525874)) * f1( 0.026121680688657018)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16420512857497094) - present_state_Q (-0.28592493919525874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.00225441240129 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24970924597207042) - present_state_Q ( -0.28724336076309315)) * f1( 0.025403827727883837)
w2 ( -15.7450205863967 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.24970924597207042) - present_state_Q (-0.28724336076309315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.003876355930345 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.3206141517843655) - present_state_Q ( -3.179425843608954)) * f1( 0.0036360140463332536)
w2 ( -15.834236057425473 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -3.3206141517843655) - present_state_Q (-3.179425843608954)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.009852463044442 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.3300646445767494) - present_state_Q ( -3.4017284598194335)) * f1( 0.01796839767721137)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.3300646445767494) - present_state_Q (-3.4017284598194335)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.023752003291253 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15174750999677217) - present_state_Q ( -0.2996240682433652)) * f1( 0.02674709495451217)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15174750999677217) - present_state_Q (-0.2996240682433652)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.03710498843798 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1569583687379908) - present_state_Q ( -0.29261463980083735)) * f1( 0.02565816926708078)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1569583687379908) - present_state_Q (-0.29261463980083735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.050313385929403 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15795444053154606) - present_state_Q ( -0.2896723815073751)) * f1( 0.02536551342844541)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15795444053154606) - present_state_Q (-0.2896723815073751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.063140035834731 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1479164846658141) - present_state_Q ( -0.2834579140609895)) * f1( 0.024607778215184332)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1479164846658141) - present_state_Q (-0.2834579140609895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.070365190932439 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0970805883683374) - present_state_Q ( -0.0970805883683374)) * f1( 0.013395468104527401)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0970805883683374) - present_state_Q (-0.0970805883683374)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.07532433904011 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1314867052218585) - present_state_Q ( -0.031772581722110255)) * f1( 0.009078565952988747)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1314867052218585) - present_state_Q (-0.031772581722110255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.080376274741914 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042703151811698176) - present_state_Q ( -0.13717287807168493)) * f1( 0.00944604695199993)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.042703151811698176) - present_state_Q (-0.13717287807168493)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.089064601908078 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08666558401755588) - present_state_Q ( -0.13553722103864924)) * f1( 0.016227025335983136)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08666558401755588) - present_state_Q (-0.13553722103864924)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.093218290777521 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047367837247641) - present_state_Q ( -0.17526212813656034)) * f1( 0.007821537993715336)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.047367837247641) - present_state_Q (-0.17526212813656034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.1039117180299 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17449990302215918) - present_state_Q ( -0.19074079724048904)) * f1( 0.020146579958963004)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17449990302215918) - present_state_Q (-0.19074079724048904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.116264323292429 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11425432761513865) - present_state_Q ( -0.1389243714433375)) * f1( 0.02307343614172933)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11425432761513865) - present_state_Q (-0.1389243714433375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.120360122336388 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13259338803004783) - present_state_Q ( -0.02509333325560496)) * f1( 0.007488749733602053)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13259338803004783) - present_state_Q (-0.02509333325560496)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.124473346408907 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07613293798236528) - present_state_Q ( -0.14790267230822532)) * f1( 0.0077014926761736335)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07613293798236528) - present_state_Q (-0.14790267230822532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.131549284370111 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.050334730164524416) - present_state_Q ( -0.1659505742721252)) * f1( 0.013300169300376126)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.050334730164524416) - present_state_Q (-0.1659505742721252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.136902205984452 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041755327866265256) - present_state_Q ( -0.041755327866265256)) * f1( 0.009833559876573104)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.041755327866265256) - present_state_Q (-0.041755327866265256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.142052007850856 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13047235994807965) - present_state_Q ( -0.15027887279687696)) * f1( 0.009636836385277979)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13047235994807965) - present_state_Q (-0.15027887279687696)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.146714067237191 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13908243065703563) - present_state_Q ( -0.023542145452208365)) * f1( 0.008520671304166713)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13908243065703563) - present_state_Q (-0.023542145452208365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.151108989161422 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04030486628353325) - present_state_Q ( -0.13225981106610485)) * f1( 0.00821039649445164)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04030486628353325) - present_state_Q (-0.13225981106610485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.156333554935133 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04008549608355884) - present_state_Q ( -0.04008549608355884)) * f1( 0.009595115353138162)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04008549608355884) - present_state_Q (-0.04008549608355884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.161387335373698 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13065563923878276) - present_state_Q ( -0.14745969545510573)) * f1( 0.009452132338891563)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13065563923878276) - present_state_Q (-0.14745969545510573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.166146312116847 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13983788149434076) - present_state_Q ( -0.025243594024899056)) * f1( 0.008700388936885347)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13983788149434076) - present_state_Q (-0.025243594024899056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.170627315186117 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05135916356358778) - present_state_Q ( -0.12843080945043983)) * f1( 0.008363499726587368)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.05135916356358778) - present_state_Q (-0.12843080945043983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.178788496719667 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.101582533761841) - present_state_Q ( -0.19309903011143983)) * f1( 0.015403795585534025)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.101582533761841) - present_state_Q (-0.19309903011143983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.190422307510067 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23171645118118347) - present_state_Q ( -0.31707718436417026)) * f1( 0.02722199343384611)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.23171645118118347) - present_state_Q (-0.31707718436417026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.199480141459844 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22709695133402968) - present_state_Q ( -0.23512933329347302)) * f1( 0.02079790461131768)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.22709695133402968) - present_state_Q (-0.23512933329347302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.2076913408541 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18345260529487084) - present_state_Q ( -0.202297430895449)) * f1( 0.018731489144977548)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.18345260529487084) - present_state_Q (-0.202297430895449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.217922186962207 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19584056783517606) - present_state_Q ( -0.27937912079980426)) * f1( 0.02374963338816693)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19584056783517606) - present_state_Q (-0.27937912079980426)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.224514958277584 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1100371935124782) - present_state_Q ( -0.1100371935124782)) * f1( 0.014753705874499319)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1100371935124782) - present_state_Q (-0.1100371935124782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.227545336221509 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15249419409941184) - present_state_Q ( -0.006367096784997653)) * f1( 0.006621651398160693)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15249419409941184) - present_state_Q (-0.006367096784997653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.230566019686478 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14208989878203987) - present_state_Q ( -0.01980223061320408)) * f1( 0.00662141188547358)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14208989878203987) - present_state_Q (-0.01980223061320408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.233725946584665 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14736834528655227) - present_state_Q ( -0.011824205672909989)) * f1( 0.006913746159097514)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14736834528655227) - present_state_Q (-0.011824205672909989)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.234882567484412 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11440417464532653) - present_state_Q ( -0.08613512986218361)) * f1( 0.0025743352433663016)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11440417464532653) - present_state_Q (-0.08613512986218361)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.238384583953863 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14665064141104978) - present_state_Q ( -0.014928614384122348)) * f1( 0.007667547728309221)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14665064141104978) - present_state_Q (-0.014928614384122348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.241807591681626 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1381912552000099) - present_state_Q ( -0.021482716571787377)) * f1( 0.007506723234637857)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1381912552000099) - present_state_Q (-0.021482716571787377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.24517861916464 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14429999836013938) - present_state_Q ( -0.01471997801075357)) * f1( 0.007380794438704484)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14429999836013938) - present_state_Q (-0.01471997801075357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.248494717702624 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15122916118119303) - present_state_Q ( -0.01038985915836356)) * f1( 0.00725255242358128)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15122916118119303) - present_state_Q (-0.01038985915836356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.251778754345233 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13948267541219145) - present_state_Q ( -0.022160026437134786)) * f1( 0.007202822795277092)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13948267541219145) - present_state_Q (-0.022160026437134786)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.258373353564872 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08620282581093962) - present_state_Q ( -0.13994667669040517)) * f1( 0.014865223076563419)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.08620282581093962) - present_state_Q (-0.13994667669040517)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.26261735325492 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12068885141051676) - present_state_Q ( -0.03869089917169589)) * f1( 0.009346031990716114)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12068885141051676) - present_state_Q (-0.03869089917169589)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.266699687241646 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12713640969048617) - present_state_Q ( -0.03272472181747553)) * f1( 0.008976946715943157)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12713640969048617) - present_state_Q (-0.03272472181747553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.270629071666168 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13455819827940257) - present_state_Q ( -0.026617690538385082)) * f1( 0.008627620327268014)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13455819827940257) - present_state_Q (-0.026617690538385082)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.2744168161359 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14334493283397537) - present_state_Q ( -0.02141846630453682)) * f1( 0.008305542350681688)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14334493283397537) - present_state_Q (-0.02141846630453682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.277373315793373 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.144492416984439) - present_state_Q ( -0.018620908805582165)) * f1( 0.006478700569320417)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.144492416984439) - present_state_Q (-0.018620908805582165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.282985301085112 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12575741972752713) - present_state_Q ( -0.12447255641360655)) * f1( 0.012595099790394054)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12575741972752713) - present_state_Q (-0.12447255641360655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.293218304308802 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.145820928388995) - present_state_Q ( -0.281464060409866)) * f1( 0.023793784881778274)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.145820928388995) - present_state_Q (-0.281464060409866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.300886142534972 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11249533264952766) - present_state_Q ( -0.11249533264952766)) * f1( 0.017168055889504378)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11249533264952766) - present_state_Q (-0.11249533264952766)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.304868299507769 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14086807510498667) - present_state_Q ( -0.026038026716994834)) * f1( 0.008741167711804785)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14086807510498667) - present_state_Q (-0.026038026716994834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.30871108471022 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13152763311007007) - present_state_Q ( -0.028390460120047933)) * f1( 0.008441324666098055)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13152763311007007) - present_state_Q (-0.028390460120047933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.312437882256411 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13848474680272882) - present_state_Q ( -0.022344768775667226)) * f1( 0.008174433039315412)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13848474680272882) - present_state_Q (-0.022344768775667226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.314681308214498 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18000607810164898) - present_state_Q ( -0.048972510233093264)) * f1( 0.004945154673564405)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.18000607810164898) - present_state_Q (-0.048972510233093264)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.322218501568111 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11245423036235898) - present_state_Q ( -0.11842434716548826)) * f1( 0.01689799344368847)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.11245423036235898) - present_state_Q (-0.11842434716548826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.3258617223629 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14735948793251188) - present_state_Q ( -0.018061895312451764)) * f1( 0.007982061851408338)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14735948793251188) - present_state_Q (-0.018061895312451764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.329424831722013 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.135476307068967) - present_state_Q ( -0.02527884177050803)) * f1( 0.0078209454394426)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.135476307068967) - present_state_Q (-0.02527884177050803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.332919708735767 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14133669398296309) - present_state_Q ( -0.01889372342777342)) * f1( 0.007659456821906679)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14133669398296309) - present_state_Q (-0.01889372342777342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.336342815004876 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14906694630734355) - present_state_Q ( -0.01324006619393255)) * f1( 0.007491610487766567)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14906694630734355) - present_state_Q (-0.01324006619393255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.339694739443692 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13884719152942862) - present_state_Q ( -0.021706073838649958)) * f1( 0.007351090516771444)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13884719152942862) - present_state_Q (-0.021706073838649958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.343042363521418 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06050079327294283) - present_state_Q ( -0.11653969295889609)) * f1( 0.007510772954440276)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.06050079327294283) - present_state_Q (-0.11653969295889609)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.34952597803311 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10203915617720699) - present_state_Q ( -0.18629549278477942)) * f1( 0.014764026859413689)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.10203915617720699) - present_state_Q (-0.18629549278477942)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.35839788841994 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09838814885889774) - present_state_Q ( -0.15773148107193652)) * f1( 0.02007358710245639)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.09838814885889774) - present_state_Q (-0.15773148107193652)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.36224001395163 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15057503796869323) - present_state_Q ( -0.15057503796869323)) * f1( 0.008668921420095163)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15057503796869323) - present_state_Q (-0.15057503796869323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.366522572469464 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12331608525437779) - present_state_Q ( -0.03815860977035856)) * f1( 0.009429294454279993)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12331608525437779) - present_state_Q (-0.03815860977035856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.370627911442922 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13062783412369988) - present_state_Q ( -0.031825658924948555)) * f1( 0.009025056914361538)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13062783412369988) - present_state_Q (-0.031825658924948555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.37456914415096 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13903029088647537) - present_state_Q ( -0.02581831421738307)) * f1( 0.008651267287113327)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13903029088647537) - present_state_Q (-0.02581831421738307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.378362940332803 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14378736227245612) - present_state_Q ( -0.021903659519309247)) * f1( 0.008319616602717902)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14378736227245612) - present_state_Q (-0.021903659519309247)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.38229580260381 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1416270113120282) - present_state_Q ( -0.0652083902324862)) * f1( 0.00870768728617412)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1416270113120282) - present_state_Q (-0.0652083902324862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.391710435542395 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21537176341163458) - present_state_Q ( -0.2406353028708167)) * f1( 0.021650359155888407)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21537176341163458) - present_state_Q (-0.2406353028708167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.403389418738417 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19686114712773584) - present_state_Q ( -0.32333991624992026)) * f1( 0.027390169454241758)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19686114712773584) - present_state_Q (-0.32333991624992026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.406240287287424 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1573034221989149) - present_state_Q ( -0.030515445040581513)) * f1( 0.006261790191853309)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1573034221989149) - present_state_Q (-0.030515445040581513)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.410412894280146 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1403278040610292) - present_state_Q ( -0.030735580208134956)) * f1( 0.009168784588574279)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1403278040610292) - present_state_Q (-0.030735580208134956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.414429226411258 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12917697965731323) - present_state_Q ( -0.03242642453572443)) * f1( 0.00883083483412906)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12917697965731323) - present_state_Q (-0.03242642453572443)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.418305337604988 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13631485158167364) - present_state_Q ( -0.02629500226827099)) * f1( 0.008509718894787705)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13631485158167364) - present_state_Q (-0.02629500226827099)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.422049572567188 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1448542154331262) - present_state_Q ( -0.020835081767511567)) * f1( 0.008208815523668849)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1448542154331262) - present_state_Q (-0.020835081767511567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.425669790535103 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13464955457893332) - present_state_Q ( -0.02692050332396356)) * f1( 0.007949306277530068)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13464955457893332) - present_state_Q (-0.02692050332396356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.429340699856429 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14181006333326582) - present_state_Q ( -0.0208210457193116)) * f1( 0.008048569665015621)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14181006333326582) - present_state_Q (-0.0208210457193116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.432908852396801 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1501911604117922) - present_state_Q ( -0.01644948732930446)) * f1( 0.00781434678312038)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1501911604117922) - present_state_Q (-0.01644948732930446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.436411091905851 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1375931258634355) - present_state_Q ( -0.025091138864783806)) * f1( 0.007686663634249182)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1375931258634355) - present_state_Q (-0.025091138864783806)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.44752245025982 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22846450297011042) - present_state_Q ( -0.3049990997764035)) * f1( 0.025928201023209505)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.22846450297011042) - present_state_Q (-0.3049990997764035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.456946218946324 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18236241298004358) - present_state_Q ( -0.18236241298004358)) * f1( 0.02140082695385793)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.18236241298004358) - present_state_Q (-0.18236241298004358)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.464398635510216 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09067923015772499) - present_state_Q ( -0.09067923015772499)) * f1( 0.0166127022101575)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.09067923015772499) - present_state_Q (-0.09067923015772499)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.468093103971928 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1326281334000774) - present_state_Q ( -0.030320947972884375)) * f1( 0.008118768587432728)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1326281334000774) - present_state_Q (-0.030320947972884375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.47171046942896 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1378671008612941) - present_state_Q ( -0.024378466594074968)) * f1( 0.007938050994324368)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1378671008612941) - present_state_Q (-0.024378466594074968)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.475244822731515 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14466580451587788) - present_state_Q ( -0.018047864394675546)) * f1( 0.007743973414980065)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14466580451587788) - present_state_Q (-0.018047864394675546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.478698024384691 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15227338255189024) - present_state_Q ( -0.01381649472040273)) * f1( 0.00755789857597218)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15227338255189024) - present_state_Q (-0.01381649472040273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.482099142467893 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14016198332513455) - present_state_Q ( -0.023753918119217468)) * f1( 0.00746211303435344)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14016198332513455) - present_state_Q (-0.023753918119217468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.485457380649118 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14565958187120948) - present_state_Q ( -0.01681289704395955)) * f1( 0.007355944348500659)
w2 ( -15.900754032165947 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14565958187120948) - present_state_Q (-0.01681289704395955)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.489663072419827 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1438781931395847) - present_state_Q ( -0.03180320207389345)) * f1( 0.009242931540479199)
w2 ( -15.991757449259113 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1438781931395847) - present_state_Q (-0.03180320207389345)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.49197638515487 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.3749343672853547) - present_state_Q ( -3.4164650134539705)) * f1( 0.01554003729862091)
w2 ( -16.021529742472968 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.3749343672853547) - present_state_Q (-3.4164650134539705)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.492140684664387 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.401369284348204) - present_state_Q ( -3.266277254286434)) * f1( 0.001000943792040705)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.401369284348204) - present_state_Q (-3.266277254286434)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.498443452587976 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08897107507651113) - present_state_Q ( -0.14456357333643777)) * f1( 0.017913720446728305)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08897107507651113) - present_state_Q (-0.14456357333643777)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.501572049437971 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12868262432500266) - present_state_Q ( -0.0341050034486304)) * f1( 0.008612004726965775)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12868262432500266) - present_state_Q (-0.0341050034486304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.509889009584608 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1263152822925804) - present_state_Q ( -0.30321217129597045)) * f1( 0.024727185852692228)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1263152822925804) - present_state_Q (-0.30321217129597045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.515297422672168 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11073205715587663) - present_state_Q ( -0.1554756954592871)) * f1( 0.015410048468326177)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11073205715587663) - present_state_Q (-0.1554756954592871)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.520942792778015 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09550665170920654) - present_state_Q ( -0.147747309020004)) * f1( 0.016056811937822105)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09550665170920654) - present_state_Q (-0.147747309020004)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.523530738458712 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1420531041579467) - present_state_Q ( -0.02570937253239686)) * f1( 0.007104734526300888)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1420531041579467) - present_state_Q (-0.02570937253239686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.526113242987702 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14611237966644866) - present_state_Q ( -0.018491175468348452)) * f1( 0.0070749884235968685)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14611237966644866) - present_state_Q (-0.018491175468348452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.528677282750833 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15236873275891752) - present_state_Q ( -0.010636114392736823)) * f1( 0.00700812027092622)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15236873275891752) - present_state_Q (-0.010636114392736823)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.53121829050944 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15487770574217938) - present_state_Q ( -0.008435673249351794)) * f1( 0.006940518212984488)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15487770574217938) - present_state_Q (-0.008435673249351794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.533724632844338 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14761841423888156) - present_state_Q ( -0.018306150078608013)) * f1( 0.006865704395182352)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14761841423888156) - present_state_Q (-0.018306150078608013)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.54227997458907 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1722574625940319) - present_state_Q ( -0.302221404781351)) * f1( 0.025393753424608714)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1722574625940319) - present_state_Q (-0.302221404781351)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.546609528905835 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11950356990486423) - present_state_Q ( -0.12246197961312948)) * f1( 0.012218101382254756)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11950356990486423) - present_state_Q (-0.12246197961312948)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.54912234331859 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1567652819327719) - present_state_Q ( -0.007034615806839256)) * f1( 0.006860531665664741)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1567652819327719) - present_state_Q (-0.007034615806839256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.551615586320809 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1472727151752898) - present_state_Q ( -0.019858204077217986)) * f1( 0.0068327906996934855)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1472727151752898) - present_state_Q (-0.019858204077217986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.554107200152757 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1526208178266283) - present_state_Q ( -0.011293330709453243)) * f1( 0.006811339872239884)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1526208178266283) - present_state_Q (-0.011293330709453243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.556589252322834 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15847120820665248) - present_state_Q ( -0.00573082415521383)) * f1( 0.0067738173043634)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15847120820665248) - present_state_Q (-0.00573082415521383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.559066647527798 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14698800956610347) - present_state_Q ( -0.02127327489516722)) * f1( 0.006792046414601163)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14698800956610347) - present_state_Q (-0.02127327489516722)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.561547157541463 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.151841438386113) - present_state_Q ( -0.012759450350408076)) * f1( 0.006783848759022085)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.151841438386113) - present_state_Q (-0.012759450350408076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.564021215955941 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15928897858274377) - present_state_Q ( -0.005328033909441559)) * f1( 0.00675110859401494)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15928897858274377) - present_state_Q (-0.005328033909441559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.569973325513393 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18844436313934568) - present_state_Q ( -0.22710330664439965)) * f1( 0.017273469375428778)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18844436313934568) - present_state_Q (-0.22710330664439965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.576413267174113 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22414360169951103) - present_state_Q ( -0.2495032343890357)) * f1( 0.018791885109537998)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.22414360169951103) - present_state_Q (-0.2495032343890357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.58307645867765 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17590594984987382) - present_state_Q ( -0.2159448267000674)) * f1( 0.01928165942426122)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17590594984987382) - present_state_Q (-0.2159448267000674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.591317771774435 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1461900613490503) - present_state_Q ( -0.2089064489227644)) * f1( 0.023820327915156245)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1461900613490503) - present_state_Q (-0.2089064489227644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.594310063147011 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1482165249637871) - present_state_Q ( -0.02092001451661924)) * f1( 0.008202619148519415)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1482165249637871) - present_state_Q (-0.02092001451661924)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.597203813999661 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13788951333516591) - present_state_Q ( -0.026218153212854912)) * f1( 0.007946285248040863)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13788951333516591) - present_state_Q (-0.026218153212854912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.600129302551094 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1455111167822344) - present_state_Q ( -0.02014171241576478)) * f1( 0.008018379826765187)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1455111167822344) - present_state_Q (-0.02014171241576478)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.602973778487664 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15083097484930294) - present_state_Q ( -0.01661909848803362)) * f1( 0.0077876803445440825)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15083097484930294) - present_state_Q (-0.01661909848803362)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.605749963239157 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14159417280090258) - present_state_Q ( -0.02309117644350496)) * f1( 0.007616132331200472)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14159417280090258) - present_state_Q (-0.02309117644350496)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.60848031051643 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14791474381950445) - present_state_Q ( -0.016275170612387632)) * f1( 0.007475108882409071)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14791474381950445) - present_state_Q (-0.016275170612387632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.6111625458 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15609012086185609) - present_state_Q ( -0.011536416483123122)) * f1( 0.007332234745019665)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15609012086185609) - present_state_Q (-0.011536416483123122)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.615076667147287 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10153416393576256) - present_state_Q ( -0.10153416393576256)) * f1( 0.010986426768174818)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10153416393576256) - present_state_Q (-0.10153416393576256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.618153381713766 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15069378225561905) - present_state_Q ( -0.15069378225561905)) * f1( 0.008744530406431146)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15069378225561905) - present_state_Q (-0.15069378225561905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.62175517893483 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13007778746872445) - present_state_Q ( -0.040656802821263334)) * f1( 0.009932101783501372)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13007778746872445) - present_state_Q (-0.040656802821263334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.625174524623082 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13995074186539097) - present_state_Q ( -0.033981088685828366)) * f1( 0.00940910185971623)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13995074186539097) - present_state_Q (-0.033981088685828366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.628426737113191 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12965156112258638) - present_state_Q ( -0.03649500372796877)) * f1( 0.0089579325896784)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12965156112258638) - present_state_Q (-0.03649500372796877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.633198687251834 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14799518632855582) - present_state_Q ( -0.17043997578436465)) * f1( 0.01364026760565841)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14799518632855582) - present_state_Q (-0.17043997578436465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.635678385476432 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15696398035514308) - present_state_Q ( -0.0075544941406453184)) * f1( 0.006771041582493157)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15696398035514308) - present_state_Q (-0.0075544941406453184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.638139245002568 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14621658708108004) - present_state_Q ( -0.020872635454021807)) * f1( 0.006746113751398858)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14621658708108004) - present_state_Q (-0.020872635454021807)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.640706423014917 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1518421721923178) - present_state_Q ( -0.012939497988025088)) * f1( 0.007021219229391918)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1518421721923178) - present_state_Q (-0.012939497988025088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.643251956835794 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15892339430416969) - present_state_Q ( -0.007503960460007237)) * f1( 0.0069503438933898026)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15892339430416969) - present_state_Q (-0.007503960460007237)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.645788375556737 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14630540739845765) - present_state_Q ( -0.022529625333877486)) * f1( 0.006956392013515853)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14630540739845765) - present_state_Q (-0.022529625333877486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.64832036459875 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15116660515468167) - present_state_Q ( -0.014472255905250685)) * f1( 0.006928009856509869)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15116660515468167) - present_state_Q (-0.014472255905250685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.650838229837014 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15848965079046523) - present_state_Q ( -0.006972988543146365)) * f1( 0.006873882248604056)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15848965079046523) - present_state_Q (-0.006972988543146365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.65333568618694 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14790630101901658) - present_state_Q ( -0.018101101623157367)) * f1( 0.006840924529326264)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14790630101901658) - present_state_Q (-0.018101101623157367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.655876953651823 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1262472917930718) - present_state_Q ( -0.05509212544829551)) * f1( 0.007036400330923708)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1262472917930718) - present_state_Q (-0.05509212544829551)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.661117925674942 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1758695454914083) - present_state_Q ( -0.24954837992393059)) * f1( 0.015315041752036325)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1758695454914083) - present_state_Q (-0.24954837992393059)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.664336676703508 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13995596844087435) - present_state_Q ( -0.030910874176703795)) * f1( 0.008849642558172357)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13995596844087435) - present_state_Q (-0.030910874176703795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.667497733224723 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1449600399169532) - present_state_Q ( -0.026305716013666255)) * f1( 0.008678834426706762)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1449600399169532) - present_state_Q (-0.026305716013666255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.670543474341303 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13702462392938444) - present_state_Q ( -0.028055706662867348)) * f1( 0.008368074128339459)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13702462392938444) - present_state_Q (-0.028055706662867348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.673580818407284 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13696429396198442) - present_state_Q ( -0.029115475027573487)) * f1( 0.008347447851468726)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13696429396198442) - present_state_Q (-0.029115475027573487)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.676623271664141 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1453768761699138) - present_state_Q ( -0.02316694996998054)) * f1( 0.008345915676061851)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1453768761699138) - present_state_Q (-0.02316694996998054)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.679307081449526 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13799303450944342) - present_state_Q ( -0.03563660953822687)) * f1( 0.007388872789474275)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13799303450944342) - present_state_Q (-0.03563660953822687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.682000411835187 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13978412451894737) - present_state_Q ( -0.030414704117352996)) * f1( 0.007404074597588099)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13978412451894737) - present_state_Q (-0.030414704117352996)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.684682548632312 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1432230235500543) - present_state_Q ( -0.024181739144027922)) * f1( 0.007359996065016433)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1432230235500543) - present_state_Q (-0.024181739144027922)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.687338413412352 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14853090022163706) - present_state_Q ( -0.016978449140792804)) * f1( 0.007272469261656453)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14853090022163706) - present_state_Q (-0.016978449140792804)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.689960650317262 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15602408756805147) - present_state_Q ( -0.010315480372174623)) * f1( 0.007165842742388916)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15602408756805147) - present_state_Q (-0.010315480372174623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.69254321917688 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14540757603641094) - present_state_Q ( -0.020698794493481835)) * f1( 0.007079583062361254)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14540757603641094) - present_state_Q (-0.020698794493481835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.69520217971074 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15195106060277766) - present_state_Q ( -0.013494525488896241)) * f1( 0.007273326402498545)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15195106060277766) - present_state_Q (-0.013494525488896241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.697823165738711 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15644329394164352) - present_state_Q ( -0.010221958921266818)) * f1( 0.007162159354516361)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15644329394164352) - present_state_Q (-0.010221958921266818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.700406953900082 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1471861149909852) - present_state_Q ( -0.02061012443453746)) * f1( 0.007082408072688364)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1471861149909852) - present_state_Q (-0.02061012443453746)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.703794426397188 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036127464375130626) - present_state_Q ( -0.13732112605161062)) * f1( 0.009622515622904662)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.036127464375130626) - present_state_Q (-0.13732112605161062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.707935964949534 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10008090907353835) - present_state_Q ( -0.13529109786667307)) * f1( 0.011736440159103828)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10008090907353835) - present_state_Q (-0.13529109786667307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.711135170707017 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13591770460163863) - present_state_Q ( -0.14623924104649985)) * f1( 0.009084983833671848)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13591770460163863) - present_state_Q (-0.14623924104649985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.714517704626529 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14162195489704835) - present_state_Q ( -0.0331412852202423)) * f1( 0.00930522769002441)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14162195489704835) - present_state_Q (-0.0331412852202423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.717758591449021 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13332028390723188) - present_state_Q ( -0.03305274819328067)) * f1( 0.008917381003705524)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13332028390723188) - present_state_Q (-0.03305274819328067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.720880244089644 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14119520477760875) - present_state_Q ( -0.026736010468232166)) * f1( 0.008572548016680554)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14119520477760875) - present_state_Q (-0.026736010468232166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.723891883046488 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15055549834814252) - present_state_Q ( -0.021666569476037827)) * f1( 0.008256816065796448)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15055549834814252) - present_state_Q (-0.021666569476037827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.726803444082046 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14015943510047613) - present_state_Q ( -0.025999273708598312)) * f1( 0.007994213501747106)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14015943510047613) - present_state_Q (-0.025999273708598312)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.729735817349118 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14824313915826318) - present_state_Q ( -0.020046889268436327)) * f1( 0.00803643928490378)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14824313915826318) - present_state_Q (-0.020046889268436327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.732575752147929 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13766255902498886) - present_state_Q ( -0.028287237784607514)) * f1( 0.007802987603204135)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13766255902498886) - present_state_Q (-0.028287237784607514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.73548027028433 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14426519129463367) - present_state_Q ( -0.021934358013059957)) * f1( 0.00796508824640336)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14426519129463367) - present_state_Q (-0.021934358013059957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.738310814821718 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1526211642579224) - present_state_Q ( -0.01640469995212737)) * f1( 0.007748703977232603)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1526211642579224) - present_state_Q (-0.01640469995212737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.740982873334326 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05316642982753157) - present_state_Q ( -0.12760866612470367)) * f1( 0.00756576232795159)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.05316642982753157) - present_state_Q (-0.12760866612470367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.746360765527509 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09794810148040398) - present_state_Q ( -0.20046711292011773)) * f1( 0.01552779735892428)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09794810148040398) - present_state_Q (-0.20046711292011773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.749956961254483 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17483149101871817) - present_state_Q ( -0.17483149101871817)) * f1( 0.010284481058024812)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17483149101871817) - present_state_Q (-0.17483149101871817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.752917059694353 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12659416713520855) - present_state_Q ( -0.06837604115826217)) * f1( 0.008226260630441778)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12659416713520855) - present_state_Q (-0.06837604115826217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.757782595165054 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16673798872609186) - present_state_Q ( -0.23980118961878075)) * f1( 0.01418134156481272)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16673798872609186) - present_state_Q (-0.23980118961878075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.761461390257416 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11979889309360983) - present_state_Q ( -0.04785465279082927)) * f1( 0.010167489256009568)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11979889309360983) - present_state_Q (-0.04785465279082927)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.764393395192716 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14751502266139624) - present_state_Q ( -0.020576906812752774)) * f1( 0.00803675759766479)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14751502266139624) - present_state_Q (-0.020576906812752774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.767243328075367 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15277163808307637) - present_state_Q ( -0.017036677359097207)) * f1( 0.007803098015321636)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15277163808307637) - present_state_Q (-0.017036677359097207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.77002340119062 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14358092537223818) - present_state_Q ( -0.02338834328838757)) * f1( 0.0076270056758766585)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14358092537223818) - present_state_Q (-0.02338834328838757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.777907072189427 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14933245571658518) - present_state_Q ( -0.14933245571658518)) * f1( 0.02239889390781854)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14933245571658518) - present_state_Q (-0.14933245571658518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.782579543084234 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11687053505000235) - present_state_Q ( -0.11687053505000235)) * f1( 0.013166023202949248)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11687053505000235) - present_state_Q (-0.11687053505000235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.785478076842319 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13892251077498255) - present_state_Q ( -0.027762672831859)) * f1( 0.007962570458165707)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.13892251077498255) - present_state_Q (-0.027762672831859)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.788416717972565 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14635259120640237) - present_state_Q ( -0.021481741136357226)) * f1( 0.00805720277242436)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14635259120640237) - present_state_Q (-0.021481741136357226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.79127347694787 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1546769421278818) - present_state_Q ( -0.017050928048760808)) * f1( 0.007821410310866991)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1546769421278818) - present_state_Q (-0.017050928048760808)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.794073849604246 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14203286117486402) - present_state_Q ( -0.025682117675745998)) * f1( 0.007687860930324424)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14203286117486402) - present_state_Q (-0.025682117675745998)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.796828812972718 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14791167643899855) - present_state_Q ( -0.01888791848799604)) * f1( 0.00754790225242523)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14791167643899855) - present_state_Q (-0.01888791848799604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.799534064097926 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15580759198782407) - present_state_Q ( -0.01280810334063162)) * f1( 0.007397780345018218)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15580759198782407) - present_state_Q (-0.01280810334063162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.802186067162086 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14513936992727738) - present_state_Q ( -0.022101323959084002)) * f1( 0.007272772367756632)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14513936992727738) - present_state_Q (-0.022101323959084002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.804905761596661 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15195913283663076) - present_state_Q ( -0.015208089589203876)) * f1( 0.007442945124527729)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15195913283663076) - present_state_Q (-0.015208089589203876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.80749639434787 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07445111712677352) - present_state_Q ( -0.11487333736284736)) * f1( 0.007304468998658217)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07445111712677352) - present_state_Q (-0.11487333736284736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.812306891248811 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11503091160999547) - present_state_Q ( -0.18397379639628075)) * f1( 0.013816921159690763)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.11503091160999547) - present_state_Q (-0.18397379639628075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.818545638955618 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15041858941119993) - present_state_Q ( -0.2514802059245454)) * f1( 0.01825459904414949)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15041858941119993) - present_state_Q (-0.2514802059245454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.824821991457304 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21562041779253413) - present_state_Q ( -0.2183590288521302)) * f1( 0.018154060474531712)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.21562041779253413) - present_state_Q (-0.2183590288521302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.830235642070726 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10503472705325205) - present_state_Q ( -0.10503472705325205)) * f1( 0.015208858609077407)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10503472705325205) - present_state_Q (-0.10503472705325205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.833383428734798 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06397105232423486) - present_state_Q ( -0.1377931510476992)) * f1( 0.008935790276325663)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06397105232423486) - present_state_Q (-0.1377931510476992)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.838469466290471 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10733776215224212) - present_state_Q ( -0.19362136688314552)) * f1( 0.014652180254098174)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10733776215224212) - present_state_Q (-0.19362136688314552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.840670017621852 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16632389345498863) - present_state_Q ( -0.03462483902950712)) * f1( 0.006051993974882518)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16632389345498863) - present_state_Q (-0.03462483902950712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.843896726419013 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1471295983463505) - present_state_Q ( -0.028596949611275027)) * f1( 0.008864134094889101)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1471295983463505) - present_state_Q (-0.028596949611275027)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.863222056096541 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08374899690924037) - present_state_Q ( -0.08374899690924037)) * f1( 0.007071067811865476)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.08374899690924037) - present_state_Q (-0.08374899690924037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.882568011190006 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1437541180015492) - present_state_Q ( -0.06058076607113722)) * f1( 0.007071067811865476)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.1437541180015492) - present_state_Q (-0.06058076607113722)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.943571857195748 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16249394682976115) - present_state_Q ( -0.28992174356126515)) * f1( 0.022484223183381533)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.16249394682976115) - present_state_Q (-0.28992174356126515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.016563588357325 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12210190985640824) - present_state_Q ( -0.3084599219912163)) * f1( 0.026925010290510226)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.12210190985640824) - present_state_Q (-0.3084599219912163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.086514978874186 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18567081054742754) - present_state_Q ( -0.33889775729407956)) * f1( 0.025826438328444917)
w2 ( -16.0543586607043 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.18567081054742754) - present_state_Q (-0.33889775729407956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.138413449052877 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.449423471818892) - present_state_Q ( -3.4741341888195234)) * f1( 0.022214140283845215)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -3.449423471818892) - present_state_Q (-3.4741341888195234)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.176858379465676 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11894109370409282) - present_state_Q ( -0.11894109370409282)) * f1( 0.01565479993887292)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11894109370409282) - present_state_Q (-0.11894109370409282)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.198988473070631 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1354517815675713) - present_state_Q ( -0.03857929546974138)) * f1( 0.008981394102822508)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1354517815675713) - present_state_Q (-0.03857929546974138)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.221055518638892 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1443382633563314) - present_state_Q ( -0.03180319841028992)) * f1( 0.00895302133754961)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1443382633563314) - present_state_Q (-0.03180319841028992)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.242203196801663 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15462145865340904) - present_state_Q ( -0.02622268459675751)) * f1( 0.008577716474666194)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.15462145865340904) - present_state_Q (-0.02622268459675751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.26256871882796 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1440127441389466) - present_state_Q ( -0.02992332008743359)) * f1( 0.008262061540503095)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1440127441389466) - present_state_Q (-0.02992332008743359)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.282976755522515 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1528767975809141) - present_state_Q ( -0.02368781540552221)) * f1( 0.00827691782788665)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1528767975809141) - present_state_Q (-0.02368781540552221)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.30271685643204 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15708353414099308) - present_state_Q ( -0.02024143044733398)) * f1( 0.008004766674491839)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.15708353414099308) - present_state_Q (-0.02024143044733398)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.355907907874778 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11640967603010953) - present_state_Q ( -0.14061297963275768)) * f1( 0.02167878430279887)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.11640967603010953) - present_state_Q (-0.14061297963275768)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.377135285012532 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14380170926292363) - present_state_Q ( -0.03305334995538923)) * f1( 0.008612807437525918)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.14380170926292363) - present_state_Q (-0.03305334995538923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.398251511047674 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1534053782844643) - present_state_Q ( -0.026797665748437862)) * f1( 0.008565201171578298)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.1534053782844643) - present_state_Q (-0.026797665748437862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.409009809531605 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19049657104712972) - present_state_Q ( -0.062160760449743196)) * f1( 0.004369410293419985)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.19049657104712972) - present_state_Q (-0.062160760449743196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.456979031164469 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23748393887442734) - present_state_Q ( -0.23748393887442734)) * f1( 0.019618326507072098)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.23748393887442734) - present_state_Q (-0.23748393887442734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.508030241640158 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12435875945490772) - present_state_Q ( -0.2260172746507529)) * f1( 0.020878658600301313)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.12435875945490772) - present_state_Q (-0.2260172746507529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.556460344501525 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.203024818553616) - present_state_Q ( -0.3169812868331465)) * f1( 0.01987423371435976)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.203024818553616) - present_state_Q (-0.3169812868331465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.618290661769091 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22611988198214306) - present_state_Q ( -0.32419344801834576)) * f1( 0.02537837793943643)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.22611988198214306) - present_state_Q (-0.32419344801834576)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.657980760343657 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1394762057943815) - present_state_Q ( -3.4898814455261746)) * f1( 0.0204989691054116)
w2 ( -16.52161482741206 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1394762057943815) - present_state_Q (-3.4898814455261746)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.664033282933165 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15962946985826285) - present_state_Q ( -3.3298972901220862)) * f1( 0.0031000428026511655)
w2 ( -16.912094764291155 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.15962946985826285) - present_state_Q (-3.3298972901220862)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.708782466798004 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22196927607728037) - present_state_Q ( -0.3215762158539692)) * f1( 0.019854507097740278)
w2 ( -16.912094764291155 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.22196927607728037) - present_state_Q (-0.3215762158539692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.76031424067134 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22152318037863375) - present_state_Q ( -0.31820742831047166)) * f1( 0.023826021326889723)
w2 ( -16.912094764291155 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.22152318037863375) - present_state_Q (-0.31820742831047166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.807276895514503 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25577082481234514) - present_state_Q ( -3.726539595352409)) * f1( 0.025770450253052676)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.25577082481234514) - present_state_Q (-3.726539595352409)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.817460317072483 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21263209420393855) - present_state_Q ( -0.0767526948854185)) * f1( 0.005081068491406238)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.21263209420393855) - present_state_Q (-0.0767526948854185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.821573710347465 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1109763089030419) - present_state_Q ( -0.10495468369656005)) * f1( 0.002154736598486204)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.1109763089030419) - present_state_Q (-0.10495468369656005)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.83727240233122 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15411421642023254) - present_state_Q ( -0.02744112455232957)) * f1( 0.008188414951186818)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15411421642023254) - present_state_Q (-0.02744112455232957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.852507943728451 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16297770045136487) - present_state_Q ( -0.020912016253381568)) * f1( 0.007943763715968838)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.16297770045136487) - present_state_Q (-0.020912016253381568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.867316177819268 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1515174987551281) - present_state_Q ( -0.030383678623952)) * f1( 0.007725243735970806)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.1515174987551281) - present_state_Q (-0.030383678623952)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.88246152186593 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15879402372325005) - present_state_Q ( -0.023397692268805288)) * f1( 0.007897930971569104)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15879402372325005) - present_state_Q (-0.023397692268805288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.897217323580847 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16805804684376885) - present_state_Q ( -0.017329238605936763)) * f1( 0.007691988291004285)
w2 ( -17.27656379282588 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.16805804684376885) - present_state_Q (-0.017329238605936763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.937382624830335 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.708828691860725) - present_state_Q ( -3.806286443469981)) * f1( 0.02550427455240199)
w2 ( -17.59153296528333 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -3.708828691860725) - present_state_Q (-3.806286443469981)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.94106667977726 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.735193795682231) - present_state_Q ( -3.5825458812577375)) * f1( 0.0024460374831383365)
w2 ( -17.892759334242996 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -3.735193795682231) - present_state_Q (-3.5825458812577375)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.974300898842918 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21411957190311534) - present_state_Q ( -3.895470834628076)) * f1( 0.023085272756391002)
w2 ( -18.180685055687697 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.21411957190311534) - present_state_Q (-3.895470834628076)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.011477272793964 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.885321544535604) - present_state_Q ( -3.9799166080723842)) * f1( 0.02532629879414136)
w2 ( -18.474264265608777 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -3.885321544535604) - present_state_Q (-3.9799166080723842)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.032699660580448 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12563493452674762) - present_state_Q ( -3.829965456238223)) * f1( 0.01567451101321108)
w2 ( -18.74505278039685 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.12563493452674762) - present_state_Q (-3.829965456238223)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.045438351882995 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15913667904187695) - present_state_Q ( -3.786621762230023)) * f1( 0.00937625434651177)
w2 ( -19.016775172554116 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.15913667904187695) - present_state_Q (-3.786621762230023)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.0912951077767 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25425371974454525) - present_state_Q ( -0.37261558460670086)) * f1( 0.026959278774670997)
w2 ( -19.016775172554116 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.25425371974454525) - present_state_Q (-0.37261558460670086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.115557324250613 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.0265018485690565) - present_state_Q ( -4.0265018485690565)) * f1( 0.017667122119731943)
w2 ( -19.291434693323655 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -4.0265018485690565) - present_state_Q (-4.0265018485690565)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.143059796959415 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.145111229688837) - present_state_Q ( -4.145111229688837)) * f1( 0.021633838855739454)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -4.145111229688837) - present_state_Q (-4.145111229688837)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.165587949726781 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11621297007920622) - present_state_Q ( -0.11621297007920622)) * f1( 0.016567051060964153)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.11621297007920622) - present_state_Q (-0.11621297007920622)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.177462403277548 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15678395316708368) - present_state_Q ( -0.03167108800896043)) * f1( 0.00867586509025978)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.15678395316708368) - present_state_Q (-0.03167108800896043)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.204808993053021 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2072374077877532) - present_state_Q ( -0.2791203525329087)) * f1( 0.020340563256960847)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.2072374077877532) - present_state_Q (-0.2791203525329087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.209622561972413 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22315114480317763) - present_state_Q ( -0.062038714312398616)) * f1( 0.0035230597592593406)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.22315114480317763) - present_state_Q (-0.062038714312398616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.230170148854208 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11496382356241135) - present_state_Q ( -0.15913250370937368)) * f1( 0.015158540060410692)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.11496382356241135) - present_state_Q (-0.15913250370937368)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.24041323212805 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16405421169810525) - present_state_Q ( -0.02357605614598404)) * f1( 0.007479111678917579)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.16405421169810525) - present_state_Q (-0.02357605614598404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.250486194402114 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1718290843464217) - present_state_Q ( -0.01570170932967763)) * f1( 0.007350252432251984)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.1718290843464217) - present_state_Q (-0.01570170932967763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.260392728614441 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17621973599984658) - present_state_Q ( -0.012529888740949938)) * f1( 0.0072269054857334696)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.17621973599984658) - present_state_Q (-0.012529888740949938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.269499409864993 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1668534213021511) - present_state_Q ( -0.022859661761514046)) * f1( 0.007124018944088283)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1668534213021511) - present_state_Q (-0.022859661761514046)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.29394096937691 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20416194368801055) - present_state_Q ( -0.3057577787936468)) * f1( 0.019547149086130172)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.20416194368801055) - present_state_Q (-0.3057577787936468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.318954844697124 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2627195737436124) - present_state_Q ( -0.26774099150525366)) * f1( 0.019934913115840093)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2627195737436124) - present_state_Q (-0.26774099150525366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.348162662177378 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22329191486072647) - present_state_Q ( -0.3239212436356686)) * f1( 0.023389363774684632)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.22329191486072647) - present_state_Q (-0.3239212436356686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.36641990329453 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13165324457428085) - present_state_Q ( -0.13849896512708604)) * f1( 0.014416751425184216)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.13165324457428085) - present_state_Q (-0.13849896512708604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.377214197711616 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15308447530480965) - present_state_Q ( -0.036621935499854145)) * f1( 0.008454225647810497)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15308447530480965) - present_state_Q (-0.036621935499854145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.388062766810748 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16211648735438822) - present_state_Q ( -0.02947622018196751)) * f1( 0.008491381277309864)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.16211648735438822) - present_state_Q (-0.02947622018196751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.419017146588534 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1988418955453177) - present_state_Q ( -0.3546732036086586)) * f1( 0.024854067170173622)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1988418955453177) - present_state_Q (-0.3546732036086586)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.45893597739626 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27898032488731744) - present_state_Q ( -0.44929720753240654)) * f1( 0.03227630970107341)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.27898032488731744) - present_state_Q (-0.44929720753240654)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.473687286076052 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011128674505825067) - present_state_Q ( -0.19384441231285635)) * f1( 0.011710631574605229)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.011128674505825067) - present_state_Q (-0.19384441231285635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.491864811701229 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04992237218126327) - present_state_Q ( -0.17327905605961297)) * f1( 0.014402655056715494)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.04992237218126327) - present_state_Q (-0.17327905605961297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.496568638520197 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1043922137754743) - present_state_Q ( -0.1043922137754743)) * f1( 0.00370517521457203)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1043922137754743) - present_state_Q (-0.1043922137754743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.53181816407346 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2370355061110268) - present_state_Q ( -0.39043836139765603)) * f1( 0.0283755336463903)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2370355061110268) - present_state_Q (-0.39043836139765603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.56601641392117 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23420026282923423) - present_state_Q ( -0.39340168466272185)) * f1( 0.027536463831855987)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.23420026282923423) - present_state_Q (-0.39340168466272185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.581569757951291 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18593049362279834) - present_state_Q ( -0.18593049362279834)) * f1( 0.01232250221903698)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.18593049362279834) - present_state_Q (-0.18593049362279834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.593950487032044 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2050472787323982) - present_state_Q ( -0.2050472787323982)) * f1( 0.009822312429128049)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2050472787323982) - present_state_Q (-0.2050472787323982)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.625377307556157 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2057264041758588) - present_state_Q ( -0.36591757123341384)) * f1( 0.02525480699583)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.2057264041758588) - present_state_Q (-0.36591757123341384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.655369675902975 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08601690428240959) - present_state_Q ( -0.18651711384048786)) * f1( 0.023782089407932527)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.08601690428240959) - present_state_Q (-0.18651711384048786)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.669135339833726 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15762653829858614) - present_state_Q ( -0.03556338297259912)) * f1( 0.010780161963040468)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.15762653829858614) - present_state_Q (-0.03556338297259912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.67903329463217 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17811049015334243) - present_state_Q ( -0.018994445479843938)) * f1( 0.007739998321538875)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.17811049015334243) - present_state_Q (-0.018994445479843938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.688713561271372 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1673271124638819) - present_state_Q ( -0.02692584914970109)) * f1( 0.007575107622724417)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1673271124638819) - present_state_Q (-0.02692584914970109)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.698228143820428 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17481156161721081) - present_state_Q ( -0.018825332667389297)) * f1( 0.0074403026037644215)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.17481156161721081) - present_state_Q (-0.018825332667389297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.728609403514351 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24645684195705156) - present_state_Q ( -0.35319144441892025)) * f1( 0.024381672110678997)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.24645684195705156) - present_state_Q (-0.35319144441892025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.75861573635239 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24434435589983114) - present_state_Q ( -0.3501147901844105)) * f1( 0.024075248296041688)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.24434435589983114) - present_state_Q (-0.3501147901844105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.765794298975422 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1887515901832825) - present_state_Q ( -0.024114129081437785)) * f1( 0.005615270042386119)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1887515901832825) - present_state_Q (-0.024114129081437785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.777644674632029 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07919187665711944) - present_state_Q ( -0.16114361351069553)) * f1( 0.009378252356024531)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.07919187665711944) - present_state_Q (-0.16114361351069553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.795689473442026 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12551063985409402) - present_state_Q ( -0.22026744505616452)) * f1( 0.014342298481266245)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.12551063985409402) - present_state_Q (-0.22026744505616452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.803786611661378 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17555910010816048) - present_state_Q ( -0.04078110613660182)) * f1( 0.006342729046366716)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.17555910010816048) - present_state_Q (-0.04078110613660182)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.816418424095202 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14361672398317193) - present_state_Q ( -0.052871965997775314)) * f1( 0.009906735750482478)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.14361672398317193) - present_state_Q (-0.052871965997775314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.82643982521964 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1765399724155928) - present_state_Q ( -0.021139874532662844)) * f1( 0.007837942040947392)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1765399724155928) - present_state_Q (-0.021139874532662844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.836194963470799 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16411694639829158) - present_state_Q ( -0.031640048994292035)) * f1( 0.007636705995864948)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.16411694639829158) - present_state_Q (-0.031640048994292035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.846184472751652 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1718400948465738) - present_state_Q ( -0.0240323469829742)) * f1( 0.007815054021097214)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1718400948465738) - present_state_Q (-0.0240323469829742)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.855931799600016 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18176973780782718) - present_state_Q ( -0.017626162061747098)) * f1( 0.007621176824178507)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.18176973780782718) - present_state_Q (-0.017626162061747098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.86546665438255 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16941647913673435) - present_state_Q ( -0.026603613355535103)) * f1( 0.007461008230383891)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.16941647913673435) - present_state_Q (-0.026603613355535103)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.875173741083984 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17802260435529293) - present_state_Q ( -0.018941197305277394)) * f1( 0.0075907169810810115)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.17802260435529293) - present_state_Q (-0.018941197305277394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.88467641131975 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18270484517845711) - present_state_Q ( -0.015606518672118559)) * f1( 0.007428658918179549)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.18270484517845711) - present_state_Q (-0.015606518672118559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.893998223914886 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17314252582502507) - present_state_Q ( -0.02469276661575224)) * f1( 0.007292999957097368)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.17314252582502507) - present_state_Q (-0.02469276661575224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.903204848680687 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1806440974528747) - present_state_Q ( -0.01585027171214714)) * f1( 0.007197480139619929)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1806440974528747) - present_state_Q (-0.01585027171214714)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.91228849404494 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18728251435082183) - present_state_Q ( -0.011106893922906895)) * f1( 0.007098337686668481)
w2 ( -19.545688900283366 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.18728251435082183) - present_state_Q (-0.011106893922906895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.921296548223236 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17481068494399826) - present_state_Q ( -0.025423873155195825)) * f1( 0.007047839517598578)
w2 ( -19.80131487348557 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.17481068494399826) - present_state_Q (-0.025423873155195825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.927476157008961 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18138706862810255) - present_state_Q ( -3.976151370407978)) * f1( 0.006997450666058297)
w2 ( -19.977939449510085 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.18138706862810255) - present_state_Q (-3.976151370407978)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.94327155893958 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.204760129772895) - present_state_Q ( -4.282664417819352)) * f1( 0.019710902029150587)
w2 ( -20.13821016575899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -4.204760129772895) - present_state_Q (-4.282664417819352)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.959940395360638 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.242273150335958) - present_state_Q ( -4.376628400838716)) * f1( 0.023779312433041357)
w2 ( -20.27840628343896 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -4.242273150335958) - present_state_Q (-4.376628400838716)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.981205102818647 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28611638212706797) - present_state_Q ( -0.3941714625057055)) * f1( 0.027067781597620848)
w2 ( -20.27840628343896 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.28611638212706797) - present_state_Q (-0.3941714625057055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.982130875874665 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.272052203237404) - present_state_Q ( -4.115366623756959)) * f1( 0.002042074152903679)
w2 ( -20.369076159917352 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -4.272052203237404) - present_state_Q (-4.115366623756959)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.98311333739517 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.332736449808641) - present_state_Q ( -4.15271639224383)) * f1( 0.0021821749303258197)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -4.332736449808641) - present_state_Q (-4.15271639224383)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.996135787966852 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21605330367109768) - present_state_Q ( -0.26756462237933337)) * f1( 0.018439706990240685)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.21605330367109768) - present_state_Q (-0.26756462237933337)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.008160820570184 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22377606565647673) - present_state_Q ( -0.2879778186572342)) * f1( 0.017074857103859253)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.22377606565647673) - present_state_Q (-0.2879778186572342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.017431689911085 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11548409666870334) - present_state_Q ( -0.11548409666870334)) * f1( 0.012868696580161083)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11548409666870334) - present_state_Q (-0.11548409666870334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.023424175081447 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16430139283404227) - present_state_Q ( -0.033981194798754936)) * f1( 0.00821948250880183)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16430139283404227) - present_state_Q (-0.033981194798754936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.029258200421017 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17231589243727993) - present_state_Q ( -0.026409991765165675)) * f1( 0.007992954718747673)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17231589243727993) - present_state_Q (-0.026409991765165675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.034936707182421 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1824125493778073) - present_state_Q ( -0.019873579258530095)) * f1( 0.007771850223768481)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1824125493778073) - present_state_Q (-0.019873579258530095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.04047138104558 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1698476946236204) - present_state_Q ( -0.028660794879853922)) * f1( 0.007585421396984655)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1698476946236204) - present_state_Q (-0.028660794879853922)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.046104498037984 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1784906121136467) - present_state_Q ( -0.020984907236276086)) * f1( 0.007711314643299086)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1784906121136467) - present_state_Q (-0.020984907236276086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.051608683977577 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18508548831228558) - present_state_Q ( -0.016782187204239178)) * f1( 0.007529805865680432)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.18508548831228558) - present_state_Q (-0.016782187204239178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.056975064637742 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13707074995569435) - present_state_Q ( -0.07250926509150642)) * f1( 0.007402582547957094)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13707074995569435) - present_state_Q (-0.07250926509150642)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.067676487888484 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1987171902572451) - present_state_Q ( -0.29873807491761967)) * f1( 0.015224085542858895)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1987171902572451) - present_state_Q (-0.29873807491761967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.07426066247958 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1747511406482367) - present_state_Q ( -0.03610404650154043)) * f1( 0.009032397858694394)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1747511406482367) - present_state_Q (-0.03610404650154043)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.080557987961937 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16235931282471683) - present_state_Q ( -0.038748530973077996)) * f1( 0.008643493391096699)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16235931282471683) - present_state_Q (-0.038748530973077996)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.086838528638044 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17280058975357138) - present_state_Q ( -0.03137671356048066)) * f1( 0.008610508764647699)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17280058975357138) - present_state_Q (-0.03137671356048066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.09288606127709 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17797013131348582) - present_state_Q ( -0.026755957570612086)) * f1( 0.008285223050004703)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17797013131348582) - present_state_Q (-0.026755957570612086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.098737437116176 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16898227549657543) - present_state_Q ( -0.030329981375782963)) * f1( 0.008021400256251861)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16898227549657543) - present_state_Q (-0.030329981375782963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.10444298574944 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17760642390349884) - present_state_Q ( -0.022669540498888453)) * f1( 0.007812364324588653)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17760642390349884) - present_state_Q (-0.022669540498888453)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.11000882113946 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.186153984936666) - present_state_Q ( -0.01777436393870467)) * f1( 0.007615065495516002)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.186153984936666) - present_state_Q (-0.01777436393870467)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.11548016646452 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17232255656708154) - present_state_Q ( -0.02878822352152033)) * f1( 0.007498504527269491)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17232255656708154) - present_state_Q (-0.02878822352152033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.120874272709083 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17935856371064887) - present_state_Q ( -0.020294169247162472)) * f1( 0.007383340985400904)
w2 ( -20.459120409519148 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17935856371064887) - present_state_Q (-0.020294169247162472)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.126181954031303 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18898994158595384) - present_state_Q ( -0.013211609732333483)) * f1( 0.007257052305530101)
w2 ( -20.605396916805056 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.18898994158595384) - present_state_Q (-0.013211609732333483)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.13356320854033 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.40433537883377) - present_state_Q ( -4.441712355797014)) * f1( 0.022321042861294577)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -4.40433537883377) - present_state_Q (-4.441712355797014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.149033754576308 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21822344148751083) - present_state_Q ( -0.37292866017933834)) * f1( 0.02559859209421657)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.21822344148751083) - present_state_Q (-0.37292866017933834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.157265785065567 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1331115169273419) - present_state_Q ( -0.16699709052538902)) * f1( 0.013190380546456881)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1331115169273419) - present_state_Q (-0.16699709052538902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.161500131589708 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18810458198369273) - present_state_Q ( -0.01412662429605743)) * f1( 0.0066168854447852836)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18810458198369273) - present_state_Q (-0.01412662429605743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.165736978575245 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19672814066636748) - present_state_Q ( -0.004665001061452818)) * f1( 0.0066101287391822465)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19672814066636748) - present_state_Q (-0.004665001061452818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.170005083384002 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18058326053087717) - present_state_Q ( -0.02753871572527086)) * f1( 0.006684433825525961)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18058326053087717) - present_state_Q (-0.02753871572527086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.173985663402217 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18161937571342193) - present_state_Q ( -0.027158203733834534)) * f1( 0.006233658179039154)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18161937571342193) - present_state_Q (-0.027158203733834534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.178227343429258 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1869034112932352) - present_state_Q ( -0.01631671215367606)) * f1( 0.006630739043108239)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1869034112932352) - present_state_Q (-0.01631671215367606)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.19237117607432 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12142506088380527) - present_state_Q ( -0.1723011631955466)) * f1( 0.022686532493551043)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12142506088380527) - present_state_Q (-0.1723011631955466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.197565101305006 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17000104112132647) - present_state_Q ( -0.030769773598522933)) * f1( 0.008139862816869025)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17000104112132647) - present_state_Q (-0.030769773598522933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.202516198326151 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09502714948603783) - present_state_Q ( -0.14283231164750956)) * f1( 0.007907469743622761)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.09502714948603783) - present_state_Q (-0.14283231164750956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.21081321318405 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15281974275603696) - present_state_Q ( -0.1881434674151074)) * f1( 0.013335480430618267)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.15281974275603696) - present_state_Q (-0.1881434674151074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.219630592005634 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19417503828722485) - present_state_Q ( -0.2232484305616865)) * f1( 0.014242736546366706)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19417503828722485) - present_state_Q (-0.2232484305616865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.22574027515646 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09852672720272153) - present_state_Q ( -0.16614063015811012)) * f1( 0.009793775630825082)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.09852672720272153) - present_state_Q (-0.16614063015811012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.23397539644026 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15775951870446508) - present_state_Q ( -0.19128868376243927)) * f1( 0.013241643790631464)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.15775951870446508) - present_state_Q (-0.19128868376243927)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.238722982838866 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17829833657105582) - present_state_Q ( -0.02305843974480642)) * f1( 0.0074304195215457005)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17829833657105582) - present_state_Q (-0.02305843974480642)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.243412089686979 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08898813179212095) - present_state_Q ( -0.14090579971526032)) * f1( 0.00748745977064443)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08898813179212095) - present_state_Q (-0.14090579971526032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.252009525391044 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1512437400944971) - present_state_Q ( -0.19679467880247592)) * f1( 0.013837927413492964)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1512437400944971) - present_state_Q (-0.19679467880247592)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.256869076715148 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1845505838872218) - present_state_Q ( -0.019557513859188087)) * f1( 0.007600746746358307)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1845505838872218) - present_state_Q (-0.019557513859188087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.261665741644343 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17169019281043008) - present_state_Q ( -0.03015517432773755)) * f1( 0.007516357872053167)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17169019281043008) - present_state_Q (-0.03015517432773755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.266575335060441 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17985684160248314) - present_state_Q ( -0.02216478929335728)) * f1( 0.007682713813019932)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17985684160248314) - present_state_Q (-0.02216478929335728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.27137767215069 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19046619833112685) - present_state_Q ( -0.016474599755437226)) * f1( 0.007506944492799474)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19046619833112685) - present_state_Q (-0.016474599755437226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.276122801367324 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17401232971806518) - present_state_Q ( -0.03025351776180847)) * f1( 0.007435445606651258)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17401232971806518) - present_state_Q (-0.03025351776180847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.280812690505725 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18052786194784146) - present_state_Q ( -0.02164246548233851)) * f1( 0.0073382355670574195)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18052786194784146) - present_state_Q (-0.02164246548233851)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.285434998110173 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1896399775871133) - present_state_Q ( -0.013520312211269066)) * f1( 0.00722228293748977)
w2 ( -20.671534100044166 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1896399775871133) - present_state_Q (-0.013520312211269066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.289985089101071 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17666818909575668) - present_state_Q ( -0.026320407371688233)) * f1( 0.007125139995791046)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17666818909575668) - present_state_Q (-0.026320407371688233)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.298103487946774 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11064282112568984) - present_state_Q ( -0.17841817655087705)) * f1( 0.01527809827597435)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11064282112568984) - present_state_Q (-0.17841817655087705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.30293408588487 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15802214500548512) - present_state_Q ( -0.04419036627744612)) * f1( 0.00885906863457563)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15802214500548512) - present_state_Q (-0.04419036627744612)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.30761084933749 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16561127152759741) - present_state_Q ( -0.036863781942005645)) * f1( 0.008564244692518898)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16561127152759741) - present_state_Q (-0.036863781942005645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.312137363710443 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17469354785090824) - present_state_Q ( -0.029232875607005472)) * f1( 0.00827616202810778)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17469354785090824) - present_state_Q (-0.029232875607005472)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.316522652514934 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1857668430358007) - present_state_Q ( -0.023165932840659036)) * f1( 0.008007445274899898)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1857668430358007) - present_state_Q (-0.023165932840659036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.320780636931204 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17304618419080253) - present_state_Q ( -0.02955501348561065)) * f1( 0.007785881877471716)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17304618419080253) - present_state_Q (-0.02955501348561065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.325082548833633 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1826485090566428) - present_state_Q ( -0.022191223753472822)) * f1( 0.007854250126333066)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1826485090566428) - present_state_Q (-0.022191223753472822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.32706801842937 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2281038133480337) - present_state_Q ( -0.0781249412144603)) * f1( 0.0036593196366000677)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2281038133480337) - present_state_Q (-0.0781249412144603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.329926972023943 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2123451908081676) - present_state_Q ( -0.03903873349521222)) * f1( 0.005233016631574336)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2123451908081676) - present_state_Q (-0.03903873349521222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.334563374573937 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17444687041410403) - present_state_Q ( -0.03113853150084856)) * f1( 0.008480071651664986)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17444687041410403) - present_state_Q (-0.03113853150084856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.339039762580843 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18508365286659134) - present_state_Q ( -0.02544540411948776)) * f1( 0.008177296125367172)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18508365286659134) - present_state_Q (-0.02544540411948776)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.343406238843324 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16973173713642498) - present_state_Q ( -0.03346580380549132)) * f1( 0.007990461495786031)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16973173713642498) - present_state_Q (-0.03346580380549132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.34782900086923 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16662026323936074) - present_state_Q ( -0.03629944513315515)) * f1( 0.008098122321707923)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16662026323936074) - present_state_Q (-0.03629944513315515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.352313584756281 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17548001272118632) - present_state_Q ( -0.028631851976535665)) * f1( 0.008198478592314838)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17548001272118632) - present_state_Q (-0.028631851976535665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.356664086934519 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18637333294685077) - present_state_Q ( -0.022406486784408264)) * f1( 0.007942736219856627)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18637333294685077) - present_state_Q (-0.022406486784408264)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.360892025098154 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1735867044294223) - present_state_Q ( -0.029497974929488753)) * f1( 0.007730784156122538)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1735867044294223) - present_state_Q (-0.029497974929488753)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.365172362683339 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18300023065269647) - present_state_Q ( -0.021979990475411222)) * f1( 0.00781450908719382)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18300023065269647) - present_state_Q (-0.021979990475411222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.369334501636315 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16998009137897835) - present_state_Q ( -0.03354974839856765)) * f1( 0.007616615500615259)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16998009137897835) - present_state_Q (-0.03354974839856765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.373611201582927 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1776452506458759) - present_state_Q ( -0.025651250814259337)) * f1( 0.007813869128170638)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1776452506458759) - present_state_Q (-0.025651250814259337)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.377789240610726 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18755272444465534) - present_state_Q ( -0.018576055081578363)) * f1( 0.007622374642476146)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18755272444465534) - present_state_Q (-0.018576055081578363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.381829252718832 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13908335495334068) - present_state_Q ( -0.07895500262374246)) * f1( 0.007459323718408579)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13908335495334068) - present_state_Q (-0.07895500262374246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.389536309849758 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2005473448926756) - present_state_Q ( -0.30046343525784047)) * f1( 0.014819283674248078)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2005473448926756) - present_state_Q (-0.30046343525784047)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.394615068752987 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1759475947077037) - present_state_Q ( -0.04059188242756125)) * f1( 0.009304983388815934)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1759475947077037) - present_state_Q (-0.04059188242756125)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.399496803375584 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.161896437305112) - present_state_Q ( -0.04214471366992644)) * f1( 0.008948857576828045)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.161896437305112) - present_state_Q (-0.04214471366992644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.404200834884222 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17104768821222985) - present_state_Q ( -0.034362401962214635)) * f1( 0.008609378149133363)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17104768821222985) - present_state_Q (-0.034362401962214635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.408738710400677 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1819036313591157) - present_state_Q ( -0.027410404942997527)) * f1( 0.00829307781067709)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1819036313591157) - present_state_Q (-0.027410404942997527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.41312004930936 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1690369746680849) - present_state_Q ( -0.03481910292172983)) * f1( 0.008019747526479503)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1690369746680849) - present_state_Q (-0.03481910292172983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.417559226086988 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17811489754145443) - present_state_Q ( -0.027137239417664088)) * f1( 0.008112860193570002)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17811489754145443) - present_state_Q (-0.027137239417664088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.421870168558403 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18930333789922682) - present_state_Q ( -0.021467524545325564)) * f1( 0.00786874200284996)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18930333789922682) - present_state_Q (-0.021467524545325564)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.433105807984655 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18291804064263045) - present_state_Q ( -0.3073171972753908)) * f1( 0.021639966171219195)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18291804064263045) - present_state_Q (-0.3073171972753908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.445275971431052 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14234867585070707) - present_state_Q ( -0.1790328101238544)) * f1( 0.022892144459001118)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.14234867585070707) - present_state_Q (-0.1790328101238544)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.44944122887324 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17507875531982334) - present_state_Q ( -0.029209662523719485)) * f1( 0.007615563222902336)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17507875531982334) - present_state_Q (-0.029209662523719485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.453673979706435 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18417718155654236) - present_state_Q ( -0.02141633797274226)) * f1( 0.007726669966053267)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18417718155654236) - present_state_Q (-0.02141633797274226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.457809195492018 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1895464818634708) - present_state_Q ( -0.017644674377428636)) * f1( 0.007542692313750845)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1895464818634708) - present_state_Q (-0.017644674377428636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.461856260397262 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17906856976018537) - present_state_Q ( -0.026636595733942902)) * f1( 0.007395446826153059)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17906856976018537) - present_state_Q (-0.026636595733942902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.465850214616358 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18685384293351107) - present_state_Q ( -0.017685448808849587)) * f1( 0.007285441212662166)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18685384293351107) - present_state_Q (-0.017685448808849587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.469786486324404 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1947369285773424) - present_state_Q ( -0.012386798789005954)) * f1( 0.00717225795068843)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1947369285773424) - present_state_Q (-0.012386798789005954)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.473683976186537 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18063528496938142) - present_state_Q ( -0.02747606650936051)) * f1( 0.007123008042604211)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18063528496938142) - present_state_Q (-0.02747606650936051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.477556733625626 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18728889444937302) - present_state_Q ( -0.017835691768046575)) * f1( 0.007064501610146148)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18728889444937302) - present_state_Q (-0.017835691768046575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.48945437098452 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1245358871616238) - present_state_Q ( -0.1712976927192603)) * f1( 0.022354486144725044)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1245358871616238) - present_state_Q (-0.1712976927192603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.493938665116609 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06763058234176529) - present_state_Q ( -0.16294552008919977)) * f1( 0.00842133453750917)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06763058234176529) - present_state_Q (-0.16294552008919977)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.50199004121946 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11655851750309532) - present_state_Q ( -0.24202436478358308)) * f1( 0.015333807830686328)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11655851750309532) - present_state_Q (-0.24202436478358308)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.506316459578018 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17840592745286957) - present_state_Q ( -0.10797704703993406)) * f1( 0.00802531036453382)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17840592745286957) - present_state_Q (-0.10797704703993406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.518515407382132 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1882886096802198) - present_state_Q ( -0.375517804882839)) * f1( 0.023805544431549776)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1882886096802198) - present_state_Q (-0.375517804882839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.523810531455778 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22030522291830204) - present_state_Q ( -0.22030522291830204)) * f1( 0.010023274063712458)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.22030522291830204) - present_state_Q (-0.22030522291830204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.536053458565181 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26121594242458496) - present_state_Q ( -0.36387360287870474)) * f1( 0.023803403609297922)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.26121594242458496) - present_state_Q (-0.36387360287870474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.54246607066167 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20036181378936668) - present_state_Q ( -0.16679433680355946)) * f1( 0.012021366635287958)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.20036181378936668) - present_state_Q (-0.16679433680355946)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.555409635800805 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2253301675406373) - present_state_Q ( -0.4036189485139901)) * f1( 0.025379452137693558)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.2253301675406373) - present_state_Q (-0.4036189485139901)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.560159589195106 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11310560458407483) - present_state_Q ( -0.19704232825756623)) * f1( 0.008970009374335692)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.11310560458407483) - present_state_Q (-0.19704232825756623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.572577266921801 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1930928242189753) - present_state_Q ( -0.38411984470205496)) * f1( 0.02427084984088977)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1930928242189753) - present_state_Q (-0.38411984470205496)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.574066222053519 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12589360378188633) - present_state_Q ( -0.12057150466506372)) * f1( 0.0027711176378312397)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12589360378188633) - present_state_Q (-0.12057150466506372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.578589050729454 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17435451540562683) - present_state_Q ( -0.03213370842563608)) * f1( 0.008273862780521892)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17435451540562683) - present_state_Q (-0.03213370842563608)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.582977707506119 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18434722800366876) - present_state_Q ( -0.024706754316465875)) * f1( 0.008016058236735034)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18434722800366876) - present_state_Q (-0.024706754316465875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.587369446359496 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.169672811117866) - present_state_Q ( -0.03658695885743252)) * f1( 0.008041292455400015)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.169672811117866) - present_state_Q (-0.03658695885743252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.591830051984784 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17851343831257377) - present_state_Q ( -0.02877760558919739)) * f1( 0.008154407879906294)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17851343831257377) - present_state_Q (-0.02877760558919739)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.596160851749776 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18943867910983478) - present_state_Q ( -0.02232626960391043)) * f1( 0.007906207160250233)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18943867910983478) - present_state_Q (-0.02232626960391043)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.600371477873788 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17641549259224074) - present_state_Q ( -0.030022598349778724)) * f1( 0.007699469434467554)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.17641549259224074) - present_state_Q (-0.030022598349778724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.60464052993959 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18582710371373204) - present_state_Q ( -0.022286046993258864)) * f1( 0.007793938639199688)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18582710371373204) - present_state_Q (-0.022286046993258864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.608806027587484 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1906420129732095) - present_state_Q ( -0.018681176020850024)) * f1( 0.007599211710469187)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1906420129732095) - present_state_Q (-0.018681176020850024)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.612874862282158 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1808524792741325) - present_state_Q ( -0.026725131527821116)) * f1( 0.007435105998158387)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1808524792741325) - present_state_Q (-0.026725131527821116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.616886232779812 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18893021976011587) - present_state_Q ( -0.01780536731418502)) * f1( 0.007317093467801615)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18893021976011587) - present_state_Q (-0.01780536731418502)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.620836714257807 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.195973406690023) - present_state_Q ( -0.012998043358716616)) * f1( 0.007198789064888642)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.195973406690023) - present_state_Q (-0.012998043358716616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.624739591454427 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18284573704807894) - present_state_Q ( -0.027043064322363582)) * f1( 0.007132001364488677)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18284573704807894) - present_state_Q (-0.027043064322363582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.628615270920715 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18982142801527657) - present_state_Q ( -0.01727368124961898)) * f1( 0.007068780589484126)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18982142801527657) - present_state_Q (-0.01727368124961898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.632453977179681 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1998236948092894) - present_state_Q ( -0.009924793881692982)) * f1( 0.006990700539780328)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1998236948092894) - present_state_Q (-0.009924793881692982)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.636259566216728 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18670215035822257) - present_state_Q ( -0.020857093792298484)) * f1( 0.006945878739563196)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.18670215035822257) - present_state_Q (-0.020857093792298484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.640148790766924 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19586438786928245) - present_state_Q ( -0.012036679722675187)) * f1( 0.007085935979780834)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.19586438786928245) - present_state_Q (-0.012036679722675187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.643950428785915 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1449688719009385) - present_state_Q ( -0.07421772114693599)) * f1( 0.00701230343578213)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1449688719009385) - present_state_Q (-0.07421772114693599)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.651868545291336 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20898758410291723) - present_state_Q ( -0.3117661874737693)) * f1( 0.015255792646490395)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.20898758410291723) - present_state_Q (-0.3117661874737693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.656771857469831 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1810170249629269) - present_state_Q ( -0.036919029199522314)) * f1( 0.008976666612632536)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.1810170249629269) - present_state_Q (-0.036919029199522314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.660703853607119 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16827507756914692) - present_state_Q ( -0.03920497531682765)) * f1( 0.008650859259455298)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.16827507756914692) - present_state_Q (-0.03920497531682765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.664507892469782 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17744707212895697) - present_state_Q ( -0.03136202620415981)) * f1( 0.008353238528420301)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.17744707212895697) - present_state_Q (-0.03136202620415981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.668141316684586 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1570864559269806) - present_state_Q ( -0.08327803763383936)) * f1( 0.008074245807630293)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1570864559269806) - present_state_Q (-0.08327803763383936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.67428367796634 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21078955717681927) - present_state_Q ( -0.30111007755897207)) * f1( 0.014326022910908776)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21078955717681927) - present_state_Q (-0.30111007755897207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.678846466214239 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14859126684895524) - present_state_Q ( -0.0598862902934582)) * f1( 0.010088952235743584)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14859126684895524) - present_state_Q (-0.0598862902934582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.688923370855635 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2520317561562044) - present_state_Q ( -0.367786272289806)) * f1( 0.02385064414324195)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2520317561562044) - present_state_Q (-0.367786272289806)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.69884724891769 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24991711913342848) - present_state_Q ( -0.36372465907890944)) * f1( 0.023467066320154956)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.24991711913342848) - present_state_Q (-0.36372465907890944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.708523500443922 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13104274323048748) - present_state_Q ( -0.1727234658644403)) * f1( 0.02195173290889847)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13104274323048748) - present_state_Q (-0.1727234658644403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.712351437959418 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18001125398262302) - present_state_Q ( -0.03292040128107943)) * f1( 0.008408121103160847)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.18001125398262302) - present_state_Q (-0.03292040128107943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.716128520444023 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18645942881248048) - present_state_Q ( -0.027847962358829897)) * f1( 0.008286011674354408)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.18645942881248048) - present_state_Q (-0.027847962358829897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.719786103674155 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17583136758241832) - present_state_Q ( -0.0323175755865458)) * f1( 0.008033609245230831)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.17583136758241832) - present_state_Q (-0.0323175755865458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.723309296220295 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15046676306572546) - present_state_Q ( -0.08051687658288767)) * f1( 0.007825636916229438)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15046676306572546) - present_state_Q (-0.08051687658288767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.729573646619514 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21128425433629067) - present_state_Q ( -0.30564816319930194)) * f1( 0.014625853695539509)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.21128425433629067) - present_state_Q (-0.30564816319930194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.733817509465498 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16616296578602335) - present_state_Q ( -0.048624311212068304)) * f1( 0.00935682869422169)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.16616296578602335) - present_state_Q (-0.048624311212068304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.737271151715339 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1473191266460007) - present_state_Q ( -0.08207509740264873)) * f1( 0.007674346050674408)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1473191266460007) - present_state_Q (-0.08207509740264873)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.743515393053144 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20844464214190384) - present_state_Q ( -0.3050870729188072)) * f1( 0.014577960301730027)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.20844464214190384) - present_state_Q (-0.3050870729188072)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.74692100419942 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16232179940675995) - present_state_Q ( -0.051018732742098644)) * f1( 0.009409630791272378)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16232179940675995) - present_state_Q (-0.051018732742098644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.749732721813698 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17677396384964716) - present_state_Q ( -0.03177899947855838)) * f1( 0.007724568149824939)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17677396384964716) - present_state_Q (-0.03177899947855838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.752592982089947 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18591256419308996) - present_state_Q ( -0.023821056417022767)) * f1( 0.007838822505900282)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18591256419308996) - present_state_Q (-0.023821056417022767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.755383978149958 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.194066682470307) - present_state_Q ( -0.018949099972499217)) * f1( 0.007637093384206866)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.194066682470307) - present_state_Q (-0.018949099972499217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.758118560530255 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18045393839235985) - present_state_Q ( -0.029638280387731752)) * f1( 0.007507482005866185)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18045393839235985) - present_state_Q (-0.029638280387731752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.760816839428845 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18796428085481892) - present_state_Q ( -0.020751303095158657)) * f1( 0.0073882655735309334)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18796428085481892) - present_state_Q (-0.020751303095158657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.763474232860089 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19820549603919463) - present_state_Q ( -0.013743218404991708)) * f1( 0.007260347532829351)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.19820549603919463) - present_state_Q (-0.013743218404991708)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.766088260309507 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18493825508006356) - present_state_Q ( -0.024160880750530238)) * f1( 0.007164856000546798)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18493825508006356) - present_state_Q (-0.024160880750530238)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.768759461799663 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19406967911298223) - present_state_Q ( -0.01563218166115081)) * f1( 0.007302666963381053)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.19406967911298223) - present_state_Q (-0.01563218166115081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.771378341110962 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18057860495268027) - present_state_Q ( -0.02945157495324477)) * f1( 0.007189439390685354)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18057860495268027) - present_state_Q (-0.02945157495324477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.774085767151917 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1882868166678294) - present_state_Q ( -0.020615325572110238)) * f1( 0.007412970259261806)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1882868166678294) - present_state_Q (-0.020615325572110238)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.776750430961641 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19872748102884571) - present_state_Q ( -0.01398481863934996)) * f1( 0.00728058790965932)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.19872748102884571) - present_state_Q (-0.01398481863934996)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.779371693154708 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1854856728971399) - present_state_Q ( -0.023691256201320784)) * f1( 0.007183653427946461)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1854856728971399) - present_state_Q (-0.023691256201320784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.78204506497686 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19484218660689986) - present_state_Q ( -0.01536874778263846)) * f1( 0.007307919681986895)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.19484218660689986) - present_state_Q (-0.01536874778263846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.78454242850566 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10628690971675561) - present_state_Q ( -0.19376689282900916)) * f1( 0.007195083052009447)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10628690971675561) - present_state_Q (-0.19376689282900916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.789248986864353 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14378379340692968) - present_state_Q ( -0.07791873234500712)) * f1( 0.013108260188443181)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14378379340692968) - present_state_Q (-0.07791873234500712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.792563855708075 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1835013368521645) - present_state_Q ( -0.03920771386038164)) * f1( 0.009123798397245218)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1835013368521645) - present_state_Q (-0.03920771386038164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.795760691043657 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16668983633681406) - present_state_Q ( -0.04263798547507962)) * f1( 0.008811320928515361)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16668983633681406) - present_state_Q (-0.04263798547507962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.798852076107979 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17553554068687316) - present_state_Q ( -0.03473553243998794)) * f1( 0.008500085525046918)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17553554068687316) - present_state_Q (-0.03473553243998794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.801842989076079 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18614303909384106) - present_state_Q ( -0.027330687995139276)) * f1( 0.008204728846536304)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18614303909384106) - present_state_Q (-0.027330687995139276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.804730683341642 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17283717644648924) - present_state_Q ( -0.03649281890440574)) * f1( 0.007944444501317592)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17283717644648924) - present_state_Q (-0.03649281890440574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.807673649101764 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1815446347441812) - present_state_Q ( -0.028534253264052487)) * f1( 0.008076884713450862)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1815446347441812) - present_state_Q (-0.028534253264052487)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.810536998488036 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19240566603235493) - present_state_Q ( -0.021833158312608673)) * f1( 0.007841620960945529)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.19240566603235493) - present_state_Q (-0.021833158312608673)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.81332094450316 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17914178159164496) - present_state_Q ( -0.030310858131699626)) * f1( 0.007644691169789771)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17914178159164496) - present_state_Q (-0.030310858131699626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.816047821499433 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09924238015476834) - present_state_Q ( -0.1470639705503652)) * f1( 0.0077535736339781995)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09924238015476834) - present_state_Q (-0.1470639705503652)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.820683206673689 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16071824507099514) - present_state_Q ( -0.19732102516954733)) * f1( 0.01334761218700403)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16071824507099514) - present_state_Q (-0.19732102516954733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.825597442444181 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20415521172896398) - present_state_Q ( -0.23413741576209515)) * f1( 0.014284127834488914)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.20415521172896398) - present_state_Q (-0.23413741576209515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.828977416312426 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10272509610953193) - present_state_Q ( -0.17156667683068952)) * f1( 0.009677044870640973)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10272509610953193) - present_state_Q (-0.17156667683068952)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.833562852449363 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16573466036849685) - present_state_Q ( -0.2001442428671305)) * f1( 0.013212616331634779)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16573466036849685) - present_state_Q (-0.2001442428671305)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.839053840490992 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1439248146298291) - present_state_Q ( -0.16590330730276856)) * f1( 0.015677078821752736)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1439248146298291) - present_state_Q (-0.16590330730276856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.841624890446052 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18923956158576535) - present_state_Q ( -0.020872355012863317)) * f1( 0.007039882421568692)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18923956158576535) - present_state_Q (-0.020872355012863317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.844184919674325 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18927956583366856) - present_state_Q ( -0.18927956583366856)) * f1( 0.007348556006485284)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18927956583366856) - present_state_Q (-0.18927956583366856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.845476915824626 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2353883677426063) - present_state_Q ( -0.036898851782502975)) * f1( 0.003548748771736873)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2353883677426063) - present_state_Q (-0.036898851782502975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.847782297045947 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1647233065421883) - present_state_Q ( -0.046544974004934084)) * f1( 0.008505438105340364)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1647233065421883) - present_state_Q (-0.046544974004934084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.850038913460766 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17061346995711135) - present_state_Q ( -0.039750953905396395)) * f1( 0.008302909795570626)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.17061346995711135) - present_state_Q (-0.039750953905396395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.85224334125502 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1780239626868325) - present_state_Q ( -0.031998553907235786)) * f1( 0.00808562098992181)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1780239626868325) - present_state_Q (-0.031998553907235786)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.854394193941015 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18738859027483556) - present_state_Q ( -0.024055218672423946)) * f1( 0.007863501253000223)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18738859027483556) - present_state_Q (-0.024055218672423946)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.856492830977611 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19469960282979132) - present_state_Q ( -0.019448228749362192)) * f1( 0.007657656532539363)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.19469960282979132) - present_state_Q (-0.019448228749362192)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.858543680533733 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18203514753798378) - present_state_Q ( -0.029274395229623162)) * f1( 0.007513698244572042)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18203514753798378) - present_state_Q (-0.029274395229623162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.860568100329877 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18980451796886605) - present_state_Q ( -0.02033075886631215)) * f1( 0.007390547369003302)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18980451796886605) - present_state_Q (-0.02033075886631215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.86256267712703 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20034978045245838) - present_state_Q ( -0.013794499871779947)) * f1( 0.007261476795287294)
w2 ( -20.799253442922634 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20034978045245838) - present_state_Q (-0.013794499871779947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.86154186248337 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18703650547066547) - present_state_Q ( -4.1831899289378365)) * f1( 0.007168971732159733)
w2 ( -20.770774752203838 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18703650547066547) - present_state_Q (-4.1831899289378365)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.859720917364404 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.262809983885165) - present_state_Q ( -4.347829681574319)) * f1( 0.015418711566128725)
w2 ( -20.74715481338914 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.262809983885165) - present_state_Q (-4.347829681574319)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.85878119443107 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.273258730619269) - present_state_Q ( -4.245867001694345)) * f1( 0.00871736721510759)
w2 ( -20.725595025665513 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -4.273258730619269) - present_state_Q (-4.245867001694345)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.862316015241966 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18842775542513931) - present_state_Q ( -0.30358578400583364)) * f1( 0.014393713818078522)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18842775542513931) - present_state_Q (-0.30358578400583364)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.86277984347423 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1507142892354931) - present_state_Q ( -0.11202540189676584)) * f1( 0.0026809632636604313)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1507142892354931) - present_state_Q (-0.11202540189676584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.864146055890261 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.178167089246319) - present_state_Q ( -0.032308662004088336)) * f1( 0.007537546757980304)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.178167089246319) - present_state_Q (-0.032308662004088336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.865440402549712 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0753920635826982) - present_state_Q ( -0.15658473720424645)) * f1( 0.007713678022477649)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0753920635826982) - present_state_Q (-0.15658473720424645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.867829846834935 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1296802579664693) - present_state_Q ( -0.24660366978564865)) * f1( 0.014995895621210335)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1296802579664693) - present_state_Q (-0.24660366978564865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.871061262017504 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21118678394405935) - present_state_Q ( -0.30439399430477093)) * f1( 0.020932119636317883)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21118678394405935) - present_state_Q (-0.30439399430477093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.873874036460528 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27976906125498774) - present_state_Q ( -0.3118554058990612)) * f1( 0.01822741480307406)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.27976906125498774) - present_state_Q (-0.3118554058990612)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.876655200266804 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13591854314727925) - present_state_Q ( -0.16172379214486945)) * f1( 0.01656536762335454)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13591854314727925) - present_state_Q (-0.16172379214486945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.878535394125576 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18256158321419852) - present_state_Q ( -0.03185013228441086)) * f1( 0.010368103265559371)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18256158321419852) - present_state_Q (-0.03185013228441086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.880024080765766 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19012197299219027) - present_state_Q ( -0.02665135794337572)) * f1( 0.008182315366575139)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.19012197299219027) - present_state_Q (-0.02665135794337572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.881372301351888 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1146276244764715) - present_state_Q ( -0.1441195461528661)) * f1( 0.007957025031438256)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1146276244764715) - present_state_Q (-0.1441195461528661)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.883418302943396 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17319309274469033) - present_state_Q ( -0.18553638465047748)) * f1( 0.012334097576639646)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17319309274469033) - present_state_Q (-0.18553638465047748)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.886018506455471 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2693144554579977) - present_state_Q ( -0.2901654512077194)) * f1( 0.016627463220908193)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2693144554579977) - present_state_Q (-0.2901654512077194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.8869661927536 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2096627411466531) - present_state_Q ( -0.027499188125138716)) * f1( 0.005205632931285991)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2096627411466531) - present_state_Q (-0.027499188125138716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.888603907652914 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1839404757346374) - present_state_Q ( -0.038665275103229514)) * f1( 0.009064357953413697)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1839404757346374) - present_state_Q (-0.038665275103229514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.890058176525345 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10983403560797726) - present_state_Q ( -0.16053653485677646)) * f1( 0.00866935933791097)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10983403560797726) - present_state_Q (-0.16053653485677646)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.89213675025766 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17852819717804352) - present_state_Q ( -0.20112072500252393)) * f1( 0.012645187859158814)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17852819717804352) - present_state_Q (-0.20112072500252393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.89361909335165 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1838980869297465) - present_state_Q ( -0.028445465063989536)) * f1( 0.008158284854516173)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1838980869297465) - present_state_Q (-0.028445465063989536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.895061117213318 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19487141848702433) - present_state_Q ( -0.02270576912405423)) * f1( 0.00790663074718527)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.19487141848702433) - present_state_Q (-0.02270576912405423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.89646775201434 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1784767034393832) - present_state_Q ( -0.03320850964209976)) * f1( 0.007764283576234789)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1784767034393832) - present_state_Q (-0.03320850964209976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.897716802162625 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10337385353897668) - present_state_Q ( -0.19687392371477982)) * f1( 0.007613847632389172)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10337385353897668) - present_state_Q (-0.19687392371477982)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.900046931317975 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1497729683039474) - present_state_Q ( -0.07891929250906367)) * f1( 0.013216148082363889)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1497729683039474) - present_state_Q (-0.07891929250906367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.901696366584165 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16479487027444853) - present_state_Q ( -0.04932874700477795)) * f1( 0.009193227287921582)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16479487027444853) - present_state_Q (-0.04932874700477795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.9030686784687 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17885697970453396) - present_state_Q ( -0.032052062542586225)) * f1( 0.007569838431854813)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17885697970453396) - present_state_Q (-0.032052062542586225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.904477143480488 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18740897002800103) - present_state_Q ( -0.023761193604978498)) * f1( 0.007730263644777721)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18740897002800103) - present_state_Q (-0.023761193604978498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.905857592401391 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.198465960767523) - present_state_Q ( -0.01776240957893621)) * f1( 0.007547071356781461)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.198465960767523) - present_state_Q (-0.01776240957893621)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.90720541030774 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1851604860939023) - present_state_Q ( -0.025641635115674674)) * f1( 0.00740596356269054)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1851604860939023) - present_state_Q (-0.025641635115674674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.908576552208572 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19498763986330972) - present_state_Q ( -0.01784640633885974)) * f1( 0.007497958980087867)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.19498763986330972) - present_state_Q (-0.01784640633885974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.909911554364848 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18145250265119553) - present_state_Q ( -0.02987774648519555)) * f1( 0.007354160118801015)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18145250265119553) - present_state_Q (-0.02987774648519555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.911286794432055 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18978690591193706) - present_state_Q ( -0.021269338153559823)) * f1( 0.0075366198338620875)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18978690591193706) - present_state_Q (-0.021269338153559823)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.91270790538859 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038737569384893526) - present_state_Q ( -0.038737569384893526)) * f1( 0.00792955141157121)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.038737569384893526) - present_state_Q (-0.038737569384893526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.914654195085433 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09024361380977432) - present_state_Q ( -0.16427347962404376)) * f1( 0.011641983019715219)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09024361380977432) - present_state_Q (-0.16427347962404376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.915680178171339 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14622560865686612) - present_state_Q ( -0.14622560865686612)) * f1( 0.006051457212352795)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14622560865686612) - present_state_Q (-0.14622560865686612)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.919225170850293 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23900127653723555) - present_state_Q ( -0.3678768573842019)) * f1( 0.023903267708439084)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23900127653723555) - present_state_Q (-0.3678768573842019)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.92290190899202 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28111976866086785) - present_state_Q ( -0.4051235707455458)) * f1( 0.025356414295036104)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.28111976866086785) - present_state_Q (-0.4051235707455458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.926162449596408 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3229708270320492) - present_state_Q ( -0.3229708270320492)) * f1( 0.021222493499867247)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3229708270320492) - present_state_Q (-0.3229708270320492)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.929971356028132 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.291499149877379) - present_state_Q ( -0.4112151916950064)) * f1( 0.02635977558269651)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.291499149877379) - present_state_Q (-0.4112151916950064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.93358073258584 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2607770770519196) - present_state_Q ( -0.3910858753703257)) * f1( 0.024687493585487733)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2607770770519196) - present_state_Q (-0.3910858753703257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.936547246358016 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28524762654481794) - present_state_Q ( -0.28524762654481794)) * f1( 0.01889124243536602)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.28524762654481794) - present_state_Q (-0.28524762654481794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.940276091996289 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27528568108378126) - present_state_Q ( -0.40677947165964445)) * f1( 0.02575554565185744)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.27528568108378126) - present_state_Q (-0.40677947165964445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.943838323318554 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.263930466986443) - present_state_Q ( -0.3785904844522473)) * f1( 0.024153389046206)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.263930466986443) - present_state_Q (-0.3785904844522473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.947563088743852 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11204779488823098) - present_state_Q ( -0.1840420865123253)) * f1( 0.022517058138205467)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11204779488823098) - present_state_Q (-0.1840420865123253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.949085034864286 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16640409375854537) - present_state_Q ( -0.04635778957092345)) * f1( 0.008467877526592652)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16640409375854537) - present_state_Q (-0.04635778957092345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.950577447975057 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17233541576319988) - present_state_Q ( -0.0394680792361206)) * f1( 0.008269133123193766)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17233541576319988) - present_state_Q (-0.0394680792361206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.952038147462746 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17983265724386083) - present_state_Q ( -0.031615405362910555)) * f1( 0.008055021453989864)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.17983265724386083) - present_state_Q (-0.031615405362910555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.953466146008854 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18933519750873515) - present_state_Q ( -0.02364561233763483)) * f1( 0.007836146580003499)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.18933519750873515) - present_state_Q (-0.02364561233763483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.954861143866083 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19563121271946232) - present_state_Q ( -0.01939022544644328)) * f1( 0.0076345896161475095)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.19563121271946232) - present_state_Q (-0.01939022544644328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.956220465412702 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1840330844019804) - present_state_Q ( -0.028575949789582274)) * f1( 0.00748170007484482)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1840330844019804) - present_state_Q (-0.028575949789582274)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.975531789633404 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10575644912014345) - present_state_Q ( -0.10575644912014345)) * f1( 0.007071067811865476)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.10575644912014345) - present_state_Q (-0.10575644912014345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.994777074616792 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11352774127094946) - present_state_Q ( -0.1999271633948483)) * f1( 0.007071067811865476)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.11352774127094946) - present_state_Q (-0.1999271633948483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.013978438959107 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014311181903160436) - present_state_Q ( -0.014311181903160436)) * f1( 0.007009680772759351)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.014311181903160436) - present_state_Q (-0.014311181903160436)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.070612358562784 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13812006126799448) - present_state_Q ( -0.16227021820007967)) * f1( 0.021498217543800033)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.13812006126799448) - present_state_Q (-0.16227021820007967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.094949534292558 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04101400216457966) - present_state_Q ( -0.04101400216457966)) * f1( 0.009199431172059822)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.04101400216457966) - present_state_Q (-0.04101400216457966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.118417767142073 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043949950756340034) - present_state_Q ( -0.043949950756340034)) * f1( 0.008871857649884745)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.043949950756340034) - present_state_Q (-0.043949950756340034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.141039349194571 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035897804731244834) - present_state_Q ( -0.035897804731244834)) * f1( 0.008549450205954644)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.035897804731244834) - present_state_Q (-0.035897804731244834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.162862007580038 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02842057449709847) - present_state_Q ( -0.02842057449709847)) * f1( 0.008245413250732758)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.02842057449709847) - present_state_Q (-0.02842057449709847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.183867627338994 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06507091448052353) - present_state_Q ( -0.17164099024237253)) * f1( 0.00797877709293697)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.06507091448052353) - present_state_Q (-0.17164099024237253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.225110568255547 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10090286925202918) - present_state_Q ( -0.25992032371232227)) * f1( 0.015716284398793846)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.10090286925202918) - present_state_Q (-0.25992032371232227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.261013935406973 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10474793996465775) - present_state_Q ( -0.23371828098187325)) * f1( 0.01416013728948135)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.10474793996465775) - present_state_Q (-0.23371828098187325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.280200585302444 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08176797326950487) - present_state_Q ( -0.08176797326950487)) * f1( 0.007522733317526593)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.08176797326950487) - present_state_Q (-0.08176797326950487)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.28882807316107 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05145327755750805) - present_state_Q ( -0.05145327755750805)) * f1( 0.0037853766573818733)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.05145327755750805) - present_state_Q (-0.05145327755750805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.300902442112331 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07284445312091573) - present_state_Q ( -0.07284445312091573)) * f1( 0.00530220101200769)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.07284445312091573) - present_state_Q (-0.07284445312091573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.357432014332701 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14264874113307482) - present_state_Q ( -0.3972208108318566)) * f1( 0.0251746312743905)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.14264874113307482) - present_state_Q (-0.3972208108318566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.391717448285988 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12815426150183798) - present_state_Q ( -0.18299221440380578)) * f1( 0.015125200913172375)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.12815426150183798) - present_state_Q (-0.18299221440380578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.411326612395717 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03965495549824648) - present_state_Q ( -0.03965495549824648)) * f1( 0.008599665043629651)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.03965495549824648) - present_state_Q (-0.03965495549824648)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.430266059432268 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03149581115487272) - present_state_Q ( -0.03149581115487272)) * f1( 0.008303284374622476)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.03149581115487272) - present_state_Q (-0.03149581115487272)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.44858499111747 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025264152865486788) - present_state_Q ( -0.025264152865486788)) * f1( 0.008029268621254044)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.025264152865486788) - present_state_Q (-0.025264152865486788)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.466390618326994 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03134817826616398) - present_state_Q ( -0.03134817826616398)) * f1( 0.0078061584737013695)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.03134817826616398) - present_state_Q (-0.03134817826616398)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.48427515061386 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08478469215664865) - present_state_Q ( -0.08478469215664865)) * f1( 0.007857317882635726)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.08478469215664865) - present_state_Q (-0.08478469215664865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.5170221148386 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011021072565310498) - present_state_Q ( -0.3206956865615489)) * f1( 0.01454235423374099)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.011021072565310498) - present_state_Q (-0.3206956865615489)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.537622107578498 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04943727929346925) - present_state_Q ( -0.04943727929346925)) * f1( 0.009415021798236459)
w2 ( -20.774711200345266 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.04943727929346925) - present_state_Q (-0.04943727929346925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.555009544380264 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17311322923646416) - present_state_Q ( -0.05021734715517689)) * f1( 0.008288418598821144)
w2 ( -21.19427101370311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.17311322923646416) - present_state_Q (-0.05021734715517689)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -15.5993380401749 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.36023252597576) - present_state_Q ( -4.669719860185186)) * f1( 0.026421867517390205)
w2 ( -21.52981501539383 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -4.36023252597576) - present_state_Q (-4.669719860185186)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -15.603586417612124 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.363945903184118) - present_state_Q ( -4.363945903184118)) * f1( 0.0024868533316571946)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -4.363945903184118) - present_state_Q (-4.363945903184118)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -15.62907705350934 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1047507572716254) - present_state_Q ( -0.15851224015611565)) * f1( 0.01406588840631591)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.1047507572716254) - present_state_Q (-0.15851224015611565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.64132715798556 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025211495247975023) - present_state_Q ( -0.025211495247975023)) * f1( 0.006713248766539392)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.025211495247975023) - present_state_Q (-0.025211495247975023)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.654016513559151 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014249362987335706) - present_state_Q ( -0.014249362987335706)) * f1( 0.006950207481688459)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.014249362987335706) - present_state_Q (-0.014249362987335706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.665971542758383 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010541754155996452) - present_state_Q ( -0.010541754155996452)) * f1( 0.006891563268003479)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.010541754155996452) - present_state_Q (-0.010541754155996452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.677819485881301 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02544481557708159) - present_state_Q ( -0.02544481557708159)) * f1( 0.006835117553725421)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.02544481557708159) - present_state_Q (-0.02544481557708159)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.69314257112225 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03772508968186981) - present_state_Q ( -0.03772508968186981)) * f1( 0.008845578539068267)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03772508968186981) - present_state_Q (-0.03772508968186981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.70783423977952 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04331936316087779) - present_state_Q ( -0.04331936316087779)) * f1( 0.008483545536673118)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.04331936316087779) - present_state_Q (-0.04331936316087779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.72258894250542 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034923788195641146) - present_state_Q ( -0.034923788195641146)) * f1( 0.0085162281292346)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.034923788195641146) - present_state_Q (-0.034923788195641146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.736815755016401 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02835342826273751) - present_state_Q ( -0.02835342826273751)) * f1( 0.008208734842074347)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.02835342826273751) - present_state_Q (-0.02835342826273751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.750599252768872 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03395032956822529) - present_state_Q ( -0.03395032956822529)) * f1( 0.007955258597939021)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03395032956822529) - present_state_Q (-0.03395032956822529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.764453686997705 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026091211359046334) - present_state_Q ( -0.026091211359046334)) * f1( 0.007992937162268808)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.026091211359046334) - present_state_Q (-0.026091211359046334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.77790880008272 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03688190044058762) - present_state_Q ( -0.03688190044058762)) * f1( 0.007766911429431941)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03688190044058762) - present_state_Q (-0.03688190044058762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.791498485583636 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15175931917856472) - present_state_Q ( -0.213155465199958)) * f1( 0.007919928754624139)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.15175931917856472) - present_state_Q (-0.213155465199958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.814840231373015 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08790308282207876) - present_state_Q ( -0.08790308282207876)) * f1( 0.013509741240159816)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08790308282207876) - present_state_Q (-0.08790308282207876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.831131443726356 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05427855260382) - present_state_Q ( -0.05427855260382)) * f1( 0.009412545642266202)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.05427855260382) - present_state_Q (-0.05427855260382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.844512221599865 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03281813416100449) - present_state_Q ( -0.03281813416100449)) * f1( 0.007722371361422378)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03281813416100449) - present_state_Q (-0.03281813416100449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.858057767691033 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024462013849762678) - present_state_Q ( -0.024462013849762678)) * f1( 0.007814071578819196)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.024462013849762678) - present_state_Q (-0.024462013849762678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.871262843576554 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020512403881724955) - present_state_Q ( -0.020512403881724955)) * f1( 0.007616101468411565)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.020512403881724955) - present_state_Q (-0.020512403881724955)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.884174477265605 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029187868099474757) - present_state_Q ( -0.029187868099474757)) * f1( 0.007450212062685297)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.029187868099474757) - present_state_Q (-0.029187868099474757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.896884194881805 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01954503914059531) - present_state_Q ( -0.01954503914059531)) * f1( 0.007330032742395859)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.01954503914059531) - present_state_Q (-0.01954503914059531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.90938861383382 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014271182674048507) - present_state_Q ( -0.014271182674048507)) * f1( 0.007209657950076978)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.014271182674048507) - present_state_Q (-0.014271182674048507)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.921767382907031 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029585594039491084) - present_state_Q ( -0.029585594039491084)) * f1( 0.007142888481602278)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.029585594039491084) - present_state_Q (-0.029585594039491084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.934041205640046 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019006785607354224) - present_state_Q ( -0.019006785607354224)) * f1( 0.007078442791407278)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.019006785607354224) - present_state_Q (-0.019006785607354224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.946182217753567 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010934400419146532) - present_state_Q ( -0.010934400419146532)) * f1( 0.006998917042034)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.010934400419146532) - present_state_Q (-0.010934400419146532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.958234322089803 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02298460235974427) - present_state_Q ( -0.02298460235974427)) * f1( 0.006952010800372331)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.02298460235974427) - present_state_Q (-0.02298460235974427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.97053965254944 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013343203862796277) - present_state_Q ( -0.013343203862796277)) * f1( 0.007094528105627352)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.013343203862796277) - present_state_Q (-0.013343203862796277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -15.994274375761849 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10219559512286773) - present_state_Q ( -0.14996442781015182)) * f1( 0.014557995357896454)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.10219559512286773) - present_state_Q (-0.14996442781015182)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.00471651875924 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11766033209394354) - present_state_Q ( -0.13669617055387154)) * f1( 0.006399007005787299)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.11766033209394354) - present_state_Q (-0.13669617055387154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.0259095114736 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03519271320243569) - present_state_Q ( -0.21854916993029938)) * f1( 0.013059296480057818)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.03519271320243569) - present_state_Q (-0.21854916993029938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.039026579416564 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027986356734501297) - present_state_Q ( -0.027986356734501297)) * f1( 0.007989383534139081)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.027986356734501297) - present_state_Q (-0.027986356734501297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.05177721462619 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022414840861806843) - present_state_Q ( -0.022414840861806843)) * f1( 0.0077638243738660075)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.022414840861806843) - present_state_Q (-0.022414840861806843)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.06428852937349 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03353511341497529) - present_state_Q ( -0.03353511341497529)) * f1( 0.007622748142437731)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.03353511341497529) - present_state_Q (-0.03353511341497529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.076584988789488 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024137066613181376) - present_state_Q ( -0.024137066613181376)) * f1( 0.007487984775800482)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.024137066613181376) - present_state_Q (-0.024137066613181376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.087981140955428 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01633988364786236) - present_state_Q ( -0.01633988364786236)) * f1( 0.007345206595886861)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01633988364786236) - present_state_Q (-0.01633988364786236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.099195166670288 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02799748288805243) - present_state_Q ( -0.02799748288805243)) * f1( 0.007232710930549736)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.02799748288805243) - present_state_Q (-0.02799748288805243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.110548159209475 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.106983551036064) - present_state_Q ( -0.1552280751988466)) * f1( 0.007379134292166715)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.106983551036064) - present_state_Q (-0.1552280751988466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.131171997430613 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05365459763537359) - present_state_Q ( -0.2184494726611792)) * f1( 0.013464927720101294)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.05365459763537359) - present_state_Q (-0.2184494726611792)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.1405986018891 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08589742039525554) - present_state_Q ( -0.08589742039525554)) * f1( 0.006100380706312737)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.08589742039525554) - present_state_Q (-0.08589742039525554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.14868142536093 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07351162618489597) - present_state_Q ( -0.07351162618489597)) * f1( 0.005555161039071549)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.07351162618489597) - present_state_Q (-0.07351162618489597)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.178759131851532 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12312703130874876) - present_state_Q ( -0.34875144347289155)) * f1( 0.02106305977336231)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.12312703130874876) - present_state_Q (-0.34875144347289155)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.180099990316375 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11096504985802844) - present_state_Q ( -0.11096504985802844)) * f1( 0.0009236847840665215)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.11096504985802844) - present_state_Q (-0.11096504985802844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.190111585138514 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021468011607273317) - present_state_Q ( -0.021468011607273317)) * f1( 0.006858687304366176)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.021468011607273317) - present_state_Q (-0.021468011607273317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.200070441614915 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009891386154528167) - present_state_Q ( -0.009891386154528167)) * f1( 0.006817691306195354)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.009891386154528167) - present_state_Q (-0.009891386154528167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.209976383112522 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027247722263052318) - present_state_Q ( -0.027247722263052318)) * f1( 0.006788726132507607)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.027247722263052318) - present_state_Q (-0.027247722263052318)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.22023704342277 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016230250435262245) - present_state_Q ( -0.016230250435262245)) * f1( 0.007027046358141577)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.016230250435262245) - present_state_Q (-0.016230250435262245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.230396278786152 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0112351586375869) - present_state_Q ( -0.0112351586375869)) * f1( 0.006955443703712926)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0112351586375869) - present_state_Q (-0.0112351586375869)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.240478987923822 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028000950345533) - present_state_Q ( -0.028000950345533)) * f1( 0.006910189321560446)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.028000950345533) - present_state_Q (-0.028000950345533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.2505199501903 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01627381394637023) - present_state_Q ( -0.01627381394637023)) * f1( 0.006876603907653417)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.01627381394637023) - present_state_Q (-0.01627381394637023)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.285110256680742 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18777180072248836) - present_state_Q ( -0.4175655706950097)) * f1( 0.024329408640090536)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.18777180072248836) - present_state_Q (-0.4175655706950097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.30297515493421 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10346300402859852) - present_state_Q ( -0.22548523454711175)) * f1( 0.012405199826733434)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.10346300402859852) - present_state_Q (-0.22548523454711175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.319583549435702 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13038918733667318) - present_state_Q ( -0.13553841222179025)) * f1( 0.011458983474850895)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.13038918733667318) - present_state_Q (-0.13553841222179025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.331246725943913 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03715011410688289) - present_state_Q ( -0.03715011410688289)) * f1( 0.00799787683521127)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03715011410688289) - present_state_Q (-0.03715011410688289)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.342633972719703 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028157038054080943) - present_state_Q ( -0.028157038054080943)) * f1( 0.00780432992681553)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.028157038054080943) - present_state_Q (-0.028157038054080943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.35374564305597 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020640876987535) - present_state_Q ( -0.020640876987535)) * f1( 0.0076119326796098945)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.020640876987535) - present_state_Q (-0.020640876987535)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.364514921392473 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07570054764365973) - present_state_Q ( -0.17552175443444712)) * f1( 0.007453652162465109)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.07570054764365973) - present_state_Q (-0.17552175443444712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.386680326558437 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1007222788618193) - present_state_Q ( -0.2787812932082474)) * f1( 0.015448894805008289)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.1007222788618193) - present_state_Q (-0.2787812932082474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.390027732108493 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1389619038339409) - present_state_Q ( -0.1389619038339409)) * f1( 0.002643208382279601)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.1389619038339409) - present_state_Q (-0.1389619038339409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.40084515799636 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03678340451972678) - present_state_Q ( -0.03678340451972678)) * f1( 0.008480174206408403)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.03678340451972678) - present_state_Q (-0.03678340451972678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.41128698049098 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029303599181786398) - present_state_Q ( -0.029303599181786398)) * f1( 0.008181407431404247)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.029303599181786398) - present_state_Q (-0.029303599181786398)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.421400127637963 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03707903825733531) - present_state_Q ( -0.03707903825733531)) * f1( 0.007928229815041462)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.03707903825733531) - present_state_Q (-0.03707903825733531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.4316185678612 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028453866150159168) - present_state_Q ( -0.028453866150159168)) * f1( 0.008005902578879297)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.028453866150159168) - present_state_Q (-0.028453866150159168)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.441548764510447 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023331214435982732) - present_state_Q ( -0.023331214435982732)) * f1( 0.007777261413418225)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.023331214435982732) - present_state_Q (-0.023331214435982732)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.451262109755575 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03300383972066815) - present_state_Q ( -0.03300383972066815)) * f1( 0.007612615157584863)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.03300383972066815) - present_state_Q (-0.03300383972066815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.460121753269217 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023339302384679853) - present_state_Q ( -0.023339302384679853)) * f1( 0.007473516351905542)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.023339302384679853) - present_state_Q (-0.023339302384679853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.468817312523953 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01633707139741445) - present_state_Q ( -0.01633707139741445)) * f1( 0.007331206447137204)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.01633707139741445) - present_state_Q (-0.01633707139741445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.477382632162424 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02639471412945269) - present_state_Q ( -0.02639471412945269)) * f1( 0.007226917026908164)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.02639471412945269) - present_state_Q (-0.02639471412945269)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.48463585222744 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11230575907491865) - present_state_Q ( -0.11230575907491865)) * f1( 0.006160030794973041)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.11230575907491865) - present_state_Q (-0.11230575907491865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.508772074467487 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1777667695027109) - present_state_Q ( -0.4003733243236788)) * f1( 0.021000569334345257)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.1777667695027109) - present_state_Q (-0.4003733243236788)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.5335634579228 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12247621302476894) - present_state_Q ( -0.3458455057969881)) * f1( 0.021479043583915877)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.12247621302476894) - present_state_Q (-0.3458455057969881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.53659896671831 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11720908635228781) - present_state_Q ( -0.11720908635228781)) * f1( 0.0025789701967770897)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.11720908635228781) - present_state_Q (-0.11720908635228781)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.544996655047328 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033232694687839404) - present_state_Q ( -0.033232694687839404)) * f1( 0.007089160582386238)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.033232694687839404) - present_state_Q (-0.033232694687839404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.55334790606524 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022319397598994747) - present_state_Q ( -0.022319397598994747)) * f1( 0.007044118496004117)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.022319397598994747) - present_state_Q (-0.022319397598994747)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.561621688130185 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011834778470498021) - present_state_Q ( -0.011834778470498021)) * f1( 0.0069732247943658345)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.011834778470498021) - present_state_Q (-0.011834778470498021)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.569788070211892 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08231184244806586) - present_state_Q ( -0.08231184244806586)) * f1( 0.006919698973132413)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.08231184244806586) - present_state_Q (-0.08231184244806586)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.587469802298173 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02954309139639341) - present_state_Q ( -0.3541179897657892)) * f1( 0.015342651921549353)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.02954309139639341) - present_state_Q (-0.3541179897657892)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.598010617118074 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04075613177339892) - present_state_Q ( -0.04075613177339892)) * f1( 0.008903434338252898)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.04075613177339892) - present_state_Q (-0.04075613177339892)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.60815870035999 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042808361662823084) - present_state_Q ( -0.042808361662823084)) * f1( 0.008573046056550727)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.042808361662823084) - present_state_Q (-0.042808361662823084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.614817582468472 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07809451049930323) - present_state_Q ( -0.07809451049930323)) * f1( 0.005640520457945457)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.07809451049930323) - present_state_Q (-0.07809451049930323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.622638300916808 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028678494227378105) - present_state_Q ( -0.028678494227378105)) * f1( 0.006599810539120975)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.028678494227378105) - present_state_Q (-0.028678494227378105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.630498243739645 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22160336943926734) - present_state_Q ( -0.015438832974292082)) * f1( 0.006614751716286754)
w2 ( -21.87148192297899 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.22160336943926734) - present_state_Q (-0.015438832974292082)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.638357805431372 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23278483216806967) - present_state_Q ( -0.0057834299845877036)) * f1( 0.006608439240002788)
w2 ( -22.10934630838938 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.23278483216806967) - present_state_Q (-0.0057834299845877036)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -16.643314380449063 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2131606769697951) - present_state_Q ( -4.4525275308492445)) * f1( 0.006658024751097417)
w2 ( -22.25823656347208 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.2131606769697951) - present_state_Q (-4.4525275308492445)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -16.655551641940626 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27345560216508774) - present_state_Q ( -4.725102914859504)) * f1( 0.017048367212793358)
w2 ( -22.40179590072497 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.27345560216508774) - present_state_Q (-4.725102914859504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -16.668618935047373 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.592488323503794) - present_state_Q ( -4.828033278777345)) * f1( 0.01740695077569376)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -4.592488323503794) - present_state_Q (-4.828033278777345)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -16.686699262636626 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0831147319161467) - present_state_Q ( -0.2734474531202639)) * f1( 0.01690212919626744)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.0831147319161467) - present_state_Q (-0.2734474531202639)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.68868485527913 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1289812868114329) - present_state_Q ( -0.1289812868114329)) * f1( 0.001830693321396357)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1289812868114329) - present_state_Q (-0.1289812868114329)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.69714225337866 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022905777108474593) - present_state_Q ( -0.022905777108474593)) * f1( 0.0077295865875785475)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.022905777108474593) - present_state_Q (-0.022905777108474593)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.715947913514245 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11428546919494759) - present_state_Q ( -0.2641573324829068)) * f1( 0.017559828604215757)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11428546919494759) - present_state_Q (-0.2641573324829068)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.732849697290597 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053915653650992566) - present_state_Q ( -0.2841363197918259)) * f1( 0.01582051158661325)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.053915653650992566) - present_state_Q (-0.2841363197918259)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.742196396255444 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043927895862096514) - present_state_Q ( -0.043927895862096514)) * f1( 0.008557154404736731)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.043927895862096514) - present_state_Q (-0.043927895862096514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.76888781634071 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1579824543037531) - present_state_Q ( -0.43725706957909993)) * f1( 0.025322130570272754)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1579824543037531) - present_state_Q (-0.43725706957909993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.778891895939044 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08329279393708802) - present_state_Q ( -0.08329279393708802)) * f1( 0.009188808571652612)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.08329279393708802) - present_state_Q (-0.08329279393708802)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.80271598061703 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1555580305983123) - present_state_Q ( -0.3924834487436953)) * f1( 0.022506807754899553)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1555580305983123) - present_state_Q (-0.3924834487436953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.81245111649146 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36377043444250723) - present_state_Q ( -0.12944419792744097)) * f1( 0.00895667554179962)
w2 ( -22.551934696142176 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.36377043444250723) - present_state_Q (-0.12944419792744097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.82645458730178 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.662982868821272) - present_state_Q ( -4.8736977830744745)) * f1( 0.02136366455718806)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -4.662982868821272) - present_state_Q (-4.8736977830744745)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -16.835548248448035 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1377571038309698) - present_state_Q ( -0.1377571038309698)) * f1( 0.009162648256174103)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.1377571038309698) - present_state_Q (-0.1377571038309698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.843311217941867 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02777016087113763) - present_state_Q ( -0.02777016087113763)) * f1( 0.007744617401158097)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.02777016087113763) - present_state_Q (-0.02777016087113763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.850348323463344 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013550316683814836) - present_state_Q ( -0.013550316683814836)) * f1( 0.007011517383320303)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.013550316683814836) - present_state_Q (-0.013550316683814836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.857310229190595 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030951543927685086) - present_state_Q ( -0.030951543927685086)) * f1( 0.0069474319111507985)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.030951543927685086) - present_state_Q (-0.030951543927685086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.864521641730988 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020150769296189985) - present_state_Q ( -0.020150769296189985)) * f1( 0.007189445885899656)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.020150769296189985) - present_state_Q (-0.020150769296189985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.87163957753671 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013077114048398071) - present_state_Q ( -0.013077114048398071)) * f1( 0.0070917528275746804)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.013077114048398071) - present_state_Q (-0.013077114048398071)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.878724196999734 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03283437837932698) - present_state_Q ( -0.03283437837932698)) * f1( 0.007071086265327291)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.03283437837932698) - present_state_Q (-0.03283437837932698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.885769335138967 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021524144704537826) - present_state_Q ( -0.021524144704537826)) * f1( 0.007024543587903038)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.021524144704537826) - present_state_Q (-0.021524144704537826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.892751322274727 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01129617929178359) - present_state_Q ( -0.01129617929178359)) * f1( 0.006955193531766545)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.01129617929178359) - present_state_Q (-0.01129617929178359)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.89967680541071 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02670577839628044) - present_state_Q ( -0.02670577839628044)) * f1( 0.006908450681217941)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.02670577839628044) - present_state_Q (-0.02670577839628044)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.906796754788513 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015787952455650164) - present_state_Q ( -0.015787952455650164)) * f1( 0.007095483748864152)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.015787952455650164) - present_state_Q (-0.015787952455650164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.913824450311477 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033740724879164664) - present_state_Q ( -0.033740724879164664)) * f1( 0.007014842178588055)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.033740724879164664) - present_state_Q (-0.033740724879164664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.92112631621667 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02326371137990598) - present_state_Q ( -0.02326371137990598)) * f1( 0.007281657570889242)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.02326371137990598) - present_state_Q (-0.02326371137990598)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.928323190263452 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014489736885676936) - present_state_Q ( -0.014489736885676936)) * f1( 0.007171309076366641)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.014489736885676936) - present_state_Q (-0.014489736885676936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.935428872353466 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027809744183682306) - present_state_Q ( -0.027809744183682306)) * f1( 0.007088909055929773)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.027809744183682306) - present_state_Q (-0.027809744183682306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.942700604599345 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017629233146779104) - present_state_Q ( -0.017629233146779104)) * f1( 0.007247942026794424)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.017629233146779104) - present_state_Q (-0.017629233146779104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.94920340687985 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034431354214292775) - present_state_Q ( -0.034431354214292775)) * f1( 0.007142652320759919)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.034431354214292775) - present_state_Q (-0.034431354214292775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.9558012274684 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1598075912255147) - present_state_Q ( -0.22054514836454273)) * f1( 0.007387873843880428)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1598075912255147) - present_state_Q (-0.22054514836454273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.96758716405327 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08612801178135499) - present_state_Q ( -0.08612801178135499)) * f1( 0.013012124704044867)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.08612801178135499) - present_state_Q (-0.08612801178135499)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.975806021879446 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05246794762374637) - present_state_Q ( -0.05246794762374637)) * f1( 0.009043686013481803)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.05246794762374637) - present_state_Q (-0.05246794762374637)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.98945085871651 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12082050047734501) - present_state_Q ( -0.18114223413037003)) * f1( 0.015218231039927324)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.12082050047734501) - present_state_Q (-0.18114223413037003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.995932208985817 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03177037217750154) - present_state_Q ( -0.03177037217750154)) * f1( 0.007117217310425451)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03177037217750154) - present_state_Q (-0.03177037217750154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -16.99902326596637 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08713035374075324) - present_state_Q ( -0.08713035374075324)) * f1( 0.003412985033987608)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.08713035374075324) - present_state_Q (-0.08713035374075324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.01653005183775 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13378574479798025) - present_state_Q ( -0.36362621014403823)) * f1( 0.01992821360669994)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.13378574479798025) - present_state_Q (-0.36362621014403823)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.040507237246764 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.233207723043866) - present_state_Q ( -0.48448996906156294)) * f1( 0.027642582830938442)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.233207723043866) - present_state_Q (-0.48448996906156294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.049066051664955 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23067515069152977) - present_state_Q ( -0.255658966270829)) * f1( 0.009613857369530545)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.23067515069152977) - present_state_Q (-0.255658966270829)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.07176998381764 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25955036259446573) - present_state_Q ( -0.48651444068077365)) * f1( 0.02617284709806416)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.25955036259446573) - present_state_Q (-0.48651444068077365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.086387532382957 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15200038834357474) - present_state_Q ( -0.15200038834357474)) * f1( 0.01624465888021571)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15200038834357474) - present_state_Q (-0.15200038834357474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.09467692088401 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047039884376932246) - present_state_Q ( -0.047039884376932246)) * f1( 0.009116394459666041)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.047039884376932246) - present_state_Q (-0.047039884376932246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.102608610604058 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038878871432129494) - present_state_Q ( -0.038878871432129494)) * f1( 0.008715968718083597)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.038878871432129494) - present_state_Q (-0.038878871432129494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.110180031245903 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10798102194154849) - present_state_Q ( -0.10798102194154849)) * f1( 0.008377328382264631)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10798102194154849) - present_state_Q (-0.10798102194154849)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.122402360841836 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017871512791644194) - present_state_Q ( -0.34330634342782745)) * f1( 0.013899035137075463)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.017871512791644194) - present_state_Q (-0.34330634342782745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.132751659229797 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056617265378598666) - present_state_Q ( -0.056617265378598666)) * f1( 0.01139261475246525)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.056617265378598666) - present_state_Q (-0.056617265378598666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.136367314214404 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07990255015562223) - present_state_Q ( -0.07990255015562223)) * f1( 0.003989353624323983)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07990255015562223) - present_state_Q (-0.07990255015562223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.15880075409371 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09727621190534062) - present_state_Q ( -0.33498947467342566)) * f1( 0.025463867732021993)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09727621190534062) - present_state_Q (-0.33498947467342566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.174491930768422 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10311899500351468) - present_state_Q ( -0.345185901113037)) * f1( 0.01783027782802303)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10311899500351468) - present_state_Q (-0.345185901113037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.194772941570783 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09987097940930932) - present_state_Q ( -0.3419746290860531)) * f1( 0.023038265050757248)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09987097940930932) - present_state_Q (-0.3419746290860531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.215629316938728 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09068197762883135) - present_state_Q ( -0.42478246581811274)) * f1( 0.023919347823384247)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09068197762883135) - present_state_Q (-0.42478246581811274)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.22179146547188 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05835123758902123) - present_state_Q ( -0.05835123758902123)) * f1( 0.006784522377846567)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.05835123758902123) - present_state_Q (-0.05835123758902123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.23378697045638 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11927836285207045) - present_state_Q ( -0.21178263570106087)) * f1( 0.013424822060322375)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.11927836285207045) - present_state_Q (-0.21178263570106087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.240096350962094 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02831903866307698) - present_state_Q ( -0.02831903866307698)) * f1( 0.006926013654921596)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.02831903866307698) - present_state_Q (-0.02831903866307698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.246376723573487 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015857841283418706) - present_state_Q ( -0.015857841283418706)) * f1( 0.00688569365280759)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.015857841283418706) - present_state_Q (-0.015857841283418706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.25199142036761 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010297646739351298) - present_state_Q ( -0.010297646739351298)) * f1( 0.006836863092369698)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.010297646739351298) - present_state_Q (-0.010297646739351298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.25747607869717 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08023442126271628) - present_state_Q ( -0.173537612198276)) * f1( 0.006808046545565328)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08023442126271628) - present_state_Q (-0.173537612198276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.269968060212786 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10432316860653326) - present_state_Q ( -0.30452976380459196)) * f1( 0.015757666940614555)
w2 ( -22.683030845614404 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.10432316860653326) - present_state_Q (-0.30452976380459196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.276106479021816 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.833286469633218) - present_state_Q ( -4.819038650874825)) * f1( 0.015796462523454347)
w2 ( -22.760749750083228 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -4.833286469633218) - present_state_Q (-4.819038650874825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -17.28689026667067 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2777211263907118) - present_state_Q ( -0.2777211263907118)) * f1( 0.013527577860880515)
w2 ( -22.92018387435525 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2777211263907118) - present_state_Q (-0.2777211263907118)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -17.28546499998311 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.8937646445567555) - present_state_Q ( -9.477801419427806)) * f1( 0.018587936328509466)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -4.8937646445567555) - present_state_Q (-9.477801419427806)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -17.29482953249867 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12748248168696094) - present_state_Q ( -0.2205759240788312)) * f1( 0.015136327383314965)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12748248168696094) - present_state_Q (-0.2205759240788312)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.300712472677358 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061562499331007645) - present_state_Q ( -0.061562499331007645)) * f1( 0.009280235263333141)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.061562499331007645) - present_state_Q (-0.061562499331007645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.314404514412566 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14960296137997847) - present_state_Q ( -0.3877094291920602)) * f1( 0.02273718645334009)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.14960296137997847) - present_state_Q (-0.3877094291920602)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.32495697237887 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07767431033498515) - present_state_Q ( -0.2888365221414815)) * f1( 0.017260765240056195)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.07767431033498515) - present_state_Q (-0.2888365221414815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.339001210493205 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14753100444057074) - present_state_Q ( -0.3631437281948416)) * f1( 0.023228090680119655)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.14753100444057074) - present_state_Q (-0.3631437281948416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.351825071656794 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15941064375848307) - present_state_Q ( -0.46771969642831934)) * f1( 0.021578667154680323)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.15941064375848307) - present_state_Q (-0.46771969642831934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.368157939995182 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15051977416396745) - present_state_Q ( -0.49709183341599167)) * f1( 0.02762392376503642)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.15051977416396745) - present_state_Q (-0.49709183341599167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.380577880952703 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12449280226645326) - present_state_Q ( -0.3639729453910995)) * f1( 0.02055227777969857)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12449280226645326) - present_state_Q (-0.3639729453910995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.395307085214867 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10730143561361845) - present_state_Q ( -0.4534450992000228)) * f1( 0.024747038704325788)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10730143561361845) - present_state_Q (-0.4534450992000228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.397633467115142 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041819637526190674) - present_state_Q ( -0.041819637526190674)) * f1( 0.003659569136532168)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.041819637526190674) - present_state_Q (-0.041819637526190674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.402756757292835 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032515041800848146) - present_state_Q ( -0.032515041800848146)) * f1( 0.008048708062929491)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.032515041800848146) - present_state_Q (-0.032515041800848146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.407713388629432 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13632970019697258) - present_state_Q ( -0.17364581671475)) * f1( 0.007950189374337308)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.13632970019697258) - present_state_Q (-0.17364581671475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.415247278021727 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06304178039854316) - present_state_Q ( -0.21208241882432974)) * f1( 0.012173341626517961)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.06304178039854316) - present_state_Q (-0.21208241882432974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -17.428397905406772 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3709582371266925) - present_state_Q ( -0.3709582371266925)) * f1( 0.021697990074738518)
w2 ( -22.889513085250474 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.3709582371266925) - present_state_Q (-0.3709582371266925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9803629323799651 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4158113883008419) - present_state_Q ( 0.4070710678118655)) * f1( 0.007071067811865476)
w2 ( 1.4445798529301799 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.4158113883008419) - present_state_Q (0.4070710678118655)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.9607980296785554 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31199097349975824) - present_state_Q ( 0.294632419684233)) * f1( 0.007071067811865476)
w2 ( 0.8912008379933102 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.31199097349975824) - present_state_Q (0.294632419684233)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.9467404028172903 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20085466739468102) - present_state_Q ( 0.18384253469037984)) * f1( 0.005099019513592802)
w2 ( 0.3398153481441074 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.20085466739468102) - present_state_Q (0.18384253469037984)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8820364364394166 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09395967156650349) - present_state_Q ( 0.09395967156650349)) * f1( 0.02353720459187961)
w2 ( -0.2099862744342742 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.09395967156650349) - present_state_Q (0.09395967156650349)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8068444228976462 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024667988479132753) - present_state_Q ( -0.024667988479132753)) * f1( 0.02745906043549199)
w2 ( -0.7576525991318344 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.024667988479132753) - present_state_Q (-0.024667988479132753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.7549333701695824 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1351931654404198) - present_state_Q ( -0.1351931654404198)) * f1( 0.01902629759044051)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.1351931654404198) - present_state_Q (-0.1351931654404198)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.7371797736969251 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014895269246628167) - present_state_Q ( 0.0022769293902251235)) * f1( 0.006701294236926926)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.014895269246628167) - present_state_Q (0.0022769293902251235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.6848782663629687 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017608606934417237) - present_state_Q ( 0.017608606934417237)) * f1( 0.01973057469069384)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.017608606934417237) - present_state_Q (0.017608606934417237)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.621557390290538 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01902307781276738) - present_state_Q ( 0.01902307781276738)) * f1( 0.02388644881846232)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.01902307781276738) - present_state_Q (0.01902307781276738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5479242285034668 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019750346012845674) - present_state_Q ( 0.019750346012845674)) * f1( 0.027775852654500232)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.019750346012845674) - present_state_Q (0.019750346012845674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.4636883054362579 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019602190194994214) - present_state_Q ( 0.019602190194994214)) * f1( 0.03177557908789993)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.019602190194994214) - present_state_Q (0.019602190194994214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.36885281995480473 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018443293703229113) - present_state_Q ( 0.018443293703229113)) * f1( 0.03577536669355403)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( 0.018443293703229113) - present_state_Q (0.018443293703229113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2670560992508606 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016146553715603727) - present_state_Q ( 0.016146553715603727)) * f1( 0.03977519701704978)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( 0.016146553715603727) - present_state_Q (0.016146553715603727)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.159034802339254 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012758589892864198) - present_state_Q ( 0.012758589892864198)) * f1( 0.04377505834869895)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( 0.012758589892864198) - present_state_Q (0.012758589892864198)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04117087098899885 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0063256405956113974) - present_state_Q ( 0.0063256405956113974)) * f1( 0.04777494290021569)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( 0.0063256405956113974) - present_state_Q (0.0063256405956113974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06680855824434224 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0021316154759108813) - present_state_Q ( 0.0021316154759108813)) * f1( 0.04377505834869895)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( 0.0021316154759108813) - present_state_Q (0.0021316154759108813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1155859231237702 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010540724032377976) - present_state_Q ( -0.0010540724032377976)) * f1( 0.0197767317651518)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.0010540724032377976) - present_state_Q (-0.0010540724032377976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.15449915334745828 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013614640526985792) - present_state_Q ( -0.0013614640526985792)) * f1( 0.015777505621101515)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.0013614640526985792) - present_state_Q (-0.0013614640526985792)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18355026106112204 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001202225929315385) - present_state_Q ( -0.001202225929315385)) * f1( 0.011778805030096219)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.001202225929315385) - present_state_Q (-0.001202225929315385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2027426695733927 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.000695589538073495) - present_state_Q ( -0.000695589538073495)) * f1( 0.007781440242663719)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.000695589538073495) - present_state_Q (-0.000695589538073495)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.21208977665816167 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.406877684401268e-05) - present_state_Q ( -8.406877684401268e-05)) * f1( 0.0037896406905238034)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -8.406877684401268e-05) - present_state_Q (-8.406877684401268e-05)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.21307461481449017 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.000899383326332204) - present_state_Q ( -0.000899383326332204)) * f1( 0.00041465754111311945)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.000899383326332204) - present_state_Q (-0.000899383326332204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2231459331771607 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017543828009209275) - present_state_Q ( -0.0017543828009209275)) * f1( 0.004240578402710076)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0017543828009209275) - present_state_Q (-0.0017543828009209275)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.24270198708272406 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.252914396065122e-05) - present_state_Q ( -9.252914396065122e-05)) * f1( 0.008233654686872725)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -9.252914396065122e-05) - present_state_Q (-9.252914396065122e-05)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2527732123445998 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001998324353457029) - present_state_Q ( -0.001998324353457029)) * f1( 0.004240578402710218)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.001998324353457029) - present_state_Q (-0.001998324353457029)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.27232695114095756 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019669396468055427) - present_state_Q ( -0.0030917339784019052)) * f1( 0.008233654686872868)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0019669396468055427) - present_state_Q (-0.0030917339784019052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3013759806644162 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017951379361070107) - present_state_Q ( -0.0017951379361070107)) * f1( 0.012231256428339473)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0017951379361070107) - present_state_Q (-0.0017951379361070107)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.33321891077416743 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002835398076962143) - present_state_Q ( -0.002835398076962143)) * f1( 0.013408172462850917)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.002835398076962143) - present_state_Q (-0.002835398076962143)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35837445314886834 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016982903984198625) - present_state_Q ( -0.0016982903984198625)) * f1( 0.010591843005395401)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0016982903984198625) - present_state_Q (-0.0016982903984198625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3937937390040227 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032899286789928933) - present_state_Q ( -0.003934415977837919)) * f1( 0.014914738082972423)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0032899286789928933) - present_state_Q (-0.003934415977837919)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4155947386257087 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0028799381329569287) - present_state_Q ( -0.0037404082672983677)) * f1( 0.009180142864776862)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0028799381329569287) - present_state_Q (-0.0037404082672983677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4459345696482184 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00409623239270592) - present_state_Q ( -0.002940861369872259)) * f1( 0.012775248139335171)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.00409623239270592) - present_state_Q (-0.002940861369872259)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.47671764229423025 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003999393738067223) - present_state_Q ( -0.003999393738067223)) * f1( 0.012962467746726871)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.003999393738067223) - present_state_Q (-0.003999393738067223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5030015375332911 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005093630426102991) - present_state_Q ( -0.0026255174096567226)) * f1( 0.011067214407945898)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.005093630426102991) - present_state_Q (-0.0026255174096567226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5374449257334964 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004880802433641783) - present_state_Q ( -0.005284424565362913)) * f1( 0.014504523866900512)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.004880802433641783) - present_state_Q (-0.005284424565362913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5600113219940941 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003498007018288215) - present_state_Q ( -0.007256759069811298)) * f1( 0.009503824819363406)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.003498007018288215) - present_state_Q (-0.007256759069811298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.592076211054286 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005700782923142133) - present_state_Q ( -0.004348576712178785)) * f1( 0.013502330606075379)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.005700782923142133) - present_state_Q (-0.004348576712178785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6211316081729991 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005193603530051531) - present_state_Q ( -0.004852411724149399)) * f1( 0.012235337492512537)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.005193603530051531) - present_state_Q (-0.004852411724149399)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6496758023800825 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005936768820077303) - present_state_Q ( -0.0039773359319364935)) * f1( 0.012019587480871992)
w2 ( -1.3033294706440914 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.005936768820077303) - present_state_Q (-0.0039773359319364935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6820203970471255 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26313179780878054) - present_state_Q ( -0.27138824001340683)) * f1( 0.013759925511243672)
w2 ( -1.7734569381309342 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.26313179780878054) - present_state_Q (-0.27138824001340683)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6964409909236746 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006518743169029705) - present_state_Q ( -0.006518743169029705)) * f1( 0.0060729588098889465)
w2 ( -1.7734569381309342 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.006518743169029705) - present_state_Q (-0.006518743169029705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7191373177576819 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006138512713236311) - present_state_Q ( -0.006138512713236311)) * f1( 0.009557988583997262)
w2 ( -1.7734569381309342 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.006138512713236311) - present_state_Q (-0.006138512713236311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7437386813905001 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00941991342419941) - present_state_Q ( -0.006683320586423168)) * f1( 0.010360341565611062)
w2 ( -1.7734569381309342 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.00941991342419941) - present_state_Q (-0.006683320586423168)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7748397452533009 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005240054921554179) - present_state_Q ( -0.008722753453316018)) * f1( 0.013098907804661464)
w2 ( -1.7734569381309342 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.005240054921554179) - present_state_Q (-0.008722753453316018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8055800231155036 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014403748947452737) - present_state_Q ( -0.36564010534321195)) * f1( 0.013144762026432279)
w2 ( -1.7734569381309342 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0014403748947452737) - present_state_Q (-0.36564010534321195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8605864558614277 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010322229292603475) - present_state_Q ( -0.37460406368205035)) * f1( 0.02352926816307206)
w2 ( -2.2410144700073715 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.010322229292603475) - present_state_Q (-0.37460406368205035)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8764130016355598 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4546982374543956) - present_state_Q ( -0.4590568062669181)) * f1( 0.007057751892207888)
w2 ( -2.6895013540987622 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.4546982374543956) - present_state_Q (-0.4590568062669181)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9037014059499737 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5487156101262115) - present_state_Q ( -0.5487156101262115)) * f1( 0.012212807865411898)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.5487156101262115) - present_state_Q (-0.5487156101262115)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9256015368301768 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006180291749678673) - present_state_Q ( -0.006180291749678673)) * f1( 0.010425985338940982)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.006180291749678673) - present_state_Q (-0.006180291749678673)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9414677678398976 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003925154773671677) - present_state_Q ( -0.003925154773671677)) * f1( 0.007552699600874159)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.003925154773671677) - present_state_Q (-0.003925154773671677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9654893471898904 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007827100297318649) - present_state_Q ( -0.010882850996725908)) * f1( 0.011438413438974886)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.007827100297318649) - present_state_Q (-0.010882850996725908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.982948203428849 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012050953926271899) - present_state_Q ( -0.012050953926271899)) * f1( 0.008313720941585858)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.012050953926271899) - present_state_Q (-0.012050953926271899)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9932954774723348 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008171957063340592) - present_state_Q ( -0.008621602432569037)) * f1( 0.004926547911556725)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.008171957063340592) - present_state_Q (-0.008621602432569037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.011717458653939 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007882327044839816) - present_state_Q ( -0.008798921115578655)) * f1( 0.008771166580796456)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.007882327044839816) - present_state_Q (-0.008798921115578655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.037499094913579 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006710346406941879) - present_state_Q ( -0.006710346406941879)) * f1( 0.01227413059407359)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.006710346406941879) - present_state_Q (-0.006710346406941879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0658905843714328 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00989698620238851) - present_state_Q ( -0.00989698620238851)) * f1( 0.013518476231453719)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.00989698620238851) - present_state_Q (-0.00989698620238851)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0881403080003014 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007448165569631617) - present_state_Q ( -0.010144505460680362)) * f1( 0.01059435126241579)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.007448165569631617) - present_state_Q (-0.010144505460680362)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1157005004502563 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010538650075307923) - present_state_Q ( -0.008044479694371226)) * f1( 0.013121460526602189)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.010538650075307923) - present_state_Q (-0.008044479694371226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1422828685538025 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009668016834351065) - present_state_Q ( -0.009668016834351065)) * f1( 0.012656947285673757)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.009668016834351065) - present_state_Q (-0.009668016834351065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1661909148122318 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011984246432753415) - present_state_Q ( -0.006621689480340434)) * f1( 0.01138181738259332)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.011984246432753415) - present_state_Q (-0.006621689480340434)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1960570069676768 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011206342134521017) - present_state_Q ( -0.0119231485030564)) * f1( 0.014221884865149359)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.011206342134521017) - present_state_Q (-0.0119231485030564)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2166182420823828 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0076508735713642665) - present_state_Q ( -0.012471592597403254)) * f1( 0.009791441923102757)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.0076508735713642665) - present_state_Q (-0.012471592597403254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.245199484783375 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009574119660810504) - present_state_Q ( -0.009574119660810504)) * f1( 0.013608638072715068)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.009574119660810504) - present_state_Q (-0.009574119660810504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2695739949273745 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010048880391523622) - present_state_Q ( -0.010048880391523622)) * f1( 0.012133663439607534)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.010048880391523622) - present_state_Q (-0.010048880391523622)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2939188893602296 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008235454296465206) - present_state_Q ( -0.008235454296465206)) * f1( 0.01211793617361763)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.008235454296465206) - present_state_Q (-0.008235454296465206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3213314335228192 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012506189891420398) - present_state_Q ( -0.012506189891420398)) * f1( 0.013647503288790858)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.012506189891420398) - present_state_Q (-0.012506189891420398)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3423142130333596 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004270151424629645) - present_state_Q ( -0.022331945729904743)) * f1( 0.010451946974330611)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.004270151424629645) - present_state_Q (-0.022331945729904743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3607449424462887 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0071122187329761746) - present_state_Q ( -0.019735135997274826)) * f1( 0.009179401861331307)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.0071122187329761746) - present_state_Q (-0.019735135997274826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3902732060297953 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012652510108910397) - present_state_Q ( -0.014563238188796141)) * f1( 0.014702322157997117)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.012652510108910397) - present_state_Q (-0.014563238188796141)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.408943873756134 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009318409476614624) - present_state_Q ( -0.018488023626104366)) * f1( 0.009298223138102765)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.009318409476614624) - present_state_Q (-0.018488023626104366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.435656808953168 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010700344926146571) - present_state_Q ( -0.010700344926146571)) * f1( 0.013298122661013252)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.010700344926146571) - present_state_Q (-0.010700344926146571)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4605954079572308 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01197525833269857) - present_state_Q ( -0.01197525833269857)) * f1( 0.012415539154634255)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.01197525833269857) - present_state_Q (-0.01197525833269857)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.484325991317979 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009059551557017431) - present_state_Q ( -0.009059551557017431)) * f1( 0.011812592252194004)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.009059551557017431) - present_state_Q (-0.009059551557017431)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5122538945217383 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014722837048623838) - present_state_Q ( -0.014722837048623838)) * f1( 0.013905459059198783)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.014722837048623838) - present_state_Q (-0.014722837048623838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5326863010476797 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010367522257793138) - present_state_Q ( -0.013968677811525608)) * f1( 0.010173247985316432)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.010367522257793138) - present_state_Q (-0.013968677811525608)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5595061204388843 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014195461046049606) - present_state_Q ( -0.01166152398638163)) * f1( 0.013351737864883913)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.014195461046049606) - present_state_Q (-0.01166152398638163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5846400994822551 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013310665251917842) - present_state_Q ( -0.013310665251917842)) * f1( 0.012513556730853281)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.013310665251917842) - present_state_Q (-0.013310665251917842)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6079083969685375 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015933142714629017) - present_state_Q ( -0.00948588550681898)) * f1( 0.01158232543862821)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.015933142714629017) - present_state_Q (-0.00948588550681898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.636207314971924 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014785056506013847) - present_state_Q ( -0.01624046731916258)) * f1( 0.014091249822082099)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.014785056506013847) - present_state_Q (-0.01624046731916258)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.656217125315016 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011135019999151487) - present_state_Q ( -0.01707056865175668)) * f1( 0.009964339162063159)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.011135019999151487) - present_state_Q (-0.01707056865175668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6826934095280892 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009886011891604828) - present_state_Q ( -0.03229041164425164)) * f1( 0.013195133957434031)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.0009886011891604828) - present_state_Q (-0.03229041164425164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.696281046509424 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013259683727813401) - present_state_Q ( -0.020848799759109797)) * f1( 0.006767473860912653)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.013259683727813401) - present_state_Q (-0.020848799759109797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7211660080723516 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014316387208633893) - present_state_Q ( -0.014316387208633893)) * f1( 0.012390135743716281)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.014316387208633893) - present_state_Q (-0.014316387208633893)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7449180511893672 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010739055289433324) - present_state_Q ( -0.010739055289433324)) * f1( 0.011824164147470903)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.010739055289433324) - present_state_Q (-0.010739055289433324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7729922669322196 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01746129534723381) - present_state_Q ( -0.01746129534723381)) * f1( 0.01398002444072903)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.01746129534723381) - present_state_Q (-0.01746129534723381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.793435951334157 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01292368208397751) - present_state_Q ( -0.016043028898018577)) * f1( 0.010179785038040184)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.01292368208397751) - present_state_Q (-0.016043028898018577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8193994398340685 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016710700973182696) - present_state_Q ( -0.012955779995628162)) * f1( 0.012926100789021948)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.016710700973182696) - present_state_Q (-0.012955779995628162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8453371275515102 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01626134873711017) - present_state_Q ( -0.01626134873711017)) * f1( 0.012915410075129323)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.01626134873711017) - present_state_Q (-0.01626134873711017)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8678206652808238 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01871699464350113) - present_state_Q ( -0.010392506979283317)) * f1( 0.011192043378593449)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.01871699464350113) - present_state_Q (-0.010392506979283317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8968637703048394 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017055488047944857) - present_state_Q ( -0.019564044838131962)) * f1( 0.014464042791833056)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.017055488047944857) - present_state_Q (-0.019564044838131962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9161463031291135 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013075344026573097) - present_state_Q ( -0.01908064047195341)) * f1( 0.009603043256497686)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.013075344026573097) - present_state_Q (-0.01908064047195341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9424958958290588 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004820039732951533) - present_state_Q ( -0.026617469004426176)) * f1( 0.0131280316888752)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.004820039732951533) - present_state_Q (-0.026617469004426176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9587331901652674 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020059515728792415) - present_state_Q ( -0.02658921416010616)) * f1( 0.008089204009716607)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.020059515728792415) - present_state_Q (-0.02658921416010616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.984968994520689 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01890670915387955) - present_state_Q ( -0.018980214578342165)) * f1( 0.013688170058530735)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.01890670915387955) - present_state_Q (-0.018980214578342165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0047528218094883 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012276101347883825) - present_state_Q ( -0.019171834406727407)) * f1( 0.010322400062855806)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.012276101347883825) - present_state_Q (-0.019171834406727407)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0315474111657648 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01638716023673846) - present_state_Q ( -0.01638716023673846)) * f1( 0.013978001122854305)
w2 ( -3.136383096858311 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.01638716023673846) - present_state_Q (-0.01638716023673846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0543611149172336 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024702766703854052) - present_state_Q ( -0.01608163738095779)) * f1( 0.011900576547541187)
w2 ( -3.5197881135872287 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.024702766703854052) - present_state_Q (-0.01608163738095779)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.0776732489167995 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013377609935739125) - present_state_Q ( -0.013377609935739125)) * f1( 0.01215958169033271)
w2 ( -3.5197881135872287 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.013377609935739125) - present_state_Q (-0.013377609935739125)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1026396834706627 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02182155004313171) - present_state_Q ( -0.723753011317789)) * f1( 0.013522933905084281)
w2 ( -3.8890339404040883 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.02182155004313171) - present_state_Q (-0.723753011317789)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.121808746468582 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013374602732413095) - present_state_Q ( -0.020435675017768076)) * f1( 0.010502878667041813)
w2 ( -3.8890339404040883 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.013374602732413095) - present_state_Q (-0.020435675017768076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1469277411280037 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02022992008464681) - present_state_Q ( -0.01692240515479409)) * f1( 0.013759726986209203)
w2 ( -3.8890339404040883 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.02022992008464681) - present_state_Q (-0.01692240515479409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1680299674094012 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020420340331138213) - present_state_Q ( -0.7951623146774943)) * f1( 0.012074128862572135)
w2 ( -4.2385784337846575 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.020420340331138213) - present_state_Q (-0.7951623146774943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.1898645957342837 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013863376189657061) - present_state_Q ( -0.023711983010740026)) * f1( 0.011965478532653622)
w2 ( -4.2385784337846575 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.013863376189657061) - present_state_Q (-0.023711983010740026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.21470550807522 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00880910027005458) - present_state_Q ( -0.0293575973248958)) * f1( 0.01361752644172555)
w2 ( -4.2385784337846575 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.00880910027005458) - present_state_Q (-0.0293575973248958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.257908100521115 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8633299683276959) - present_state_Q ( -0.907511938074262)) * f1( 0.02612694129541907)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.8633299683276959) - present_state_Q (-0.907511938074262)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.265058765468353 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01858814650389367) - present_state_Q ( -0.019650420689248452)) * f1( 0.004609762899130128)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01858814650389367) - present_state_Q (-0.019650420689248452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2785535867915447 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013735691234919436) - present_state_Q ( -0.025103642851549757)) * f1( 0.008702932012473503)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.013735691234919436) - present_state_Q (-0.025103642851549757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3001905526822064 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022677805093194402) - present_state_Q ( -0.022677805093194402)) * f1( 0.013950887431089654)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.022677805093194402) - present_state_Q (-0.022677805093194402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3157931355217034 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014366754346703368) - present_state_Q ( -0.021694383082507623)) * f1( 0.010059993634190592)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.014366754346703368) - present_state_Q (-0.021694383082507623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3374959401296347 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021036642876997896) - present_state_Q ( -0.018963908971331236)) * f1( 0.013990136269099417)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.021036642876997896) - present_state_Q (-0.018963908971331236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3560174359216415 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01861936786437616) - present_state_Q ( -0.01861936786437616)) * f1( 0.011939310950658307)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01861936786437616) - present_state_Q (-0.01861936786437616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.374891900576275 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02314382091022147) - present_state_Q ( -0.015359598817326457)) * f1( 0.01216393065492519)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.02314382091022147) - present_state_Q (-0.015359598817326457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.395922492583324 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021795012283663893) - present_state_Q ( -0.02272958484890931)) * f1( 0.013560037610762236)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.021795012283663893) - present_state_Q (-0.02272958484890931)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4122046229046408 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01595808170929031) - present_state_Q ( -0.0229592654934286)) * f1( 0.010498890335059564)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01595808170929031) - present_state_Q (-0.0229592654934286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4330908616727047 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01858683858507919) - present_state_Q ( -0.01858683858507919)) * f1( 0.013463644158898438)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01858683858507919) - present_state_Q (-0.01858683858507919)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4522486591395127 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02034311681118171) - present_state_Q ( -0.02034311681118171)) * f1( 0.012350718325980612)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.02034311681118171) - present_state_Q (-0.02034311681118171)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4703927608947547 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01491891070255562) - present_state_Q ( -0.01491891070255562)) * f1( 0.011693525104965697)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01491891070255562) - present_state_Q (-0.01491891070255562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.492008345182154 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024559060329722014) - present_state_Q ( -0.024559060329722014)) * f1( 0.013938622846877288)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.024559060329722014) - present_state_Q (-0.024559060329722014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5076300680354286 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0064368712599882074) - present_state_Q ( -0.02897533443763098)) * f1( 0.010077580615801633)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0064368712599882074) - present_state_Q (-0.02897533443763098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5206346343142645 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02538791152592019) - present_state_Q ( -0.03500970985868504)) * f1( 0.00839149243767922)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.02538791152592019) - present_state_Q (-0.03500970985868504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5422844069532653 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02350301226166877) - present_state_Q ( -0.025125296978425294)) * f1( 0.013961273756026125)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.02350301226166877) - present_state_Q (-0.025125296978425294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5578934162795393 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008776809984478435) - present_state_Q ( -0.042147236480198504)) * f1( 0.010077790289922393)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.008776809984478435) - present_state_Q (-0.042147236480198504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5725302376835266 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009267929842073301) - present_state_Q ( -0.009267929842073301)) * f1( 0.009430059338284643)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.009267929842073301) - present_state_Q (-0.009267929842073301)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5870614137424868 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01837379558258688) - present_state_Q ( -0.02882951402670282)) * f1( 0.009373258413603043)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01837379558258688) - present_state_Q (-0.02882951402670282)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.605940827886232 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018679189364297088) - present_state_Q ( -0.018679189364297088)) * f1( 0.01293157971577444)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.018679189364297088) - present_state_Q (-0.018679189364297088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6246334946268255 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022962276278382685) - present_state_Q ( -0.022962276278382685)) * f1( 0.012807047361819042)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.022962276278382685) - present_state_Q (-0.022962276278382685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.640988371800924 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005867384483620775) - present_state_Q ( -0.032538730633739736)) * f1( 0.011214011160534252)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.005867384483620775) - present_state_Q (-0.032538730633739736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6531222210266354 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02462981325647494) - present_state_Q ( -0.037512983721799224)) * f1( 0.008321556267771688)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02462981325647494) - present_state_Q (-0.037512983721799224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.673820571158267 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010949937912784617) - present_state_Q ( -0.04532240135389019)) * f1( 0.014204145736627623)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.010949937912784617) - present_state_Q (-0.04532240135389019)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.682577336791403 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024936005796966962) - present_state_Q ( -0.024936005796966962)) * f1( 0.006000318970626509)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.024936005796966962) - present_state_Q (-0.024936005796966962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.696189600929611 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02245852569798693) - present_state_Q ( -0.02245852569798693)) * f1( 0.009325983226378195)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02245852569798693) - present_state_Q (-0.02245852569798693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7108019576278366 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010443682081795929) - present_state_Q ( -0.037214798031994914)) * f1( 0.01002276611749436)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0010443682081795929) - present_state_Q (-0.037214798031994914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.717184230132345 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022694822099838363) - present_state_Q ( -0.022694822099838363)) * f1( 0.004372662252263206)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.022694822099838363) - present_state_Q (-0.022694822099838363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.729420176917583 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010525004713094897) - present_state_Q ( -0.0010525004713094897)) * f1( 0.008371995614057364)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0010525004713094897) - present_state_Q (-0.0010525004713094897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7358023880804243 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022850693750074067) - present_state_Q ( -0.022850693750074067)) * f1( 0.0043726622522633485)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.022850693750074067) - present_state_Q (-0.022850693750074067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7480125389465235 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02087289374181946) - present_state_Q ( -0.03384669060291393)) * f1( 0.008371995614057506)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02087289374181946) - present_state_Q (-0.03384669060291393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7660748295774047 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018514587208608747) - present_state_Q ( -0.018514587208608747)) * f1( 0.012371760018333218)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.018514587208608747) - present_state_Q (-0.018514587208608747)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.785438888508512 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02558711347689481) - present_state_Q ( -0.02558711347689481)) * f1( 0.013269191010886932)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02558711347689481) - present_state_Q (-0.02558711347689481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8013691428229563 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01975558073078299) - present_state_Q ( -0.027138161720658336)) * f1( 0.010917778462597172)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.01975558073078299) - present_state_Q (-0.027138161720658336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8202935366718926 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02803148764354217) - present_state_Q ( -0.020313286733443785)) * f1( 0.01296300939406583)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02803148764354217) - present_state_Q (-0.020313286733443785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.838922141010538 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02472592456751913) - present_state_Q ( -0.02472592456751913)) * f1( 0.012764543960196557)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.02472592456751913) - present_state_Q (-0.02472592456751913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.855347301875398 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03072140377646531) - present_state_Q ( -0.016127380789472286)) * f1( 0.01124762898579118)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03072140377646531) - present_state_Q (-0.016127380789472286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8762395803430887 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028345290402420602) - present_state_Q ( -0.029467242186856604)) * f1( 0.014319937859050226)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.028345290402420602) - present_state_Q (-0.029467242186856604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8903404146474014 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006675861660034577) - present_state_Q ( -0.05085320208909815)) * f1( 0.00968058962390006)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.006675861660034577) - present_state_Q (-0.05085320208909815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9026159083546896 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027869335638281374) - present_state_Q ( -0.04124822367955243)) * f1( 0.0084206679708092)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.027869335638281374) - present_state_Q (-0.04124822367955243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.923435833486663 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025187181638398632) - present_state_Q ( -0.029882557377072294)) * f1( 0.014271060761742276)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.025187181638398632) - present_state_Q (-0.029882557377072294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.937825728564872 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007531702742380077) - present_state_Q ( -0.05215373127981113)) * f1( 0.00987986143668734)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.007531702742380077) - present_state_Q (-0.05215373127981113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9493279013764915 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03003990377414375) - present_state_Q ( -0.041093913401846635)) * f1( 0.008417454523804305)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.03003990377414375) - present_state_Q (-0.041093913401846635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9684577988927714 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027784510085123906) - present_state_Q ( -0.029471669434766402)) * f1( 0.013987866265274018)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.027784510085123906) - present_state_Q (-0.029471669434766402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9821682668878786 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010294494717318454) - present_state_Q ( -0.04912706803162666)) * f1( 0.010040870220820404)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.010294494717318454) - present_state_Q (-0.04912706803162666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9951092188836332 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010721058370512541) - present_state_Q ( -0.010721058370512541)) * f1( 0.009450703472648085)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.010721058370512541) - present_state_Q (-0.010721058370512541)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0079473372664265 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008979228748618086) - present_state_Q ( -0.008979228748618086)) * f1( 0.009374531434242848)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.008979228748618086) - present_state_Q (-0.008979228748618086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0193697643459663 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007467156785495899) - present_state_Q ( -0.007467156785495899)) * f1( 0.008935973630235752)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.007467156785495899) - present_state_Q (-0.007467156785495899)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.030103212580029 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029700104912253247) - present_state_Q ( -0.029700104912253247)) * f1( 0.008410138038348032)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.029700104912253247) - present_state_Q (-0.029700104912253247)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0458756965777574 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025005981861715872) - present_state_Q ( -0.025005981861715872)) * f1( 0.012354358935236256)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.025005981861715872) - present_state_Q (-0.025005981861715872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.061053781744686 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03669408778154517) - present_state_Q ( -0.007143822150523182)) * f1( 0.011871079072466317)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.03669408778154517) - present_state_Q (-0.007143822150523182)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0690305634931896 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028102166755064453) - present_state_Q ( -0.03131413702714114)) * f1( 0.006251035036757528)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.028102166755064453) - present_state_Q (-0.03131413702714114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0820623572675783 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010273116094334008) - present_state_Q ( -0.05128715238269055)) * f1( 0.010229855226291796)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.010273116094334008) - present_state_Q (-0.05128715238269055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0939758095183274 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008757167228428411) - present_state_Q ( -0.008757167228428411)) * f1( 0.009320958205896908)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008757167228428411) - present_state_Q (-0.008757167228428411)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.103413638136017 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030848178415633015) - present_state_Q ( -0.04036653301262206)) * f1( 0.007401080934235082)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.030848178415633015) - present_state_Q (-0.04036653301262206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1200665432370958 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028089341136298126) - present_state_Q ( -0.028089341136298126)) * f1( 0.013046815973298239)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.028089341136298126) - present_state_Q (-0.028089341136298126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1340432727456675 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008079563973905806) - present_state_Q ( -0.054353706812153915)) * f1( 0.010974453082148313)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008079563973905806) - present_state_Q (-0.054353706812153915)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1451079345950954 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00894699756243482) - present_state_Q ( -0.00894699756243482)) * f1( 0.00865698934965027)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.00894699756243482) - present_state_Q (-0.00894699756243482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.156169170563973 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0072810192238691295) - present_state_Q ( -0.0072810192238691295)) * f1( 0.008653293810453721)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.0072810192238691295) - present_state_Q (-0.0072810192238691295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.166809010848432 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006029995252176225) - present_state_Q ( -0.006029995252176225)) * f1( 0.008322899483736663)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.006029995252176225) - present_state_Q (-0.006029995252176225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1771471807209517 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02814490772473284) - present_state_Q ( -0.04319893224590843)) * f1( 0.008109095469207338)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.02814490772473284) - present_state_Q (-0.04319893224590843)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1945547189511583 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025977502498125832) - present_state_Q ( -0.030794888669784033)) * f1( 0.013641154896908304)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.025977502498125832) - present_state_Q (-0.030794888669784033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.20810975750623 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011072380205790818) - present_state_Q ( -0.05438896028971313)) * f1( 0.010643122959816438)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.011072380205790818) - present_state_Q (-0.05438896028971313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.219909096301917 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009247006924622632) - present_state_Q ( -0.009247006924622632)) * f1( 0.009231995484302685)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.009247006924622632) - present_state_Q (-0.009247006924622632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.231178488955406 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007645434281971779) - present_state_Q ( -0.007645434281971779)) * f1( 0.008816362665034178)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.007645434281971779) - present_state_Q (-0.007645434281971779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2419909509927387 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008740718407192646) - present_state_Q ( -0.008740718407192646)) * f1( 0.008459545386254382)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008740718407192646) - present_state_Q (-0.008740718407192646)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2528353305211986 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007040975876229743) - present_state_Q ( -0.007040975876229743)) * f1( 0.008483501895122992)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.007040975876229743) - present_state_Q (-0.007040975876229743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2632926756752068 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005784030671886635) - present_state_Q ( -0.005784030671886635)) * f1( 0.008180003003125175)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.005784030671886635) - present_state_Q (-0.005784030671886635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.273496691887255 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01862689400178027) - present_state_Q ( -0.01862689400178027)) * f1( 0.007989065867331032)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.01862689400178027) - present_state_Q (-0.01862689400178027)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2922805573122966 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02793865380731735) - present_state_Q ( -0.029007308240484253)) * f1( 0.014717405143038383)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.02793865380731735) - present_state_Q (-0.029007308240484253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.305760627663768 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02524673743432525) - present_state_Q ( -0.03798645410523653)) * f1( 0.01138494220316578)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.02524673743432525) - present_state_Q (-0.03798645410523653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3205847128626713 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022594683962638737) - present_state_Q ( -0.022594683962638737)) * f1( 0.012504090078150466)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.022594683962638737) - present_state_Q (-0.022594683962638737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3363145959238905 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03085727916555389) - present_state_Q ( -0.03085727916555389)) * f1( 0.013276456704885595)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.03085727916555389) - present_state_Q (-0.03085727916555389)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3491082761694213 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009524645582948625) - present_state_Q ( -0.03914091022299816)) * f1( 0.010807722563508959)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.009524645582948625) - present_state_Q (-0.03914091022299816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.359594337519578 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007983054856187027) - present_state_Q ( -0.007983054856187027)) * f1( 0.008835174183396915)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.007983054856187027) - present_state_Q (-0.007983054856187027)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.36975108346431 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009061530461564467) - present_state_Q ( -0.009061530461564467)) * f1( 0.008558404911550375)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.009061530461564467) - present_state_Q (-0.009061530461564467)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.376511420669641 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032380040964350265) - present_state_Q ( -0.03848386724578944)) * f1( 0.005709513588164021)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.032380040964350265) - present_state_Q (-0.03848386724578944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.390062002128612 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01163340858115424) - present_state_Q ( -0.01163340858115424)) * f1( 0.01142038871495092)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.01163340858115424) - present_state_Q (-0.01163340858115424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4012379896561966 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009954522983796768) - present_state_Q ( -0.009954522983796768)) * f1( 0.009417888859783464)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.009954522983796768) - present_state_Q (-0.009954522983796768)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.411929881662308 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009789917289724183) - present_state_Q ( -0.009789917289724183)) * f1( 0.009009834101986426)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.009789917289724183) - present_state_Q (-0.009789917289724183)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4221693491905993 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017518930309347657) - present_state_Q ( -0.038465768320833704)) * f1( 0.008648921854530449)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.017518930309347657) - present_state_Q (-0.038465768320833704)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4397768418391945 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03125603029456348) - present_state_Q ( -0.03719716617188238)) * f1( 0.014869117650087638)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.03125603029456348) - present_state_Q (-0.03719716617188238)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.450586351418886 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008035281764964384) - present_state_Q ( -0.041380410763429104)) * f1( 0.009133396715728302)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.008035281764964384) - present_state_Q (-0.041380410763429104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4604627066126525 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006579795817185906) - present_state_Q ( -0.006579795817185906)) * f1( 0.008320572542042992)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.006579795817185906) - present_state_Q (-0.006579795817185906)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.469991007401399 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016378830839014324) - present_state_Q ( -0.03862844921187828)) * f1( 0.008048412160884338)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.016378830839014324) - present_state_Q (-0.03862844921187828)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.488070112388671 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03061385981515923) - present_state_Q ( -0.03945520905332845)) * f1( 0.015270377956598958)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.03061385981515923) - present_state_Q (-0.03945520905332845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4977089152177827 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0126170088066875) - present_state_Q ( -0.03816903807432695)) * f1( 0.008822460850722863)
w2 ( -4.569291409003611 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.0126170088066875) - present_state_Q (-0.03816903807432695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5072166382674856 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04105228630260166) - present_state_Q ( -0.9244037767315759)) * f1( 0.009468043971756334)
w2 ( -4.770129577437658 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04105228630260166) - present_state_Q (-0.9244037767315759)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.514842002890204 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9736626748066306) - present_state_Q ( -0.9901570032190133)) * f1( 0.007572797116998066)
w2 ( -4.971517902118965 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.9736626748066306) - present_state_Q (-0.9901570032190133)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.529323454220637 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0128068903666192) - present_state_Q ( -1.0508042346865776)) * f1( 0.014463107158166247)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -1.0128068903666192) - present_state_Q (-1.0508042346865776)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.5438383208518287 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025600519749758837) - present_state_Q ( -0.04648189800655425)) * f1( 0.015965753474227892)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.025600519749758837) - present_state_Q (-0.04648189800655425)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5516428319506668 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031477443887215295) - present_state_Q ( -0.03459225312878945)) * f1( 0.008572873802801603)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.031477443887215295) - present_state_Q (-0.03459225312878945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.561757224775883 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024269321063612504) - present_state_Q ( -0.047667278944409204)) * f1( 0.011127027884028182)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.024269321063612504) - present_state_Q (-0.047667278944409204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.573984686264249 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027343590082745065) - present_state_Q ( -0.027343590082745065)) * f1( 0.013421191600572327)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.027343590082745065) - present_state_Q (-0.027343590082745065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5853262292231762 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014422383378796793) - present_state_Q ( -0.05056448656177829)) * f1( 0.012482368769774767)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.014422383378796793) - present_state_Q (-0.05056448656177829)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5948207097006444 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003569279618955698) - present_state_Q ( -0.04761125275321293)) * f1( 0.010447365745059193)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.003569279618955698) - present_state_Q (-0.04761125275321293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6013563582362407 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03156048128823128) - present_state_Q ( -0.04596180526519446)) * f1( 0.007188060863046368)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03156048128823128) - present_state_Q (-0.04596180526519446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.613015640107706 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01785876653029287) - present_state_Q ( -0.01785876653029287)) * f1( 0.012785562612668349)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01785876653029287) - present_state_Q (-0.01785876653029287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6216111027156184 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01057317405458223) - present_state_Q ( -0.06298126201033546)) * f1( 0.00947341250410252)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01057317405458223) - present_state_Q (-0.06298126201033546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6296270841554983 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00865033300480904) - present_state_Q ( -0.00865033300480904)) * f1( 0.008782339662022517)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.00865033300480904) - present_state_Q (-0.00865033300480904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6373224426185535 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0073515065569389575) - present_state_Q ( -0.0073515065569389575)) * f1( 0.008429984285349131)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0073515065569389575) - present_state_Q (-0.0073515065569389575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.64476715021635 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008257114021645987) - present_state_Q ( -0.008257114021645987)) * f1( 0.008156133635798164)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.008257114021645987) - present_state_Q (-0.008257114021645987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6520057228547578 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006305080714332578) - present_state_Q ( -0.006305080714332578)) * f1( 0.007928774085887652)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.006305080714332578) - present_state_Q (-0.006305080714332578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6580704697445285 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02781736369519272) - present_state_Q ( -0.04522750359281787)) * f1( 0.006669888015762186)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.02781736369519272) - present_state_Q (-0.04522750359281787)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6693226564228842 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014883479673612362) - present_state_Q ( -0.050806667786772404)) * f1( 0.012384291544171986)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.014883479673612362) - present_state_Q (-0.050806667786772404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.678151265165434 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01276677350286152) - present_state_Q ( -0.01276677350286152)) * f1( 0.009676584931514534)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01276677350286152) - present_state_Q (-0.01276677350286152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6880909378716233 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010659082644458119) - present_state_Q ( -0.0638257014826545)) * f1( 0.010955930968978804)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.010659082644458119) - present_state_Q (-0.0638257014826545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.696132873876957 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008772834586437267) - present_state_Q ( -0.008772834586437267)) * f1( 0.008810882010651331)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.008772834586437267) - present_state_Q (-0.008772834586437267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.70384806816905 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010212975433865965) - present_state_Q ( -0.010212975433865965)) * f1( 0.008454098757993392)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.010212975433865965) - present_state_Q (-0.010212975433865965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7116055117641453 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008242813543684923) - present_state_Q ( -0.008242813543684923)) * f1( 0.008498743112312183)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.008242813543684923) - present_state_Q (-0.008242813543684923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.71908682710292 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006650937090121051) - present_state_Q ( -0.006650937090121051)) * f1( 0.008194941828658733)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.006650937090121051) - present_state_Q (-0.006650937090121051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.72633626680092 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00814027058626331) - present_state_Q ( -0.00814027058626331)) * f1( 0.007942114179284141)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.00814027058626331) - present_state_Q (-0.00814027058626331)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.733634387802322 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006249817941080914) - present_state_Q ( -0.006249817941080914)) * f1( 0.007993956864274563)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.006249817941080914) - present_state_Q (-0.006249817941080914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7407236423590935 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008975539433031785) - present_state_Q ( -0.008975539433031785)) * f1( 0.007767263245763354)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.008975539433031785) - present_state_Q (-0.008975539433031785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.747970419082627 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006960987733778133) - present_state_Q ( -0.006960987733778133)) * f1( 0.00793827372592257)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.006960987733778133) - present_state_Q (-0.006960987733778133)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.753958112299959 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02878758172879861) - present_state_Q ( -0.03488367586015624)) * f1( 0.006577593106731018)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.02878758172879861) - present_state_Q (-0.03488367586015624)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.762416531350773 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005495803501799512) - present_state_Q ( -0.047831053749502805)) * f1( 0.009307350901850114)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.005495803501799512) - present_state_Q (-0.047831053749502805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7692455841421997 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02367082342904204) - present_state_Q ( -0.03723336658473125)) * f1( 0.00750420112071035)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.02367082342904204) - present_state_Q (-0.03723336658473125)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7817995657179555 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0366114117889149) - present_state_Q ( -0.0366114117889149)) * f1( 0.013792216113914185)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0366114117889149) - present_state_Q (-0.0366114117889149)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7912026738766134 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010404564175485562) - present_state_Q ( -0.04380856790567304)) * f1( 0.01034171796907972)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.010404564175485562) - present_state_Q (-0.04380856790567304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7990908667789673 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008476336574990985) - present_state_Q ( -0.008476336574990985)) * f1( 0.008642185783454676)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.008476336574990985) - present_state_Q (-0.008476336574990985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.806665942318391 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024065959956383953) - present_state_Q ( -0.024065959956383953)) * f1( 0.008311915994704565)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.024065959956383953) - present_state_Q (-0.024065959956383953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8194220335524935 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03342071507737296) - present_state_Q ( -0.03127224333826206)) * f1( 0.014006535468911646)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03342071507737296) - present_state_Q (-0.03127224333826206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8303634893197493 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015904072190047303) - present_state_Q ( -0.015904072190047303)) * f1( 0.011996080627211725)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.015904072190047303) - present_state_Q (-0.015904072190047303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8394442242876545 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028536139532842675) - present_state_Q ( -0.028536139532842675)) * f1( 0.009968434038006787)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.028536139532842675) - present_state_Q (-0.028536139532842675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8509195024743734 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021396266791005324) - present_state_Q ( -0.045343202384501666)) * f1( 0.01262133462402816)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.021396266791005324) - present_state_Q (-0.045343202384501666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8641416333592207 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03679173599112307) - present_state_Q ( -0.040405682416604395)) * f1( 0.014532295882428268)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03679173599112307) - present_state_Q (-0.040405682416604395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8728260349193846 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010519788421476653) - present_state_Q ( -0.04643011958591101)) * f1( 0.009554013260335064)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.010519788421476653) - present_state_Q (-0.04643011958591101)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8808152764763375 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011936400059878394) - present_state_Q ( -0.011936400059878394)) * f1( 0.00875588042944683)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.011936400059878394) - present_state_Q (-0.011936400059878394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.888806515035745 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018992859258933844) - present_state_Q ( -0.04453709103958555)) * f1( 0.008788790859403976)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.018992859258933844) - present_state_Q (-0.04453709103958555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9025552264669368 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03457377031980782) - present_state_Q ( -0.0432319088153158)) * f1( 0.015116118631942581)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03457377031980782) - present_state_Q (-0.0432319088153158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.910636953201697 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009881330315692216) - present_state_Q ( -0.045953989335520876)) * f1( 0.008890586401285648)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.009881330315692216) - present_state_Q (-0.045953989335520876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.918344983865115 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007982544182985218) - present_state_Q ( -0.007982544182985218)) * f1( 0.008444391586772846)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.007982544182985218) - present_state_Q (-0.007982544182985218)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.925779787727311 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009846387234849286) - present_state_Q ( -0.009846387234849286)) * f1( 0.008146560077940354)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.009846387234849286) - present_state_Q (-0.009846387234849286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9325361887070205 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007773565640014097) - present_state_Q ( -0.007773565640014097)) * f1( 0.008224810019110376)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.007773565640014097) - present_state_Q (-0.007773565640014097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9390788393264424 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006198691716063716) - present_state_Q ( -0.006198691716063716)) * f1( 0.007963230010364863)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.006198691716063716) - present_state_Q (-0.006198691716063716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9454461364972224 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007825298805867376) - present_state_Q ( -0.007825298805867376)) * f1( 0.007751183884819359)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.007825298805867376) - present_state_Q (-0.007825298805867376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.95186185782187 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0058557745080882575) - present_state_Q ( -0.0058557745080882575)) * f1( 0.007808447755298356)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0058557745080882575) - present_state_Q (-0.0058557745080882575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9581148678194777 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008728176458497135) - present_state_Q ( -0.008728176458497135)) * f1( 0.007612810267702521)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.008728176458497135) - present_state_Q (-0.008728176458497135)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9644772767009706 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0200935284987919) - present_state_Q ( -0.041882285009795475)) * f1( 0.007776311629839981)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0200935284987919) - present_state_Q (-0.041882285009795475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9767337416468385 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03600978946582191) - present_state_Q ( -0.04361964640642859)) * f1( 0.014980456362422519)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.03600978946582191) - present_state_Q (-0.04361964640642859)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9841612152269783 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012238306372195107) - present_state_Q ( -0.04564429250762687)) * f1( 0.009083111581304702)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.012238306372195107) - present_state_Q (-0.04564429250762687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9915247819514628 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0035493608117341193) - present_state_Q ( -0.05528921735146573)) * f1( 0.009016552387548855)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0035493608117341193) - present_state_Q (-0.05528921735146573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9973197848638544 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0073775626174081834) - present_state_Q ( -0.0073775626174081834)) * f1( 0.007054159505994061)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0073775626174081834) - present_state_Q (-0.0073775626174081834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.003296903565463 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004900163946168941) - present_state_Q ( -0.004900163946168941)) * f1( 0.0072738717703584865)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.004900163946168941) - present_state_Q (-0.004900163946168941)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.009183554807862 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033813188078000013) - present_state_Q ( -0.0033813188078000013)) * f1( 0.007162585620836844)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0033813188078000013) - present_state_Q (-0.0033813188078000013)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.015032572488361 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0076919857427035455) - present_state_Q ( -0.0076919857427035455)) * f1( 0.007120156040491111)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0076919857427035455) - present_state_Q (-0.0076919857427035455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.020812894043538 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020535514593512043) - present_state_Q ( -0.0402898897878252)) * f1( 0.007063455547814943)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.020535514593512043) - present_state_Q (-0.0402898897878252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.03318386801456 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03593667324824603) - present_state_Q ( -0.04451556625224237)) * f1( 0.015122084383360697)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.03593667324824603) - present_state_Q (-0.04451556625224237)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.040490502338734 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010217164313927527) - present_state_Q ( -0.04757144646694619)) * f1( 0.008937663650423247)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.010217164313927527) - present_state_Q (-0.04757144646694619)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.046666584186077 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008279617085465646) - present_state_Q ( -0.008279617085465646)) * f1( 0.008459590746453024)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.008279617085465646) - present_state_Q (-0.008279617085465646)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.0526225584463065 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010066005141699272) - present_state_Q ( -0.010066005141699272)) * f1( 0.008159898813025174)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.010066005141699272) - present_state_Q (-0.010066005141699272)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.058628154282489 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007932582466939437) - present_state_Q ( -0.007932582466939437)) * f1( 0.008225718315309424)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.007932582466939437) - present_state_Q (-0.007932582466939437)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.064407228584909 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04287787010523462) - present_state_Q ( -0.05510230232762778)) * f1( 0.007963093185315736)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.04287787010523462) - present_state_Q (-0.05510230232762778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.074280914009363 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034740420846440086) - present_state_Q ( -0.039027037764206525)) * f1( 0.013576583080045461)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.034740420846440086) - present_state_Q (-0.039027037764206525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.081964616942531 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013316469037461498) - present_state_Q ( -0.06936244558074545)) * f1( 0.010612692244568145)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.013316469037461498) - present_state_Q (-0.06936244558074545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.0886333660700505 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011034592764785777) - present_state_Q ( -0.011034592764785777)) * f1( 0.009137517294164713)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.011034592764785777) - present_state_Q (-0.011034592764785777)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.095005331745057 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009413883520014628) - present_state_Q ( -0.009413883520014628)) * f1( 0.008729120106066862)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.009413883520014628) - present_state_Q (-0.009413883520014628)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.101106367372243 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03795665528300778) - present_state_Q ( -0.05708904469262108)) * f1( 0.008409602083818805)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.03795665528300778) - present_state_Q (-0.05708904469262108)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.11124252488094 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03457953348671388) - present_state_Q ( -0.04091567135526643)) * f1( 0.01394114050354435)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.03457953348671388) - present_state_Q (-0.04091567135526643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.117739669426352 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014902971028631565) - present_state_Q ( -0.0686416423534724)) * f1( 0.010268156426993826)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.014902971028631565) - present_state_Q (-0.0686416423534724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.123778375015076 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01244035224339611) - present_state_Q ( -0.01244035224339611)) * f1( 0.00945997821198742)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01244035224339611) - present_state_Q (-0.01244035224339611)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.1295270035746 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010478360223480908) - present_state_Q ( -0.010478360223480908)) * f1( 0.009003065487627436)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010478360223480908) - present_state_Q (-0.010478360223480908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.135042455231385 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017391321838946334) - present_state_Q ( -0.04799213092241308)) * f1( 0.008687983885295687)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.017391321838946334) - present_state_Q (-0.04799213092241308)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.145067589737651 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0339532969544727) - present_state_Q ( -0.048750093360510546)) * f1( 0.015789438583379663)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0339532969544727) - present_state_Q (-0.048750093360510546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.150280382901248 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011363747834389492) - present_state_Q ( -0.0472952565520768)) * f1( 0.008211112055576893)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.011363747834389492) - present_state_Q (-0.0472952565520768)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.155599821537439 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04023759384997399) - present_state_Q ( -0.057870840551188706)) * f1( 0.008389258044509282)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.04023759384997399) - present_state_Q (-0.057870840551188706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.164463796478462 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036890805606770126) - present_state_Q ( -0.04139829034268534)) * f1( 0.013943838780052294)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.036890805606770126) - present_state_Q (-0.04139829034268534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.17089420911079 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014610306423456098) - present_state_Q ( -0.06925673736556812)) * f1( 0.010163727629964727)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.014610306423456098) - present_state_Q (-0.06925673736556812)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.176915231274318 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012261252707986251) - present_state_Q ( -0.012261252707986251)) * f1( 0.00943203794253312)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.012261252707986251) - present_state_Q (-0.012261252707986251)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.182645837460595 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012954803572498827) - present_state_Q ( -0.012954803572498827)) * f1( 0.008977974045157553)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.012954803572498827) - present_state_Q (-0.012954803572498827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.188346027057585 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010649427584569045) - present_state_Q ( -0.010649427584569045)) * f1( 0.008927419322275879)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010649427584569045) - present_state_Q (-0.010649427584569045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.193808807194931 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008901544167019148) - present_state_Q ( -0.008901544167019148)) * f1( 0.008553490348749183)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.008901544167019148) - present_state_Q (-0.008901544167019148)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.199077476030607 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017700287833110447) - present_state_Q ( -0.04739971857333779)) * f1( 0.008298434822679961)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.017700287833110447) - present_state_Q (-0.04739971857333779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.209102604823848 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03467142362818717) - present_state_Q ( -0.04999219568010165)) * f1( 0.015792340412097056)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03467142362818717) - present_state_Q (-0.04999219568010165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.214347093453194 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015738664798543314) - present_state_Q ( -0.04456210954302808)) * f1( 0.008256914483264573)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.015738664798543314) - present_state_Q (-0.04456210954302808)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.220380677891899 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01324132521441457) - present_state_Q ( -0.01324132521441457)) * f1( 0.009453023160457784)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01324132521441457) - present_state_Q (-0.01324132521441457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.226134270035777 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010924347890698863) - present_state_Q ( -0.010924347890698863)) * f1( 0.009011405555812904)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010924347890698863) - present_state_Q (-0.010924347890698863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.231640977003128 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00932281106286751) - present_state_Q ( -0.00932281106286751)) * f1( 0.008622781817756517)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.00932281106286751) - present_state_Q (-0.00932281106286751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.236920520037387 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0188071523746002) - present_state_Q ( -0.04755430569283265)) * f1( 0.008315619765349714)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0188071523746002) - present_state_Q (-0.04755430569283265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.246801491491503 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03603356216917348) - present_state_Q ( -0.049447551636727025)) * f1( 0.015563583623685973)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03603356216917348) - present_state_Q (-0.049447551636727025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.252202431922098 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015607830023103864) - present_state_Q ( -0.045614443930680884)) * f1( 0.008504658512889809)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.015607830023103864) - present_state_Q (-0.045614443930680884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.258237221408694 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013055749567548847) - present_state_Q ( -0.013055749567548847)) * f1( 0.009454663747247278)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.013055749567548847) - present_state_Q (-0.013055749567548847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.26398613786003 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010841975248406762) - present_state_Q ( -0.010841975248406762)) * f1( 0.009003977833444754)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010841975248406762) - present_state_Q (-0.010841975248406762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.268416792383127 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016400450286475278) - present_state_Q ( -0.04872398465599472)) * f1( 0.006980116331349935)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.016400450286475278) - present_state_Q (-0.04872398465599472)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.273362234148652 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015295829383477056) - present_state_Q ( -0.015295829383477056)) * f1( 0.007750438090104294)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.015295829383477056) - present_state_Q (-0.015295829383477056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.278328760190572 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013957601180168847) - present_state_Q ( -0.013957601180168847)) * f1( 0.00778201224164892)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.013957601180168847) - present_state_Q (-0.013957601180168847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.282412917997226 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01987576060963577) - present_state_Q ( -0.04206302471075393)) * f1( 0.006427144074505487)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01987576060963577) - present_state_Q (-0.04206302471075393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.292566344150946 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03463882204239444) - present_state_Q ( -0.051083875669603714)) * f1( 0.015997203358523348)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03463882204239444) - present_state_Q (-0.051083875669603714)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.2977249137391595 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018953658009952024) - present_state_Q ( -0.018953658009952024)) * f1( 0.008088622630671992)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.018953658009952024) - present_state_Q (-0.018953658009952024)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.307734985415578 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03586923435970574) - present_state_Q ( -0.05064245492374368)) * f1( 0.01576993911776376)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03586923435970574) - present_state_Q (-0.05064245492374368)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.313034613308507 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015380912889572326) - present_state_Q ( -0.046331949596319054)) * f1( 0.008346098244919622)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.015380912889572326) - present_state_Q (-0.046331949596319054)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.31897866624406 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012860489230750014) - present_state_Q ( -0.012860489230750014)) * f1( 0.009312251021790176)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.012860489230750014) - present_state_Q (-0.012860489230750014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.324653395631477 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010592317713559205) - present_state_Q ( -0.010592317713559205)) * f1( 0.008887472970834498)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010592317713559205) - present_state_Q (-0.010592317713559205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.330091152073115 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012257322537320932) - present_state_Q ( -0.012257322537320932)) * f1( 0.008518337130712188)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.012257322537320932) - present_state_Q (-0.012257322537320932)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.3355583231206145 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009932956584209302) - present_state_Q ( -0.009932956584209302)) * f1( 0.008561609930590505)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.009932956584209302) - present_state_Q (-0.009932956584209302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.340827266404721 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007997600732344375) - present_state_Q ( -0.007997600732344375)) * f1( 0.008248934466092936)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007997600732344375) - present_state_Q (-0.007997600732344375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.345926824699392 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009828099886034381) - present_state_Q ( -0.009828099886034381)) * f1( 0.007985808991559677)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.009828099886034381) - present_state_Q (-0.009828099886034381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.351053542786454 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024603980006431894) - present_state_Q ( -0.024603980006431894)) * f1( 0.008045094499486479)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.024603980006431894) - present_state_Q (-0.024603980006431894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.360206750801393 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04289345528036645) - present_state_Q ( -0.03724254769099513)) * f1( 0.014388064090829973)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.04289345528036645) - present_state_Q (-0.03724254769099513)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.367493124347648 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03247613797239922) - present_state_Q ( -0.03247613797239922)) * f1( 0.011446857173791172)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03247613797239922) - present_state_Q (-0.03247613797239922)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.375420619850524 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004829886470944753) - present_state_Q ( -0.08254203453912973)) * f1( 0.012558285996377141)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.004829886470944753) - present_state_Q (-0.08254203453912973)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.38012192727329 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008743365738234546) - present_state_Q ( -0.008743365738234546)) * f1( 0.007361030522597679)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.008743365738234546) - present_state_Q (-0.008743365738234546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.384765503510237 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006026449413977533) - present_state_Q ( -0.006026449413977533)) * f1( 0.007267855865672936)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.006026449413977533) - present_state_Q (-0.006026449413977533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.389341574876613 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037188773504338215) - present_state_Q ( -0.0037188773504338215)) * f1( 0.007159873838219206)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0037188773504338215) - present_state_Q (-0.0037188773504338215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.3938636565724 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007242276157679383) - present_state_Q ( -0.007242276157679383)) * f1( 0.007078912033182801)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007242276157679383) - present_state_Q (-0.007242276157679383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.3984918262652055 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004588159659355234) - present_state_Q ( -0.004588159659355234)) * f1( 0.007242275141776619)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.004588159659355234) - present_state_Q (-0.004588159659355234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.403050312147532 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00900011818784449) - present_state_Q ( -0.00900011818784449)) * f1( 0.0071376672045130795)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.00900011818784449) - present_state_Q (-0.00900011818784449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.407770272550169 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006360164713239902) - present_state_Q ( -0.006360164713239902)) * f1( 0.007387755210111417)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.006360164713239902) - present_state_Q (-0.006360164713239902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.412410622688316 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004144416299073845) - present_state_Q ( -0.004144416299073845)) * f1( 0.007260881642429099)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.004144416299073845) - present_state_Q (-0.004144416299073845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.416985979322514 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007507572239722068) - present_state_Q ( -0.007507572239722068)) * f1( 0.007162576869799439)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007507572239722068) - present_state_Q (-0.007507572239722068)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.421662081688342 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004907154505353827) - present_state_Q ( -0.004907154505353827)) * f1( 0.00731761012822125)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.004907154505353827) - present_state_Q (-0.004907154505353827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.426260528613533 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009247791074146347) - present_state_Q ( -0.009247791074146347)) * f1( 0.007200489433006822)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.009247791074146347) - present_state_Q (-0.009247791074146347)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.431017874345088 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0066373560324676836) - present_state_Q ( -0.0066373560324676836)) * f1( 0.007446562078302089)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0066373560324676836) - present_state_Q (-0.0066373560324676836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.435690087286938 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004395558225168818) - present_state_Q ( -0.004395558225168818)) * f1( 0.007310996804338053)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.004395558225168818) - present_state_Q (-0.004395558225168818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.440291001314387 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007808837217539627) - present_state_Q ( -0.007808837217539627)) * f1( 0.0072028918979534975)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007808837217539627) - present_state_Q (-0.007808837217539627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.444995587129922 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0052042423905679985) - present_state_Q ( -0.0052042423905679985)) * f1( 0.007362491803409576)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0052042423905679985) - present_state_Q (-0.0052042423905679985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.449620875437968 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004326047172076693) - present_state_Q ( -0.004326047172076693)) * f1( 0.007237499111147302)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.004326047172076693) - present_state_Q (-0.004326047172076693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.454168318764887 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007417866968132525) - present_state_Q ( -0.007417866968132525)) * f1( 0.007118789492663998)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007417866968132525) - present_state_Q (-0.007417866968132525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.459955458067828 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011610757378410424) - present_state_Q ( -0.011610757378410424)) * f1( 0.009064824950556258)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.011610757378410424) - present_state_Q (-0.011610757378410424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.465488216253123 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012938483063678905) - present_state_Q ( -0.012938483063678905)) * f1( 0.008667991389394024)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.012938483063678905) - present_state_Q (-0.012938483063678905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.4710275977254765 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01052740442383997) - present_state_Q ( -0.01052740442383997)) * f1( 0.008675418562097949)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01052740442383997) - present_state_Q (-0.01052740442383997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.476356058117517 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008617652966799285) - present_state_Q ( -0.008617652966799285)) * f1( 0.008342841944508997)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.008617652966799285) - present_state_Q (-0.008617652966799285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.48150734956012 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010067151574897128) - present_state_Q ( -0.010067151574897128)) * f1( 0.008067093886586482)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010067151574897128) - present_state_Q (-0.010067151574897128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.486676603974166 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007833046332949551) - present_state_Q ( -0.007833046332949551)) * f1( 0.008092676263925175)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007833046332949551) - present_state_Q (-0.007833046332949551)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.490971882966376 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01078976758008974) - present_state_Q ( -0.01078976758008974)) * f1( 0.007850430836114215)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.01078976758008974) - present_state_Q (-0.01078976758008974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.495348389462344 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00837074569388569) - present_state_Q ( -0.00837074569388569)) * f1( 0.007995707819332403)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.00837074569388569) - present_state_Q (-0.00837074569388569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.499604485525878 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00634253046076235) - present_state_Q ( -0.00634253046076235)) * f1( 0.007773130344479112)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.00634253046076235) - present_state_Q (-0.00634253046076235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.503757283013999 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009001172682099838) - present_state_Q ( -0.009001172682099838)) * f1( 0.007587786670340232)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.009001172682099838) - present_state_Q (-0.009001172682099838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.50796183942859 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024071635468596097) - present_state_Q ( -0.024071635468596097)) * f1( 0.007701444057885372)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.024071635468596097) - present_state_Q (-0.024071635468596097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.515972135885544 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04422768740219882) - present_state_Q ( -0.03990605514972112)) * f1( 0.014709612798873043)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04422768740219882) - present_state_Q (-0.03990605514972112)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.522071605952573 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014278686931134559) - present_state_Q ( -0.014278686931134559)) * f1( 0.011154331079493452)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.014278686931134559) - present_state_Q (-0.014278686931134559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.5270880994409115 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015032157422011352) - present_state_Q ( -0.015032157422011352)) * f1( 0.009174988742598672)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.015032157422011352) - present_state_Q (-0.015032157422011352)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.5320766819528995 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012449667372712922) - present_state_Q ( -0.012449667372712922)) * f1( 0.009120063658831645)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.012449667372712922) - present_state_Q (-0.012449667372712922)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.536833085377798 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02843947186836762) - present_state_Q ( -0.02843947186836762)) * f1( 0.008718534478512896)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02843947186836762) - present_state_Q (-0.02843947186836762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.5442319284894985 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014977477201685231) - present_state_Q ( -0.09066561328236475)) * f1( 0.013725485229154415)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0014977477201685231) - present_state_Q (-0.09066561328236475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.547693546922841 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0073286006915349675) - present_state_Q ( -0.0073286006915349675)) * f1( 0.006323159259313196)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0073286006915349675) - present_state_Q (-0.0073286006915349675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.553121909407159 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015547021793332862) - present_state_Q ( -0.015547021793332862)) * f1( 0.009929124029496328)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.015547021793332862) - present_state_Q (-0.015547021793332862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.558258844889523 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016421305458633063) - present_state_Q ( -0.016421305458633063)) * f1( 0.009397421724461223)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.016421305458633063) - present_state_Q (-0.016421305458633063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.5633713848777955 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013741500966908895) - present_state_Q ( -0.013741500966908895)) * f1( 0.009348668253451526)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.013741500966908895) - present_state_Q (-0.013741500966908895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.568205158144966 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04835347747856274) - present_state_Q ( -0.06597245213872498)) * f1( 0.008918456181853341)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04835347747856274) - present_state_Q (-0.06597245213872498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.576066448638354 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04397368688587546) - present_state_Q ( -0.04777845238256573)) * f1( 0.0144569544257007)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04397368688587546) - present_state_Q (-0.04777845238256573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.581262291538392 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029289329169567533) - present_state_Q ( -0.04672041985077303)) * f1( 0.009555903545685732)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.029289329169567533) - present_state_Q (-0.04672041985077303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.588641855674119 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012804614413110548) - present_state_Q ( -0.06536171014592071)) * f1( 0.01362291645475552)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.012804614413110548) - present_state_Q (-0.06536171014592071)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.593062523061675 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007739601432491306) - present_state_Q ( -0.007739601432491306)) * f1( 0.00807555000229691)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.007739601432491306) - present_state_Q (-0.007739601432491306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.597375074601806 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01009157700955931) - present_state_Q ( -0.01009157700955931)) * f1( 0.007881094550691161)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.01009157700955931) - present_state_Q (-0.01009157700955931)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.601592752450121 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007522072607519473) - present_state_Q ( -0.007522072607519473)) * f1( 0.007704458919436081)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.007522072607519473) - present_state_Q (-0.007522072607519473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.605714791029865 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005432935874119295) - present_state_Q ( -0.005432935874119295)) * f1( 0.007527168766979321)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005432935874119295) - present_state_Q (-0.005432935874119295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.60975668918434 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00839615656924687) - present_state_Q ( -0.00839615656924687)) * f1( 0.007384422196129959)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.00839615656924687) - present_state_Q (-0.00839615656924687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.613869225261486 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0058451966333971815) - present_state_Q ( -0.0058451966333971815)) * f1( 0.0075103252985812665)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0058451966333971815) - present_state_Q (-0.0058451966333971815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.617897662780747 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010035549282910203) - present_state_Q ( -0.010035549282910203)) * f1( 0.007361814490581581)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.010035549282910203) - present_state_Q (-0.010035549282910203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.622051430286082 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007399055544258315) - present_state_Q ( -0.007399055544258315)) * f1( 0.0075875600357208795)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.007399055544258315) - present_state_Q (-0.007399055544258315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.625427641305135 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004397665734365086) - present_state_Q ( -0.004397665734365086)) * f1( 0.006164179801688389)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.004397665734365086) - present_state_Q (-0.004397665734365086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.630854587442035 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01868563735988595) - present_state_Q ( -0.01868563735988595)) * f1( 0.009931664866063665)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.01868563735988595) - present_state_Q (-0.01868563735988595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.6362155965418195 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015740237172642932) - present_state_Q ( -0.015740237172642932)) * f1( 0.009806238545637969)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.015740237172642932) - present_state_Q (-0.015740237172642932)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.641304210633789 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013125440616070867) - present_state_Q ( -0.013125440616070867)) * f1( 0.009303974637608703)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.013125440616070867) - present_state_Q (-0.013125440616070867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.646154007080681 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014364451296985858) - present_state_Q ( -0.014364451296985858)) * f1( 0.008869130966539915)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.014364451296985858) - present_state_Q (-0.014364451296985858)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.651001687391183 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011799172345106208) - present_state_Q ( -0.011799172345106208)) * f1( 0.00886151957828948)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.011799172345106208) - present_state_Q (-0.011799172345106208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.655653985857838 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009658660874286974) - present_state_Q ( -0.009658660874286974)) * f1( 0.008501369378322337)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.009658660874286974) - present_state_Q (-0.009658660874286974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.660110387610166 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03434635368939243) - present_state_Q ( -0.048084525174542515)) * f1( 0.008197258882789714)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.03434635368939243) - present_state_Q (-0.048084525174542515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.666958163593837 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018448015253129918) - present_state_Q ( -0.05796569514850444)) * f1( 0.01262266904723426)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.018448015253129918) - present_state_Q (-0.05796569514850444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.672344280978602 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01598390342050335) - present_state_Q ( -0.01598390342050335)) * f1( 0.009852561285447132)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.01598390342050335) - present_state_Q (-0.01598390342050335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.677453047627929 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013361048124969964) - present_state_Q ( -0.013361048124969964)) * f1( 0.00934118354760872)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.013361048124969964) - present_state_Q (-0.013361048124969964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.682289157250176 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03527022687006852) - present_state_Q ( -0.051306283523007236)) * f1( 0.008900830178377856)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.03527022687006852) - present_state_Q (-0.051306283523007236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.6890693888941835 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011719603815104007) - present_state_Q ( -0.06203129566983552)) * f1( 0.012509088994617897)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.011719603815104007) - present_state_Q (-0.06203129566983552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.693657285496639 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028205400334895162) - present_state_Q ( -0.028205400334895162)) * f1( 0.008409335081483038)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.028205400334895162) - present_state_Q (-0.028205400334895162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.701253776624491 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003301176068866279) - present_state_Q ( -0.09515820565863572)) * f1( 0.014103421322078896)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.003301176068866279) - present_state_Q (-0.09515820565863572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.706582982262982 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017590990203513627) - present_state_Q ( -0.017590990203513627)) * f1( 0.009751035327532211)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.017590990203513627) - present_state_Q (-0.017590990203513627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.7118279326272745 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014713972828018474) - present_state_Q ( -0.014713972828018474)) * f1( 0.009592325867903796)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.014713972828018474) - present_state_Q (-0.014713972828018474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.716813026940417 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012480165745343995) - present_state_Q ( -0.012480165745343995)) * f1( 0.009113732311358022)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.012480165745343995) - present_state_Q (-0.012480165745343995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.721605655668601 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012904526378695828) - present_state_Q ( -0.012904526378695828)) * f1( 0.008762479214976599)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.012904526378695828) - present_state_Q (-0.012904526378695828)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.726226698811924 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010382271755877645) - present_state_Q ( -0.010382271755877645)) * f1( 0.008445260055240668)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.010382271755877645) - present_state_Q (-0.010382271755877645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.730688214559144 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00829110662679296) - present_state_Q ( -0.00829110662679296)) * f1( 0.008150909693730498)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.00829110662679296) - present_state_Q (-0.00829110662679296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.734250417148043 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0443062421916692) - present_state_Q ( -0.06514935936204382)) * f1( 0.007903943453187854)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0443062421916692) - present_state_Q (-0.06514935936204382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.740506295167659 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027786288536907504) - present_state_Q ( -0.027786288536907504)) * f1( 0.013771645140666944)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.027786288536907504) - present_state_Q (-0.027786288536907504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.744882675047621 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014754475189356523) - present_state_Q ( -0.014754475189356523)) * f1( 0.00960932075595304)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.014754475189356523) - present_state_Q (-0.014754475189356523)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.749366166512236 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035636658626101006) - present_state_Q ( -0.035636658626101006)) * f1( 0.009885301227813238)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.035636658626101006) - present_state_Q (-0.035636658626101006)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.754969076643793 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009343172280007783) - present_state_Q ( -0.08901993887733096)) * f1( 0.01250789008005217)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.009343172280007783) - present_state_Q (-0.08901993887733096)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.75845336731876 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006734158616740798) - present_state_Q ( -0.006734158616740798)) * f1( 0.007638432973889382)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.006734158616740798) - present_state_Q (-0.006734158616740798)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.761861038571242 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005479539444261463) - present_state_Q ( -0.005479539444261463)) * f1( 0.007468615340083279)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.005479539444261463) - present_state_Q (-0.005479539444261463)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.765205998865084 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008711187249572906) - present_state_Q ( -0.008711187249572906)) * f1( 0.0073358475887568345)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.008711187249572906) - present_state_Q (-0.008711187249572906)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.7685072722721715 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005712642362466231) - present_state_Q ( -0.005712642362466231)) * f1( 0.007235755190255884)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.005712642362466231) - present_state_Q (-0.005712642362466231)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.771761710152893 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038921671347750333) - present_state_Q ( -0.0038921671347750333)) * f1( 0.0071305400801805205)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0038921671347750333) - present_state_Q (-0.0038921671347750333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.774995128370459 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009090538967089485) - present_state_Q ( -0.009090538967089485)) * f1( 0.007091755159269622)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.009090538967089485) - present_state_Q (-0.009090538967089485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.778206498833732 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005890332128804456) - present_state_Q ( -0.005890332128804456)) * f1( 0.007038951986910784)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.005890332128804456) - present_state_Q (-0.005890332128804456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.781386502743619 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003175126797898016) - present_state_Q ( -0.003175126797898016)) * f1( 0.006966468704524255)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.003175126797898016) - present_state_Q (-0.003175126797898016)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.788917687449247 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046081612280747315) - present_state_Q ( -0.04881996738953837)) * f1( 0.01664948330075769)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.046081612280747315) - present_state_Q (-0.04881996738953837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.793304213256685 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017962941025086877) - present_state_Q ( -0.017962941025086877)) * f1( 0.009637709115191821)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.017962941025086877) - present_state_Q (-0.017962941025086877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.7976470569982865 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01503951966825131) - present_state_Q ( -0.01503951966825131)) * f1( 0.009536221830995286)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01503951966825131) - present_state_Q (-0.01503951966825131)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.80178076682589 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012513493545786562) - present_state_Q ( -0.012513493545786562)) * f1( 0.009072466626656388)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.012513493545786562) - present_state_Q (-0.012513493545786562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.805732482605287 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013732013559357756) - present_state_Q ( -0.013732013559357756)) * f1( 0.00867512290939193)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013732013559357756) - present_state_Q (-0.013732013559357756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.8096489445566375 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03247199380219409) - present_state_Q ( -0.05141455605453895)) * f1( 0.008665852966268794)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.03247199380219409) - present_state_Q (-0.05141455605453895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.8156111563435955 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01144311859532184) - present_state_Q ( -0.06802785980981992)) * f1( 0.013247290919504582)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01144311859532184) - present_state_Q (-0.06802785980981992)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.819168838180461 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02625761806151692) - present_state_Q ( -0.02625761806151692)) * f1( 0.007829484053833083)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.02625761806151692) - present_state_Q (-0.02625761806151692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.825761844684964 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04758092488265481) - present_state_Q ( -0.04210476490497796)) * f1( 0.014553328604158712)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.04758092488265481) - present_state_Q (-0.04210476490497796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.830892188945818 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015944960419685253) - present_state_Q ( -0.015944960419685253)) * f1( 0.011267468931063547)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.015944960419685253) - present_state_Q (-0.015944960419685253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.835016093862808 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05067250637731737) - present_state_Q ( -0.07090916742618367)) * f1( 0.009160682222869825)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.05067250637731737) - present_state_Q (-0.07090916742618367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.841651390168105 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04553610995407654) - present_state_Q ( -0.05165226463694307)) * f1( 0.014678275699971117)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.04553610995407654) - present_state_Q (-0.05165226463694307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.8458465263945465 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013235606358991363) - present_state_Q ( -0.08397745790299291)) * f1( 0.009353845071161795)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013235606358991363) - present_state_Q (-0.08397745790299291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.849833920308767 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011212898026338866) - present_state_Q ( -0.011212898026338866)) * f1( 0.008749091855596495)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.011212898026338866) - present_state_Q (-0.011212898026338866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.853680475830598 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012086731260729632) - present_state_Q ( -0.012086731260729632)) * f1( 0.008441522631005208)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.012086731260729632) - present_state_Q (-0.012086731260729632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.857406406300434 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009501034598185788) - present_state_Q ( -0.009501034598185788)) * f1( 0.008172629118937177)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.009501034598185788) - present_state_Q (-0.009501034598185788)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.861004829872861 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026890888088536652) - present_state_Q ( -0.026890888088536652)) * f1( 0.00792013887757595)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.026890888088536652) - present_state_Q (-0.026890888088536652)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.867557652106237 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047358041435965684) - present_state_Q ( -0.042130925864737594)) * f1( 0.014464780990519316)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.047358041435965684) - present_state_Q (-0.042130925864737594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.872723210170835 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01619050403138549) - present_state_Q ( -0.01619050403138549)) * f1( 0.01134535754856141)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01619050403138549) - present_state_Q (-0.01619050403138549)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.876919275511446 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013443285568498832) - present_state_Q ( -0.013443285568498832)) * f1( 0.009211013185479362)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013443285568498832) - present_state_Q (-0.013443285568498832)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.88092255365169 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015135767921462814) - present_state_Q ( -0.015135767921462814)) * f1( 0.008790754822726583)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.015135767921462814) - present_state_Q (-0.015135767921462814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.884939991431423 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012454448889714146) - present_state_Q ( -0.012454448889714146)) * f1( 0.008817175518895094)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.012454448889714146) - present_state_Q (-0.012454448889714146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.888800156630908 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010069291315386206) - present_state_Q ( -0.010069291315386206)) * f1( 0.00846801575496073)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.010069291315386206) - present_state_Q (-0.010069291315386206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.892521223877975 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012317058064864455) - present_state_Q ( -0.012317058064864455)) * f1( 0.008166501753981105)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.012317058064864455) - present_state_Q (-0.012317058064864455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.896277582985918 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009722580509511846) - present_state_Q ( -0.009722580509511846)) * f1( 0.00823973307663131)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.009722580509511846) - present_state_Q (-0.009722580509511846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.899914918898601 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0077738039042355685) - present_state_Q ( -0.0077738039042355685)) * f1( 0.007975582240685185)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0077738039042355685) - present_state_Q (-0.0077738039042355685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.903453490614107 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009717173248876942) - present_state_Q ( -0.009717173248876942)) * f1( 0.00776199890522788)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.009717173248876942) - present_state_Q (-0.009717173248876942)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.907017754623321 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0072805945533475375) - present_state_Q ( -0.0072805945533475375)) * f1( 0.007814596957265494)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0072805945533475375) - present_state_Q (-0.0072805945533475375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.910490061247882 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010770230880356318) - present_state_Q ( -0.010770230880356318)) * f1( 0.007618227549421395)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.010770230880356318) - present_state_Q (-0.010770230880356318)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.914548993921845 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012496540834250202) - present_state_Q ( -0.012496540834250202)) * f1( 0.00890831951173445)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.012496540834250202) - present_state_Q (-0.012496540834250202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.918360416049256 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010145223132491926) - present_state_Q ( -0.010145223132491926)) * f1( 0.008361213785699494)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.010145223132491926) - present_state_Q (-0.010145223132491926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.922043588927709 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008280556630193225) - present_state_Q ( -0.008280556630193225)) * f1( 0.008076896624788471)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.008280556630193225) - present_state_Q (-0.008280556630193225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.925642960001668 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011114523077324408) - present_state_Q ( -0.011114523077324408)) * f1( 0.007897543374081365)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.011114523077324408) - present_state_Q (-0.011114523077324408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.929638190656569 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01175546670916917) - present_state_Q ( -0.01175546670916917)) * f1( 0.008767226500573223)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01175546670916917) - present_state_Q (-0.01175546670916917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.933423268057506 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010344509831039569) - present_state_Q ( -0.010344509831039569)) * f1( 0.008303747409451553)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.010344509831039569) - present_state_Q (-0.010344509831039569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.9370862859390545 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00813226989661126) - present_state_Q ( -0.00813226989661126)) * f1( 0.008032463286267553)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00813226989661126) - present_state_Q (-0.00813226989661126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.939951220855716 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007820924833617342) - present_state_Q ( -0.007820924833617342)) * f1( 0.006281997887967384)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.007820924833617342) - present_state_Q (-0.007820924833617342)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.9429573704956 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003756274369629769) - present_state_Q ( -0.003756274369629769)) * f1( 0.006586358962776667)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.003756274369629769) - present_state_Q (-0.003756274369629769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.945964403412827 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022256285945813315) - present_state_Q ( -0.0022256285945813315)) * f1( 0.006586306292429617)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0022256285945813315) - present_state_Q (-0.0022256285945813315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.948965425057593 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007908219397853805) - present_state_Q ( -0.007908219397853805)) * f1( 0.006580511237068971)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.007908219397853805) - present_state_Q (-0.007908219397853805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.951974330451951 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00385474066814218) - present_state_Q ( -0.00385474066814218)) * f1( 0.006592524717479472)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00385474066814218) - present_state_Q (-0.00385474066814218)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.954983632886536 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0021657884692046856) - present_state_Q ( -0.0021657884692046856)) * f1( 0.006591199467378993)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0021657884692046856) - present_state_Q (-0.0021657884692046856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.957989264040407 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008051100975422349) - present_state_Q ( -0.008051100975422349)) * f1( 0.006590804614087306)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.008051100975422349) - present_state_Q (-0.008051100975422349)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.961002561291041 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004020906675074326) - present_state_Q ( -0.004020906675074326)) * f1( 0.006602363628841509)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.004020906675074326) - present_state_Q (-0.004020906675074326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.964015496954797 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020655833175565867) - present_state_Q ( -0.0020655833175565867)) * f1( 0.006599026887878362)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0020655833175565867) - present_state_Q (-0.0020655833175565867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.967028916148449 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00829029303713183) - present_state_Q ( -0.00829029303713183)) * f1( 0.0066081943171328)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00829029303713183) - present_state_Q (-0.00829029303713183)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.97004970933657 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004305798204852136) - present_state_Q ( -0.004305798204852136)) * f1( 0.00661915966090427)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.004305798204852136) - present_state_Q (-0.004305798204852136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.973068908893902 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019220001581354775) - present_state_Q ( -0.0019220001581354775)) * f1( 0.006612559108697736)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0019220001581354775) - present_state_Q (-0.0019220001581354775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.975838712818357 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0027699264752663755) - present_state_Q ( -0.0027699264752663755)) * f1( 0.006067354466285666)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0027699264752663755) - present_state_Q (-0.0027699264752663755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.980298720749914 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01858618537188231) - present_state_Q ( -0.01858618537188231)) * f1( 0.009800365720840441)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01858618537188231) - present_state_Q (-0.01858618537188231)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.984676528447501 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0155636130520205) - present_state_Q ( -0.0155636130520205)) * f1( 0.009613993106000277)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0155636130520205) - present_state_Q (-0.0155636130520205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.988833303672126 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016766716800177887) - present_state_Q ( -0.016766716800177887)) * f1( 0.009130760464242459)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.016766716800177887) - present_state_Q (-0.016766716800177887)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.992982292075679 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013924543835453997) - present_state_Q ( -0.013924543835453997)) * f1( 0.009108538051968115)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013924543835453997) - present_state_Q (-0.013924543835453997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.996953325376883 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01140868905690765) - present_state_Q ( -0.01140868905690765)) * f1( 0.008713530496703153)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01140868905690765) - present_state_Q (-0.01140868905690765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.001606747548256 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011714020863706785) - present_state_Q ( -0.011714020863706785)) * f1( 0.010211493592770267)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.011714020863706785) - present_state_Q (-0.011714020863706785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.005402593013922 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00956352798646822) - present_state_Q ( -0.00956352798646822)) * f1( 0.008326086638810749)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00956352798646822) - present_state_Q (-0.00956352798646822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.00907258459078 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011248244079218312) - present_state_Q ( -0.011248244079218312)) * f1( 0.00805270766569432)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.011248244079218312) - present_state_Q (-0.011248244079218312)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.009974337766645 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02209121132904878) - present_state_Q ( -0.02209121132904878)) * f1( 0.0019828756371831563)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.02209121132904878) - present_state_Q (-0.02209121132904878)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0129518407193965 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017492449264932641) - present_state_Q ( -0.0017492449264932641)) * f1( 0.006521014414517819)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0017492449264932641) - present_state_Q (-0.0017492449264932641)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.015340089183961 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008490961149713051) - present_state_Q ( -0.008490961149713051)) * f1( 0.006549557643031929)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.008490961149713051) - present_state_Q (-0.008490961149713051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0177383680561265 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004408890296752631) - present_state_Q ( -0.004408890296752631)) * f1( 0.006570445255107748)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004408890296752631) - present_state_Q (-0.004408890296752631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.020138600179159 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017047706191576789) - present_state_Q ( -0.0017047706191576789)) * f1( 0.006571414983170057)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0017047706191576789) - present_state_Q (-0.0017047706191576789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.022549493333379 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008938143590106123) - present_state_Q ( -0.008938143590106123)) * f1( 0.0066123885013844134)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.008938143590106123) - present_state_Q (-0.008938143590106123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.024968528371107 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004994574857401961) - present_state_Q ( -0.004994574857401961)) * f1( 0.0066282670859582615)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004994574857401961) - present_state_Q (-0.004994574857401961)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.027386401549912 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017031872466389101) - present_state_Q ( -0.0017031872466389101)) * f1( 0.0066197105201251305)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0017031872466389101) - present_state_Q (-0.0017031872466389101)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.029825231963561 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00973443676005771) - present_state_Q ( -0.00973443676005771)) * f1( 0.006690327444911492)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00973443676005771) - present_state_Q (-0.00973443676005771)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.032271199583515 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005996439961337516) - present_state_Q ( -0.005996439961337516)) * f1( 0.006703719846350727)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005996439961337516) - present_state_Q (-0.005996439961337516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0347125411318885 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022219048855607504) - present_state_Q ( -0.0022219048855607504)) * f1( 0.006684817193163012)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0022219048855607504) - present_state_Q (-0.0022219048855607504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.037150377257398 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007550497092142214) - present_state_Q ( -0.007550497092142214)) * f1( 0.006683995859011265)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.007550497092142214) - present_state_Q (-0.007550497092142214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.039668908687651 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0039694924876283125) - present_state_Q ( -0.0039694924876283125)) * f1( 0.006899147737925354)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0039694924876283125) - present_state_Q (-0.0039694924876283125)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.042166345088179 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009667639115900824) - present_state_Q ( -0.009667639115900824)) * f1( 0.0068509854750643554)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009667639115900824) - present_state_Q (-0.009667639115900824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.044769512108379 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006390517898289919) - present_state_Q ( -0.006390517898289919)) * f1( 0.0071352534509396895)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.006390517898289919) - present_state_Q (-0.006390517898289919)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.047342320931994 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037131112518568304) - present_state_Q ( -0.0037131112518568304)) * f1( 0.007047387246020657)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0037131112518568304) - present_state_Q (-0.0037131112518568304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.049891112470002 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0077443809759574) - present_state_Q ( -0.0077443809759574)) * f1( 0.006988544882170348)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0077443809759574) - present_state_Q (-0.0077443809759574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.052499042362039 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004646681906710433) - present_state_Q ( -0.004646681906710433)) * f1( 0.007145234658030431)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004646681906710433) - present_state_Q (-0.004646681906710433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0550721341322316 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009636271679701271) - present_state_Q ( -0.009636271679701271)) * f1( 0.007058469165974698)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009636271679701271) - present_state_Q (-0.009636271679701271)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.057240633425583 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009133454413152907) - present_state_Q ( -0.009133454413152907)) * f1( 0.0059478584114527645)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009133454413152907) - present_state_Q (-0.009133454413152907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.060235108031965 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012812535456724597) - present_state_Q ( -0.012812535456724597)) * f1( 0.008220847240059427)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.012812535456724597) - present_state_Q (-0.012812535456724597)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.063252768939465 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010136786516662763) - present_state_Q ( -0.010136786516662763)) * f1( 0.008279028016148148)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.010136786516662763) - present_state_Q (-0.010136786516662763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.066173045106719 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008196514983960585) - present_state_Q ( -0.008196514983960585)) * f1( 0.008008014019263179)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.008196514983960585) - present_state_Q (-0.008196514983960585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.069031762626938 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011414131503437898) - present_state_Q ( -0.011414131503437898)) * f1( 0.007845437305358968)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.011414131503437898) - present_state_Q (-0.011414131503437898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.071832498646883 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00857294577451048) - present_state_Q ( -0.00857294577451048)) * f1( 0.00768092326706341)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00857294577451048) - present_state_Q (-0.00857294577451048)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0745726665411155 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006030389781560863) - present_state_Q ( -0.006030389781560863)) * f1( 0.007510104212057893)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.006030389781560863) - present_state_Q (-0.006030389781560863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.077257725410435 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009852423701264119) - present_state_Q ( -0.009852423701264119)) * f1( 0.007366008825580655)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009852423701264119) - present_state_Q (-0.009852423701264119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.079316277650781 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006947893289238014) - present_state_Q ( -0.006947893289238014)) * f1( 0.007528629870078696)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.006947893289238014) - present_state_Q (-0.006947893289238014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.084460472679967 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03185510390171951) - present_state_Q ( -0.12756325082270842)) * f1( 0.019663046208765796)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03185510390171951) - present_state_Q (-0.12756325082270842)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.089742090002706 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049560803680232975) - present_state_Q ( -0.11665430705218222)) * f1( 0.02009095330364271)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.049560803680232975) - present_state_Q (-0.11665430705218222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.095748593001181 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0288047782942922) - present_state_Q ( -0.0742660609244032)) * f1( 0.02250329368751161)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0288047782942922) - present_state_Q (-0.0742660609244032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.09728173860889 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03501842026374494) - present_state_Q ( -0.03501842026374494)) * f1( 0.005659378763193262)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03501842026374494) - present_state_Q (-0.03501842026374494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.10073795064017 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01160252794702531) - present_state_Q ( -0.07999085715666905)) * f1( 0.012984876521516437)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01160252794702531) - present_state_Q (-0.07999085715666905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.102619341269969 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008277791092579102) - present_state_Q ( -0.008277791092579102)) * f1( 0.006883719727027866)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.008277791092579102) - present_state_Q (-0.008277791092579102)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.10449363958245 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004529329006275098) - present_state_Q ( -0.004529329006275098)) * f1( 0.006849315543748655)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.004529329006275098) - present_state_Q (-0.004529329006275098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.105735389058773 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029757077310633997) - present_state_Q ( -0.0029757077310633997)) * f1( 0.006806507242248348)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0029757077310633997) - present_state_Q (-0.0029757077310633997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.125091040462319 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03610300116547594) - present_state_Q ( -0.03610300116547594)) * f1( 0.007071067811865476)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03610300116547594) - present_state_Q (-0.03610300116547594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.144437792672608 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03604720088343909) - present_state_Q ( -0.048682780934736755)) * f1( 0.007071067811865476)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03604720088343909) - present_state_Q (-0.048682780934736755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.170386663079155 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015399627438088443) - present_state_Q ( -0.08932395518116648)) * f1( 0.009498910468202186)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.015399627438088443) - present_state_Q (-0.08932395518116648)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.194155673860785 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011733113316808241) - present_state_Q ( -0.011733113316808241)) * f1( 0.0086764181602848)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.011733113316808241) - present_state_Q (-0.011733113316808241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.216955997674937 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04800272058039856) - present_state_Q ( -0.07392241490657737)) * f1( 0.008340647543038803)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.04800272058039856) - present_state_Q (-0.07392241490657737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.255888969684317 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04314608387030467) - present_state_Q ( -0.05359404765784315)) * f1( 0.014231844316601176)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.04314608387030467) - present_state_Q (-0.05359404765784315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.283311741722494 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040636629855787414) - present_state_Q ( -0.05324852768134659)) * f1( 0.010024286690556368)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.040636629855787414) - present_state_Q (-0.05324852768134659)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.31685248312279 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009973777360725028) - present_state_Q ( -0.07104090790068276)) * f1( 0.012270042044976129)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.009973777360725028) - present_state_Q (-0.07104090790068276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.337726402624641 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.065332533911756) - present_state_Q ( -0.00989034522452574)) * f1( 0.007880327971016721)
w2 ( -5.17177157060204 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.065332533911756) - present_state_Q (-0.00989034522452574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.366158183609349 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0955718544331075) - present_state_Q ( -2.118241126784615)) * f1( 0.011565478274829722)
w2 ( -6.155104220029411 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -2.0955718544331075) - present_state_Q (-2.118241126784615)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -5.378293735123201 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2751370046017254) - present_state_Q ( -2.5061578486076077)) * f1( 0.005032709465790891)
w2 ( -6.155104220029411 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -1.2751370046017254) - present_state_Q (-2.5061578486076077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.399277780570227 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06012885269716925) - present_state_Q ( -0.06012885269716925)) * f1( 0.008221181539260915)
w2 ( -7.176078898513991 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.06012885269716925) - present_state_Q (-0.06012885269716925)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -5.4359970341822565 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4984594889373475) - present_state_Q ( -2.9336752686401457)) * f1( 0.016108713139755884)
w2 ( -7.631971970709902 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -1.4984594889373475) - present_state_Q (-2.9336752686401457)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.458573800898088 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5979022603242476) - present_state_Q ( -1.5979022603242476)) * f1( 0.009352286596505072)
w2 ( -8.597586806519907 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -1.5979022603242476) - present_state_Q (-1.5979022603242476)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -5.509702054967745 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.473624921887509) - present_state_Q ( -3.5611894347770248)) * f1( 0.024894924706644866)
w2 ( -9.419091666199362 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -3.473624921887509) - present_state_Q (-3.5611894347770248)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -5.5429190496256435 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.9135895190805017) - present_state_Q ( -1.9898426238695164)) * f1( 0.01815245852566347)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -1.9135895190805017) - present_state_Q (-1.9898426238695164)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.580523697404215 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04247082640876402) - present_state_Q ( -0.11819321359024275)) * f1( 0.021808795486535196)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.04247082640876402) - present_state_Q (-0.11819321359024275)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.62264999888114 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038412591227943216) - present_state_Q ( -0.10575576974530987)) * f1( 0.0244140906472446)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.038412591227943216) - present_state_Q (-0.10575576974530987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.636411618471605 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011612012872211497) - present_state_Q ( -0.011612012872211497)) * f1( 0.007933425912082088)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.011612012872211497) - present_state_Q (-0.011612012872211497)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.6501573556030955 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013315010764398325) - present_state_Q ( -0.06910172853775232)) * f1( 0.007950541636860636)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.013315010764398325) - present_state_Q (-0.06910172853775232)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.672782436817372 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03456588287628854) - present_state_Q ( -0.06373564264528438)) * f1( 0.013080691226288847)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03456588287628854) - present_state_Q (-0.06373564264528438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.683400025037094 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014025285409225749) - present_state_Q ( -0.0014025285409225749)) * f1( 0.006117684995447174)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.0014025285409225749) - present_state_Q (-0.0014025285409225749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.700444570719029 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018002343943121956) - present_state_Q ( -0.07790166259816647)) * f1( 0.009863327160962327)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.018002343943121956) - present_state_Q (-0.07790166259816647)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.715068759657243 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017047981072298716) - present_state_Q ( -0.0717024423675447)) * f1( 0.008931722680387989)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.017047981072298716) - present_state_Q (-0.0717024423675447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.7298166415503955 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021657511988120978) - present_state_Q ( -0.0713402906510741)) * f1( 0.009006815363865044)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.021657511988120978) - present_state_Q (-0.0713402906510741)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.745704893427134 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013589980377720778) - present_state_Q ( -0.07866773506065025)) * f1( 0.009708083960159888)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.013589980377720778) - present_state_Q (-0.07866773506065025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.7592680045831655 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010793349946866053) - present_state_Q ( -0.010793349946866053)) * f1( 0.008253282357123979)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.010793349946866053) - present_state_Q (-0.010793349946866053)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.772347151678545 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02123102418041369) - present_state_Q ( -0.06705949957586677)) * f1( 0.007985619970226869)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.02123102418041369) - present_state_Q (-0.06705949957586677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.787403717069998 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021869233579953743) - present_state_Q ( -0.07389197224554934)) * f1( 0.009740250694155527)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.021869233579953743) - present_state_Q (-0.07389197224554934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.802397245414092 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018358483696495863) - present_state_Q ( -0.018358483696495863)) * f1( 0.009664969275567748)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.018358483696495863) - present_state_Q (-0.018358483696495863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.81657883287445 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022156774689450764) - present_state_Q ( -0.07328026158197766)) * f1( 0.009173838163611066)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.022156774689450764) - present_state_Q (-0.07328026158197766)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.831591116223427 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014772045892517353) - present_state_Q ( -0.07924305133340105)) * f1( 0.00971541338800245)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.014772045892517353) - present_state_Q (-0.07924305133340105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.844666095512948 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011881691063687987) - present_state_Q ( -0.011881691063687987)) * f1( 0.008425088796829776)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011881691063687987) - present_state_Q (-0.011881691063687987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.857238878693806 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02511491350453151) - present_state_Q ( -0.06687902106674694)) * f1( 0.008129606932931443)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.02511491350453151) - present_state_Q (-0.06687902106674694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.881525522508303 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04908314041949688) - present_state_Q ( -0.0693288890097925)) * f1( 0.01570388531333896)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.04908314041949688) - present_state_Q (-0.0693288890097925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.894460906542792 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023636144708907552) - present_state_Q ( -0.0959737188798976)) * f1( 0.008379910984686127)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.023636144708907552) - present_state_Q (-0.0959737188798976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.90964197403676 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018293479595254982) - present_state_Q ( -0.0787233652200745)) * f1( 0.009824090164148314)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.018293479595254982) - present_state_Q (-0.0787233652200745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.923560051465362 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015086520711433276) - present_state_Q ( -0.015086520711433276)) * f1( 0.008970020797528842)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.015086520711433276) - present_state_Q (-0.015086520711433276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.942281919132998 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047771534855022384) - present_state_Q ( -0.08015255468300522)) * f1( 0.012114249726274087)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.047771534855022384) - present_state_Q (-0.08015255468300522)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.9708655408498 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05066562311906663) - present_state_Q ( -0.13000126572556717)) * f1( 0.019724621214290854)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.05066562311906663) - present_state_Q (-0.13000126572556717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.996479502775869 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03112372348596243) - present_state_Q ( -0.11500959699712093)) * f1( 0.017659468491471483)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.03112372348596243) - present_state_Q (-0.11500959699712093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.008726369762687 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011958110544634241) - present_state_Q ( -0.011958110544634241)) * f1( 0.00838509844412479)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.011958110544634241) - present_state_Q (-0.011958110544634241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.020461002062105 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020701776337045567) - present_state_Q ( -0.06971400777600696)) * f1( 0.008065797660486632)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.020701776337045567) - present_state_Q (-0.06971400777600696)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.034268994368543 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019185061103478507) - present_state_Q ( -0.019185061103478507)) * f1( 0.009458170704342145)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.019185061103478507) - present_state_Q (-0.019185061103478507)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.04744159376929 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015838208250188945) - present_state_Q ( -0.015838208250188945)) * f1( 0.009021079263246277)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.015838208250188945) - present_state_Q (-0.015838208250188945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.060128240802788 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0250232993065012) - present_state_Q ( -0.07029050355047557)) * f1( 0.008720251355335738)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0250232993065012) - present_state_Q (-0.07029050355047557)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.074834661057157 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012357320634908561) - present_state_Q ( -0.012357320634908561)) * f1( 0.010069335715753478)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.012357320634908561) - present_state_Q (-0.012357320634908561)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.086474197823627 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009655820577923312) - present_state_Q ( -0.009655820577923312)) * f1( 0.007968145447582783)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.009655820577923312) - present_state_Q (-0.009655820577923312)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.097759344590585 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.045367026848303516) - present_state_Q ( -0.060176915529777164)) * f1( 0.007750449199559206)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.045367026848303516) - present_state_Q (-0.060176915529777164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.115986374881705 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011624732460289961) - present_state_Q ( -0.11462138014019772)) * f1( 0.01256792402793554)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.011624732460289961) - present_state_Q (-0.11462138014019772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.12701494017227 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01684496571101457) - present_state_Q ( -0.07205685456295731)) * f1( 0.0075819045640042266)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.01684496571101457) - present_state_Q (-0.07205685456295731)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.139675166622635 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014697135820486951) - present_state_Q ( -0.014697135820486951)) * f1( 0.008669577737726822)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.014697135820486951) - present_state_Q (-0.014697135820486951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.151857738031452 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01187125653837147) - present_state_Q ( -0.01187125653837147)) * f1( 0.008341032360461418)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.01187125653837147) - present_state_Q (-0.01187125653837147)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.163584004258549 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0213069972344283) - present_state_Q ( -0.0727681776520907)) * f1( 0.008061706085822436)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0213069972344283) - present_state_Q (-0.0727681776520907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.177468999432008 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06622031113548375) - present_state_Q ( -0.07586882919191462)) * f1( 0.00954490089980238)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.06622031113548375) - present_state_Q (-0.07586882919191462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.200306045353703 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0359484987758678) - present_state_Q ( -0.11157197840790094)) * f1( 0.015740677930187245)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0359484987758678) - present_state_Q (-0.11157197840790094)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.21243286870244 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020900840244787482) - present_state_Q ( -0.06764403259682132)) * f1( 0.008334172929309793)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.020900840244787482) - present_state_Q (-0.06764403259682132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.22567973577125 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019517727364863582) - present_state_Q ( -0.07850280717506841)) * f1( 0.009110809961582676)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.019517727364863582) - present_state_Q (-0.07850280717506841)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.239011277115482 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016594756167014422) - present_state_Q ( -0.016594756167014422)) * f1( 0.009130354285367091)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.016594756167014422) - present_state_Q (-0.016594756167014422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.2518200585170955 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017043013950211242) - present_state_Q ( -0.017043013950211242)) * f1( 0.008772574790761725)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.017043013950211242) - present_state_Q (-0.017043013950211242)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.259481539880277 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018009547047926375) - present_state_Q ( -0.07049518520977316)) * f1( 0.006022918056970431)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.018009547047926375) - present_state_Q (-0.07049518520977316)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.269475611425413 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023221669598146977) - present_state_Q ( -0.0721738316703825)) * f1( 0.007857351415623168)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.023221669598146977) - present_state_Q (-0.0721738316703825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.281849960167951 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022964484151920095) - present_state_Q ( -0.08076562847419473)) * f1( 0.009735324074227062)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.022964484151920095) - present_state_Q (-0.08076562847419473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.294016992354333 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.044248892795017716) - present_state_Q ( -0.044248892795017716)) * f1( 0.009543206364806057)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.044248892795017716) - present_state_Q (-0.044248892795017716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.3104551413262655 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0045936035004749244) - present_state_Q ( -0.12130951545651009)) * f1( 0.012975719308095558)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.0045936035004749244) - present_state_Q (-0.12130951545651009)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.31934386852908 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022039918092816472) - present_state_Q ( -0.06984381051405533)) * f1( 0.006987113297611447)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.022039918092816472) - present_state_Q (-0.06984381051405533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.331100747610619 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01690600328393146) - present_state_Q ( -0.08260231869637824)) * f1( 0.009251317298299682)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.01690600328393146) - present_state_Q (-0.08260231869637824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.3421788194983755 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014132091756955689) - present_state_Q ( -0.014132091756955689)) * f1( 0.00867064713148082)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.014132091756955689) - present_state_Q (-0.014132091756955689)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.352919125769267 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016296334528142983) - present_state_Q ( -0.016296334528142983)) * f1( 0.008407564611399009)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.016296334528142983) - present_state_Q (-0.016296334528142983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.363338598843174 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012922496119015359) - present_state_Q ( -0.012922496119015359)) * f1( 0.008154476526108411)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.012922496119015359) - present_state_Q (-0.012922496119015359)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.373447447309054 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009891592852019036) - present_state_Q ( -0.009891592852019036)) * f1( 0.00790968724788832)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.009891592852019036) - present_state_Q (-0.009891592852019036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.383239339156562 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023101866698857026) - present_state_Q ( -0.07303680564232992)) * f1( 0.007698927088756979)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.023101866698857026) - present_state_Q (-0.07303680564232992)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.395461306969183 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02221508856652161) - present_state_Q ( -0.0822681534351066)) * f1( 0.009616634244367563)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.02221508856652161) - present_state_Q (-0.0822681534351066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.407448032227329 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018574386012253134) - present_state_Q ( -0.018574386012253134)) * f1( 0.00938477373197036)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.018574386012253134) - present_state_Q (-0.018574386012253134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.41880868837012 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023398884512210828) - present_state_Q ( -0.08004375293715967)) * f1( 0.008937279265520863)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.023398884512210828) - present_state_Q (-0.08004375293715967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.430987207844535 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01440285248533863) - present_state_Q ( -0.08785581307980896)) * f1( 0.009587252108209997)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.01440285248533863) - present_state_Q (-0.08785581307980896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.440672193862453 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011417935539288264) - present_state_Q ( -0.011417935539288264)) * f1( 0.008162343264623487)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.011417935539288264) - present_state_Q (-0.011417935539288264)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.450009671491835 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022780330658016584) - present_state_Q ( -0.07515986719279075)) * f1( 0.007911210983130622)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.022780330658016584) - present_state_Q (-0.07515986719279075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.461313870245755 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023744190811544327) - present_state_Q ( -0.08203237844207248)) * f1( 0.009583024125795862)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.023744190811544327) - present_state_Q (-0.08203237844207248)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.472678555741182 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01993437644599299) - present_state_Q ( -0.01993437644599299)) * f1( 0.009584156860116329)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.01993437644599299) - present_state_Q (-0.01993437644599299)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.482577887270003 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06333483652616644) - present_state_Q ( -0.09683887542163308)) * f1( 0.009105595345646878)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06333483652616644) - present_state_Q (-0.09683887542163308)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.498880321824734 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055264080272664814) - present_state_Q ( -0.07123978583661357)) * f1( 0.0149611748193085)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.055264080272664814) - present_state_Q (-0.07123978583661357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.508985771657313 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.063591219499198) - present_state_Q ( -0.034688440702588294)) * f1( 0.009242329350198667)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.063591219499198) - present_state_Q (-0.034688440702588294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.525287805709429 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055489207909847155) - present_state_Q ( -0.0715299932295466)) * f1( 0.0149611748193085)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.055489207909847155) - present_state_Q (-0.0715299932295466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.535354177120231 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023687471221549437) - present_state_Q ( -0.07298007108798997)) * f1( 0.009242329350198667)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.023687471221549437) - present_state_Q (-0.07298007108798997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5440830024113925 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012800218904639976) - present_state_Q ( -0.012800218904639976)) * f1( 0.007971030011424456)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.012800218904639976) - present_state_Q (-0.012800218904639976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.55272268302359 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013575401022006646) - present_state_Q ( -0.0814374376105596)) * f1( 0.007939330752893835)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.013575401022006646) - present_state_Q (-0.0814374376105596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.561641641175731 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014161487262971935) - present_state_Q ( -0.014161487262971935)) * f1( 0.0081455677479813)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.014161487262971935) - present_state_Q (-0.014161487262971935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.570432346081448 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010773337876648983) - present_state_Q ( -0.010773337876648983)) * f1( 0.008026200506198915)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.010773337876648983) - present_state_Q (-0.010773337876648983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.578957736286659 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05073044764537499) - present_state_Q ( -0.06277891603234519)) * f1( 0.007818230406117338)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.05073044764537499) - present_state_Q (-0.06277891603234519)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.592380651456566 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020938690453815714) - present_state_Q ( -0.08243053162712051)) * f1( 0.012335117760959602)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.020938690453815714) - present_state_Q (-0.08243053162712051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.602386547098062 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022467660359767105) - present_state_Q ( -0.08263672633719503)) * f1( 0.009195059641931288)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.022467660359767105) - present_state_Q (-0.08263672633719503)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.612592050061829 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041060349769493625) - present_state_Q ( -0.07618546235141764)) * f1( 0.009371334655177468)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.041060349769493625) - present_state_Q (-0.07618546235141764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.627749520625613 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03907702555930036) - present_state_Q ( -0.03907702555930036)) * f1( 0.013871527722084182)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03907702555930036) - present_state_Q (-0.03907702555930036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6438191189853875 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04319336949450827) - present_state_Q ( -0.04319336949450827)) * f1( 0.014711259330897172)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04319336949450827) - present_state_Q (-0.04319336949450827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.653880598499945 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014640656453808952) - present_state_Q ( -0.0888768294539923)) * f1( 0.009252110268909933)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.014640656453808952) - present_state_Q (-0.0888768294539923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.662879809928935 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01206924050403538) - present_state_Q ( -0.01206924050403538)) * f1( 0.00821744882829231)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.01206924050403538) - present_state_Q (-0.01206924050403538)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.671654966684855 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015427683963344362) - present_state_Q ( -0.015427683963344362)) * f1( 0.008015069991957726)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.015427683963344362) - present_state_Q (-0.015427683963344362)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.680220825044178 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011774475210415861) - present_state_Q ( -0.011774475210415861)) * f1( 0.007821551721252599)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.011774475210415861) - present_state_Q (-0.011774475210415861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.690600334917021 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007961151838692287) - present_state_Q ( -0.09577228335720617)) * f1( 0.009551200358499674)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.007961151838692287) - present_state_Q (-0.09577228335720617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.698256176688899 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015838865847988055) - present_state_Q ( -0.07921022188895142)) * f1( 0.007033658005722372)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.015838865847988055) - present_state_Q (-0.07921022188895142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.70702932689331 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011243571590866308) - present_state_Q ( -0.011243571590866308)) * f1( 0.008010482019839886)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.011243571590866308) - present_state_Q (-0.011243571590866308)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.71550183065521 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02545810784764384) - present_state_Q ( -0.07661459913920338)) * f1( 0.00778140727157141)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.02545810784764384) - present_state_Q (-0.07661459913920338)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.726161498489637 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024211596266480808) - present_state_Q ( -0.08721682104023773)) * f1( 0.009799820501597379)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.024211596266480808) - present_state_Q (-0.08721682104023773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.736531573152691 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020224597867794594) - present_state_Q ( -0.020224597867794594)) * f1( 0.009475575735304926)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.020224597867794594) - present_state_Q (-0.020224597867794594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.746339944675734 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02608831034607697) - present_state_Q ( -0.08382760151765331)) * f1( 0.009014228619102128)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.02608831034607697) - present_state_Q (-0.08382760151765331)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.756976952682603 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01630583368582011) - present_state_Q ( -0.09297799853067899)) * f1( 0.009784882192068274)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.01630583368582011) - present_state_Q (-0.09297799853067899)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.766036442654393 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012921719728377938) - present_state_Q ( -0.012921719728377938)) * f1( 0.008273070562972348)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.012921719728377938) - present_state_Q (-0.012921719728377938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.774049345335436 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02027188652424903) - present_state_Q ( -0.02027188652424903)) * f1( 0.007321756491790418)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.02027188652424903) - present_state_Q (-0.02027188652424903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.782036675870549 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05408534765159222) - present_state_Q ( -0.087057239708163)) * f1( 0.007340919776982825)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.05408534765159222) - present_state_Q (-0.087057239708163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.796009248255917 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0350397942634801) - present_state_Q ( -0.09345091237978775)) * f1( 0.01285158038715942)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.0350397942634801) - present_state_Q (-0.09345091237978775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.807703169881808 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04665577922996407) - present_state_Q ( -0.04665577922996407)) * f1( 0.01070850686739732)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04665577922996407) - present_state_Q (-0.04665577922996407)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.822671731118134 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03069226640832666) - present_state_Q ( -0.08941398822922182)) * f1( 0.01376310305312504)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03069226640832666) - present_state_Q (-0.08941398822922182)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8336400474566705 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015314400195101677) - present_state_Q ( -0.08678380914077506)) * f1( 0.010083995696597528)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.015314400195101677) - present_state_Q (-0.08678380914077506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.842782197344723 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01721444206063527) - present_state_Q ( -0.01721444206063527)) * f1( 0.008351501566113447)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.01721444206063527) - present_state_Q (-0.01721444206063527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.851873266686209 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017639786235887713) - present_state_Q ( -0.08477836272164316)) * f1( 0.008356382396825266)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.017639786235887713) - present_state_Q (-0.08477836272164316)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.86142463235788 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019042920463174986) - present_state_Q ( -0.019042920463174986)) * f1( 0.008726638677580228)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.019042920463174986) - present_state_Q (-0.019042920463174986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.870602710502659 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049051970446506304) - present_state_Q ( -0.07086864555557106)) * f1( 0.008423158060706126)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.049051970446506304) - present_state_Q (-0.07086864555557106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.884523806044673 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014120754586305552) - present_state_Q ( -0.13145493241617912)) * f1( 0.012851629089984296)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.014120754586305552) - present_state_Q (-0.13145493241617912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.891913974369391 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04924915448671326) - present_state_Q ( -0.06498175634053653)) * f1( 0.0073985783764845235)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.04924915448671326) - present_state_Q (-0.06498175634053653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.904822868258489 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021170989484538612) - present_state_Q ( -0.09072867616642152)) * f1( 0.012960635148677457)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.021170989484538612) - present_state_Q (-0.09072867616642152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.913680965306545 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04647753073782806) - present_state_Q ( -0.07526013157616401)) * f1( 0.008877559015459061)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.04647753073782806) - present_state_Q (-0.07526013157616401)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.926852617613907 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003040714910242378) - present_state_Q ( -0.13524001760050694)) * f1( 0.013286241119855822)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.003040714910242378) - present_state_Q (-0.13524001760050694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.933567434524438 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024504451937801416) - present_state_Q ( -0.07550760917939645)) * f1( 0.006731219169576523)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.024504451937801416) - present_state_Q (-0.07550760917939645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.94271424044853 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01660926214782811) - present_state_Q ( -0.09145602360985121)) * f1( 0.00918455939999584)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.01660926214782811) - present_state_Q (-0.09145602360985121)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.95116044400096 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013888002757771854) - present_state_Q ( -0.013888002757771854)) * f1( 0.00841574652826209)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.013888002757771854) - present_state_Q (-0.013888002757771854)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.959359733341033 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016447365320397617) - present_state_Q ( -0.016447365320397617)) * f1( 0.008171598168927955)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.016447365320397617) - present_state_Q (-0.016447365320397617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.967297052566709 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04912202537490253) - present_state_Q ( -0.06922810626368665)) * f1( 0.007949741636168815)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.04912202537490253) - present_state_Q (-0.06922810626368665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.980157625812161 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011462419154317591) - present_state_Q ( -0.1336609623187778)) * f1( 0.012969288302159892)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011462419154317591) - present_state_Q (-0.1336609623187778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.989348286994728 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015695695329416153) - present_state_Q ( -0.09302836750253052)) * f1( 0.00923013762801343)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.015695695329416153) - present_state_Q (-0.09302836750253052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.002592319045477 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07018491357521069) - present_state_Q ( -0.08572355602954694)) * f1( 0.016264414967742648)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07018491357521069) - present_state_Q (-0.08572355602954694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.008898162784837 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022182225400055332) - present_state_Q ( -0.07685091539036419)) * f1( 0.007740059282583402)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.022182225400055332) - present_state_Q (-0.07685091539036419)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.015927414625999 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025926184542978625) - present_state_Q ( -0.08309608605055432)) * f1( 0.00863422283307607)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.025926184542978625) - present_state_Q (-0.08309608605055432)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.022386487551124 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07061031192654198) - present_state_Q ( -0.07061031192654198)) * f1( 0.007917368280564673)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07061031192654198) - present_state_Q (-0.07061031192654198)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.033350433238278 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031839484074287355) - present_state_Q ( -0.1061314517116562)) * f1( 0.01350454537817356)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.031839484074287355) - present_state_Q (-0.1061314517116562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.041248419415956 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03615460784610587) - present_state_Q ( -0.08306440888229401)) * f1( 0.009700056602349292)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.03615460784610587) - present_state_Q (-0.08306440888229401)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.050159066812403 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017819266366944694) - present_state_Q ( -0.0941142770273331)) * f1( 0.0109611188180243)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.017819266366944694) - present_state_Q (-0.0941142770273331)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.057062308221824 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015178491886723561) - present_state_Q ( -0.0899505051977879)) * f1( 0.008487707484784654)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.015178491886723561) - present_state_Q (-0.0899505051977879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.06379260113847 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02330569050652603) - present_state_Q ( -0.08528954869897669)) * f1( 0.008269497609698999)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.02330569050652603) - present_state_Q (-0.08528954869897669)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.0714270044160665 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023706581809214373) - present_state_Q ( -0.023706581809214373)) * f1( 0.009309885377425487)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.023706581809214373) - present_state_Q (-0.023706581809214373)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.07891373758156 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01927618652140832) - present_state_Q ( -0.09190703146253512)) * f1( 0.009206876093246526)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01927618652140832) - present_state_Q (-0.09190703146253512)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.086075405240837 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0161777606247038) - present_state_Q ( -0.0161777606247038)) * f1( 0.008726190506400324)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0161777606247038) - present_state_Q (-0.0161777606247038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.0930051567858525 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018172563093036748) - present_state_Q ( -0.018172563093036748)) * f1( 0.008445457950470244)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.018172563093036748) - present_state_Q (-0.018172563093036748)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.099722533628772 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014390467615514704) - present_state_Q ( -0.014390467615514704)) * f1( 0.00818323696562405)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.014390467615514704) - present_state_Q (-0.014390467615514704)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.10621609571325 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039390329286580684) - present_state_Q ( -0.039390329286580684)) * f1( 0.00793232386978711)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.039390329286580684) - present_state_Q (-0.039390329286580684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.1178935362767115 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004725405123324428) - present_state_Q ( -0.14672495434152052)) * f1( 0.014460505355727913)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.004725405123324428) - present_state_Q (-0.14672495434152052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.126092925132592 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028872542725988194) - present_state_Q ( -0.028872542725988194)) * f1( 0.010004537660751168)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.028872542725988194) - present_state_Q (-0.028872542725988194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.133929325172579 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025029945833909283) - present_state_Q ( -0.025029945833909283)) * f1( 0.009557601462131011)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.025029945833909283) - present_state_Q (-0.025029945833909283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.141428594279961 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021073822231009072) - present_state_Q ( -0.021073822231009072)) * f1( 0.009142452312650998)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.021073822231009072) - present_state_Q (-0.021073822231009072)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.148615095729022 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01719819131289877) - present_state_Q ( -0.01719819131289877)) * f1( 0.00875742940457505)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01719819131289877) - present_state_Q (-0.01719819131289877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.155519396480475 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01423532873919977) - present_state_Q ( -0.01423532873919977)) * f1( 0.00841080818397544)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01423532873919977) - present_state_Q (-0.01423532873919977)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.1622380371810745 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017461521511817627) - present_state_Q ( -0.017461521511817627)) * f1( 0.008187533441469304)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.017461521511817627) - present_state_Q (-0.017461521511817627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.167618029737155 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0024228037722492765) - present_state_Q ( -0.0024228037722492765)) * f1( 0.006545421392305849)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0024228037722492765) - present_state_Q (-0.0024228037722492765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.172984676136215 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0335002165137482) - present_state_Q ( -0.07101831916837449)) * f1( 0.0065816225182107144)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0335002165137482) - present_state_Q (-0.07101831916837449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.185897606150497 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.058883089602118854) - present_state_Q ( -0.08462064506019579)) * f1( 0.015857856185955605)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.058883089602118854) - present_state_Q (-0.08462064506019579)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.1925854332409775 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026182189786953625) - present_state_Q ( -0.07733659895849833)) * f1( 0.008209008141062514)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.026182189786953625) - present_state_Q (-0.07733659895849833)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.20031391313955 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021878730912454136) - present_state_Q ( -0.021878730912454136)) * f1( 0.00942271820550995)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.021878730912454136) - present_state_Q (-0.021878730912454136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.206836457324744 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04691209847631123) - present_state_Q ( -0.04691209847631123)) * f1( 0.008976904227351071)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.04691209847631123) - present_state_Q (-0.04691209847631123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.216503284048589 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0021298363614449586) - present_state_Q ( -0.1424019628770778)) * f1( 0.013489946304386796)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0021298363614449586) - present_state_Q (-0.1424019628770778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.2212945777276945 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012859813666892473) - present_state_Q ( -0.012859813666892473)) * f1( 0.0065665066217521365)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.012859813666892473) - present_state_Q (-0.012859813666892473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.226071778965274 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0518316982797693) - present_state_Q ( -0.06339300865463529)) * f1( 0.006589308396776306)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0518316982797693) - present_state_Q (-0.06339300865463529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.235509213828303 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006133466358854073) - present_state_Q ( -0.1384846225022494)) * f1( 0.013161902133655761)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.006133466358854073) - present_state_Q (-0.1384846225022494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.2406787244759565 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011792647687230708) - present_state_Q ( -0.011792647687230708)) * f1( 0.0070839235674701176)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.011792647687230708) - present_state_Q (-0.011792647687230708)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.245249574092138 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019947945857861752) - present_state_Q ( -0.08410754628111561)) * f1( 0.007240940732531125)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.019947945857861752) - present_state_Q (-0.08410754628111561)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.250692526176498 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015814677344771003) - present_state_Q ( -0.015814677344771003)) * f1( 0.008530754699351837)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.015814677344771003) - present_state_Q (-0.015814677344771003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.2559382757659465 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01321417238080794) - present_state_Q ( -0.01321417238080794)) * f1( 0.008218663881889045)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01321417238080794) - present_state_Q (-0.01321417238080794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.261019713086566 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041314910977638085) - present_state_Q ( -0.041314910977638085)) * f1( 0.007992901967942383)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.041314910977638085) - present_state_Q (-0.041314910977638085)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.270178891135673 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011363824695385083) - present_state_Q ( -0.15061999714470273)) * f1( 0.014666095998330108)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.011363824695385083) - present_state_Q (-0.15061999714470273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.276204104712323 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02149185435047494) - present_state_Q ( -0.02149185435047494)) * f1( 0.00945090318675534)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.02149185435047494) - present_state_Q (-0.02149185435047494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.281875044228181 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025231139551389805) - present_state_Q ( -0.09201867362269098)) * f1( 0.008994174358973662)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.025231139551389805) - present_state_Q (-0.09201867362269098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.287381665539078 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038017988165714695) - present_state_Q ( -0.08204599607804793)) * f1( 0.008718006724702342)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.038017988165714695) - present_state_Q (-0.08204599607804793)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.296674093811802 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025035194340092364) - present_state_Q ( -0.11181431484247682)) * f1( 0.014784359667064567)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.025035194340092364) - present_state_Q (-0.11181431484247682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.301679781625197 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013649084989328708) - present_state_Q ( -0.013649084989328708)) * f1( 0.007843033299099186)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.013649084989328708) - present_state_Q (-0.013649084989328708)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.306680692684656 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010630933519194108) - present_state_Q ( -0.010630933519194108)) * f1( 0.00783221555698475)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010630933519194108) - present_state_Q (-0.010630933519194108)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.310810897979918 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03462231982926297) - present_state_Q ( -0.07907643162284896)) * f1( 0.0076407612286328265)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.03462231982926297) - present_state_Q (-0.07907643162284896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.3190825243797235 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06393051922070178) - present_state_Q ( -0.08304249930914494)) * f1( 0.015305202602128094)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06393051922070178) - present_state_Q (-0.08304249930914494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.323770786125785 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02535600789401724) - present_state_Q ( -0.12235069268432341)) * f1( 0.008744655020193)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02535600789401724) - present_state_Q (-0.12235069268432341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.328817995872235 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017946812967992613) - present_state_Q ( -0.09765134457099381)) * f1( 0.009372290469919721)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.017946812967992613) - present_state_Q (-0.09765134457099381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.3334271364178605 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014715816878824946) - present_state_Q ( -0.014715816878824946)) * f1( 0.00842951571196535)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.014715816878824946) - present_state_Q (-0.014715816878824946)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.337818557468816 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024160137956745893) - present_state_Q ( -0.08787595399567232)) * f1( 0.008138827212312046)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.024160137956745893) - present_state_Q (-0.08787595399567232)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.3428879848287885 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02842069774268279) - present_state_Q ( -0.09106807201929457)) * f1( 0.009400227200182699)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02842069774268279) - present_state_Q (-0.09106807201929457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.3482322163404366 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024001890798382212) - present_state_Q ( -0.024001890798382212)) * f1( 0.009788863020632277)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.024001890798382212) - present_state_Q (-0.024001890798382212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.353232740770873 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025292591911653305) - present_state_Q ( -0.0950932947004609)) * f1( 0.009279925436938733)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.025292591911653305) - present_state_Q (-0.0950932947004609)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.358264257495748 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018303077708811195) - present_state_Q ( -0.0976504116010267)) * f1( 0.009343086275757218)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.018303077708811195) - present_state_Q (-0.0976504116010267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.362897269089646 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0150735988154263) - present_state_Q ( -0.0150735988154263)) * f1( 0.008473671756537132)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0150735988154263) - present_state_Q (-0.0150735988154263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.367308384255625 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02401998802546603) - present_state_Q ( -0.08858146863314072)) * f1( 0.008176417598572664)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02401998802546603) - present_state_Q (-0.08858146863314072)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.372365217318039 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028785745570511518) - present_state_Q ( -0.09109944455634869)) * f1( 0.009376864700225668)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.028785745570511518) - present_state_Q (-0.09109944455634869)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.377727942951662 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024334252151068686) - present_state_Q ( -0.024334252151068686)) * f1( 0.009823276345671676)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.024334252151068686) - present_state_Q (-0.024334252151068686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.3828688058130965 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024544998024911054) - present_state_Q ( -0.024544998024911054)) * f1( 0.009417202109573438)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.024544998024911054) - present_state_Q (-0.024544998024911054)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.3877914988664 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020426822491023063) - present_state_Q ( -0.020426822491023063)) * f1( 0.009011433212499741)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.020426822491023063) - present_state_Q (-0.020426822491023063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.39172380384401 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016579731507629376) - present_state_Q ( -0.016579731507629376)) * f1( 0.008637370479848588)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.016579731507629376) - present_state_Q (-0.016579731507629376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.395459808798156 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05765840814504226) - present_state_Q ( -0.07634355787641817)) * f1( 0.008307756005431877)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.05765840814504226) - present_state_Q (-0.07634355787641817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.40091306519964 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013564041575528648) - present_state_Q ( -0.09923366300568706)) * f1( 0.012200473067999427)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013564041575528648) - present_state_Q (-0.09923366300568706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.404492122359623 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013939752797437466) - present_state_Q ( -0.013939752797437466)) * f1( 0.007857355365974152)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013939752797437466) - present_state_Q (-0.013939752797437466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.4079851598877084 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010100603579228075) - present_state_Q ( -0.010100603579228075)) * f1( 0.007662697877574128)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.010100603579228075) - present_state_Q (-0.010100603579228075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.411395719723684 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015989783354348554) - present_state_Q ( -0.015989783354348554)) * f1( 0.007490475396041231)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.015989783354348554) - present_state_Q (-0.015989783354348554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.414893636568781 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011820153565315251) - present_state_Q ( -0.011820153565315251)) * f1( 0.007676007645783344)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.011820153565315251) - present_state_Q (-0.011820153565315251)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.418314700928962 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008555558414326928) - present_state_Q ( -0.008555558414326928)) * f1( 0.007502521299847104)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.008555558414326928) - present_state_Q (-0.008555558414326928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.4216702930752545 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01312328814486412) - present_state_Q ( -0.01312328814486412)) * f1( 0.007365578724728689)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01312328814486412) - present_state_Q (-0.01312328814486412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.42508190065469 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009056318743719094) - present_state_Q ( -0.009056318743719094)) * f1( 0.0074825217762426255)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.009056318743719094) - present_state_Q (-0.009056318743719094)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.43622474915999 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09303729236798662) - present_state_Q ( -0.19044832428303302)) * f1( 0.02540293343393167)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.09303729236798662) - present_state_Q (-0.19044832428303302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.4438632669372895 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06809664396860088) - present_state_Q ( -0.0754871173428939)) * f1( 0.016978601130264132)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.06809664396860088) - present_state_Q (-0.0754871173428939)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.4480252706397545 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025149622847919002) - present_state_Q ( -0.025149622847919002)) * f1( 0.009157421442418507)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.025149622847919002) - present_state_Q (-0.025149622847919002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.452178002501185 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02088518912694494) - present_state_Q ( -0.02088518912694494)) * f1( 0.00912931187534202)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.02088518912694494) - present_state_Q (-0.02088518912694494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.45615232224082 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017131271417966627) - present_state_Q ( -0.017131271417966627)) * f1( 0.008730608488783393)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.017131271417966627) - present_state_Q (-0.017131271417966627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.459968187012077 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019918701304932656) - present_state_Q ( -0.019918701304932656)) * f1( 0.008387143806896152)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.019918701304932656) - present_state_Q (-0.019918701304932656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.463756159405236 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05105331488340308) - present_state_Q ( -0.07797485037631796)) * f1( 0.008427610971660076)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.05105331488340308) - present_state_Q (-0.07797485037631796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.469698499207424 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05658928223794713) - present_state_Q ( -0.05658928223794713)) * f1( 0.013156503297102263)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.05658928223794713) - present_state_Q (-0.05658928223794713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.475410786885581 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03505629891300884) - present_state_Q ( -0.10501506936650529)) * f1( 0.012790392858421071)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.03505629891300884) - present_state_Q (-0.10501506936650529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.480042741583609 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030639656030091205) - present_state_Q ( -0.030639656030091205)) * f1( 0.010202519685990152)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.030639656030091205) - present_state_Q (-0.030639656030091205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.484451873975062 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026242374900135482) - present_state_Q ( -0.026242374900135482)) * f1( 0.009703264403298235)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.026242374900135482) - present_state_Q (-0.026242374900135482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.488656997119855 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021895507251907247) - present_state_Q ( -0.021895507251907247)) * f1( 0.009246336501999537)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.021895507251907247) - present_state_Q (-0.021895507251907247)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.492676681977847 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01798218189940455) - present_state_Q ( -0.01798218189940455)) * f1( 0.008831750340354721)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01798218189940455) - present_state_Q (-0.01798218189940455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.496471854555725 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027415948614706788) - present_state_Q ( -0.09038982557414678)) * f1( 0.008471484587950695)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.027415948614706788) - present_state_Q (-0.09038982557414678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.499965744732284 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02793646108423628) - present_state_Q ( -0.02793646108423628)) * f1( 0.009627889926965134)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02793646108423628) - present_state_Q (-0.02793646108423628)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.503353819421936 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022984940893125798) - present_state_Q ( -0.0980552278204797)) * f1( 0.009521577775377295)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.022984940893125798) - present_state_Q (-0.0980552278204797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.506626074669368 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019061491371456952) - present_state_Q ( -0.019061491371456952)) * f1( 0.008997341054553073)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.019061491371456952) - present_state_Q (-0.019061491371456952)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.509695808229176 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026346771528975778) - present_state_Q ( -0.09210155343516355)) * f1( 0.008611714468918813)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.026346771528975778) - present_state_Q (-0.09210155343516355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.513342686595791 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032840574506961276) - present_state_Q ( -0.032840574506961276)) * f1( 0.010061707316237008)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.032840574506961276) - present_state_Q (-0.032840574506961276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.516847990597268 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029388679145691297) - present_state_Q ( -0.029388679145691297)) * f1( 0.009662822443036637)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.029388679145691297) - present_state_Q (-0.029388679145691297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.520220708179115 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02573087969689308) - present_state_Q ( -0.02573087969689308)) * f1( 0.009288901319074413)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02573087969689308) - present_state_Q (-0.02573087969689308)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.5234673902820575 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021812819037796492) - present_state_Q ( -0.021812819037796492)) * f1( 0.008933107654504618)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.021812819037796492) - present_state_Q (-0.021812819037796492)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.526593999821726 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017748973922531462) - present_state_Q ( -0.017748973922531462)) * f1( 0.008594084580225023)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.017748973922531462) - present_state_Q (-0.017748973922531462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.529608650148055 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014180318531418034) - present_state_Q ( -0.014180318531418034)) * f1( 0.008279034346745338)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.014180318531418034) - present_state_Q (-0.014180318531418034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.532478656412446 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056813803625782) - present_state_Q ( -0.07610403648151866)) * f1( 0.008008620262857688)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.056813803625782) - present_state_Q (-0.07610403648151866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.536857412050224 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016220454827729762) - present_state_Q ( -0.14107385562944535)) * f1( 0.012458698668404352)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.016220454827729762) - present_state_Q (-0.14107385562944535)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.539609976139784 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022797891264721534) - present_state_Q ( -0.08793103147058301)) * f1( 0.007713682321579873)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.022797891264721534) - present_state_Q (-0.08793103147058301)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.542871920823453 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02025210392897438) - present_state_Q ( -0.02025210392897438)) * f1( 0.008971634623243577)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02025210392897438) - present_state_Q (-0.02025210392897438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.545623215786366 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011169421210713041) - present_state_Q ( -0.011169421210713041)) * f1( 0.007550171506424881)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.011169421210713041) - present_state_Q (-0.011169421210713041)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.548304060256156 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014663925473647105) - present_state_Q ( -0.09304094978347782)) * f1( 0.007525189219060998)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.014663925473647105) - present_state_Q (-0.09304094978347782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.551157923945444 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01296432697498964) - present_state_Q ( -0.01296432697498964)) * f1( 0.007835116491399672)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01296432697498964) - present_state_Q (-0.01296432697498964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.553921071399813 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039825294851188964) - present_state_Q ( -0.039825294851188964)) * f1( 0.007636745971291522)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.039825294851188964) - present_state_Q (-0.039825294851188964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.559071326469638 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007186534097254965) - present_state_Q ( -0.15788783009522772)) * f1( 0.014728060124317986)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.007186534097254965) - present_state_Q (-0.15788783009522772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.562615219163897 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028737928287987023) - present_state_Q ( -0.028737928287987023)) * f1( 0.009767620176923815)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.028737928287987023) - present_state_Q (-0.028737928287987023)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.566008409622304 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024623459694917384) - present_state_Q ( -0.024623459694917384)) * f1( 0.009342721715587367)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.024623459694917384) - present_state_Q (-0.024623459694917384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.56926106140955 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02041959565516106) - present_state_Q ( -0.02041959565516106)) * f1( 0.008946446413297863)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02041959565516106) - present_state_Q (-0.02041959565516106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.572537119012987 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03101572175636681) - present_state_Q ( -0.091273747950125)) * f1( 0.00918719125698544)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03101572175636681) - present_state_Q (-0.091273747950125)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.57610160587689 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015432393290234303) - present_state_Q ( -0.09689247523928277)) * f1( 0.010016205574695966)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015432393290234303) - present_state_Q (-0.09689247523928277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.579072982570763 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01760609438680815) - present_state_Q ( -0.01760609438680815)) * f1( 0.008167108727101703)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01760609438680815) - present_state_Q (-0.01760609438680815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.581986856461212 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01883535012662092) - present_state_Q ( -0.09341732815203596)) * f1( 0.008179214322067604)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01883535012662092) - present_state_Q (-0.09341732815203596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.58511444699213 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019730234000760304) - present_state_Q ( -0.019730234000760304)) * f1( 0.008600996616114232)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.019730234000760304) - present_state_Q (-0.019730234000760304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.588138173971919 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015685824969502993) - present_state_Q ( -0.015685824969502993)) * f1( 0.008307052372047565)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015685824969502993) - present_state_Q (-0.015685824969502993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.591042705189824 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0428034644836329) - present_state_Q ( -0.0428034644836329)) * f1( 0.008033451436695252)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0428034644836329) - present_state_Q (-0.0428034644836329)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.596066538854193 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00498215107540214) - present_state_Q ( -0.1561207592642907)) * f1( 0.014360184562586194)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00498215107540214) - present_state_Q (-0.1561207592642907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.59970674144191 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030238397327001516) - present_state_Q ( -0.030238397327001516)) * f1( 0.010036803670027484)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.030238397327001516) - present_state_Q (-0.030238397327001516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.603179706165602 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025895983172545132) - present_state_Q ( -0.025895983172545132)) * f1( 0.009565386397982384)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.025895983172545132) - present_state_Q (-0.025895983172545132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.60566445789105 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021554891298956637) - present_state_Q ( -0.021554891298956637)) * f1( 0.009131248143168905)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021554891298956637) - present_state_Q (-0.021554891298956637)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.608044549713575 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01761557741860815) - present_state_Q ( -0.01761557741860815)) * f1( 0.008735250896384068)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01761557741860815) - present_state_Q (-0.01761557741860815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.610269816073896 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02801254984710692) - present_state_Q ( -0.09114927552912064)) * f1( 0.008390254270217409)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02801254984710692) - present_state_Q (-0.09114927552912064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.612004459880248 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04471479703424675) - present_state_Q ( -0.10802135234846842)) * f1( 0.006578090922931524)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04471479703424675) - present_state_Q (-0.10802135234846842)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6144955815692175 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025148272958990962) - present_state_Q ( -0.025148272958990962)) * f1( 0.009165550310741932)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.025148272958990962) - present_state_Q (-0.025148272958990962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.616904792386297 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022092067784871466) - present_state_Q ( -0.09740125296200938)) * f1( 0.009107309000281434)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.022092067784871466) - present_state_Q (-0.09740125296200938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.619330393606574 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018719675275989583) - present_state_Q ( -0.018719675275989583)) * f1( 0.008905524202970718)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.018719675275989583) - present_state_Q (-0.018719675275989583)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.621666055941809 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01975139713057109) - present_state_Q ( -0.01975139713057109)) * f1( 0.008578240652134873)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01975139713057109) - present_state_Q (-0.01975139713057109)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.623925865088514 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01568404025397655) - present_state_Q ( -0.01568404025397655)) * f1( 0.008288509463133906)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01568404025397655) - present_state_Q (-0.01568404025397655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.626114298607053 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012420058383584114) - present_state_Q ( -0.012420058383584114)) * f1( 0.008018079483335598)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.012420058383584114) - present_state_Q (-0.012420058383584114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.628182973577758 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02567661242478432) - present_state_Q ( -0.08904185229241224)) * f1( 0.007794327522993805)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02567661242478432) - present_state_Q (-0.08904185229241224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6306691170663905 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026880901901154698) - present_state_Q ( -0.0964321568118072)) * f1( 0.009392988811903986)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.026880901901154698) - present_state_Q (-0.0964321568118072)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.633242039079417 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022602061595945917) - present_state_Q ( -0.022602061595945917)) * f1( 0.009458542244510993)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.022602061595945917) - present_state_Q (-0.022602061595945917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.635624049380282 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025942133663213435) - present_state_Q ( -0.0968692598335106)) * f1( 0.009001365218082744)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.025942133663213435) - present_state_Q (-0.0968692598335106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.638090300259537 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01676534498010591) - present_state_Q ( -0.1028648300883899)) * f1( 0.00934411243055236)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01676534498010591) - present_state_Q (-0.1028648300883899)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.640323922530334 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013619440156963921) - present_state_Q ( -0.013619440156963921)) * f1( 0.008186881872503594)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.013619440156963921) - present_state_Q (-0.013619440156963921)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.64242940605339 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02481693057339603) - present_state_Q ( -0.09053675896930351)) * f1( 0.007937742360540288)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02481693057339603) - present_state_Q (-0.09053675896930351)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6448956685961065 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027918069238301715) - present_state_Q ( -0.09537451022895878)) * f1( 0.009313789195632177)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.027918069238301715) - present_state_Q (-0.09537451022895878)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.647503427251356 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023576927627640343) - present_state_Q ( -0.023576927627640343)) * f1( 0.00958970132550624)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.023576927627640343) - present_state_Q (-0.023576927627640343)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.649929824915598 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07156922556202956) - present_state_Q ( -0.11223997999827352)) * f1( 0.009206702694076873)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07156922556202956) - present_state_Q (-0.11223997999827352)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.65378115887774 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0455635248021599) - present_state_Q ( -0.12099058569603138)) * f1( 0.014676682536462034)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0455635248021599) - present_state_Q (-0.12099058569603138)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.656576777422291 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01851608339533919) - present_state_Q ( -0.01851608339533919)) * f1( 0.01026334157809884)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01851608339533919) - present_state_Q (-0.01851608339533919)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.658792212898355 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016502356485673806) - present_state_Q ( -0.09704756535417343)) * f1( 0.008375448576418722)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.016502356485673806) - present_state_Q (-0.09704756535417343)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.661063790418226 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019482430395392894) - present_state_Q ( -0.019482430395392894)) * f1( 0.00834213321768974)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.019482430395392894) - present_state_Q (-0.019482430395392894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.662887674639378 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003154296666159435) - present_state_Q ( -0.003154296666159435)) * f1( 0.006662072701289488)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.003154296666159435) - present_state_Q (-0.003154296666159435)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6647125064048245 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013708965239311574) - present_state_Q ( -0.013708965239311574)) * f1( 0.0066887420989582115)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.013708965239311574) - present_state_Q (-0.013708965239311574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.666542183156856 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007833591628647834) - present_state_Q ( -0.007833591628647834)) * f1( 0.006693527497050848)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.007833591628647834) - present_state_Q (-0.007833591628647834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.66836963204833 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029736307381049254) - present_state_Q ( -0.0029736307381049254)) * f1( 0.006674696889262413)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0029736307381049254) - present_state_Q (-0.0029736307381049254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.670207835857366 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015022630061274675) - present_state_Q ( -0.015022630061274675)) * f1( 0.006740677153993199)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.015022630061274675) - present_state_Q (-0.015022630061274675)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.672051413996237 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00942603789889266) - present_state_Q ( -0.00942603789889266)) * f1( 0.006747921101539441)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.00942603789889266) - present_state_Q (-0.00942603789889266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.673834872028991 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0771321323022804) - present_state_Q ( -0.09533618411912757)) * f1( 0.006722600515592334)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0771321323022804) - present_state_Q (-0.09533618411912757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.677103878344657 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025343275314205386) - present_state_Q ( -0.11239712871613773)) * f1( 0.012426426645838734)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.025343275314205386) - present_state_Q (-0.11239712871613773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.679413352516346 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029894109335989672) - present_state_Q ( -0.09043368682767743)) * f1( 0.00870478938678584)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.029894109335989672) - present_state_Q (-0.09043368682767743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.682013496096707 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028461385865249225) - present_state_Q ( -0.10284341788521217)) * f1( 0.00984696061620789)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.028461385865249225) - present_state_Q (-0.10284341788521217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.684519483333366 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02412370535428734) - present_state_Q ( -0.09974034606866557)) * f1( 0.009480798667231343)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02412370535428734) - present_state_Q (-0.09974034606866557)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6869909652374036 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020075525216524417) - present_state_Q ( -0.020075525216524417)) * f1( 0.009078040910631153)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.020075525216524417) - present_state_Q (-0.020075525216524417)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.689289653307066 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026656725640389036) - present_state_Q ( -0.09496192795312328)) * f1( 0.008680008710988299)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.026656725640389036) - present_state_Q (-0.09496192795312328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.691853066721206 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028099821793634416) - present_state_Q ( -0.028099821793634416)) * f1( 0.00944075932956042)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.028099821793634416) - present_state_Q (-0.028099821793634416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6943229660230985 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040720876455175206) - present_state_Q ( -0.1144407070663391)) * f1( 0.009390598244911522)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.040720876455175206) - present_state_Q (-0.1144407070663391)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.69832900492586 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05332264388429659) - present_state_Q ( -0.05332264388429659)) * f1( 0.014878171238276339)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05332264388429659) - present_state_Q (-0.05332264388429659)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7007334025399645 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018409253848010166) - present_state_Q ( -0.018409253848010166)) * f1( 0.00882677038985558)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.018409253848010166) - present_state_Q (-0.018409253848010166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.702977498951037 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027761097081711344) - present_state_Q ( -0.09310250951277418)) * f1( 0.008467568304327489)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.027761097081711344) - present_state_Q (-0.09310250951277418)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7055781580849185 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028358161235268214) - present_state_Q ( -0.028358161235268214)) * f1( 0.009578751288773784)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.028358161235268214) - present_state_Q (-0.028358161235268214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.708082509520214 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023469916435856462) - present_state_Q ( -0.1005080256920224)) * f1( 0.009477597038666614)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.023469916435856462) - present_state_Q (-0.1005080256920224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.710529720350507 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019488220121310868) - present_state_Q ( -0.019488220121310868)) * f1( 0.008987145560976012)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.019488220121310868) - present_state_Q (-0.019488220121310868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7128083571997905 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02665540928099215) - present_state_Q ( -0.09476804833371064)) * f1( 0.008603664435518065)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02665540928099215) - present_state_Q (-0.09476804833371064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.715370742255947 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027885104320957522) - present_state_Q ( -0.027885104320957522)) * f1( 0.009436300420996156)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.027885104320957522) - present_state_Q (-0.027885104320957522)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.717848991272768 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024450035055574706) - present_state_Q ( -0.09923200050369527)) * f1( 0.009373939270060248)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.024450035055574706) - present_state_Q (-0.09923200050369527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.720332338637029 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020455275112246672) - present_state_Q ( -0.020455275112246672)) * f1( 0.009122769381461239)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.020455275112246672) - present_state_Q (-0.020455275112246672)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.722640283926778 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02582921794722928) - present_state_Q ( -0.09615411662721758)) * f1( 0.008719162261414885)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02582921794722928) - present_state_Q (-0.09615411662721758)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.725099044705617 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014615072992292978) - present_state_Q ( -0.10541120685516107)) * f1( 0.009325490683982541)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.014615072992292978) - present_state_Q (-0.10541120685516107)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7272536869877095 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011678160559867935) - present_state_Q ( -0.011678160559867935)) * f1( 0.007892342891624028)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.011678160559867935) - present_state_Q (-0.011678160559867935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.73150254082889 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07258002614720847) - present_state_Q ( -0.07258002614720847)) * f1( 0.01588220185778288)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07258002614720847) - present_state_Q (-0.07258002614720847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.734196216501108 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026266557485803626) - present_state_Q ( -0.026266557485803626)) * f1( 0.009914475809823504)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.026266557485803626) - present_state_Q (-0.026266557485803626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.73666462004316 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07605259302171898) - present_state_Q ( -0.11788667353848384)) * f1( 0.009384600189473617)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07605259302171898) - present_state_Q (-0.11788667353848384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.740655396055185 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04876394108374287) - present_state_Q ( -0.12719794932526138)) * f1( 0.015242265678102345)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04876394108374287) - present_state_Q (-0.12719794932526138)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7433344855858115 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029988788337989306) - present_state_Q ( -0.1018672660050195)) * f1( 0.010141599769221122)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.029988788337989306) - present_state_Q (-0.1018672660050195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7459645457204145 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025130413687739404) - present_state_Q ( -0.025130413687739404)) * f1( 0.009676687426692383)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.025130413687739404) - present_state_Q (-0.025130413687739404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.748465568311947 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021014958287620247) - present_state_Q ( -0.021014958287620247)) * f1( 0.009189401140917404)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.021014958287620247) - present_state_Q (-0.021014958287620247)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.750787756306827 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026364808160733014) - present_state_Q ( -0.09666147140947244)) * f1( 0.008774473914724667)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.026364808160733014) - present_state_Q (-0.09666147140947244)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.753258755105497 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01529415741203685) - present_state_Q ( -0.1057585534745615)) * f1( 0.009372899941185205)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01529415741203685) - present_state_Q (-0.1057585534745615)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.75543095194456 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012183352238390224) - present_state_Q ( -0.012183352238390224)) * f1( 0.007957969676585049)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.012183352238390224) - present_state_Q (-0.012183352238390224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7574857693309935 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02537878613125848) - present_state_Q ( -0.09068998816420974)) * f1( 0.007747012783993689)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02537878613125848) - present_state_Q (-0.09068998816420974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.759943335153863 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0267956289656136) - present_state_Q ( -0.09767631479106603)) * f1( 0.009289415006081887)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0267956289656136) - present_state_Q (-0.09767631479106603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.761869682178197 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015399272675269814) - present_state_Q ( -0.015399272675269814)) * f1( 0.007064775720296912)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.015399272675269814) - present_state_Q (-0.015399272675269814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7631440719449065 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024982088073683413) - present_state_Q ( -0.08779601855392875)) * f1( 0.00731677628152975)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.024982088073683413) - present_state_Q (-0.08779601855392875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.764775226771633 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020346061415753423) - present_state_Q ( -0.020346061415753423)) * f1( 0.009018267534222007)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.020346061415753423) - present_state_Q (-0.020346061415753423)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.766338664190387 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017062275627472447) - present_state_Q ( -0.017062275627472447)) * f1( 0.00862977324385102)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.017062275627472447) - present_state_Q (-0.017062275627472447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.767851060827929 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019349703964015315) - present_state_Q ( -0.019349703964015315)) * f1( 0.00835753825086283)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.019349703964015315) - present_state_Q (-0.019349703964015315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.769321238217403 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01518025682276569) - present_state_Q ( -0.01518025682276569)) * f1( 0.00810742183941525)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01518025682276569) - present_state_Q (-0.01518025682276569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7707284734343 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04263640550310861) - present_state_Q ( -0.04263640550310861)) * f1( 0.007867531312840145)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04263640550310861) - present_state_Q (-0.04263640550310861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.773148246655054 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005687256595936632) - present_state_Q ( -0.16087908709800883)) * f1( 0.014518138737397254)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005687256595936632) - present_state_Q (-0.16087908709800883)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.774938200972686 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030999833128690018) - present_state_Q ( -0.030999833128690018)) * f1( 0.009948973649396904)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.030999833128690018) - present_state_Q (-0.030999833128690018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.77665199136599 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02675585380248531) - present_state_Q ( -0.02675585380248531)) * f1( 0.009505457033085111)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02675585380248531) - present_state_Q (-0.02675585380248531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.778295029514836 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022407744408207118) - present_state_Q ( -0.022407744408207118)) * f1( 0.009093296138827976)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.022407744408207118) - present_state_Q (-0.022407744408207118)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.77984842547073 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.048845391155549166) - present_state_Q ( -0.048845391155549166)) * f1( 0.008711900162148033)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.048845391155549166) - present_state_Q (-0.048845391155549166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.782151012043957 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002467775286413255) - present_state_Q ( -0.15553122236258127)) * f1( 0.013773509820102328)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.002467775286413255) - present_state_Q (-0.15553122236258127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7833031620176465 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01193676651929698) - present_state_Q ( -0.01193676651929698)) * f1( 0.006343420281869414)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01193676651929698) - present_state_Q (-0.01193676651929698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.7845104625297585 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005595351416955407) - present_state_Q ( -0.005595351416955407)) * f1( 0.006626242522739133)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005595351416955407) - present_state_Q (-0.005595351416955407)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.785711019993904 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015244835472789658) - present_state_Q ( -0.015244835472789658)) * f1( 0.00662079137667655)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015244835472789658) - present_state_Q (-0.015244835472789658)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.786516165883414 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0522377268162471) - present_state_Q ( -0.0522377268162471)) * f1( 0.0045232393168524385)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0522377268162471) - present_state_Q (-0.0522377268162471)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.787738329174994 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0060838694037024755) - present_state_Q ( -0.0060838694037024755)) * f1( 0.006709435616361729)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0060838694037024755) - present_state_Q (-0.0060838694037024755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.788958085195587 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0041669599028945035) - present_state_Q ( -0.0041669599028945035)) * f1( 0.00668988411031853)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0041669599028945035) - present_state_Q (-0.0041669599028945035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.790167562883647 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012274773460059542) - present_state_Q ( -0.012274773460059542)) * f1( 0.006660166375753309)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012274773460059542) - present_state_Q (-0.012274773460059542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.791380571602277 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006042445410952076) - present_state_Q ( -0.006042445410952076)) * f1( 0.0066590425252889465)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006042445410952076) - present_state_Q (-0.006042445410952076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.792592766219064 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038559812821025493) - present_state_Q ( -0.0038559812821025493)) * f1( 0.006647392372633798)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0038559812821025493) - present_state_Q (-0.0038559812821025493)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.793796570718167 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012412860742113395) - present_state_Q ( -0.012412860742113395)) * f1( 0.006629379815216824)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012412860742113395) - present_state_Q (-0.012412860742113395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.795004914136517 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006123703363232074) - present_state_Q ( -0.006123703363232074)) * f1( 0.006633697795325003)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006123703363232074) - present_state_Q (-0.006123703363232074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.796170031709601 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046164785317734186) - present_state_Q ( -0.07320215717293987)) * f1( 0.006625825911261377)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.046164785317734186) - present_state_Q (-0.07320215717293987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.798642461032104 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026013224007937763) - present_state_Q ( -0.11393124568889752)) * f1( 0.014410577224741965)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.026013224007937763) - present_state_Q (-0.11393124568889752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.800144779868628 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01987502551735576) - present_state_Q ( -0.01987502551735576)) * f1( 0.008304017638001612)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01987502551735576) - present_state_Q (-0.01987502551735576)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.80139649902057 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013341466909642033) - present_state_Q ( -0.013341466909642033)) * f1( 0.006896420997751542)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013341466909642033) - present_state_Q (-0.013341466909642033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.804618487202779 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04638019190548148) - present_state_Q ( -0.1968402561009633)) * f1( 0.019708371708046533)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04638019190548148) - present_state_Q (-0.1968402561009633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.807315702124752 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055863742883083656) - present_state_Q ( -0.10314467016454754)) * f1( 0.015595559646273623)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.055863742883083656) - present_state_Q (-0.10314467016454754)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.808886566807039 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027020954687808716) - present_state_Q ( -0.027020954687808716)) * f1( 0.008713879500105767)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.027020954687808716) - present_state_Q (-0.027020954687808716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.828230054030285 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05521716644905796) - present_state_Q ( -0.05521716644905796)) * f1( 0.007071067811865476)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.05521716644905796) - present_state_Q (-0.05521716644905796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.847587121765793 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09485958381365946) - present_state_Q ( -0.039975663560182735)) * f1( 0.007071067811865476)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.09485958381365946) - present_state_Q (-0.039975663560182735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.8705851435709455 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07152767371828203) - present_state_Q ( -0.06762551395379167)) * f1( 0.008410306904240258)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07152767371828203) - present_state_Q (-0.06762551395379167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.895495559114633 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046391289577748816) - present_state_Q ( -0.07994349657108592)) * f1( 0.009114607153566398)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.046391289577748816) - present_state_Q (-0.07994349657108592)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.934225369402446 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0732032354805801) - present_state_Q ( -0.08035463653960526)) * f1( 0.014169883552033455)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0732032354805801) - present_state_Q (-0.08035463653960526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.961214778836143 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05340850754051321) - present_state_Q ( -0.08233874752143039)) * f1( 0.009875913477969168)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.05340850754051321) - present_state_Q (-0.08233874752143039)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.997513122165015 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06021733509327754) - present_state_Q ( -0.06021733509327754)) * f1( 0.013271147758002333)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.06021733509327754) - present_state_Q (-0.06021733509327754)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.031535477081237 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06704096939615486) - present_state_Q ( -0.06704096939615486)) * f1( 0.012441810304180878)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.06704096939615486) - present_state_Q (-0.06704096939615486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.063793216113838 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04967972022791889) - present_state_Q ( -0.04967972022791889)) * f1( 0.011789762119665313)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.04967972022791889) - present_state_Q (-0.04967972022791889)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.101897721105946 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07226242805470508) - present_state_Q ( -0.08028888565804286)) * f1( 0.013941120655463496)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07226242805470508) - present_state_Q (-0.08028888565804286)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.129649105389218 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07437478259967856) - present_state_Q ( -0.07437478259967856)) * f1( 0.010150996928622103)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07437478259967856) - present_state_Q (-0.07437478259967856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.165865119295312 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013880507803764743) - present_state_Q ( -0.11897133668237393)) * f1( 0.013271804938452869)
w2 ( -9.785069581652936 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.013880507803764743) - present_state_Q (-0.11897133668237393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.22242966815951 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.0236340695553743) - present_state_Q ( -4.080769617238128)) * f1( 0.024042283963019835)
w2 ( -10.726154030222 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -2.0236340695553743) - present_state_Q (-4.080769617238128)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.245617936351914 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005181754197724947) - present_state_Q ( -2.257183941950498)) * f1( 0.009567958832193288)
w2 ( -11.695567406641795 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.005181754197724947) - present_state_Q (-2.257183941950498)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.273849791788063 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.795303174028646) - present_state_Q ( -4.785613043308923)) * f1( 0.013271589085336584)
w2 ( -12.54646341478723 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -4.795303174028646) - present_state_Q (-4.785613043308923)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.310802857501876 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.6557744530187097) - present_state_Q ( -2.6557744530187097)) * f1( 0.01658965184064032)
w2 ( -12.99195878827406 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -2.6557744530187097) - present_state_Q (-2.6557744530187097)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.34277221006384 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.7415019345849556) - present_state_Q ( -5.339893692239768)) * f1( 0.01631154148006998)
w2 ( -13.775927675605141 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -2.7415019345849556) - present_state_Q (-5.339893692239768)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.365389058868315 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-5.633398533591273) - present_state_Q ( -5.606460292744484)) * f1( 0.012089187665669878)
w2 ( -14.524260795412713 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -5.633398533591273) - present_state_Q (-5.606460292744484)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.430059139405486 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14292040856184826) - present_state_Q ( -0.2743712783266378)) * f1( 0.031165076112541074)
w2 ( -14.524260795412713 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.14292040856184826) - present_state_Q (-0.2743712783266378)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.466045254816555 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13854554786275322) - present_state_Q ( -0.15308761432183587)) * f1( 0.01724161333976332)
w2 ( -14.524260795412713 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.13854554786275322) - present_state_Q (-0.15308761432183587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.533490021542148 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1585985139190771) - present_state_Q ( -0.2750753531858263)) * f1( 0.03250086877636758)
w2 ( -14.524260795412713 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.1585985139190771) - present_state_Q (-0.2750753531858263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.571861649353309 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14251967039349914) - present_state_Q ( -0.16762969022891694)) * f1( 0.018397024050805007)
w2 ( -14.524260795412713 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.14251967039349914) - present_state_Q (-0.16762969022891694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.648356067656021 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17321003747234973) - present_state_Q ( -0.3203997792078068)) * f1( 0.036939873921316584)
w2 ( -14.524260795412713 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.17321003747234973) - present_state_Q (-0.3203997792078068)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.690097394557238 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.0720554023963627) - present_state_Q ( -3.097817769902214)) * f1( 0.02290926192026029)
w2 ( -14.888666484661936 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -3.0720554023963627) - present_state_Q (-3.097817769902214)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.722771034491238 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.121519693542775) - present_state_Q ( -3.1412642023975135)) * f1( 0.017970535294560323)
w2 ( -15.252302173843546 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -3.121519693542775) - present_state_Q (-3.1412642023975135)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.756489178509058 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.199690070452694) - present_state_Q ( -3.2245622980606155)) * f1( 0.01862232354714408)
w2 ( -15.614428241865713 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -3.199690070452694) - present_state_Q (-3.2245622980606155)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.78670711652903 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1476537095788716) - present_state_Q ( -0.1476537095788716)) * f1( 0.015135841860181914)
w2 ( -15.614428241865713 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.1476537095788716) - present_state_Q (-0.1476537095788716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.823240645495794 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1491487382783504) - present_state_Q ( -0.17198254774210936)) * f1( 0.018321443208525526)
w2 ( -15.614428241865713 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.1491487382783504) - present_state_Q (-0.17198254774210936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.866761601679938 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.157551954564931) - present_state_Q ( -0.18547533327183524)) * f1( 0.02183947222002109)
w2 ( -15.614428241865713 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.157551954564931) - present_state_Q (-0.18547533327183524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.907722410954545 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14699431802822807) - present_state_Q ( -0.17839746540293566)) * f1( 0.02054854215334266)
w2 ( -16.013101870086512 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.14699431802822807) - present_state_Q (-0.17839746540293566)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.944535047199652 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1718411967542228) - present_state_Q ( -0.1718411967542228)) * f1( 0.018459183037611297)
w2 ( -16.013101870086512 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.1718411967542228) - present_state_Q (-0.1718411967542228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.99470979336063 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.3185929341745517) - present_state_Q ( -3.49074957178106)) * f1( 0.029621736430232404)
w2 ( -16.35187165341204 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -3.3185929341745517) - present_state_Q (-3.49074957178106)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -9.049045254908775 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.4163992705712682) - present_state_Q ( -3.5713973502170964)) * f1( 0.032212875934111804)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -3.4163992705712682) - present_state_Q (-3.5713973502170964)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -9.120454477559887 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19058597292089993) - present_state_Q ( -0.34985612919589243)) * f1( 0.03787671867626049)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.19058597292089993) - present_state_Q (-0.34985612919589243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.162308890798164 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20604460424980453) - present_state_Q ( -0.20604460424980453)) * f1( 0.022030468172980412)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.20604460424980453) - present_state_Q (-0.20604460424980453)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.19930807046229 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16490671984552333) - present_state_Q ( -0.16490671984552333)) * f1( 0.01943699106075102)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.16490671984552333) - present_state_Q (-0.16490671984552333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.259925853083178 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16819633619175095) - present_state_Q ( -0.29609151080784935)) * f1( 0.032065113073019284)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.16819633619175095) - present_state_Q (-0.29609151080784935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.286754497272547 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1424395213248978) - present_state_Q ( -0.1424395213248978)) * f1( 0.01407908981257951)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.1424395213248978) - present_state_Q (-0.1424395213248978)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.324601838353825 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20187850733195947) - present_state_Q ( -0.20187850733195947)) * f1( 0.019917376811643798)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.20187850733195947) - present_state_Q (-0.20187850733195947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.357732114341607 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15381255647221287) - present_state_Q ( -0.17141630119059578)) * f1( 0.017411486726151514)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15381255647221287) - present_state_Q (-0.17141630119059578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.395987845758677 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15883307484784095) - present_state_Q ( -0.19462439192673184)) * f1( 0.020129170022219433)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15883307484784095) - present_state_Q (-0.19462439192673184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.415931663412104 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14671700854758432) - present_state_Q ( -0.14671700854758432)) * f1( 0.010468197216643035)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.14671700854758432) - present_state_Q (-0.14671700854758432)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.441437654423309 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14310471978085523) - present_state_Q ( -0.13149003723564706)) * f1( 0.01405043719433161)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14310471978085523) - present_state_Q (-0.13149003723564706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.488486683569679 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14797545202021942) - present_state_Q ( -0.2647881469259213)) * f1( 0.02610882574111253)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14797545202021942) - present_state_Q (-0.2647881469259213)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.534229853499214 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14552958405390687) - present_state_Q ( -0.25558672798066995)) * f1( 0.025371557167436613)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.14552958405390687) - present_state_Q (-0.25558672798066995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.549048106113451 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13467654368607684) - present_state_Q ( -0.09051939136016529)) * f1( 0.008144898474954834)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.13467654368607684) - present_state_Q (-0.09051939136016529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.564225687500883 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13239100149058153) - present_state_Q ( -0.09842672859334206)) * f1( 0.008787573920385483)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.13239100149058153) - present_state_Q (-0.09842672859334206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.587137803478292 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030797647409723503) - present_state_Q ( -0.14001904271140878)) * f1( 0.013305612332065717)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.030797647409723503) - present_state_Q (-0.14001904271140878)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.612374624542438 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01808118158811366) - present_state_Q ( -0.1202197007594176)) * f1( 0.014639872299792574)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.01808118158811366) - present_state_Q (-0.1202197007594176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.61563876757147 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05494129515348701) - present_state_Q ( -0.05494129515348701)) * f1( 0.0018859832787167885)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.05494129515348701) - present_state_Q (-0.05494129515348701)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.625515137929327 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08600848106921066) - present_state_Q ( -0.08600848106921066)) * f1( 0.005715683928215843)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08600848106921066) - present_state_Q (-0.08600848106921066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.636835365745846 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12056993403398264) - present_state_Q ( -0.030340135125181196)) * f1( 0.006528937753131979)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.12056993403398264) - present_state_Q (-0.030340135125181196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.654263694454372 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0975937034309897) - present_state_Q ( -0.03922495771588366)) * f1( 0.010058270031549134)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.0975937034309897) - present_state_Q (-0.03922495771588366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.670899724856316 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10340041655413171) - present_state_Q ( -0.033929675586870156)) * f1( 0.009597762597174611)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.10340041655413171) - present_state_Q (-0.033929675586870156)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.70541768460506 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08681970178489298) - present_state_Q ( -0.13264870212166624)) * f1( 0.020030312905292474)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08681970178489298) - present_state_Q (-0.13264870212166624)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.716344835897837 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13421732824357113) - present_state_Q ( -0.0031700524858720405)) * f1( 0.006641207054796049)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.13421732824357113) - present_state_Q (-0.0031700524858720405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.727182741034087 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1254838025000065) - present_state_Q ( -0.01374732201147116)) * f1( 0.0065915531151316655)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1254838025000065) - present_state_Q (-0.01374732201147116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.738375184743816 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1310179288297195) - present_state_Q ( -0.006546851464722617)) * f1( 0.006803972823003425)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1310179288297195) - present_state_Q (-0.006546851464722617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.74950938223236 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12215151029554117) - present_state_Q ( -0.01759168461167408)) * f1( 0.006773477458888962)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.12215151029554117) - present_state_Q (-0.01759168461167408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.7610966043257 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1269949271435243) - present_state_Q ( -0.01102576356699141)) * f1( 0.0070460524399407354)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.1269949271435243) - present_state_Q (-0.01102576356699141)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.7719276103517 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1332138461964879) - present_state_Q ( -0.00647003719076662)) * f1( 0.006971264604722075)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1332138461964879) - present_state_Q (-0.00647003719076662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.777920319835989 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09270164213682618) - present_state_Q ( -0.08681377715772567)) * f1( 0.0038782116789699223)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.09270164213682618) - present_state_Q (-0.08681377715772567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.791701383406739 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11173461597298195) - present_state_Q ( -0.02873347081453892)) * f1( 0.008883997162009388)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11173461597298195) - present_state_Q (-0.02873347081453892)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.805380532591172 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11936734954484703) - present_state_Q ( -0.023539999967127703)) * f1( 0.008814912844459398)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11936734954484703) - present_state_Q (-0.023539999967127703)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.817331802837101 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11348715843158277) - present_state_Q ( -0.03022429469210979)) * f1( 0.00770506976587007)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11348715843158277) - present_state_Q (-0.03022429469210979)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.829226431257094 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11565405825533887) - present_state_Q ( -0.02606386761750366)) * f1( 0.007666388950488696)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.11565405825533887) - present_state_Q (-0.02606386761750366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.840989996800761 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1190635830697418) - present_state_Q ( -0.021050991604972452)) * f1( 0.007579300238413243)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1190635830697418) - present_state_Q (-0.021050991604972452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.852568546245019 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1239279184577067) - present_state_Q ( -0.015288448729118774)) * f1( 0.007457091416182)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1239279184577067) - present_state_Q (-0.015288448729118774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.86391058169191 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1076255960869508) - present_state_Q ( -0.04910595634646843)) * f1( 0.0073214809366738565)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.1076255960869508) - present_state_Q (-0.04910595634646843)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.886957133587261 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14694181468225564) - present_state_Q ( -0.20876584371764986)) * f1( 0.015028019273672201)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.14694181468225564) - present_state_Q (-0.20876584371764986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.901823838564484 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10237952680237247) - present_state_Q ( -0.0374709005518221)) * f1( 0.009589838535383713)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.10237952680237247) - present_state_Q (-0.0374709005518221)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.91611300282734 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10764168701619324) - present_state_Q ( -0.03254456605402429)) * f1( 0.009214052398616232)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.10764168701619324) - present_state_Q (-0.03254456605402429)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.929783404365615 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09942288893360915) - present_state_Q ( -0.1048029032560957)) * f1( 0.008856793805889005)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.09942288893360915) - present_state_Q (-0.1048029032560957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.948982261776457 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12144286839456556) - present_state_Q ( -0.1339060879834867)) * f1( 0.012460292236469288)
w2 ( -16.68922409384164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.12144286839456556) - present_state_Q (-0.1339060879834867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.958736496841782 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12264008332867697) - present_state_Q ( -3.3563336943906084)) * f1( 0.008004641700907452)
w2 ( -16.932938564264923 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.12264008332867697) - present_state_Q (-3.3563336943906084)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -9.962467607910423 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.545950200425218) - present_state_Q ( -3.4204880108042306)) * f1( 0.0029935341296407993)
w2 ( -17.182216568594125 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -3.545950200425218) - present_state_Q (-3.4204880108042306)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -9.99072823346606 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.605087593572607) - present_state_Q ( -3.6803274460629263)) * f1( 0.023145523774968996)
w2 ( -17.42641605900445 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -3.605087593572607) - present_state_Q (-3.6803274460629263)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.01736995895786 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6985834361512535) - present_state_Q ( -3.6985834361512535)) * f1( 0.021835568198928214)
w2 ( -17.670437421298164 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -3.6985834361512535) - present_state_Q (-3.6985834361512535)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.021898200322122 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.707950210171076) - present_state_Q ( -3.582362698088769)) * f1( 0.003970501701603262)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -3.707950210171076) - present_state_Q (-3.582362698088769)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -10.029552176700573 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13459831128017702) - present_state_Q ( -0.04387261476990459)) * f1( 0.006461608393605473)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.13459831128017702) - present_state_Q (-0.04387261476990459)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.055554818889068 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1497663117043716) - present_state_Q ( -0.23820944970541202)) * f1( 0.02231509236838249)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.1497663117043716) - present_state_Q (-0.23820944970541202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.060819562090762 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16509598641717327) - present_state_Q ( -0.055358869010328375)) * f1( 0.004819711435400645)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.16509598641717327) - present_state_Q (-0.055358869010328375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.08983713250067 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20643562034087654) - present_state_Q ( -0.28476040772803773)) * f1( 0.02712406614009781)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.20643562034087654) - present_state_Q (-0.28476040772803773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.109711834316903 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10291807915451465) - present_state_Q ( -0.104839412356755)) * f1( 0.01828793224746991)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.10291807915451465) - present_state_Q (-0.104839412356755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.117982883805821 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12065186142334133) - present_state_Q ( -0.022565887866895718)) * f1( 0.007552293027640475)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12065186142334133) - present_state_Q (-0.022565887866895718)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.126464305259377 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12624810553785853) - present_state_Q ( -0.016942048701690707)) * f1( 0.007740013507395749)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12624810553785853) - present_state_Q (-0.016942048701690707)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.134749911521256 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13350618366399483) - present_state_Q ( -0.012273086598804715)) * f1( 0.007557594861914841)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13350618366399483) - present_state_Q (-0.012273086598804715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.140093385969214 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17237235083908814) - present_state_Q ( -0.056781355794465416)) * f1( 0.004892098677569157)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.17237235083908814) - present_state_Q (-0.056781355794465416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.15873278909438 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1495612740867042) - present_state_Q ( -0.2095363451502863)) * f1( 0.017310595504803943)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1495612740867042) - present_state_Q (-0.2095363451502863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.166223882275135 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14305676975495515) - present_state_Q ( -0.08019682798879288)) * f1( 0.006874886250852477)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14305676975495515) - present_state_Q (-0.08019682798879288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.180986172478478 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16061430585306416) - present_state_Q ( -0.1355747510950004)) * f1( 0.013614965691024567)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.16061430585306416) - present_state_Q (-0.1355747510950004)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.19116723736 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14777748485180028) - present_state_Q ( -0.11050008029918772)) * f1( 0.00936923505687279)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14777748485180028) - present_state_Q (-0.11050008029918772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.20537859648693 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034880090895703486) - present_state_Q ( -0.1096371795294664)) * f1( 0.013090718002406758)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.034880090895703486) - present_state_Q (-0.1096371795294664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.209109876989508 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06695125938475287) - present_state_Q ( -0.06695125938475287)) * f1( 0.0034225805624929673)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06695125938475287) - present_state_Q (-0.06695125938475287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.21624396836056 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09745943016424538) - present_state_Q ( -0.09745943016424538)) * f1( 0.006560389578079932)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.09745943016424538) - present_state_Q (-0.09745943016424538)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.225285011932865 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09000047826405023) - present_state_Q ( -0.061812825182464164)) * f1( 0.00828739287701601)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.09000047826405023) - present_state_Q (-0.061812825182464164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.24104348569986 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1345610550062478) - present_state_Q ( -0.2114378286682306)) * f1( 0.014639672996362959)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1345610550062478) - present_state_Q (-0.2114378286682306)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.251645063716921 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11486634917357401) - present_state_Q ( -0.03399584391608595)) * f1( 0.009690924053720355)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11486634917357401) - present_state_Q (-0.03399584391608595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.261725761397457 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12325200799153424) - present_state_Q ( -0.028234889981660002)) * f1( 0.00920923068513916)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12325200799153424) - present_state_Q (-0.028234889981660002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.271342862408371 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11421927157585268) - present_state_Q ( -0.03168074783718838)) * f1( 0.008789203604189319)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11421927157585268) - present_state_Q (-0.03168074783718838)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.280247929880819 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1201999113000385) - present_state_Q ( -0.02526013237536372)) * f1( 0.008133249110975242)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1201999113000385) - present_state_Q (-0.02526013237536372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.28914956004313 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053510636366985276) - present_state_Q ( -0.11177891549859097)) * f1( 0.008199899855736412)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.053510636366985276) - present_state_Q (-0.11177891549859097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.305142083949175 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08731704291076475) - present_state_Q ( -0.16703329934219863)) * f1( 0.014802539777345549)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.08731704291076475) - present_state_Q (-0.16703329934219863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.326770040227181 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14426339488836837) - present_state_Q ( -0.24182381323233756)) * f1( 0.02014749892482356)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14426339488836837) - present_state_Q (-0.24182381323233756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.354268713429839 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16185900776213438) - present_state_Q ( -0.29042998657931224)) * f1( 0.025728638376749872)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.16185900776213438) - present_state_Q (-0.29042998657931224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.368859014234744 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11951079240046042) - present_state_Q ( -0.12516179136560607)) * f1( 0.013448525981911753)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11951079240046042) - present_state_Q (-0.12516179136560607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.395109924378477 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13596973517233682) - present_state_Q ( -0.26558017801498207)) * f1( 0.024510141635668076)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13596973517233682) - present_state_Q (-0.26558017801498207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.410099688522044 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16129368374396588) - present_state_Q ( -0.14188897120365668)) * f1( 0.0138327291555434)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.16129368374396588) - present_state_Q (-0.14188897120365668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.42413915589315 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11066625605025973) - present_state_Q ( -0.118359851386982)) * f1( 0.012933743834526681)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11066625605025973) - present_state_Q (-0.118359851386982)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.435251946880214 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1602105729112976) - present_state_Q ( -0.1602105729112976)) * f1( 0.010272483876208533)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1602105729112976) - present_state_Q (-0.1602105729112976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.438997907352396 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10155092414596806) - present_state_Q ( -0.07901981926552866)) * f1( 0.0034387613907980053)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.10155092414596806) - present_state_Q (-0.07901981926552866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.447299523515474 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13698975678361122) - present_state_Q ( -0.012902355597860918)) * f1( 0.007572392086724163)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13698975678361122) - present_state_Q (-0.012902355597860918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.45542696587031 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12766199266165384) - present_state_Q ( -0.020202082106017916)) * f1( 0.007419089171573593)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12766199266165384) - present_state_Q (-0.020202082106017916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.463718766285782 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13399569369222458) - present_state_Q ( -0.014326917121694038)) * f1( 0.007564628128628352)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13399569369222458) - present_state_Q (-0.014326917121694038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.471839884143085 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13844882149940033) - present_state_Q ( -0.011396679797532592)) * f1( 0.007406633158811813)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13844882149940033) - present_state_Q (-0.011396679797532592)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.479828019040076 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13017406398893955) - present_state_Q ( -0.019279431740061713)) * f1( 0.007291141875468604)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13017406398893955) - present_state_Q (-0.019279431740061713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.487721204747139 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13562282972250267) - present_state_Q ( -0.012595730898000887)) * f1( 0.00719972706015497)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.13562282972250267) - present_state_Q (-0.012595730898000887)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.495509068022441 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14211905715519405) - present_state_Q ( -0.008214662602322658)) * f1( 0.007100399595160056)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14211905715519405) - present_state_Q (-0.008214662602322658)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.503239630235626 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10282944903903941) - present_state_Q ( -0.05137752131412792)) * f1( 0.00707854835177854)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.10282944903903941) - present_state_Q (-0.05137752131412792)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.520002892516283 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1499627502630628) - present_state_Q ( -0.22662043528081266)) * f1( 0.015592886988198931)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1499627502630628) - present_state_Q (-0.22662043528081266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.529639551029058 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1302720388629978) - present_state_Q ( -0.02484153159092869)) * f1( 0.008800285823292571)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1302720388629978) - present_state_Q (-0.02484153159092869)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.538884175965421 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12100499985001976) - present_state_Q ( -0.028508551833116454)) * f1( 0.008445820177819923)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12100499985001976) - present_state_Q (-0.028508551833116454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.548166925059151 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12822272116736602) - present_state_Q ( -0.0229040447349033)) * f1( 0.008475751442392047)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12822272116736602) - present_state_Q (-0.0229040447349033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.557123129268929 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1363461088220663) - present_state_Q ( -0.018686232562877516)) * f1( 0.008173840677829319)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1363461088220663) - present_state_Q (-0.018686232562877516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.569938667928476 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14612017308565242) - present_state_Q ( -0.11829486511922667)) * f1( 0.011802284100091955)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14612017308565242) - present_state_Q (-0.11829486511922667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.57765679676032 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15010036079109593) - present_state_Q ( -0.09493722524849459)) * f1( 0.0077423138590942955)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.15010036079109593) - present_state_Q (-0.09493722524849459)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.585452839100702 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06901746018328676) - present_state_Q ( -0.13442551624096663)) * f1( 0.007857990056838874)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.06901746018328676) - present_state_Q (-0.13442551624096663)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.60790958405654 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0754741394063157) - present_state_Q ( -0.2766643726000257)) * f1( 0.022962910035581877)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.0754741394063157) - present_state_Q (-0.2766643726000257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.633434061487186 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12545031031930245) - present_state_Q ( -0.2953169539025916)) * f1( 0.026136281253653953)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.12545031031930245) - present_state_Q (-0.2953169539025916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.654901805429883 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19086995066094675) - present_state_Q ( -0.22291374903464847)) * f1( 0.021806036317325927)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.19086995066094675) - present_state_Q (-0.22291374903464847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.687651535112426 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19798179292028198) - present_state_Q ( -0.37638466457774267)) * f1( 0.033790116429427586)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.19798179292028198) - present_state_Q (-0.37638466457774267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.709669704101069 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20281550212748073) - present_state_Q ( -0.2310038027343815)) * f1( 0.02238081093095394)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.20281550212748073) - present_state_Q (-0.2310038027343815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.742935662689227 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21932701019373887) - present_state_Q ( -0.3673012605154844)) * f1( 0.03428306456103841)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.21932701019373887) - present_state_Q (-0.3673012605154844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.762527187180865 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19088063097516397) - present_state_Q ( -0.22751325843056236)) * f1( 0.02194699130401312)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.19088063097516397) - present_state_Q (-0.22751325843056236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.778850696144792 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08562346698975454) - present_state_Q ( -0.16546384572735023)) * f1( 0.018181127533252624)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.08562346698975454) - present_state_Q (-0.16546384572735023)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.786017116396922 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14140997198894403) - present_state_Q ( -0.14140997198894403)) * f1( 0.007955702736039521)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.14140997198894403) - present_state_Q (-0.14140997198894403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.797944287177792 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1595282657968525) - present_state_Q ( -0.1477023685596475)) * f1( 0.013247373659794323)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1595282657968525) - present_state_Q (-0.1477023685596475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.80583156002323 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1531295234218229) - present_state_Q ( -0.11054540282523145)) * f1( 0.008724917227537514)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1531295234218229) - present_state_Q (-0.11054540282523145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.817420854720236 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15734186557498467) - present_state_Q ( -0.13483149458459245)) * f1( 0.012854035134566599)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15734186557498467) - present_state_Q (-0.13483149458459245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.82553328369359 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15640296207010587) - present_state_Q ( -0.11426838927147918)) * f1( 0.008977357528079807)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15640296207010587) - present_state_Q (-0.11426838927147918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.832658476998594 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16638813078436854) - present_state_Q ( -0.09803733922470877)) * f1( 0.007869859956063876)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.16638813078436854) - present_state_Q (-0.09803733922470877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.858186439022553 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20559668118446872) - present_state_Q ( -0.32867435057776034)) * f1( 0.028920125623291475)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.20559668118446872) - present_state_Q (-0.32867435057776034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.869352708088602 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15204294875397817) - present_state_Q ( -0.12931366645164874)) * f1( 0.01237799684830261)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.15204294875397817) - present_state_Q (-0.12931366645164874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.879913644364594 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14954736008736821) - present_state_Q ( -0.1203958818542249)) * f1( 0.011695736995058069)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.14954736008736821) - present_state_Q (-0.1203958818542249)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.886622167179468 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03436554406443896) - present_state_Q ( -0.11965243051873771)) * f1( 0.007438247142493183)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.03436554406443896) - present_state_Q (-0.11965243051873771)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.896552249568984 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06786818187263144) - present_state_Q ( -0.11260331496501744)) * f1( 0.010997553328992955)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06786818187263144) - present_state_Q (-0.11260331496501744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.902190441025205 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10115926833012956) - present_state_Q ( -0.10115926833012956)) * f1( 0.006234089952826469)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10115926833012956) - present_state_Q (-0.10115926833012956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.906404265054329 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07263131671645659) - present_state_Q ( -0.10652233332586626)) * f1( 0.004663417965320109)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07263131671645659) - present_state_Q (-0.10652233332586626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.91519462018867 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030716876683329477) - present_state_Q ( -0.14162132045140224)) * f1( 0.009770727625984238)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.030716876683329477) - present_state_Q (-0.14162132045140224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.9307610939856 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07374980054298153) - present_state_Q ( -0.14523904263579193)) * f1( 0.01730125620205314)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07374980054298153) - present_state_Q (-0.14523904263579193)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.936867691605219 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1080248961722935) - present_state_Q ( -0.1080248961722935)) * f1( 0.006756618009043504)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1080248961722935) - present_state_Q (-0.1080248961722935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.960700398947667 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21425512797423973) - present_state_Q ( -0.3113120927630563)) * f1( 0.02694396498395742)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.21425512797423973) - present_state_Q (-0.3113120927630563)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.967627178261361 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13744683692991908) - present_state_Q ( -0.017562141649942655)) * f1( 0.0075857080366382515)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.13744683692991908) - present_state_Q (-0.017562141649942655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.97441526417416 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1449829605972856) - present_state_Q ( -0.012113980859409186)) * f1( 0.007428775622166356)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.1449829605972856) - present_state_Q (-0.012113980859409186)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.981078782733277 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13514160305955863) - present_state_Q ( -0.020302813675320844)) * f1( 0.007299779116919865)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.13514160305955863) - present_state_Q (-0.020302813675320844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.984774946963917 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16819683243923667) - present_state_Q ( -0.056722425852144175)) * f1( 0.004517570334145348)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.16819683243923667) - present_state_Q (-0.056722425852144175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.988103646281907 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16939243760539482) - present_state_Q ( -0.05171130891382409)) * f1( 0.004065893273472709)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.16939243760539482) - present_state_Q (-0.05171130891382409)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.991575144238729 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19201057597386112) - present_state_Q ( -0.05635253020786717)) * f1( 0.004241549714985009)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.19201057597386112) - present_state_Q (-0.05635253020786717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.994846392758351 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16488995068577067) - present_state_Q ( -0.03341660946357231)) * f1( 0.003987028788691049)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.16488995068577067) - present_state_Q (-0.03341660946357231)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.001424811087407 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1403545013528415) - present_state_Q ( -0.018137881339562473)) * f1( 0.008005325298475936)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1403545013528415) - present_state_Q (-0.018137881339562473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.007810429223667 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13051916589224874) - present_state_Q ( -0.02507149409000194)) * f1( 0.007778199190089529)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13051916589224874) - present_state_Q (-0.02507149409000194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.014202361633634 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07253344088848072) - present_state_Q ( -0.14866899443760595)) * f1( 0.007910572884693372)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07253344088848072) - present_state_Q (-0.14866899443760595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.025238869495576 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11178219161141834) - present_state_Q ( -0.06113720248826879)) * f1( 0.013505773504503467)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11178219161141834) - present_state_Q (-0.06113720248826879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.032953911941396 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12341603705039529) - present_state_Q ( -0.037223734374407416)) * f1( 0.00941229231896367)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12341603705039529) - present_state_Q (-0.037223734374407416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.040557923172544 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13248849054959033) - present_state_Q ( -0.03096047295588146)) * f1( 0.009268726844914907)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13248849054959033) - present_state_Q (-0.03096047295588146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.04710070126111 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1260202449593903) - present_state_Q ( -0.03351745741695364)) * f1( 0.007978277878061267)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1260202449593903) - present_state_Q (-0.03351745741695364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.053565416430335 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12924326981335243) - present_state_Q ( -0.028678695947377295)) * f1( 0.007878129751735628)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12924326981335243) - present_state_Q (-0.028678695947377295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.059923381216043 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13378128256200789) - present_state_Q ( -0.022929179050042887)) * f1( 0.007742187201979762)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13378128256200789) - present_state_Q (-0.022929179050042887)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.066156007006393 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13991169856759) - present_state_Q ( -0.01659914486020401)) * f1( 0.0075831487506878015)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13991169856759) - present_state_Q (-0.01659914486020401)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.072261560877422 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14798764040257398) - present_state_Q ( -0.011929668830091713)) * f1( 0.007423595130333895)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14798764040257398) - present_state_Q (-0.011929668830091713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.078262142097106 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13814573949742165) - present_state_Q ( -0.018481888806023375)) * f1( 0.007302653093303916)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13814573949742165) - present_state_Q (-0.018481888806023375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.084462042454714 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16456361793433724) - present_state_Q ( -0.09737573623642547)) * f1( 0.007615896731733334)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.16456361793433724) - present_state_Q (-0.09737573623642547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.107963348300814 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2016074282562255) - present_state_Q ( -0.33516730807672146)) * f1( 0.02972347305470521)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.2016074282562255) - present_state_Q (-0.33516730807672146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.117980165077805 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15389877197743376) - present_state_Q ( -0.15389877197743376)) * f1( 0.012392224963966974)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.15389877197743376) - present_state_Q (-0.15389877197743376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.128680395861918 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0784254632815148) - present_state_Q ( -0.0784254632815148)) * f1( 0.013127390331774371)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0784254632815148) - present_state_Q (-0.0784254632815148)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.13644257643657 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11438583370493817) - present_state_Q ( -0.0430916706384378)) * f1( 0.009477629482733911)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11438583370493817) - present_state_Q (-0.0430916706384378)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.143940631332633 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11940735268363097) - present_state_Q ( -0.038094960602132)) * f1( 0.009148989054134388)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11940735268363097) - present_state_Q (-0.038094960602132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.151182787967045 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12506543993913719) - present_state_Q ( -0.03259874874841735)) * f1( 0.00883021518515185)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.12506543993913719) - present_state_Q (-0.03259874874841735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.158176020299587 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13166291292922994) - present_state_Q ( -0.026681149195431582)) * f1( 0.008519874195340945)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13166291292922994) - present_state_Q (-0.026681149195431582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.164931674960425 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13955880899254525) - present_state_Q ( -0.02100007645819954)) * f1( 0.008223949539047181)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13955880899254525) - present_state_Q (-0.02100007645819954)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.171464094156093 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1295598793197814) - present_state_Q ( -0.02834755890597153)) * f1( 0.007960284325730033)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1295598793197814) - present_state_Q (-0.02834755890597153)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.178123783952048 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13597210636083581) - present_state_Q ( -0.022373777616927534)) * f1( 0.008108837438556974)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13597210636083581) - present_state_Q (-0.022373777616927534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.184593305742132 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14395987630323506) - present_state_Q ( -0.017000328021260537)) * f1( 0.007871373324074223)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14395987630323506) - present_state_Q (-0.017000328021260537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.19088837215352 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13395370309685253) - present_state_Q ( -0.024245354614287305)) * f1( 0.00766680760905633)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13395370309685253) - present_state_Q (-0.024245354614287305)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.197300277071141 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14069540747962556) - present_state_Q ( -0.018131132269221324)) * f1( 0.0078026551345430515)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14069540747962556) - present_state_Q (-0.018131132269221324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.203447626227604 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07560950089107248) - present_state_Q ( -0.14834094687883603)) * f1( 0.007607281366270664)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07560950089107248) - present_state_Q (-0.14834094687883603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.214274851987854 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11003736362189313) - present_state_Q ( -0.05988532595523937)) * f1( 0.01324792076734744)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11003736362189313) - present_state_Q (-0.05988532595523937)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.221781004360745 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11517145403939631) - present_state_Q ( -0.07542488021716884)) * f1( 0.009201256081724986)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11517145403939631) - present_state_Q (-0.07542488021716884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.232428952703424 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14160421398983047) - present_state_Q ( -0.21912911037256644)) * f1( 0.013282231120929452)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14160421398983047) - present_state_Q (-0.21912911037256644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.238082322863889 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08337127877633219) - present_state_Q ( -0.14211337345194064)) * f1( 0.006989929217785358)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08337127877633219) - present_state_Q (-0.14211337345194064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.248432706152702 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11942003389536948) - present_state_Q ( -0.05280874469895164)) * f1( 0.01265206074753197)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11942003389536948) - present_state_Q (-0.05280874469895164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.255641709840706 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1319467632039658) - present_state_Q ( -0.03062846814263504)) * f1( 0.008786944312330876)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.1319467632039658) - present_state_Q (-0.03062846814263504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.262768110057019 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14123141153043706) - present_state_Q ( -0.025235403126202045)) * f1( 0.008679572406310787)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.14123141153043706) - present_state_Q (-0.025235403126202045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.269617057664757 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13130384658545405) - present_state_Q ( -0.028726682120467063)) * f1( 0.008346208169174942)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13130384658545405) - present_state_Q (-0.028726682120467063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.275721651927968 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1392229563131421) - present_state_Q ( -0.022864270865265733)) * f1( 0.008363379006859763)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1392229563131421) - present_state_Q (-0.022864270865265733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.281621121766046 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1446073222540482) - present_state_Q ( -0.01914403752117605)) * f1( 0.00807764284293452)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1446073222540482) - present_state_Q (-0.01914403752117605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.287359186777485 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13582239352075368) - present_state_Q ( -0.023906632885211408)) * f1( 0.007862717981976063)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13582239352075368) - present_state_Q (-0.023906632885211408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.292971681814674 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1422937448667061) - present_state_Q ( -0.017585079991640253)) * f1( 0.0076833159790207885)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1422937448667061) - present_state_Q (-0.017585079991640253)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.298459806734915 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15069672993946354) - present_state_Q ( -0.01304621998733718)) * f1( 0.007507529034254401)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.15069672993946354) - present_state_Q (-0.01304621998733718)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.304059874944455 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1417476167262313) - present_state_Q ( -0.018752032349992037)) * f1( 0.007667586308268057)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1417476167262313) - present_state_Q (-0.018752032349992037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.309654822145287 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1492710394923739) - present_state_Q ( -0.014631384206403673)) * f1( 0.007655466863069915)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1492710394923739) - present_state_Q (-0.014631384206403673)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.315159577109062 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13750725123208535) - present_state_Q ( -0.02370066879454821)) * f1( 0.007542632412967816)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13750725123208535) - present_state_Q (-0.02370066879454821)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.32058283531772 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1430745994697186) - present_state_Q ( -0.016983114871707995)) * f1( 0.0074235660488453335)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1430745994697186) - present_state_Q (-0.016983114871707995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.325914894643216 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1506754711605509) - present_state_Q ( -0.011096928129619814)) * f1( 0.00729209536949414)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1506754711605509) - present_state_Q (-0.011096928129619814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.33116239350915 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14047752001324326) - present_state_Q ( -0.020112710908498494)) * f1( 0.007186313873977143)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14047752001324326) - present_state_Q (-0.020112710908498494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.336537935613007 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14712914673221156) - present_state_Q ( -0.01339280024359029)) * f1( 0.0073542279530284214)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14712914673221156) - present_state_Q (-0.01339280024359029)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.34182502766153 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15060678951099943) - present_state_Q ( -0.01081206253419477)) * f1( 0.007230323477710378)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.15060678951099943) - present_state_Q (-0.01081206253419477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.347026808517423 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14283253361006437) - present_state_Q ( -0.019364066887519283)) * f1( 0.007122744266424294)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14283253361006437) - present_state_Q (-0.019364066887519283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.361931199114256 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16715675447928327) - present_state_Q ( -0.3034485522211527)) * f1( 0.02122707688559504)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.16715675447928327) - present_state_Q (-0.3034485522211527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.367896778279173 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.155893147960793) - present_state_Q ( -0.10952919505241326)) * f1( 0.008269220060974733)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.155893147960793) - present_state_Q (-0.10952919505241326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.376329104475689 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15290265198275904) - present_state_Q ( -0.12621618813063856)) * f1( 0.011716100803766736)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.15290265198275904) - present_state_Q (-0.12621618813063856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.384283183643507 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1429594485131154) - present_state_Q ( -0.11749112800087402)) * f1( 0.011039753391851218)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1429594485131154) - present_state_Q (-0.11749112800087402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.39266482794208 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049359922473411984) - present_state_Q ( -0.11626625569818301)) * f1( 0.011646336304706106)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.049359922473411984) - present_state_Q (-0.11626625569818301)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.39579355189967 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10233963364970385) - present_state_Q ( -0.10233963364970385)) * f1( 0.004335795383615403)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10233963364970385) - present_state_Q (-0.10233963364970385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.404293319211192 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11938974572582985) - present_state_Q ( -0.11298846204578088)) * f1( 0.011793621405495778)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11938974572582985) - present_state_Q (-0.11298846204578088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.413882448725932 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03913916099837441) - present_state_Q ( -0.16590081492809713)) * f1( 0.013418593318269499)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.03913916099837441) - present_state_Q (-0.16590081492809713)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.424175014406813 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08680241297283361) - present_state_Q ( -0.24154025566058557)) * f1( 0.014547224478050527)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.08680241297283361) - present_state_Q (-0.24154025566058557)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.425711569606879 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06863559732744492) - present_state_Q ( -0.06863559732744492)) * f1( 0.002120449356691324)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.06863559732744492) - present_state_Q (-0.06863559732744492)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.430044698553996 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10642648902549913) - present_state_Q ( -0.10642648902549913)) * f1( 0.006007925932585053)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10642648902549913) - present_state_Q (-0.10642648902549913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.43353494237149 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18035626148964798) - present_state_Q ( -0.062607743977353)) * f1( 0.004805138252731598)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.18035626148964798) - present_state_Q (-0.062607743977353)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.436658360808948 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18491389246598) - present_state_Q ( -0.057131073185851367)) * f1( 0.004296607978034602)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.18491389246598) - present_state_Q (-0.057131073185851367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.452940437422994 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10451780566232147) - present_state_Q ( -0.14111493750013687)) * f1( 0.022684965138457885)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10451780566232147) - present_state_Q (-0.14111493750013687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.45863696835313 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14362685150065319) - present_state_Q ( -0.019690663173871872)) * f1( 0.007800464381102407)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14362685150065319) - present_state_Q (-0.019690663173871872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.464304472386699 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1496245432060322) - present_state_Q ( -0.016011394605447354)) * f1( 0.0077561721266386574)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1496245432060322) - present_state_Q (-0.016011394605447354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.470095855485782 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14815047131505835) - present_state_Q ( -0.017841639826513992)) * f1( 0.007927850503759507)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14815047131505835) - present_state_Q (-0.017841639826513992)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.475726745553654 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1379257371633607) - present_state_Q ( -0.02428821983768172)) * f1( 0.0077160400720554115)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1379257371633607) - present_state_Q (-0.02428821983768172)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.481353770677412 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06108729999365113) - present_state_Q ( -0.12078136068113303)) * f1( 0.007822412218047683)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.06108729999365113) - present_state_Q (-0.12078136068113303)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.49185184701233 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10100153654389801) - present_state_Q ( -0.18690694216227025)) * f1( 0.01472106126221562)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.10100153654389801) - present_state_Q (-0.18690694216227025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.49841387667752 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11618890773827861) - present_state_Q ( -0.05462409389617165)) * f1( 0.00903222262667079)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11618890773827861) - present_state_Q (-0.05462409389617165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.500660964603087 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11305139964474413) - present_state_Q ( -0.08973564374836332)) * f1( 0.003108131183731339)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11305139964474413) - present_state_Q (-0.08973564374836332)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.506363280889886 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15068872232585132) - present_state_Q ( -0.016456715534756187)) * f1( 0.007804175837710631)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.15068872232585132) - present_state_Q (-0.016456715534756187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.511923427483525 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1404728652391825) - present_state_Q ( -0.022267663777506844)) * f1( 0.00761672514172853)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1404728652391825) - present_state_Q (-0.022267663777506844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.517818662174939 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14144679516852088) - present_state_Q ( -0.023178790281171258)) * f1( 0.008076655432137758)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14144679516852088) - present_state_Q (-0.023178790281171258)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.523677044840303 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1496701133707745) - present_state_Q ( -0.018780638742422494)) * f1( 0.00802043061434912)
w2 ( -17.898531586951496 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1496701133707745) - present_state_Q (-0.018780638742422494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.529407454450993 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13755933325056757) - present_state_Q ( -0.02594536382622271)) * f1( 0.007854235226461666)
w2 ( -18.044450557938855 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.13755933325056757) - present_state_Q (-0.02594536382622271)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.532247423609027 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14357827799635856) - present_state_Q ( -3.6283764970857475)) * f1( 0.007687811137821944)
w2 ( -18.118332944150517 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.14357827799635856) - present_state_Q (-3.6283764970857475)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.543613460329196 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15603691758419924) - present_state_Q ( -0.22063201204817637)) * f1( 0.01600149408483405)
w2 ( -18.118332944150517 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.15603691758419924) - present_state_Q (-0.22063201204817637)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.555713308594184 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16096084450753406) - present_state_Q ( -0.2653428869834408)) * f1( 0.019689360489863753)
w2 ( -18.241240422747573 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.16096084450753406) - present_state_Q (-0.2653428869834408)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.560639381577417 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8539738532311567) - present_state_Q ( -3.8539738532311567)) * f1( 0.016835264739622643)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -3.8539738532311567) - present_state_Q (-3.8539738532311567)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.561695763432395 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08405952570097165) - present_state_Q ( -0.10359452684887074)) * f1( 0.0019613786561957)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08405952570097165) - present_state_Q (-0.10359452684887074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.566582577363471 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12389155523107426) - present_state_Q ( -0.04004878576452677)) * f1( 0.00896096863067582)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12389155523107426) - present_state_Q (-0.04004878576452677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.571331115632045 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12880449490111223) - present_state_Q ( -0.034869687877053406)) * f1( 0.008698367636481361)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12880449490111223) - present_state_Q (-0.034869687877053406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.575169558575503 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13464587088835778) - present_state_Q ( -0.029033453969430457)) * f1( 0.008432399595619307)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13464587088835778) - present_state_Q (-0.029033453969430457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.578892959135851 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1417373122197675) - present_state_Q ( -0.022842675970628338)) * f1( 0.008167291313520762)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1417373122197675) - present_state_Q (-0.022842675970628338)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.582506748955607 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15048239735762475) - present_state_Q ( -0.017802998481987034)) * f1( 0.007916589772571009)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15048239735762475) - present_state_Q (-0.017802998481987034)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.586020379069264 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14016229122134674) - present_state_Q ( -0.023666734911198202)) * f1( 0.007708818905178988)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14016229122134674) - present_state_Q (-0.023666734911198202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.589579275155309 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14771908863318317) - present_state_Q ( -0.017577290745044716)) * f1( 0.007796422651324764)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14771908863318317) - present_state_Q (-0.017577290745044716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.593041175493346 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13721022607662534) - present_state_Q ( -0.027025232774173073)) * f1( 0.007601418439442095)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13721022607662534) - present_state_Q (-0.027025232774173073)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.596599939141061 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1433599850396332) - present_state_Q ( -0.02064483144045632)) * f1( 0.007802120616953642)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1433599850396332) - present_state_Q (-0.02064483144045632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.600077255886918 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15132387039190923) - present_state_Q ( -0.014904295832149462)) * f1( 0.007612649060767433)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15132387039190923) - present_state_Q (-0.014904295832149462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.603473682236407 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14086874325706306) - present_state_Q ( -0.023351582110509115)) * f1( 0.00745104602079899)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14086874325706306) - present_state_Q (-0.023351582110509115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.60694746933811 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14765436315896394) - present_state_Q ( -0.01680520332924167)) * f1( 0.007608699471620565)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14765436315896394) - present_state_Q (-0.01680520332924167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.610349458377224 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15430244317362415) - present_state_Q ( -0.012817789041088518)) * f1( 0.007443853687480087)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15430244317362415) - present_state_Q (-0.012817789041088518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.613700855861907 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14306799025963302) - present_state_Q ( -0.02289072533289694)) * f1( 0.007351164259151613)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14306799025963302) - present_state_Q (-0.02289072533289694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.617015372911105 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14870048503753702) - present_state_Q ( -0.015634303904268745)) * f1( 0.007257819687845394)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.14870048503753702) - present_state_Q (-0.015634303904268745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.62018102468336 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08338148771855275) - present_state_Q ( -0.14897990860200003)) * f1( 0.00715087299667099)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.08338148771855275) - present_state_Q (-0.14897990860200003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.62598332869569 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12951491403905666) - present_state_Q ( -0.05607160838556502)) * f1( 0.012824284363899157)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12951491403905666) - present_state_Q (-0.05607160838556502)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.630262238060508 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12312529447452872) - present_state_Q ( -0.04139228660902815)) * f1( 0.00942801201988268)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12312529447452872) - present_state_Q (-0.04139228660902815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.63437882927098 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12954706819886028) - present_state_Q ( -0.035338626254899995)) * f1( 0.00905700323654596)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.12954706819886028) - present_state_Q (-0.035338626254899995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.640989286118861 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15670364357822794) - present_state_Q ( -0.21197431223527413)) * f1( 0.015122466155290096)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15670364357822794) - present_state_Q (-0.21197431223527413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.648891355907756 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13645296668738016) - present_state_Q ( -0.24488189061524468)) * f1( 0.018222861251029107)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13645296668738016) - present_state_Q (-0.24488189061524468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.65499888174011 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18130079632649682) - present_state_Q ( -0.15837146654741552)) * f1( 0.01379500815631955)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.18130079632649682) - present_state_Q (-0.15837146654741552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.658891276408076 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16705854592177682) - present_state_Q ( -0.16705854592177682)) * f1( 0.00881183805962288)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.16705854592177682) - present_state_Q (-0.16705854592177682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.662299313562121 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17041413669139507) - present_state_Q ( -0.10249844439191176)) * f1( 0.007603612026520407)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.17041413669139507) - present_state_Q (-0.10249844439191176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.674072501714166 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15582073176125416) - present_state_Q ( -0.3340257398269914)) * f1( 0.027707209056809956)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15582073176125416) - present_state_Q (-0.3340257398269914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.68362628019805 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2094877877119457) - present_state_Q ( -0.24801925623356633)) * f1( 0.022010698756178577)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2094877877119457) - present_state_Q (-0.24801925623356633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.688593902059957 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1898745424155539) - present_state_Q ( -0.1898745424155539)) * f1( 0.011298525765085972)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1898745424155539) - present_state_Q (-0.1898745424155539)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.696940658983602 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24406677370833346) - present_state_Q ( -0.24406677370833346)) * f1( 0.019197099152127965)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.24406677370833346) - present_state_Q (-0.24406677370833346)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.706579085033214 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13300231965508225) - present_state_Q ( -0.2582128115355874)) * f1( 0.02229737151707596)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13300231965508225) - present_state_Q (-0.2582128115355874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.716146128266704 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13929663057975755) - present_state_Q ( -0.24768225687918102)) * f1( 0.022075243353249997)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13929663057975755) - present_state_Q (-0.24768225687918102)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.727276453889521 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08317760474695937) - present_state_Q ( -0.24669029214956362)) * f1( 0.02570980870741338)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.08317760474695937) - present_state_Q (-0.24669029214956362)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.730455793862957 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09918221943471892) - present_state_Q ( -0.09918221943471892)) * f1( 0.007099399737451442)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.09918221943471892) - present_state_Q (-0.09918221943471892)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.740089732517726 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15580535181083302) - present_state_Q ( -0.31859842553026757)) * f1( 0.02259065366226232)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15580535181083302) - present_state_Q (-0.31859842553026757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.744012960068689 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16603668649474115) - present_state_Q ( -0.16603668649474115)) * f1( 0.008879790618356498)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.16603668649474115) - present_state_Q (-0.16603668649474115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.756120713377824 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19213585251034995) - present_state_Q ( -0.36636212050947004)) * f1( 0.02868838296781024)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.19213585251034995) - present_state_Q (-0.36636212050947004)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.76381316726985 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1964774819134881) - present_state_Q ( -0.2192298232195408)) * f1( 0.01761091245805559)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.1964774819134881) - present_state_Q (-0.2192298232195408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.771616079860314 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20260417424869653) - present_state_Q ( -0.22360197243594604)) * f1( 0.017879182214322224)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.20260417424869653) - present_state_Q (-0.22360197243594604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.785877749462658 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.237856878270944) - present_state_Q ( -0.4016114559108402)) * f1( 0.03403934355371457)
w2 ( -18.299761308037123 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.237856878270944) - present_state_Q (-0.4016114559108402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.787905313268777 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8417291739967445) - present_state_Q ( -3.887147424819071)) * f1( 0.019045101129217423)
w2 ( -18.3210535426371 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.8417291739967445) - present_state_Q (-3.887147424819071)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.79001877112449 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8211231460570145) - present_state_Q ( -3.889875628543522)) * f1( 0.019941612970339577)
w2 ( -18.342250001106706 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.8211231460570145) - present_state_Q (-3.889875628543522)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.791964418004307 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8656585046123997) - present_state_Q ( -3.901260733625618)) * f1( 0.018479084968795755)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -3.8656585046123997) - present_state_Q (-3.901260733625618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -11.798341863774622 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16679063996875548) - present_state_Q ( -0.2099146529390322)) * f1( 0.018427485612239926)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16679063996875548) - present_state_Q (-0.2099146529390322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.801086881461163 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14096158895612543) - present_state_Q ( -0.029064621000187193)) * f1( 0.0075431213442695546)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14096158895612543) - present_state_Q (-0.029064621000187193)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.803806145079536 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14531138167546315) - present_state_Q ( -0.022574199366887877)) * f1( 0.00745815765307833)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14531138167546315) - present_state_Q (-0.022574199366887877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.808391038126487 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18094792551992314) - present_state_Q ( -0.18094792551992314)) * f1( 0.013132654166318097)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18094792551992314) - present_state_Q (-0.18094792551992314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.811171235438565 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06283315552916759) - present_state_Q ( -0.12467572585260532)) * f1( 0.007863268173596746)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06283315552916759) - present_state_Q (-0.12467572585260532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.816281708801117 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10358643249763778) - present_state_Q ( -0.19207566393588246)) * f1( 0.014717613328982776)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10358643249763778) - present_state_Q (-0.19207566393588246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.825083032008427 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17799876549061336) - present_state_Q ( -0.23856983668331092)) * f1( 0.025635178092312)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17799876549061336) - present_state_Q (-0.23856983668331092)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.833455485667708 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22179981776272387) - present_state_Q ( -0.3077966442112461)) * f1( 0.024855491024556524)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.22179981776272387) - present_state_Q (-0.3077966442112461)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.840883670619212 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.236792056968116) - present_state_Q ( -0.2608389983112711)) * f1( 0.02173948595804253)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.236792056968116) - present_state_Q (-0.2608389983112711)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.84825380269643 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17387493982444574) - present_state_Q ( -0.25038026653519524)) * f1( 0.021543314468624956)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.17387493982444574) - present_state_Q (-0.25038026653519524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.853350290436454 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10532077948060402) - present_state_Q ( -0.10532077948060402)) * f1( 0.014318871589380888)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10532077948060402) - present_state_Q (-0.10532077948060402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.856937756084193 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1337985916350783) - present_state_Q ( -0.04398286182955576)) * f1( 0.009900646674604725)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1337985916350783) - present_state_Q (-0.04398286182955576)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.859883414497766 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14769766012529403) - present_state_Q ( -0.022773307707284664)) * f1( 0.008079005863688673)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14769766012529403) - present_state_Q (-0.022773307707284664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.862798618474894 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15212327309821835) - present_state_Q ( -0.019275932724749464)) * f1( 0.007986848429023315)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15212327309821835) - present_state_Q (-0.019275932724749464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.865630198110605 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14408112037297222) - present_state_Q ( -0.02388239847263504)) * f1( 0.007769257945173106)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14408112037297222) - present_state_Q (-0.02388239847263504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.868318535935158 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062266754464169405) - present_state_Q ( -0.12316373548872975)) * f1( 0.007600332353547532)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.062266754464169405) - present_state_Q (-0.12316373548872975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.873465477325734 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1055231977094531) - present_state_Q ( -0.1954266919660474)) * f1( 0.014836127567643171)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1055231977094531) - present_state_Q (-0.1954266919660474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.876709104466496 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12028110672182847) - present_state_Q ( -0.057951374833899616)) * f1( 0.00898973430477622)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12028110672182847) - present_state_Q (-0.057951374833899616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.879320708340614 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15219233434601326) - present_state_Q ( -0.15219233434601326)) * f1( 0.007425455422664634)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15219233434601326) - present_state_Q (-0.15219233434601326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.881684403168528 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16487965974002497) - present_state_Q ( -0.0031310369573245823)) * f1( 0.006445105859398034)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.16487965974002497) - present_state_Q (-0.0031310369573245823)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.884728237397255 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15106648944577517) - present_state_Q ( -0.023021571442532314)) * f1( 0.008348068053162256)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.15106648944577517) - present_state_Q (-0.023021571442532314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.888897417736889 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09567476382800028) - present_state_Q ( -0.14243661976348326)) * f1( 0.01184022640383089)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09567476382800028) - present_state_Q (-0.14243661976348326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.892282063464375 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1304106467852291) - present_state_Q ( -0.14988858252671003)) * f1( 0.009623066828029365)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1304106467852291) - present_state_Q (-0.14988858252671003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.895254328489926 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14336811310747538) - present_state_Q ( -0.026159091611354067)) * f1( 0.00816052635229306)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.14336811310747538) - present_state_Q (-0.026159091611354067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.898222211175346 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1524253929666898) - present_state_Q ( -0.02074020052646278)) * f1( 0.008134369368278396)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1524253929666898) - present_state_Q (-0.02074020052646278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.904631543209328 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2116541211723427) - present_state_Q ( -0.3210788274414426)) * f1( 0.026260884141437128)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2116541211723427) - present_state_Q (-0.3210788274414426)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.90585762473336 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1814533878725125) - present_state_Q ( -0.022943500034044804)) * f1( 0.004481695765489631)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1814533878725125) - present_state_Q (-0.022943500034044804)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.908036072814243 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06965731548150494) - present_state_Q ( -0.169717424433624)) * f1( 0.008450803161598042)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06965731548150494) - present_state_Q (-0.169717424433624)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.911852547389655 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11694815733739278) - present_state_Q ( -0.0749490988989187)) * f1( 0.014254951619869128)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.11694815733739278) - present_state_Q (-0.0749490988989187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.91358700861438 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15226018486946716) - present_state_Q ( -0.021950636441079165)) * f1( 0.006344443689151045)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15226018486946716) - present_state_Q (-0.021950636441079165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.915426486988833 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1570874399425917) - present_state_Q ( -0.012937436600322278)) * f1( 0.006705292599637529)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1570874399425917) - present_state_Q (-0.012937436600322278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.917266339204593 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16465251017471158) - present_state_Q ( -0.0047752723710773855)) * f1( 0.0066849223367865035)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.16465251017471158) - present_state_Q (-0.0047752723710773855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.919034790368928 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08832916471870363) - present_state_Q ( -0.10641986395857225)) * f1( 0.006691164285288414)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08832916471870363) - present_state_Q (-0.10641986395857225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.922366131620214 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1415397700484865) - present_state_Q ( -0.1581733210184515)) * f1( 0.012829962196087119)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1415397700484865) - present_state_Q (-0.1581733210184515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.924668471799936 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14439293675390194) - present_state_Q ( -0.025950016920034298)) * f1( 0.00843644402381671)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14439293675390194) - present_state_Q (-0.025950016920034298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.926895577728569 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15372314305707183) - present_state_Q ( -0.020795321915300587)) * f1( 0.008142600018281133)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15372314305707183) - present_state_Q (-0.020795321915300587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.92905108938358 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14311616317129108) - present_state_Q ( -0.025760694882728884)) * f1( 0.007898243197427032)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14311616317129108) - present_state_Q (-0.025760694882728884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.931228050080303 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15116609582366924) - present_state_Q ( -0.019638813590273127)) * f1( 0.007956641791088044)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15116609582366924) - present_state_Q (-0.019638813590273127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.934560656252494 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04613126862811694) - present_state_Q ( -0.12739487128877278)) * f1( 0.012730706573551387)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04613126862811694) - present_state_Q (-0.12739487128877278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.935584679851884 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10228149695019062) - present_state_Q ( -0.10228149695019062)) * f1( 0.0038664308849420117)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.10228149695019062) - present_state_Q (-0.10228149695019062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.936367633478403 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10505764966631365) - present_state_Q ( -0.10343517083809294)) * f1( 0.002957195275558522)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.10505764966631365) - present_state_Q (-0.10343517083809294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.938729963302247 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14184572330033593) - present_state_Q ( -0.028798181186525856)) * f1( 0.008666116793817303)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14184572330033593) - present_state_Q (-0.028798181186525856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.941008717887609 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15102267124443178) - present_state_Q ( -0.02313102560957385)) * f1( 0.00833937938942481)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15102267124443178) - present_state_Q (-0.02313102560957385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.943205642081406 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1403663199990452) - present_state_Q ( -0.028667552773853723)) * f1( 0.008059383736776553)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1403663199990452) - present_state_Q (-0.028667552773853723)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.945428727014109 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14809615763118592) - present_state_Q ( -0.02234857083410803)) * f1( 0.008134191478977584)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14809615763118592) - present_state_Q (-0.02234857083410803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.947587944044804 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15585361273237017) - present_state_Q ( -0.018015185151645588)) * f1( 0.007885759271891894)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15585361273237017) - present_state_Q (-0.018015185151645588)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.949697980112832 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14387770601233965) - present_state_Q ( -0.025811101291471267)) * f1( 0.0077315382410609226)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14387770601233965) - present_state_Q (-0.025811101291471267)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.9516625299003 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08296246089756301) - present_state_Q ( -0.1576236082264721)) * f1( 0.007581550260150582)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08296246089756301) - present_state_Q (-0.1576236082264721)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.955210530458661 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12118589737984582) - present_state_Q ( -0.06287433127776042)) * f1( 0.013190593476822228)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.12118589737984582) - present_state_Q (-0.06287433127776042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.961811839007174 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24491653152619197) - present_state_Q ( -0.33737664308688314)) * f1( 0.027191988116446776)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.24491653152619197) - present_state_Q (-0.33737664308688314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.963242446646252 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21333312915873373) - present_state_Q ( -0.07240190950007967)) * f1( 0.005319266050341952)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.21333312915873373) - present_state_Q (-0.07240190950007967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.964641060908358 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17814632153120052) - present_state_Q ( -0.012781232183045893)) * f1( 0.005094048042792351)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.17814632153120052) - present_state_Q (-0.012781232183045893)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.970003100617399 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14523312745831957) - present_state_Q ( -0.2848286408759587)) * f1( 0.02170649728689079)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14523312745831957) - present_state_Q (-0.2848286408759587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.976946272228501 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18223561297481836) - present_state_Q ( -0.35780385937306786)) * f1( 0.028918176546281517)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.18223561297481836) - present_state_Q (-0.35780385937306786)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.982573470571033 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2255332445167209) - present_state_Q ( -0.261017655927042)) * f1( 0.022490015008457813)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2255332445167209) - present_state_Q (-0.261017655927042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.99018072458307 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21906387119624066) - present_state_Q ( -0.3870670848639029)) * f1( 0.032025270234932554)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.21906387119624066) - present_state_Q (-0.3870670848639029)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.994550607344767 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22236869842762913) - present_state_Q ( -0.22236869842762913)) * f1( 0.01720141909550454)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.22236869842762913) - present_state_Q (-0.22236869842762913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -11.999214731260935 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20072509296749824) - present_state_Q ( -0.23688800583530917)) * f1( 0.018481027578542336)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20072509296749824) - present_state_Q (-0.23688800583530917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.00396092689114 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20176655482228417) - present_state_Q ( -0.20176655482228417)) * f1( 0.018547348187242626)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20176655482228417) - present_state_Q (-0.20176655482228417)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.012118106851362 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24452333501976017) - present_state_Q ( -0.44230101231977176)) * f1( 0.03511934042685906)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.24452333501976017) - present_state_Q (-0.44230101231977176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.015278237579619 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20735835416714346) - present_state_Q ( -0.20735835416714346)) * f1( 0.012373603383089567)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20735835416714346) - present_state_Q (-0.20735835416714346)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.0199305912997 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.215246071455274) - present_state_Q ( -0.23717917831988974)) * f1( 0.018425913629299233)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.215246071455274) - present_state_Q (-0.23717917831988974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.024342095174108 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19824937070789467) - present_state_Q ( -0.22027167670376263)) * f1( 0.01736740752651909)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.19824937070789467) - present_state_Q (-0.22027167670376263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.032535324141508 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24315171118670012) - present_state_Q ( -0.44523951646436555)) * f1( 0.03532131486989743)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.24315171118670012) - present_state_Q (-0.44523951646436555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.038862736860539 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16423738836873483) - present_state_Q ( -0.23324972618722187)) * f1( 0.025071712749468764)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.16423738836873483) - present_state_Q (-0.23324972618722187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.042671797474432 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21697951106173466) - present_state_Q ( -0.21697951106173466)) * f1( 0.014965250605180145)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.21697951106173466) - present_state_Q (-0.21697951106173466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.045531719462383 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20894249777949625) - present_state_Q ( -0.20894249777949625)) * f1( 0.011204380304165951)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20894249777949625) - present_state_Q (-0.20894249777949625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.050199602737155 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2240803989651516) - present_state_Q ( -0.26042162999759266)) * f1( 0.01865259584709246)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2240803989651516) - present_state_Q (-0.26042162999759266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.054331341611137 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1131803053204448) - present_state_Q ( -0.1131803053204448)) * f1( 0.015658299032976497)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1131803053204448) - present_state_Q (-0.1131803053204448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.057109612284878 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14742360061789997) - present_state_Q ( -0.027115049997218164)) * f1( 0.010183608261262429)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14742360061789997) - present_state_Q (-0.027115049997218164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.059376873660588 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15276529343455458) - present_state_Q ( -0.02292928742044452)) * f1( 0.008296177021813197)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15276529343455458) - present_state_Q (-0.02292928742044452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.061571317165242 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14398745582868724) - present_state_Q ( -0.02658162048584972)) * f1( 0.008043060198919254)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14398745582868724) - present_state_Q (-0.02658162048584972)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.063714392448379 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.151169810239285) - present_state_Q ( -0.019994382703153814)) * f1( 0.007833809935619683)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.151169810239285) - present_state_Q (-0.019994382703153814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.065806966513401 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15992173335082072) - present_state_Q ( -0.015348465783760831)) * f1( 0.007633801030207633)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15992173335082072) - present_state_Q (-0.015348465783760831)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.06783277878421 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11501298826053005) - present_state_Q ( -0.06406172255358064)) * f1( 0.007536528323950067)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.11501298826053005) - present_state_Q (-0.06406172255358064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.071738375496315 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06497494254089212) - present_state_Q ( -0.17230716030934579)) * f1( 0.0151688852561566)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06497494254089212) - present_state_Q (-0.17230716030934579)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.075772321409897 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12431085465494382) - present_state_Q ( -0.22020072345833877)) * f1( 0.015926936259072284)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.12431085465494382) - present_state_Q (-0.22020072345833877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.080573772453016 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15597591780690717) - present_state_Q ( -0.24596202960944008)) * f1( 0.019127859639315396)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15597591780690717) - present_state_Q (-0.24596202960944008)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.083594970400508 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1666462213196881) - present_state_Q ( -0.1666462213196881)) * f1( 0.011662289690742064)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1666462213196881) - present_state_Q (-0.1666462213196881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.086011755134095 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13701809026206022) - present_state_Q ( -0.036169757086838436)) * f1( 0.008891501941824463)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.13701809026206022) - present_state_Q (-0.036169757086838436)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.08842089634552 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14602418390602126) - present_state_Q ( -0.02959298442652951)) * f1( 0.008839064895228578)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14602418390602126) - present_state_Q (-0.02959298442652951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.090736124526414 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15232357024694995) - present_state_Q ( -0.024943449973153304)) * f1( 0.008478078650672842)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15232357024694995) - present_state_Q (-0.024943449973153304)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.092974579521451 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14265537659840183) - present_state_Q ( -0.028334199355715017)) * f1( 0.008210045427522999)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.14265537659840183) - present_state_Q (-0.028334199355715017)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.095155491715042 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1498848842457851) - present_state_Q ( -0.021804061969547654)) * f1( 0.007977771294820725)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.1498848842457851) - present_state_Q (-0.021804061969547654)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.097280400310646 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15897314098041532) - present_state_Q ( -0.016762165556985363)) * f1( 0.007756027167156341)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.15897314098041532) - present_state_Q (-0.016762165556985363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.098658650182527 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1481834370813439) - present_state_Q ( -0.022908398728979994)) * f1( 0.00757719610127937)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1481834370813439) - present_state_Q (-0.022908398728979994)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.100058223430176 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15613041407306466) - present_state_Q ( -0.016589324846540413)) * f1( 0.007664450337963269)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15613041407306466) - present_state_Q (-0.016589324846540413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.101418133930443 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14513983168108185) - present_state_Q ( -0.02632717262400758)) * f1( 0.007491706361845043)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14513983168108185) - present_state_Q (-0.02632717262400758)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.102818248734403 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15173557350080638) - present_state_Q ( -0.019514525420784325)) * f1( 0.007681570068139591)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15173557350080638) - present_state_Q (-0.019514525420784325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.10419140997164 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16035883231010684) - present_state_Q ( -0.014051952945631186)) * f1( 0.007507640263646976)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16035883231010684) - present_state_Q (-0.014051952945631186)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.10553269117084 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1494904965234835) - present_state_Q ( -0.021758850048197005)) * f1( 0.007368767181321132)
w2 ( -18.363307828191783 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1494904965234835) - present_state_Q (-0.021758850048197005)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.122267361219116 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19140527796863777) - present_state_Q ( -3.7582606081963803)) * f1( 0.007071067811865476)
w2 ( -18.836635775073976 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.19140527796863777) - present_state_Q (-3.7582606081963803)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.138879118820732 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16217982164945385) - present_state_Q ( -3.929162511614297)) * f1( 0.007071067811865476)
w2 ( -19.306487232975172 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.16217982164945385) - present_state_Q (-3.929162511614297)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.174314438627745 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18217080216201859) - present_state_Q ( -4.043468248757053)) * f1( 0.015156080524945532)
w2 ( -19.77409255809454 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.18217080216201859) - present_state_Q (-4.043468248757053)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.212851964128804 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.022069050673058) - present_state_Q ( -4.1939350697681625)) * f1( 0.016319924378529364)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -4.022069050673058) - present_state_Q (-4.1939350697681625)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -12.259341045734589 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05409695581350567) - present_state_Q ( -0.22220351847886025)) * f1( 0.017693136632009828)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.05409695581350567) - present_state_Q (-0.22220351847886025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.31148888207518 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08706204347088836) - present_state_Q ( -0.2552195533297483)) * f1( 0.02135591381029283)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.08706204347088836) - present_state_Q (-0.2552195533297483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.361835121392984 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13246928167578556) - present_state_Q ( -0.2646189819826185)) * f1( 0.02142386199179529)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.13246928167578556) - present_state_Q (-0.2646189819826185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.382175914309066 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051011821850149054) - present_state_Q ( -0.1440799761301002)) * f1( 0.008961115281348563)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.051011821850149054) - present_state_Q (-0.1440799761301002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.428543714788445 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04358390951452779) - present_state_Q ( -0.20905511608045668)) * f1( 0.020486599278289264)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.04358390951452779) - present_state_Q (-0.20905511608045668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.452489421374791 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02616276252591774) - present_state_Q ( -0.02616276252591774)) * f1( 0.010495880954215664)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.02616276252591774) - present_state_Q (-0.02616276252591774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.471238339678283 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030769714078271324) - present_state_Q ( -0.030769714078271324)) * f1( 0.008219518769685526)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.030769714078271324) - present_state_Q (-0.030769714078271324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.490081497319679 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024293871409909424) - present_state_Q ( -0.024293871409909424)) * f1( 0.008258723068627719)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.024293871409909424) - present_state_Q (-0.024293871409909424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.508314091572405 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020144529494635733) - present_state_Q ( -0.020144529494635733)) * f1( 0.007989812954285693)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.020144529494635733) - present_state_Q (-0.020144529494635733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.526100841141822 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02641084057519476) - present_state_Q ( -0.02641084057519476)) * f1( 0.00779636351024516)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.02641084057519476) - present_state_Q (-0.02641084057519476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.54351195036239 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019330770247164928) - present_state_Q ( -0.019330770247164928)) * f1( 0.007629580274819712)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.019330770247164928) - present_state_Q (-0.019330770247164928)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.56046849186557 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07684834694938897) - present_state_Q ( -0.1245281115619942)) * f1( 0.007462909157422099)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.07684834694938897) - present_state_Q (-0.1245281115619942)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.592126389298528 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04642343563573565) - present_state_Q ( -0.17498284181767831)) * f1( 0.013966150116054564)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.04642343563573565) - present_state_Q (-0.17498284181767831)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.610716857553356 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03982251275504197) - present_state_Q ( -0.03982251275504197)) * f1( 0.008152966460609699)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.03982251275504197) - present_state_Q (-0.03982251275504197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.629040225197476 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03457945884000287) - present_state_Q ( -0.03457945884000287)) * f1( 0.008034165192732392)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.03457945884000287) - present_state_Q (-0.03457945884000287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.646938795547662 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0534708742444877) - present_state_Q ( -0.1386229493154329)) * f1( 0.007883216060241368)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.0534708742444877) - present_state_Q (-0.1386229493154329)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.682776100419499 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08279681450771795) - present_state_Q ( -0.22004041191344434)) * f1( 0.015838873370344654)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.08279681450771795) - present_state_Q (-0.22004041191344434)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.703031896573137 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09773393135599798) - present_state_Q ( -0.09773393135599798)) * f1( 0.009276138291005986)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.09773393135599798) - present_state_Q (-0.09773393135599798)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.754695589005003 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06230005381921363) - present_state_Q ( -0.31620344620492113)) * f1( 0.02390239637929617)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.06230005381921363) - present_state_Q (-0.31620344620492113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.784921372576342 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10001920513840168) - present_state_Q ( -0.11922791004043418)) * f1( 0.013855385358358646)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.10001920513840168) - present_state_Q (-0.11922791004043418)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.802099827196447 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020027949994080852) - present_state_Q ( -0.020027949994080852)) * f1( 0.007841755566837813)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.020027949994080852) - present_state_Q (-0.020027949994080852)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.818837058106794 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01682789180097278) - present_state_Q ( -0.01682789180097278)) * f1( 0.007639337960056786)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.01682789180097278) - present_state_Q (-0.01682789180097278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.835198202227177 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02368233095837548) - present_state_Q ( -0.02368233095837548)) * f1( 0.007469784751289118)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02368233095837548) - present_state_Q (-0.02368233095837548)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.851294515854638 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01594348848061273) - present_state_Q ( -0.01594348848061273)) * f1( 0.00734653857686382)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.01594348848061273) - present_state_Q (-0.01594348848061273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.867124058603368 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011695537806707183) - present_state_Q ( -0.011695537806707183)) * f1( 0.007223520892693914)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.011695537806707183) - present_state_Q (-0.011695537806707183)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.88279508966573 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02398643932533678) - present_state_Q ( -0.02398643932533678)) * f1( 0.007154798636111772)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02398643932533678) - present_state_Q (-0.02398643932533678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.898249655672855 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06685572804483941) - present_state_Q ( -0.12911049764166127)) * f1( 0.007088603703384028)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.06685572804483941) - present_state_Q (-0.12911049764166127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.930891960661762 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07085142575273515) - present_state_Q ( -0.21700671880734457)) * f1( 0.015032497395133043)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.07085142575273515) - present_state_Q (-0.21700671880734457)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -12.98360010597811 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07974827333116483) - present_state_Q ( -0.333587696984925)) * f1( 0.024403273656548343)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.07974827333116483) - present_state_Q (-0.333587696984925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.020534326505482 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08501986474003473) - present_state_Q ( -0.26761833413968666)) * f1( 0.017047640527729436)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.08501986474003473) - present_state_Q (-0.26761833413968666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.077418241113707 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18625156717498903) - present_state_Q ( -0.3702272870493694)) * f1( 0.026368335806724773)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.18625156717498903) - present_state_Q (-0.3702272870493694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.111847033014298 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1114302567983433) - present_state_Q ( -0.15011962703779677)) * f1( 0.01580358078933014)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.1114302567983433) - present_state_Q (-0.15011962703779677)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.134053031498004 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03297349162556111) - present_state_Q ( -0.03297349162556111)) * f1( 0.010142162326603902)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.03297349162556111) - present_state_Q (-0.03297349162556111)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.152201034378885 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02611042208736206) - present_state_Q ( -0.02611042208736206)) * f1( 0.008286413259769334)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02611042208736206) - present_state_Q (-0.02611042208736206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.16975474767889 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021416485912720374) - present_state_Q ( -0.021416485912720374)) * f1( 0.008013513732114374)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.021416485912720374) - present_state_Q (-0.021416485912720374)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.186873794802445 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07318384615647819) - present_state_Q ( -0.07318384615647819)) * f1( 0.007831740127086149)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.07318384615647819) - present_state_Q (-0.07318384615647819)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.217653179432048 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022472221597905587) - present_state_Q ( -0.27560169938909207)) * f1( 0.014842349353964073)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.022472221597905587) - present_state_Q (-0.27560169938909207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.237181662055821 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040946108399613546) - present_state_Q ( -0.040946108399613546)) * f1( 0.00931078504056737)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.040946108399613546) - present_state_Q (-0.040946108399613546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.256261259744962 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034897618921687155) - present_state_Q ( -0.034897618921687155)) * f1( 0.009094405416219666)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.034897618921687155) - present_state_Q (-0.034897618921687155)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.274578644778398 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03545364458136836) - present_state_Q ( -0.03545364458136836)) * f1( 0.008731300420071099)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.03545364458136836) - present_state_Q (-0.03545364458136836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.292236945771892 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028407043841990238) - present_state_Q ( -0.028407043841990238)) * f1( 0.008414592811616871)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.028407043841990238) - present_state_Q (-0.028407043841990238)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.309287768359932 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022919549914777803) - present_state_Q ( -0.022919549914777803)) * f1( 0.008123203427779765)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.022919549914777803) - present_state_Q (-0.022919549914777803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.325832621308768 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027985975813103144) - present_state_Q ( -0.027985975813103144)) * f1( 0.00788386644519259)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.027985975813103144) - present_state_Q (-0.027985975813103144)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.342479329817056 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021326039855179214) - present_state_Q ( -0.021326039855179214)) * f1( 0.007930137114208756)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.021326039855179214) - present_state_Q (-0.021326039855179214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.35866645406173 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030731329033659476) - present_state_Q ( -0.030731329033659476)) * f1( 0.007714311710160915)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.030731329033659476) - present_state_Q (-0.030731329033659476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.375112544327637 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09411794388060207) - present_state_Q ( -0.13207554105193717)) * f1( 0.007873375554958508)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.09411794388060207) - present_state_Q (-0.13207554105193717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.402186998459134 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04711392382818543) - present_state_Q ( -0.1737548329982241)) * f1( 0.012990425469740326)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.04711392382818543) - present_state_Q (-0.1737548329982241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.41833790824741 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10021931542595276) - present_state_Q ( -0.13219312558695037)) * f1( 0.007731878993652007)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.10021931542595276) - present_state_Q (-0.13219312558695037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.444593277304522 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09213458263658392) - present_state_Q ( -0.09213458263658392)) * f1( 0.012545584723239413)
w2 ( -20.24636834329071 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.09213458263658392) - present_state_Q (-0.09213458263658392)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.47240591240239 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12649596727428908) - present_state_Q ( -0.19930536285466144)) * f1( 0.013355893833876926)
w2 ( -20.662853161810638 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.12649596727428908) - present_state_Q (-0.19930536285466144)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.503655374440207 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24370716972950174) - present_state_Q ( -4.3762778020916295)) * f1( 0.01875831774615921)
w2 ( -20.99603295395074 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.24370716972950174) - present_state_Q (-4.3762778020916295)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.529639361597479 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.266846297329556) - present_state_Q ( -4.444738435402818)) * f1( 0.01529113599819437)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -4.266846297329556) - present_state_Q (-4.444738435402818)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.585647435178005 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1478121904481363) - present_state_Q ( -0.39870492148298553)) * f1( 0.02841108850369003)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.1478121904481363) - present_state_Q (-0.39870492148298553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.614095079445473 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12187652350028216) - present_state_Q ( -0.15769766489692058)) * f1( 0.014942282815035419)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.12187652350028216) - present_state_Q (-0.15769766489692058)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.63063858341903 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03978453439521066) - present_state_Q ( -0.03978453439521066)) * f1( 0.008639782478786402)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.03978453439521066) - present_state_Q (-0.03978453439521066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.64644500192916 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032410223584755926) - present_state_Q ( -0.032410223584755926)) * f1( 0.008665241061271922)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.032410223584755926) - present_state_Q (-0.032410223584755926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.661655701623529 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02626527529501803) - present_state_Q ( -0.02626527529501803)) * f1( 0.008336134492470062)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.02626527529501803) - present_state_Q (-0.02626527529501803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.675620311725462 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031651907748757396) - present_state_Q ( -0.031651907748757396)) * f1( 0.00805882693204629)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.031651907748757396) - present_state_Q (-0.031651907748757396)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.68967595503733 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024601687774184516) - present_state_Q ( -0.024601687774184516)) * f1( 0.008108392135472818)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.024601687774184516) - present_state_Q (-0.024601687774184516)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.703221185771001 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08640006395060758) - present_state_Q ( -0.13938379705656728)) * f1( 0.007863209539230604)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08640006395060758) - present_state_Q (-0.13938379705656728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.72675563625323 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06490123876603289) - present_state_Q ( -0.2096565196720624)) * f1( 0.013719782652731006)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.06490123876603289) - present_state_Q (-0.2096565196720624)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.754388675850464 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05060162056201222) - present_state_Q ( -0.22552363680235737)) * f1( 0.017033409506586966)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.05060162056201222) - present_state_Q (-0.22552363680235737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.800503077239739 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14899751425126898) - present_state_Q ( -0.40973735214413554)) * f1( 0.028734448323423506)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.14899751425126898) - present_state_Q (-0.40973735214413554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.840741668358332 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08273629113284749) - present_state_Q ( -0.3587455476153047)) * f1( 0.025004035818833363)
w2 ( -21.335889811679817 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.08273629113284749) - present_state_Q (-0.3587455476153047)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.847932488246359 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-8.646071049504009) - present_state_Q ( -8.646071049504009)) * f1( 0.008301717052592219)
w2 ( -21.682363672085895 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -8.646071049504009) - present_state_Q (-8.646071049504009)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -13.853631186520312 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.414251231908924) - present_state_Q ( -4.414251231908924)) * f1( 0.004930963483222376)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -4.414251231908924) - present_state_Q (-4.414251231908924)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -13.88784997816719 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12811461104338612) - present_state_Q ( -0.3660031768069561)) * f1( 0.02563288506339937)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.12811461104338612) - present_state_Q (-0.3660031768069561)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.91285475007123 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07293386325579214) - present_state_Q ( -0.2693265679884483)) * f1( 0.01860373671579568)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.07293386325579214) - present_state_Q (-0.2693265679884483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.944500740314396 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09505437650428923) - present_state_Q ( -0.37127038638959875)) * f1( 0.025464534334338183)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09505437650428923) - present_state_Q (-0.37127038638959875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.94993773922968 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025683062077395014) - present_state_Q ( -0.025683062077395014)) * f1( 0.0042589260151181165)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.025683062077395014) - present_state_Q (-0.025683062077395014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.961313522132407 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11969269286873985) - present_state_Q ( -0.20174117698092645)) * f1( 0.009028779228516297)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.11969269286873985) - present_state_Q (-0.20174117698092645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -13.97968669435584 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09401782445181946) - present_state_Q ( -0.09401782445181946)) * f1( 0.014461797661906027)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09401782445181946) - present_state_Q (-0.09401782445181946)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.000309362309984 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09303311714400399) - present_state_Q ( -0.25705183220406486)) * f1( 0.01644355105549661)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.09303311714400399) - present_state_Q (-0.25705183220406486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.02680086176448 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31578888487500784) - present_state_Q ( -0.31578888487500784)) * f1( 0.022854218137204032)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.31578888487500784) - present_state_Q (-0.31578888487500784)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.029017206094226 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11518576316459625) - present_state_Q ( -0.11518576316459625)) * f1( 0.0020411071599376264)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11518576316459625) - present_state_Q (-0.11518576316459625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.037994334656721 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03362905201868446) - present_state_Q ( -0.03362905201868446)) * f1( 0.008211834209365587)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03362905201868446) - present_state_Q (-0.03362905201868446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.046728160045637 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02604986763936515) - present_state_Q ( -0.02604986763936515)) * f1( 0.007984290468988383)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.02604986763936515) - present_state_Q (-0.02604986763936515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.055224936782478 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01969597945808694) - present_state_Q ( -0.01969597945808694)) * f1( 0.007763526709463553)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.01969597945808694) - present_state_Q (-0.01969597945808694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.080284956329805 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12143050263732585) - present_state_Q ( -0.2943752878297433)) * f1( 0.023464493292932362)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.12143050263732585) - present_state_Q (-0.2943752878297433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.102495677737421 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14500850587233044) - present_state_Q ( -0.37199002312829366)) * f1( 0.0209441890225011)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.14500850587233044) - present_state_Q (-0.37199002312829366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.114042828015137 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08282292171217424) - present_state_Q ( -0.08282292171217424)) * f1( 0.010605716520733648)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.08282292171217424) - present_state_Q (-0.08282292171217424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.123462655728808 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06977636615604269) - present_state_Q ( -0.06977636615604269)) * f1( 0.008642512975037753)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06977636615604269) - present_state_Q (-0.06977636615604269)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.131940187392406 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03234732534415335) - present_state_Q ( -0.16826681076874273)) * f1( 0.007851620214299958)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.03234732534415335) - present_state_Q (-0.16826681076874273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.15222761090959 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07038418149504966) - present_state_Q ( -0.2124146640880541)) * f1( 0.018860037903332842)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.07038418149504966) - present_state_Q (-0.2124146640880541)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.157632536273358 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08482176889739639) - present_state_Q ( -0.11852317194757633)) * f1( 0.004980503778089992)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.08482176889739639) - present_state_Q (-0.11852317194757633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.180149497382967 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15367518326806964) - present_state_Q ( -0.34941868425647105)) * f1( 0.02118614134486253)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.15367518326806964) - present_state_Q (-0.34941868425647105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.204233627597084 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11957473535988368) - present_state_Q ( -0.31546006381755193)) * f1( 0.022595738990929985)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11957473535988368) - present_state_Q (-0.31546006381755193)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.222471179149508 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10868765881087696) - present_state_Q ( -0.17592852085186067)) * f1( 0.016891083533036585)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.10868765881087696) - present_state_Q (-0.17592852085186067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.233058410562403 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05248066430761137) - present_state_Q ( -0.05248066430761137)) * f1( 0.009699730894581548)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.05248066430761137) - present_state_Q (-0.05248066430761137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.252844288071996 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11303483011440858) - present_state_Q ( -0.3142964273796865)) * f1( 0.018562229318576994)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.11303483011440858) - present_state_Q (-0.3142964273796865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.27775218429221 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08423388118577116) - present_state_Q ( -0.27067904895935835)) * f1( 0.023278513612812732)
w2 ( -21.913503014055973 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.08423388118577116) - present_state_Q (-0.27067904895935835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.30035214415485 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1707125608996664) - present_state_Q ( -0.3317290619913077)) * f1( 0.021225504118130373)
w2 ( -22.12645399733402 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.1707125608996664) - present_state_Q (-0.3317290619913077)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.315182732151719 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.5596605389736835) - present_state_Q ( -4.805119825883351)) * f1( 0.026020693835610002)
w2 ( -22.240444716340704 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -4.5596605389736835) - present_state_Q (-4.805119825883351)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.337860168564255 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20764158172233532) - present_state_Q ( -0.4020136400721037)) * f1( 0.023457539791888183)
w2 ( -22.240444716340704 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.20764158172233532) - present_state_Q (-0.4020136400721037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.355892221118696 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.712248730899056) - present_state_Q ( -4.9332690589020896)) * f1( 0.032277065793067036)
w2 ( -22.46391053780102 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -4.712248730899056) - present_state_Q (-4.9332690589020896)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -14.35082154208247 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-13.726401025982067) - present_state_Q ( -13.77522672785256)) * f1( 0.02154163604151604)
w2 ( -22.322676723624962 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -13.726401025982067) - present_state_Q (-13.77522672785256)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -14.359267636400281 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12084915791735368) - present_state_Q ( -0.16622181135041014)) * f1( 0.013534358134170513)
w2 ( -22.322676723624962 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.12084915791735368) - present_state_Q (-0.16622181135041014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.365827280272724 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03881538603807629) - present_state_Q ( -0.03881538603807629)) * f1( 0.010314413258245168)
w2 ( -22.322676723624962 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.03881538603807629) - present_state_Q (-0.03881538603807629)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.371205953660374 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031143616692380905) - present_state_Q ( -0.031143616692380905)) * f1( 0.008448277868582165)
w2 ( -22.322676723624962 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.031143616692380905) - present_state_Q (-0.031143616692380905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.385153772336402 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14867362897123973) - present_state_Q ( -0.3541693141946244)) * f1( 0.02303399569910414)
w2 ( -22.322676723624962 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.14867362897123973) - present_state_Q (-0.3541693141946244)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.386350655749135 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.537738428456072) - present_state_Q ( -4.537738428456072)) * f1( 0.005179842160437747)
w2 ( -22.368889846560464 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -4.537738428456072) - present_state_Q (-4.537738428456072)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.387206917737096 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.513074398840053) - present_state_Q ( -4.513074398840053)) * f1( 0.003670448605536137)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -4.513074398840053) - present_state_Q (-4.513074398840053)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.395002307562844 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06537398868727731) - present_state_Q ( -0.25503123293989927)) * f1( 0.014897709452702957)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06537398868727731) - present_state_Q (-0.25503123293989927)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.405768163154017 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06404272077000829) - present_state_Q ( -0.2380733503345049)) * f1( 0.02050860103622986)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06404272077000829) - present_state_Q (-0.2380733503345049)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.418181016344747 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07899842321491511) - present_state_Q ( -0.3670325986691968)) * f1( 0.024234525827198032)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07899842321491511) - present_state_Q (-0.3670325986691968)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.42990082956072 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06182432023511572) - present_state_Q ( -0.34787701640621205)) * f1( 0.022803815463661215)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06182432023511572) - present_state_Q (-0.34787701640621205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.438186768747732 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05747990443453601) - present_state_Q ( -0.2602003669596132)) * f1( 0.015853247165700614)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.05747990443453601) - present_state_Q (-0.2602003669596132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.450160204299886 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06899715945626207) - present_state_Q ( -0.35557354103033334)) * f1( 0.023328981293004817)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06899715945626207) - present_state_Q (-0.35557354103033334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.462125999523698 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13184946755582014) - present_state_Q ( -0.33165053014218127)) * f1( 0.023177676707289787)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.13184946755582014) - present_state_Q (-0.33165053014218127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.470485980153923 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15104394119991604) - present_state_Q ( -0.13981809363694322)) * f1( 0.015607491174439696)
w2 ( -22.41554692202905 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.15104394119991604) - present_state_Q (-0.13981809363694322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.47135382640867 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16269548732026493) - present_state_Q ( -4.527670285265622)) * f1( 0.008949611169072341)
w2 ( -22.434940976996415 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.16269548732026493) - present_state_Q (-4.527670285265622)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.472833605518328 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28151666511790036) - present_state_Q ( -4.768504860517184)) * f1( 0.019976760425807757)
w2 ( -22.449755982814345 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.28151666511790036) - present_state_Q (-4.768504860517184)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.474844358538215 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.541311327178751) - present_state_Q ( -4.74303108810005)) * f1( 0.016865853607196196)
w2 ( -22.473600053404738 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -4.541311327178751) - present_state_Q (-4.74303108810005)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.476593085552441 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.558138895814395) - present_state_Q ( -4.761184139917592)) * f1( 0.014873501597248453)
w2 ( -22.49711471809605 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -4.558138895814395) - present_state_Q (-4.761184139917592)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.484982239921095 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13357320391169647) - present_state_Q ( -0.3369253742119883)) * f1( 0.019767008541758307)
w2 ( -22.49711471809605 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.13357320391169647) - present_state_Q (-0.3369253742119883)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.495054417255481 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15154569291940517) - present_state_Q ( -0.3736599322929147)) * f1( 0.02392963602957982)
w2 ( -22.49711471809605 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.15154569291940517) - present_state_Q (-0.3736599322929147)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.506444398419262 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25798847524061136) - present_state_Q ( -0.38646294265345427)) * f1( 0.02707438069577352)
w2 ( -22.49711471809605 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.25798847524061136) - present_state_Q (-0.38646294265345427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.513619528403597 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2532983169921923) - present_state_Q ( -0.2532983169921923)) * f1( 0.016534013809903155)
w2 ( -22.583907073138555 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2532983169921923) - present_state_Q (-0.2532983169921923)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.525011415398989 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24877992111360298) - present_state_Q ( -0.4170173381757189)) * f1( 0.027283036503187436)
w2 ( -22.583907073138555 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.24877992111360298) - present_state_Q (-0.4170173381757189)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.525412616566905 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.6957659633932645) - present_state_Q ( -4.6957659633932645)) * f1( 0.011751752952175596)
w2 ( -22.59073501054584 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.6957659633932645) - present_state_Q (-4.6957659633932645)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.525721536180335 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.7254225377266135) - present_state_Q ( -4.917805548519504)) * f1( 0.025254429519288954)
w2 ( -22.593181469399266 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.7254225377266135) - present_state_Q (-4.917805548519504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.525926286620956 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.6912324621891175) - present_state_Q ( -4.961554280736321)) * f1( 0.027243681437634084)
w2 ( -22.594684573457283 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.6912324621891175) - present_state_Q (-4.961554280736321)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.526117214016265 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.715256180674967) - present_state_Q ( -4.976496241304622)) * f1( 0.030491978367701577)
w2 ( -22.595936885740905 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.715256180674967) - present_state_Q (-4.976496241304622)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.526307018378251 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.773178647030786) - present_state_Q ( -4.976229441374429)) * f1( 0.02763819433860375)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -4.773178647030786) - present_state_Q (-4.976229441374429)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.535035937493175 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09644024307349719) - present_state_Q ( -0.30153600652793816)) * f1( 0.0259621045973988)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09644024307349719) - present_state_Q (-0.30153600652793816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.537574964150973 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10039263522954672) - present_state_Q ( -0.10039263522954672)) * f1( 0.0071246612518985585)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10039263522954672) - present_state_Q (-0.10039263522954672)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.545685218576935 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1074381556064242) - present_state_Q ( -0.3678180559521084)) * f1( 0.02459893036555665)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1074381556064242) - present_state_Q (-0.3678180559521084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.551748630035986 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08941957173250832) - present_state_Q ( -0.29479386135715774)) * f1( 0.018001842828664075)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.08941957173250832) - present_state_Q (-0.29479386135715774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.55313118706927 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0432827521965147) - present_state_Q ( -0.0432827521965147)) * f1( 0.003824379638124728)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0432827521965147) - present_state_Q (-0.0432827521965147)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.556009980088184 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0780413395690048) - present_state_Q ( -0.1575909442035608)) * f1( 0.008215071935704185)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0780413395690048) - present_state_Q (-0.1575909442035608)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.56103040326345 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0828712193403829) - present_state_Q ( -0.23391832550185163)) * f1( 0.014643471715396743)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0828712193403829) - present_state_Q (-0.23391832550185163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.56769578604574 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09505736583866203) - present_state_Q ( -0.22303113840589764)) * f1( 0.019373051413502956)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09505736583866203) - present_state_Q (-0.22303113840589764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.569977795732518 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06967559106380956) - present_state_Q ( -0.1654197563215462)) * f1( 0.00652820323878711)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06967559106380956) - present_state_Q (-0.1654197563215462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.578577100580844 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10737052892706532) - present_state_Q ( -0.3986860995520355)) * f1( 0.02632880909971286)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10737052892706532) - present_state_Q (-0.3986860995520355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.583496138975288 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10129617668188737) - present_state_Q ( -0.14942920591637626)) * f1( 0.013995337480986867)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10129617668188737) - present_state_Q (-0.14942920591637626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.586018211282036 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026994133234946947) - present_state_Q ( -0.026994133234946947)) * f1( 0.006948289670728668)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.026994133234946947) - present_state_Q (-0.026994133234946947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.588635404088109 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017655058890094003) - present_state_Q ( -0.017655058890094003)) * f1( 0.007193688083395358)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.017655058890094003) - present_state_Q (-0.017655058890094003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.591220879638168 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011334569979763379) - present_state_Q ( -0.011334569979763379)) * f1( 0.0070954151944201205)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.011334569979763379) - present_state_Q (-0.011334569979763379)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.593789647741634 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028764414822279694) - present_state_Q ( -0.028764414822279694)) * f1( 0.0070800439441060185)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.028764414822279694) - present_state_Q (-0.028764414822279694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.602345056196418 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18431857001387067) - present_state_Q ( -0.3876873193224925)) * f1( 0.02604533981263961)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.18431857001387067) - present_state_Q (-0.3876873193224925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.605665553267228 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1364847293228969) - present_state_Q ( -0.1364847293228969)) * f1( 0.009403223523804672)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1364847293228969) - present_state_Q (-0.1364847293228969)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.613856252274232 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10230366375779772) - present_state_Q ( -0.3795180483840178)) * f1( 0.024935294740138284)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.10230366375779772) - present_state_Q (-0.3795180483840178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.621674664873971 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1546630791972024) - present_state_Q ( -0.3609985050505105)) * f1( 0.02363102812320087)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.1546630791972024) - present_state_Q (-0.3609985050505105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.62343735668518 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07858287432416385) - present_state_Q ( -0.07858287432416385)) * f1( 0.004919124741993088)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07858287432416385) - present_state_Q (-0.07858287432416385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.627906357600194 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030600479463219577) - present_state_Q ( -0.23673060268860893)) * f1( 0.013065731959860863)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.030600479463219577) - present_state_Q (-0.23673060268860893)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.6312237513362 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040053588843282445) - present_state_Q ( -0.040053588843282445)) * f1( 0.009169084303507595)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.040053588843282445) - present_state_Q (-0.040053588843282445)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.63630888492764 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05892917744366394) - present_state_Q ( -0.2567459499504646)) * f1( 0.014942141946330272)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.05892917744366394) - present_state_Q (-0.2567459499504646)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.637541101825285 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06226899671295848) - present_state_Q ( -0.06226899671295848)) * f1( 0.0034247017443893918)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06226899671295848) - present_state_Q (-0.06226899671295848)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.639225648062451 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0456694946888895) - present_state_Q ( -0.0456694946888895)) * f1( 0.00466250175013505)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0456694946888895) - present_state_Q (-0.0456694946888895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.642176359496819 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041389388034410314) - present_state_Q ( -0.041389388034410314)) * f1( 0.008158306536974554)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.041389388034410314) - present_state_Q (-0.041389388034410314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.64507190231312 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03421920609299638) - present_state_Q ( -0.03421920609299638)) * f1( 0.007991514444650861)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03421920609299638) - present_state_Q (-0.03421920609299638)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.652631862319737 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12721341240775602) - present_state_Q ( -0.16788882503690683)) * f1( 0.021606667102102378)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.12721341240775602) - present_state_Q (-0.16788882503690683)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.655777157026185 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03682276022377151) - present_state_Q ( -0.03682276022377151)) * f1( 0.008686431398651124)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03682276022377151) - present_state_Q (-0.03682276022377151)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.658874105186962 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03109929412981311) - present_state_Q ( -0.03109929412981311)) * f1( 0.00854076159618713)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03109929412981311) - present_state_Q (-0.03109929412981311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.66186724008698 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03475934696001933) - present_state_Q ( -0.03475934696001933)) * f1( 0.008261970894625542)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03475934696001933) - present_state_Q (-0.03475934696001933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.66477878570747 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026872965051620887) - present_state_Q ( -0.026872965051620887)) * f1( 0.008021044652687376)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.026872965051620887) - present_state_Q (-0.026872965051620887)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.667611556044314 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02080976018319238) - present_state_Q ( -0.02080976018319238)) * f1( 0.007792311521071027)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02080976018319238) - present_state_Q (-0.02080976018319238)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.67037228182529 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02799758983685953) - present_state_Q ( -0.02799758983685953)) * f1( 0.007607670980733373)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02799758983685953) - present_state_Q (-0.02799758983685953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.673167978968356 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020396353676055575) - present_state_Q ( -0.020396353676055575)) * f1( 0.007689544535424125)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.020396353676055575) - present_state_Q (-0.020396353676055575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.675891579758126 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03199538853505818) - present_state_Q ( -0.03199538853505818)) * f1( 0.007512815405623648)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03199538853505818) - present_state_Q (-0.03199538853505818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.678687991841338 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023760833304384225) - present_state_Q ( -0.023760833304384225)) * f1( 0.007697922248377294)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.023760833304384225) - present_state_Q (-0.023760833304384225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.681424655633363 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01721213908555481) - present_state_Q ( -0.01721213908555481)) * f1( 0.007521245231844854)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01721213908555481) - present_state_Q (-0.01721213908555481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.684103998318378 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026371716599409498) - present_state_Q ( -0.026371716599409498)) * f1( 0.007380429339528649)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.026371716599409498) - present_state_Q (-0.026371716599409498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.686832332346563 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018289192293117738) - present_state_Q ( -0.018289192293117738)) * f1( 0.007500350478963779)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.018289192293117738) - present_state_Q (-0.018289192293117738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.689498721855072 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03137077746145885) - present_state_Q ( -0.03137077746145885)) * f1( 0.007353862751851927)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03137077746145885) - present_state_Q (-0.03137077746145885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.692249730995913 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022921762862700354) - present_state_Q ( -0.022921762862700354)) * f1( 0.007571363756056096)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.022921762862700354) - present_state_Q (-0.022921762862700354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.694948819980999 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015895935829025383) - present_state_Q ( -0.015895935829025383)) * f1( 0.007415563173605756)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015895935829025383) - present_state_Q (-0.015895935829025383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.697595706651928 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0262445408415073) - present_state_Q ( -0.0262445408415073)) * f1( 0.007290797203100168)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0262445408415073) - present_state_Q (-0.0262445408415073)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.700450070408452 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027482196082767547) - present_state_Q ( -0.027482196082767547)) * f1( 0.007864701786798456)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.027482196082767547) - present_state_Q (-0.027482196082767547)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.703301582535808 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021570388083706635) - present_state_Q ( -0.021570388083706635)) * f1( 0.007845343298708295)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.021570388083706635) - present_state_Q (-0.021570388083706635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.706078643832234 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028153194689398864) - present_state_Q ( -0.028153194689398864)) * f1( 0.0076529817410945404)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.028153194689398864) - present_state_Q (-0.028153194689398864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.708885925702212 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020690009169655185) - present_state_Q ( -0.020690009169655185)) * f1( 0.007721969573277962)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.020690009169655185) - present_state_Q (-0.020690009169655185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.71152478047525 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06409296964999525) - present_state_Q ( -0.1609083360962893)) * f1( 0.007540511577517736)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06409296964999525) - present_state_Q (-0.1609083360962893)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.716880126967819 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09470592745089201) - present_state_Q ( -0.25387581065011366)) * f1( 0.01570637708181625)
w2 ( -22.597310378955843 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.09470592745089201) - present_state_Q (-0.25387581065011366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.71574577338221 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.552628621488068) - present_state_Q ( -4.749090844398724)) * f1( 0.017730951799584446)
w2 ( -22.584515199109536 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -4.552628621488068) - present_state_Q (-4.749090844398724)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.722297363051952 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2551719522380531) - present_state_Q ( -0.37290108006036915)) * f1( 0.019813164729962238)
w2 ( -22.650648901211497 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.2551719522380531) - present_state_Q (-0.37290108006036915)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.721244284773853 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.568297401594944) - present_state_Q ( -4.760356713724942)) * f1( 0.016214725273085834)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -4.568297401594944) - present_state_Q (-4.760356713724942)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.724722756764617 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13344009530464246) - present_state_Q ( -0.3372499553986429)) * f1( 0.02314154696116401)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.13344009530464246) - present_state_Q (-0.3372499553986429)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.728342543883238 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15902065926903503) - present_state_Q ( -0.36186452773443245)) * f1( 0.024440317807490043)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15902065926903503) - present_state_Q (-0.36186452773443245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.732246178557064 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16399645504838517) - present_state_Q ( -0.4185765941537352)) * f1( 0.027396666311746136)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16399645504838517) - present_state_Q (-0.4185765941537352)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.735902843272678 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05367672122327891) - present_state_Q ( -0.2614809708426526)) * f1( 0.02327720018260858)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05367672122327891) - present_state_Q (-0.2614809708426526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.736607854069089 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08413360715940266) - present_state_Q ( -0.08413360715940266)) * f1( 0.004025609892710691)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08413360715940266) - present_state_Q (-0.08413360715940266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.739264091618798 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09731422799563309) - present_state_Q ( -0.30521381194757025)) * f1( 0.017343435718690795)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09731422799563309) - present_state_Q (-0.30521381194757025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.742581782526708 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12922382489630468) - present_state_Q ( -0.32885654983057705)) * f1( 0.021955464155851753)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12922382489630468) - present_state_Q (-0.32885654983057705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.743738978263252 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08185033583751988) - present_state_Q ( -0.08185033583751988)) * f1( 0.006599840702995057)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08185033583751988) - present_state_Q (-0.08185033583751988)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.746700203894102 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10287401495872052) - present_state_Q ( -0.30410202738608433)) * f1( 0.019313770259396564)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10287401495872052) - present_state_Q (-0.30410202738608433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.749750528164398 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12515656654372118) - present_state_Q ( -0.3336977317021531)) * f1( 0.020256462253180754)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12515656654372118) - present_state_Q (-0.3336977317021531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.751740018002565 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049625554490318594) - present_state_Q ( -0.2126754805913148)) * f1( 0.01228594662875928)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.049625554490318594) - present_state_Q (-0.2126754805913148)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.753229355443947 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10405173104020364) - present_state_Q ( -0.10405173104020364)) * f1( 0.008592060724345402)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10405173104020364) - present_state_Q (-0.10405173104020364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.755788470853325 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0873185750585835) - present_state_Q ( -0.29594855394326813)) * f1( 0.01661959883709192)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0873185750585835) - present_state_Q (-0.29594855394326813)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.758113444254793 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11317237972770543) - present_state_Q ( -0.13167101194697717)) * f1( 0.013622778841231824)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11317237972770543) - present_state_Q (-0.13167101194697717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.759400708647755 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07240508299246529) - present_state_Q ( -0.15589719285888579)) * f1( 0.00766969382566384)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07240508299246529) - present_state_Q (-0.15589719285888579)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.76181068585716 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08679268005626675) - present_state_Q ( -0.24774140590615512)) * f1( 0.015176443082784286)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08679268005626675) - present_state_Q (-0.24774140590615512)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.764802508145443 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14239593548658303) - present_state_Q ( -0.3324281888624789)) * f1( 0.019828547693110044)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14239593548658303) - present_state_Q (-0.3324281888624789)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.766794435464536 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10321643079894534) - present_state_Q ( -0.20089260689531452)) * f1( 0.012172146101481346)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.10321643079894534) - present_state_Q (-0.20089260689531452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.770450891939714 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2107781215777277) - present_state_Q ( -0.41091796426409405)) * f1( 0.025441628638051116)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.2107781215777277) - present_state_Q (-0.41091796426409405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.774207544900904 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11327672497718302) - present_state_Q ( -0.4062074018843903)) * f1( 0.026230775133207068)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.11327672497718302) - present_state_Q (-0.4062074018843903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.778174509984419 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16978490154305215) - present_state_Q ( -0.4313620858584882)) * f1( 0.028081708549395103)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.16978490154305215) - present_state_Q (-0.4313620858584882)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.778967428182309 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07300681705088534) - present_state_Q ( -0.07300681705088534)) * f1( 0.004501819286328043)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07300681705088534) - present_state_Q (-0.07300681705088534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.781498591731586 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1620993892919155) - present_state_Q ( -0.1339741062876819)) * f1( 0.014808444915898283)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.1620993892919155) - present_state_Q (-0.1339741062876819)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.778776747955021 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07836117399977777) - present_state_Q ( -4.708371805903617)) * f1( 0.009472220797010837)
w2 ( -22.63765974153888 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07836117399977777) - present_state_Q (-4.708371805903617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.77441379824939 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.125711908869789) - present_state_Q ( -4.768625918342086)) * f1( 0.014895594292041101)
w2 ( -22.579079336889123 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.125711908869789) - present_state_Q (-4.768625918342086)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.792443163209132 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2336039934814375) - present_state_Q ( -0.10447088184798241)) * f1( 0.007071067811865476)
w2 ( -22.579079336889123 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.2336039934814375) - present_state_Q (-0.10447088184798241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.807321006057943 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.252697946398953) - present_state_Q ( -4.563305588538366)) * f1( 0.007071067811865476)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.252697946398953) - present_state_Q (-4.563305588538366)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -14.860250934387004 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28888536166466133) - present_state_Q ( -0.42008498553907514)) * f1( 0.026859553847526578)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.28888536166466133) - present_state_Q (-0.42008498553907514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.910030775051638 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15685140059683578) - present_state_Q ( -0.28548857615670437)) * f1( 0.026318962679296124)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.15685140059683578) - present_state_Q (-0.28548857615670437)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.92232219496732 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09865524642100325) - present_state_Q ( -0.11404860935741931)) * f1( 0.006442152325123095)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.09865524642100325) - present_state_Q (-0.11404860935741931)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.936978845055616 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18440859147500488) - present_state_Q ( -0.04107828495959721)) * f1( 0.007649119648247294)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.18440859147500488) - present_state_Q (-0.04107828495959721)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.957013518738002 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17807151494138404) - present_state_Q ( -0.03568568853336587)) * f1( 0.010453245728303456)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.17807151494138404) - present_state_Q (-0.03568568853336587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.972986684565075 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1899195747509051) - present_state_Q ( -0.028778819688234737)) * f1( 0.008330605543185224)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.1899195747509051) - present_state_Q (-0.028778819688234737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -14.98842116362428 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17674545494479169) - present_state_Q ( -0.03528818318889837)) * f1( 0.008052947352383108)
w2 ( -22.999888279601993 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.17674545494479169) - present_state_Q (-0.03528818318889837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9806174908211922 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0158113883008419) - present_state_Q ( 0.007071067811865476)) * f1( 0.007071067811865476)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.0158113883008419) - present_state_Q (0.007071067811865476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9612375117522225 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022750099653905387) - present_state_Q ( 0.004186823299867391)) * f1( 0.007071067811865476)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.022750099653905387) - present_state_Q (0.004186823299867391)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.8663964709823078 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.037069487049281875) - present_state_Q ( 0.037069487049281875)) * f1( 0.03456447234665168)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.037069487049281875) - present_state_Q (0.037069487049281875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.7605809167957237 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03687749566692886) - present_state_Q ( 0.03687749566692886)) * f1( 0.038564336697294074)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.03687749566692886) - present_state_Q (0.03687749566692886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.6437957814530523 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.035415792726606915) - present_state_Q ( 0.035415792726606915)) * f1( 0.04256422654297944)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.035415792726606915) - present_state_Q (0.035415792726606915)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5160479373805179 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03255292756696985) - present_state_Q ( 0.03255292756696985)) * f1( 0.04656413531358539)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.03255292756696985) - present_state_Q (0.03255292756696985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.3773463799118435 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028157636033425373) - present_state_Q ( 0.028157636033425373)) * f1( 0.05056405851790086)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.028157636033425373) - present_state_Q (0.028157636033425373)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.22770241154278636 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02209888939308364) - present_state_Q ( 0.02209888939308364)) * f1( 0.054563992981649684)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.02209888939308364) - present_state_Q (0.02209888939308364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06712982671200465 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014245947956586218) - present_state_Q ( 0.014245947956586218)) * f1( 0.058563936397763854)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.014245947956586218) - present_state_Q (0.014245947956586218)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10435641545675664 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004736936010933322) - present_state_Q ( 0.004736936010933322)) * f1( 0.06256388704916872)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.004736936010933322) - present_state_Q (0.004736936010933322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.21003335350265084 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00435235969593967) - present_state_Q ( -0.0031896166354087167)) * f1( 0.038564336697294)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.00435235969593967) - present_state_Q (-0.0031896166354087167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2803983029950886 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00713630341084112) - present_state_Q ( -0.004739437874044273)) * f1( 0.026564866195838737)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.00713630341084112) - present_state_Q (-0.004739437874044273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3401681077635042 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008458233582543894) - present_state_Q ( -0.005205762583380662)) * f1( 0.02256516784123268)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.008458233582543894) - present_state_Q (-0.005205762583380662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3893445628005374 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008983439448380695) - present_state_Q ( -0.004954979865420352)) * f1( 0.018565599462532573)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.008983439448380695) - present_state_Q (-0.004954979865420352)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.42792881688518575 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008852094616038997) - present_state_Q ( -0.0041143764608517295)) * f1( 0.014566268125479925)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.008852094616038997) - present_state_Q (-0.0041143764608517295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45592198390407 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008213729019780411) - present_state_Q ( -0.0028115129906675766)) * f1( 0.010567442964291603)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.008213729019780411) - present_state_Q (-0.0028115129906675766)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.47332468495688423 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0075911506389407316) - present_state_Q ( -0.0048245151312389465)) * f1( 0.006570048287778459)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.0075911506389407316) - present_state_Q (-0.0048245151312389465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.49633625659395186 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004132194729074676) - present_state_Q ( -0.0039873414775433765)) * f1( 0.008687406186108861)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.004132194729074676) - present_state_Q (-0.0039873414775433765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5186494763499664 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00241681481565809) - present_state_Q ( -0.004925006236356705)) * f1( 0.008424114786885853)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.00241681481565809) - present_state_Q (-0.004925006236356705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.544933557370502 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037866658410910794) - present_state_Q ( -0.003595366666474555)) * f1( 0.009922721080571406)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.0037866658410910794) - present_state_Q (-0.003595366666474555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.56427334671857 ) += alpha ( 0.1 ) * (reward ( -26.492000177025584 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0031208784915890296) - present_state_Q ( -0.0031208784915890296)) * f1( 0.007301011596001248)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -26.492000177025584) + discount_factor ( 0.1) * next_state_max_Q( -0.0031208784915890296) - present_state_Q (-0.0031208784915890296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5907948522326237 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005482288006148002) - present_state_Q ( -0.005327656938698383)) * f1( 0.010370615860388725)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.005482288006148002) - present_state_Q (-0.005327656938698383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6156405072481425 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038815507073391097) - present_state_Q ( -0.006079412048897319)) * f1( 0.009715660039640825)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.0038815507073391097) - present_state_Q (-0.006079412048897319)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6500129620747963 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006270385406797077) - present_state_Q ( -0.004745733722532514)) * f1( 0.013440199143043868)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.006270385406797077) - present_state_Q (-0.004745733722532514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6814480942370249 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005364857525723533) - present_state_Q ( -0.005364857525723533)) * f1( 0.012291998427999084)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.005364857525723533) - present_state_Q (-0.005364857525723533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7120341525165221 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0065310434633201525) - present_state_Q ( -0.0043261265707119755)) * f1( 0.011959446967676503)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.0065310434633201525) - present_state_Q (-0.0043261265707119755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7473601135353503 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00627835322032414) - present_state_Q ( -0.0070028630050643715)) * f1( 0.013814254638062178)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.00627835322032414) - present_state_Q (-0.0070028630050643715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7737125785068608 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005282799131569379) - present_state_Q ( -0.006910743137966317)) * f1( 0.01030516245868084)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.005282799131569379) - present_state_Q (-0.006910743137966317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8072273946590803 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0025186198615763287) - present_state_Q ( -0.010307156540054627)) * f1( 0.01310789096250782)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.0025186198615763287) - present_state_Q (-0.010307156540054627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8301222231342643 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00851659712310065) - present_state_Q ( -0.011696336993114037)) * f1( 0.008954612173910602)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.00851659712310065) - present_state_Q (-0.011696336993114037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8671727457734035 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007938466883487197) - present_state_Q ( -0.008709787302567764)) * f1( 0.014489519397509792)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.007938466883487197) - present_state_Q (-0.008709787302567764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8915333830814401 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005634973857920892) - present_state_Q ( -0.011728429166704227)) * f1( 0.009528037829454753)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.005634973857920892) - present_state_Q (-0.011728429166704227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9261198863743313 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009062884645236939) - present_state_Q ( -0.006933946380891166)) * f1( 0.013524905186271759)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.009062884645236939) - present_state_Q (-0.006933946380891166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9573522043242655 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.190520274445206) - present_state_Q ( -0.007615724421264164)) * f1( 0.012223130815857233)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( 0.190520274445206) - present_state_Q (-0.007615724421264164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9874712312928589 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010017265309082566) - present_state_Q ( -0.005926261205388694)) * f1( 0.011777409503809271)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.010017265309082566) - present_state_Q (-0.005926261205388694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0230448115587998 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18546130338412836) - present_state_Q ( 0.18546130338412836)) * f1( 0.013817452006059881)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( 0.18546130338412836) - present_state_Q (0.18546130338412836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0635577641015803 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1907281849197948) - present_state_Q ( 0.182469823828673)) * f1( 0.015738148357955783)
w2 ( 0.48516240130242727 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( 0.1907281849197948) - present_state_Q (0.182469823828673)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1110446671234029 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08137354947271672) - present_state_Q ( 0.08137354947271672)) * f1( 0.018512171754316728)
w2 ( -0.027871981178920424 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( 0.08137354947271672) - present_state_Q (0.08137354947271672)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1319962929585154 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010535138672446596) - present_state_Q ( -0.007099379366167487)) * f1( 0.008193050111978713)
w2 ( -0.5393207224597808 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.010535138672446596) - present_state_Q (-0.007099379366167487)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1562324523752672 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005637368574875987) - present_state_Q ( -0.019387217435452412)) * f1( 0.009482191836375978)
w2 ( -0.5393207224597808 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.005637368574875987) - present_state_Q (-0.019387217435452412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1853021668687007 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014987182124673067) - present_state_Q ( -0.014720712706286545)) * f1( 0.011370787322409434)
w2 ( -0.5393207224597808 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.014987182124673067) - present_state_Q (-0.014720712706286545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2178475311772121 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013916447083844284) - present_state_Q ( -0.0172492186134536)) * f1( 0.012731620424634809)
w2 ( -0.5393207224597808 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.013916447083844284) - present_state_Q (-0.0172492186134536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2550483568654043 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020793422080348175) - present_state_Q ( -0.017538064306952216)) * f1( 0.01455259181633163)
w2 ( -1.0505812066086413 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.020793422080348175) - present_state_Q (-0.017538064306952216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.278222572121348 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21423784650751085) - present_state_Q ( -0.2262804711174128)) * f1( 0.009133190121876324)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.21423784650751085) - present_state_Q (-0.2262804711174128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2826693900380592 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009699215361384123) - present_state_Q ( -0.009699215361384123)) * f1( 0.0018035266399512187)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.009699215361384123) - present_state_Q (-0.009699215361384123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3013841607564907 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001648516818487056) - present_state_Q ( -0.001648516818487056)) * f1( 0.007588048883605013)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.001648516818487056) - present_state_Q (-0.001648516818487056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.319695208037399 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01162393993853247) - present_state_Q ( -0.01735761201949313)) * f1( 0.007428787292910867)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.01162393993853247) - present_state_Q (-0.01735761201949313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.35257782190663 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011035129780769586) - present_state_Q ( -0.012382498023339645)) * f1( 0.013337807960873908)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.011035129780769586) - present_state_Q (-0.012382498023339645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3794418797705896 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01014621796882689) - present_state_Q ( -0.012925332807530693)) * f1( 0.010896847427431806)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.01014621796882689) - present_state_Q (-0.012925332807530693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4104221600570208 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009496439233086061) - present_state_Q ( -0.009496439233086061)) * f1( 0.01256479313915343)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.009496439233086061) - present_state_Q (-0.009496439233086061)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4428182412532318 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012897435088323814) - present_state_Q ( -0.012897435088323814)) * f1( 0.013140636342003548)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.012897435088323814) - present_state_Q (-0.012897435088323814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4696418807988567 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007697962021328213) - present_state_Q ( -0.007697962021328213)) * f1( 0.010878254008641048)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.007697962021328213) - present_state_Q (-0.007697962021328213)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5058020770772327 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013717599941874376) - present_state_Q ( -0.01568034720791357)) * f1( 0.014669059789866002)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.013717599941874376) - present_state_Q (-0.01568034720791357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.52880649058708 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01004441516003296) - present_state_Q ( -0.020077488898902825)) * f1( 0.009333974569653573)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.01004441516003296) - present_state_Q (-0.020077488898902825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5616793442357269 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011647186808216694) - present_state_Q ( -0.011647186808216694)) * f1( 0.013333418252333204)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.011647186808216694) - present_state_Q (-0.011647186808216694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5921893378629293 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0021041977529272036) - present_state_Q ( -0.03165039974985215)) * f1( 0.012385556982864856)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.0021041977529272036) - present_state_Q (-0.03165039974985215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.602059770333513 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012561431316693605) - present_state_Q ( -0.012561431316693605)) * f1( 0.004003637720683775)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.012561431316693605) - present_state_Q (-0.012561431316693605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6215050415834111 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013345089007346189) - present_state_Q ( -0.01898669986435566)) * f1( 0.007889408010704197)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.013345089007346189) - present_state_Q (-0.01898669986435566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.650725755591466 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010123861197039254) - present_state_Q ( -0.010123861197039254)) * f1( 0.011851430399755343)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.010123861197039254) - present_state_Q (-0.010123861197039254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6849663865779574 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01635426048540605) - present_state_Q ( -0.01635426048540605)) * f1( 0.013890583408009716)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.01635426048540605) - present_state_Q (-0.01635426048540605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7101278708964471 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011738157947239184) - present_state_Q ( -0.015525338017480938)) * f1( 0.01020724439372654)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.011738157947239184) - present_state_Q (-0.015525338017480938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7427538114337822 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015916715662546987) - present_state_Q ( -0.012827252196356281)) * f1( 0.013233673120906328)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.015916715662546987) - present_state_Q (-0.012827252196356281)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7738593084209575 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015055032067532625) - present_state_Q ( -0.015055032067532625)) * f1( 0.012618137957620242)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.015055032067532625) - present_state_Q (-0.015055032067532625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8021518407952195 ) += alpha ( 0.1 ) * (reward ( -24.664965682058302 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017932116873932435) - present_state_Q ( -0.010442861667834365)) * f1( 0.011474760599861435)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -24.664965682058302) + discount_factor ( 0.1) * next_state_max_Q( -0.017932116873932435) - present_state_Q (-0.010442861667834365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8358267505343944 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01660185580100072) - present_state_Q ( -0.018375914952346202)) * f1( 0.014188029904290324)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.01660185580100072) - present_state_Q (-0.018375914952346202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8592413809563315 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012467626513667097) - present_state_Q ( -0.018991292069049506)) * f1( 0.009865561024737264)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.012467626513667097) - present_state_Q (-0.018991292069049506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.890604690688662 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013959184647059444) - present_state_Q ( -0.013959184647059444)) * f1( 0.013211787120521514)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.013959184647059444) - present_state_Q (-0.013959184647059444)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9202684654825064 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015963601625390567) - present_state_Q ( -0.015963601625390567)) * f1( 0.012496808075313216)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.015963601625390567) - present_state_Q (-0.015963601625390567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.948123980573196 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011781010201339431) - present_state_Q ( -0.011781010201339431)) * f1( 0.01173316049796759)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.011781010201339431) - present_state_Q (-0.011781010201339431)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9813447575570893 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01950702465883396) - present_state_Q ( -0.01950702465883396)) * f1( 0.013997188841421534)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.01950702465883396) - present_state_Q (-0.01950702465883396)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0053136686515574 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01382125267171291) - present_state_Q ( -0.018078098197875297)) * f1( 0.010098656446155213)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.01382125267171291) - present_state_Q (-0.018078098197875297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.036778818522453 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018453834870172433) - present_state_Q ( -0.015087577817101471)) * f1( 0.013255066348505802)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.018453834870172433) - present_state_Q (-0.015087577817101471)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.066733749804636 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01760820430371477) - present_state_Q ( -0.01760820430371477)) * f1( 0.01262025394604363)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.01760820430371477) - present_state_Q (-0.01760820430371477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.094018172831611 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0206753625575421) - present_state_Q ( -0.012209347159161886)) * f1( 0.011492384753487475)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.0206753625575421) - present_state_Q (-0.012209347159161886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1276953154245373 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019052439898286956) - present_state_Q ( -0.021362944767020525)) * f1( 0.014190610038021717)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.019052439898286956) - present_state_Q (-0.021362944767020525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1511403139831384 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014691373539402162) - present_state_Q ( -0.02194529049534116)) * f1( 0.009879493465568653)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.014691373539402162) - present_state_Q (-0.02194529049534116)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1822313479812085 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015926087319235515) - present_state_Q ( -0.015926087319235515)) * f1( 0.013098067754113374)
w2 ( -1.5580537314701468 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.015926087319235515) - present_state_Q (-0.015926087319235515)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.211748277303146 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02536445280508503) - present_state_Q ( -0.33027591836543996)) * f1( 0.012601304674022961)
w2 ( -2.0265279106999414 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.02536445280508503) - present_state_Q (-0.33027591836543996)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.239330165352967 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015621679022827012) - present_state_Q ( -0.022926140823701882)) * f1( 0.011623173147315381)
w2 ( -2.0265279106999414 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.015621679022827012) - present_state_Q (-0.022926140823701882)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2700402668732016 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023906100435427066) - present_state_Q ( -0.01626582400449315)) * f1( 0.01293733830110559)
w2 ( -2.0265279106999414 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.023906100435427066) - present_state_Q (-0.01626582400449315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2997774172747585 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020179633341142936) - present_state_Q ( -0.4250295362397502)) * f1( 0.01274716800696486)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.020179633341142936) - present_state_Q (-0.4250295362397502)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -2.3269564193565877 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005518104055957929) - present_state_Q ( -0.04100437962741884)) * f1( 0.011462615166469582)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.005518104055957929) - present_state_Q (-0.04100437962741884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.385281705152107 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01006198271901093) - present_state_Q ( -0.03669302636117427)) * f1( 0.026646266591184267)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.01006198271901093) - present_state_Q (-0.03669302636117427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.3947525025177616 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02298569633771921) - present_state_Q ( -0.024336663577323888)) * f1( 0.004324095902833069)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02298569633771921) - present_state_Q (-0.024336663577323888)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4171167498824038 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005331932121825415) - present_state_Q ( -0.005331932121825415)) * f1( 0.010202846701401235)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.005331932121825415) - present_state_Q (-0.005331932121825415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4353147089783183 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022515918346439796) - present_state_Q ( -0.03431332182441421)) * f1( 0.008312473196881968)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.022515918346439796) - present_state_Q (-0.03431332182441421)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.466406060155117 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02036978148609762) - present_state_Q ( -0.02492063442281974)) * f1( 0.014195972050619237)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02036978148609762) - present_state_Q (-0.02492063442281974)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4883794774276162 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01883583061913306) - present_state_Q ( -0.02510049493762152)) * f1( 0.010032975478315685)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.01883583061913306) - present_state_Q (-0.02510049493762152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5154684773237146 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01674959108771363) - present_state_Q ( -0.01674959108771363)) * f1( 0.012364133630750992)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.01674959108771363) - present_state_Q (-0.01674959108771363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5445476014352844 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023283302902013685) - present_state_Q ( -0.023283302902013685)) * f1( 0.013276041996557016)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.023283302902013685) - present_state_Q (-0.023283302902013685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.568435394053021 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007967302469086494) - present_state_Q ( -0.02950796066206412)) * f1( 0.010909808620898062)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.007967302469086494) - present_state_Q (-0.02950796066206412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.588388167238378 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006762674243079098) - present_state_Q ( -0.006762674243079098)) * f1( 0.009103237076601154)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.006762674243079098) - present_state_Q (-0.006762674243079098)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6076323128235175 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007305198388931085) - present_state_Q ( -0.007305198388931085)) * f1( 0.008780128988048452)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.007305198388931085) - present_state_Q (-0.007305198388931085)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6261954871717403 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005960700002465048) - present_state_Q ( -0.005960700002465048)) * f1( 0.008468968743132684)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.005960700002465048) - present_state_Q (-0.005960700002465048)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.644116377587462 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004729281907351188) - present_state_Q ( -0.004729281907351188)) * f1( 0.008175529918296173)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.004729281907351188) - present_state_Q (-0.004729281907351188)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6614780265717215 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0062603870251840485) - present_state_Q ( -0.0062603870251840485)) * f1( 0.00792090130381284)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.0062603870251840485) - present_state_Q (-0.0062603870251840485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6834308087025787 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010170960742780311) - present_state_Q ( -0.010170960742780311)) * f1( 0.010017121428534326)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.010170960742780311) - present_state_Q (-0.010170960742780311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.703245236890223 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009803333887420738) - present_state_Q ( -0.03381293298018918)) * f1( 0.009051163495176736)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.009803333887420738) - present_state_Q (-0.03381293298018918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7242321962460787 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008302836760666479) - present_state_Q ( -0.008302836760666479)) * f1( 0.00957567891693194)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.008302836760666479) - present_state_Q (-0.008302836760666479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.739322981621351 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02393150682768027) - present_state_Q ( -0.02827276987655645)) * f1( 0.00689123078031977)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02393150682768027) - present_state_Q (-0.02827276987655645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7620454681606597 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008164854781570897) - present_state_Q ( -0.030902533115719222)) * f1( 0.010378252601050525)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.008164854781570897) - present_state_Q (-0.030902533115719222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7812728771163435 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0067188989769552164) - present_state_Q ( -0.0067188989769552164)) * f1( 0.008772281719996064)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.0067188989769552164) - present_state_Q (-0.0067188989769552164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8066588202944227 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02994883912789883) - present_state_Q ( -0.01668044922601356)) * f1( 0.011586079086198861)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02994883912789883) - present_state_Q (-0.01668044922601356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8373224745613177 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02379526890559446) - present_state_Q ( -0.02822810785569712)) * f1( 0.01400258590674909)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02379526890559446) - present_state_Q (-0.02822810785569712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8596019945186897 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02114833705750849) - present_state_Q ( -0.02545245395170466)) * f1( 0.010172796821714222)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02114833705750849) - present_state_Q (-0.02545245395170466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8875732653153303 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020253342777627154) - present_state_Q ( -0.020253342777627154)) * f1( 0.012768663100280133)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.020253342777627154) - present_state_Q (-0.020253342777627154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.916191444032616 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02624893658617633) - present_state_Q ( -0.02624893658617633)) * f1( 0.013067190202863057)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.02624893658617633) - present_state_Q (-0.02624893658617633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9403752164686456 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00868189765256694) - present_state_Q ( -0.03437220668290875)) * f1( 0.01104740393224292)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.00868189765256694) - present_state_Q (-0.03437220668290875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.960054194190086 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007429134756518772) - present_state_Q ( -0.007429134756518772)) * f1( 0.008978566562347537)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.007429134756518772) - present_state_Q (-0.007429134756518772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.97898553070411 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007767124416001164) - present_state_Q ( -0.007767124416001164)) * f1( 0.008637573887348579)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.007767124416001164) - present_state_Q (-0.007767124416001164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0048577162295853 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029382174987309967) - present_state_Q ( -0.01846373160538783)) * f1( 0.011808990498742749)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.029382174987309967) - present_state_Q (-0.01846373160538783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.035301638045847 ) += alpha ( 0.1 ) * (reward ( -21.92441393960738 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027260853576995165) - present_state_Q ( -0.029794825049176847)) * f1( 0.013903019538815384)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.92441393960738) + discount_factor ( 0.1) * next_state_max_Q( -0.027260853576995165) - present_state_Q (-0.029794825049176847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0566204619183694 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010421944987282656) - present_state_Q ( -0.05053585756847216)) * f1( 0.010170513797656178)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.010421944987282656) - present_state_Q (-0.05053585756847216)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0763391672738543 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00885814165061613) - present_state_Q ( -0.00885814165061613)) * f1( 0.009388552285930515)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.00885814165061613) - present_state_Q (-0.00885814165061613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0952729894214785 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024001021994932415) - present_state_Q ( -0.032695211804432314)) * f1( 0.00902444196995762)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.024001021994932415) - present_state_Q (-0.032695211804432314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1209147289368135 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02034836909658138) - present_state_Q ( -0.02034836909658138)) * f1( 0.012214666160890014)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.02034836909658138) - present_state_Q (-0.02034836909658138)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1490894104489353 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029419443689471057) - present_state_Q ( -0.029419443689471057)) * f1( 0.013426476688849507)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.029419443689471057) - present_state_Q (-0.029419443689471057)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.171265965353323 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006274501693945661) - present_state_Q ( -0.038584559600894124)) * f1( 0.010573889662414659)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.006274501693945661) - present_state_Q (-0.038584559600894124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1879586929410126 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033455574013635254) - present_state_Q ( -0.043041999802672926)) * f1( 0.007959834148219783)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.033455574013635254) - present_state_Q (-0.043041999802672926)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2164091533830956 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012938628698536028) - present_state_Q ( -0.05034461097536641)) * f1( 0.01357249763120308)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.012938628698536028) - present_state_Q (-0.05034461097536641)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2309270573178024 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03393181639911844) - present_state_Q ( -0.03393181639911844)) * f1( 0.006919759851100772)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.03393181639911844) - present_state_Q (-0.03393181639911844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.253062101445104 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03092831130190985) - present_state_Q ( -0.03210110791926839)) * f1( 0.010549595769999644)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.03092831130190985) - present_state_Q (-0.03210110791926839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.27491577759872 ) += alpha ( 0.1 ) * (reward ( -21.01089669212374 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010000670232356861) - present_state_Q ( -0.055092484240042934)) * f1( 0.010427962139143002)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -21.01089669212374) + discount_factor ( 0.1) * next_state_max_Q( -0.010000670232356861) - present_state_Q (-0.055092484240042934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.293315652607126 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009923823447558894) - present_state_Q ( -0.009923823447558894)) * f1( 0.00915943083565536)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.009923823447558894) - present_state_Q (-0.009923823447558894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.310954325677821 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008166337330202824) - present_state_Q ( -0.008166337330202824)) * f1( 0.008779814314888732)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.008166337330202824) - present_state_Q (-0.008166337330202824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.327866623134877 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03515632002292222) - present_state_Q ( -0.046385245736518824)) * f1( 0.008433164210771438)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.03515632002292222) - present_state_Q (-0.046385245736518824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3559802237006853 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032721034776860225) - present_state_Q ( -0.03331204700178622)) * f1( 0.014009630207454705)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.032721034776860225) - present_state_Q (-0.03331204700178622)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3760034251147992 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006764332286391327) - present_state_Q ( -0.06038130727595323)) * f1( 0.009992777027879268)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.006764332286391327) - present_state_Q (-0.06038130727595323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3923557733560017 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00809165555881302) - present_state_Q ( -0.00809165555881302)) * f1( 0.008139506881487742)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.00809165555881302) - present_state_Q (-0.00809165555881302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4088002828380173 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006364438584320464) - present_state_Q ( -0.006364438584320464)) * f1( 0.008184747542037329)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.006364438584320464) - present_state_Q (-0.006364438584320464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4226151237425038 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026669389887043354) - present_state_Q ( -0.03360563878888232)) * f1( 0.0068845497216734955)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.026669389887043354) - present_state_Q (-0.03360563878888232)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.442396156345953 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0360183893828438) - present_state_Q ( -0.0360183893828438)) * f1( 0.009858494485016805)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.0360183893828438) - present_state_Q (-0.0360183893828438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.469381332466515 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032519781766769354) - present_state_Q ( -0.032519781766769354)) * f1( 0.013446793900310325)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.032519781766769354) - present_state_Q (-0.032519781766769354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.4905297046588726 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010266255312164716) - present_state_Q ( -0.05916338756464715)) * f1( 0.010553478755319194)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.010266255312164716) - present_state_Q (-0.05916338756464715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5087007348108834 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010234227704557031) - present_state_Q ( -0.010234227704557031)) * f1( 0.009045637996135742)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.010234227704557031) - present_state_Q (-0.010234227704557031)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5245571920746133 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008374519343949616) - present_state_Q ( -0.008374519343949616)) * f1( 0.008682376426425336)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.008374519343949616) - present_state_Q (-0.008374519343949616)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.539808193511699 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006826007723177477) - present_state_Q ( -0.006826007723177477)) * f1( 0.00835021525655629)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.006826007723177477) - present_state_Q (-0.006826007723177477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.554548271937124 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008188441555137937) - present_state_Q ( -0.008188441555137937)) * f1( 0.008071016984420632)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -0.008188441555137937) - present_state_Q (-0.008188441555137937)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.568628881655969 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00639064485600745) - present_state_Q ( -0.00639064485600745)) * f1( 0.008115121863328684)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.00639064485600745) - present_state_Q (-0.00639064485600745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5822828680639174 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005424584343672086) - present_state_Q ( -0.005424584343672086)) * f1( 0.007868850410995543)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.005424584343672086) - present_state_Q (-0.005424584343672086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5955553058812972 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03221407892213688) - present_state_Q ( -0.04743524882734743)) * f1( 0.007666339654859344)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03221407892213688) - present_state_Q (-0.04743524882734743)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6184984046881272 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030442957603776032) - present_state_Q ( -0.03337496719706816)) * f1( 0.01324162568239183)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.030442957603776032) - present_state_Q (-0.03337496719706816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.637464938783469 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010954929387180156) - present_state_Q ( -0.06276539251771729)) * f1( 0.010966383950818675)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.010954929387180156) - present_state_Q (-0.06276539251771729)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6502091691126144 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03306592378670438) - present_state_Q ( -0.0481962414736839)) * f1( 0.007361527224773783)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03306592378670438) - present_state_Q (-0.0481962414736839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6731661155453965 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031250972719339014) - present_state_Q ( -0.03389475064594969)) * f1( 0.01324995354863898)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.031250972719339014) - present_state_Q (-0.03389475064594969)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.6920746582483845 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011010997817772306) - present_state_Q ( -0.06362595608846987)) * f1( 0.010933394003499635)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.011010997817772306) - present_state_Q (-0.06362595608846987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.707462072214101 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009072176586057289) - present_state_Q ( -0.009072176586057289)) * f1( 0.008869510113505981)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.009072176586057289) - present_state_Q (-0.009072176586057289)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7222125898410505 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010819795450972943) - present_state_Q ( -0.010819795450972943)) * f1( 0.008503165501328916)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.010819795450972943) - present_state_Q (-0.010819795450972943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7370678815773677 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01797099674329154) - present_state_Q ( -0.042138273463144874)) * f1( 0.008578698520599675)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.01797099674329154) - present_state_Q (-0.042138273463144874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7633444052257774 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03299013057182958) - present_state_Q ( -0.041753084481884674)) * f1( 0.015172627758608923)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.03299013057182958) - present_state_Q (-0.041753084481884674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7786280533152428 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008952992639869017) - present_state_Q ( -0.04465712433766355)) * f1( 0.008827811433252553)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.008952992639869017) - present_state_Q (-0.04465712433766355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.792974362965863 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007136050702325325) - present_state_Q ( -0.007136050702325325)) * f1( 0.008268573234160434)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.007136050702325325) - present_state_Q (-0.007136050702325325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.806850813597155 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009252557890859076) - present_state_Q ( -0.009252557890859076)) * f1( 0.007998645595455324)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.009252557890859076) - present_state_Q (-0.009252557890859076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8209143016706233 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007246140710781472) - present_state_Q ( -0.007246140710781472)) * f1( 0.00810561377486651)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.007246140710781472) - present_state_Q (-0.007246140710781472)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.834559131863231 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005675484242947084) - present_state_Q ( -0.005675484242947084)) * f1( 0.007863675979284074)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.005675484242947084) - present_state_Q (-0.005675484242947084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8478606944476637 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007529133149719467) - present_state_Q ( -0.007529133149719467)) * f1( 0.007666583927984645)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.007529133149719467) - present_state_Q (-0.007529133149719467)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.860566867684854 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0213064951489424) - present_state_Q ( -0.03995031363355079)) * f1( 0.007745073524091664)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.0213064951489424) - present_state_Q (-0.03995031363355079)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8843800988688164 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036975302140924544) - present_state_Q ( -0.040572600320426186)) * f1( 0.014514567379039474)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.036975302140924544) - present_state_Q (-0.040572600320426186)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9000859485298442 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011539675259237716) - present_state_Q ( -0.046090556587410285)) * f1( 0.009577687269304648)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.011539675259237716) - present_state_Q (-0.046090556587410285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9148490966589824 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012748219548253678) - present_state_Q ( -0.012748219548253678)) * f1( 0.008984478167490375)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.012748219548253678) - present_state_Q (-0.012748219548253678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.929622059937726 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010594528759639382) - present_state_Q ( -0.010594528759639382)) * f1( 0.00898939101831793)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.010594528759639382) - present_state_Q (-0.010594528759639382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.943780345911602 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008647678175257372) - present_state_Q ( -0.008647678175257372)) * f1( 0.008614439625179366)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.008647678175257372) - present_state_Q (-0.008647678175257372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9574023883206015 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010425979788306816) - present_state_Q ( -0.010425979788306816)) * f1( 0.008288975817268809)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.010425979788306816) - present_state_Q (-0.010425979788306816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9711372608691535 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00834582233713908) - present_state_Q ( -0.00834582233713908)) * f1( 0.008356680663480935)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.00834582233713908) - present_state_Q (-0.00834582233713908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9843841820863495 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024231314035243586) - present_state_Q ( -0.041448478665132626)) * f1( 0.008075281019826075)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.024231314035243586) - present_state_Q (-0.041448478665132626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.00721628388257 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03279677734521752) - present_state_Q ( -0.03279677734521752)) * f1( 0.013910313906281196)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.03279677734521752) - present_state_Q (-0.03279677734521752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.026949469883037 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01643923587164024) - present_state_Q ( -0.05421149789724907)) * f1( 0.012039226131763349)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.01643923587164024) - present_state_Q (-0.05421149789724907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.0421542216857675 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013997508281777149) - present_state_Q ( -0.013997508281777149)) * f1( 0.009798646831478982)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.013997508281777149) - present_state_Q (-0.013997508281777149)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.056602097991176 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011670722660234504) - present_state_Q ( -0.011670722660234504)) * f1( 0.009309624807568508)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011670722660234504) - present_state_Q (-0.011670722660234504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.073273188104872 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011294528828791055) - present_state_Q ( -0.011294528828791055)) * f1( 0.010741939157742837)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011294528828791055) - present_state_Q (-0.011294528828791055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.086703996501759 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009205251827878371) - present_state_Q ( -0.009205251827878371)) * f1( 0.008653030350558153)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.009205251827878371) - present_state_Q (-0.009205251827878371)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.0996177435854095 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011340453866967935) - present_state_Q ( -0.011340453866967935)) * f1( 0.00832093486193664)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011340453866967935) - present_state_Q (-0.011340453866967935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.112683116267147 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009180375458160864) - present_state_Q ( -0.009180375458160864)) * f1( 0.008417579935733941)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.009180375458160864) - present_state_Q (-0.009180375458160864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.125306245823068 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007230386380831536) - present_state_Q ( -0.007230386380831536)) * f1( 0.008131738132112591)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.007230386380831536) - present_state_Q (-0.007230386380831536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.136403967359405 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011412490083402085) - present_state_Q ( -0.011412490083402085)) * f1( 0.007150814026710979)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011412490083402085) - present_state_Q (-0.011412490083402085)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.147531707362262 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009268193632034684) - present_state_Q ( -0.009268193632034684)) * f1( 0.00716926491324257)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.009268193632034684) - present_state_Q (-0.009268193632034684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.158599278019732 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0067324516924821744) - present_state_Q ( -0.0067324516924821744)) * f1( 0.0071294513658713456)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0067324516924821744) - present_state_Q (-0.0067324516924821744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.169551787106763 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003972927078766935) - present_state_Q ( -0.003972927078766935)) * f1( 0.007054203018630001)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.003972927078766935) - present_state_Q (-0.003972927078766935)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.1803881591234875 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0031650767992522123) - present_state_Q ( -0.0031650767992522123)) * f1( 0.006979075580737311)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0031650767992522123) - present_state_Q (-0.0031650767992522123)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.191070156492395 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03677032718435793) - present_state_Q ( -0.052342475708388715)) * f1( 0.006900012409681992)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.03677032718435793) - present_state_Q (-0.052342475708388715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.210492812323831 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019644532966582032) - present_state_Q ( -0.019644532966582032)) * f1( 0.012520960665854411)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.019644532966582032) - present_state_Q (-0.019644532966582032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.2267139365329065 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011228756373274208) - present_state_Q ( -0.011228756373274208)) * f1( 0.010451965859771142)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011228756373274208) - present_state_Q (-0.011228756373274208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.241243222860044 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029571548959563673) - present_state_Q ( -0.060757728143357864)) * f1( 0.00939231705908483)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0029571548959563673) - present_state_Q (-0.060757728143357864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.251533290232464 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0081912954318971) - present_state_Q ( -0.0081912954318971)) * f1( 0.006629163933267389)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0081912954318971) - present_state_Q (-0.0081912954318971)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.262333335598573 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019963127329343203) - present_state_Q ( -0.019963127329343203)) * f1( 0.006962458896035227)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.019963127329343203) - present_state_Q (-0.019963127329343203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.286313740926383 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039573051437032554) - present_state_Q ( -0.04064667975728733)) * f1( 0.015478115674497006)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.039573051437032554) - present_state_Q (-0.04064667975728733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.302635739122045 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011127224690377167) - present_state_Q ( -0.011127224690377167)) * f1( 0.010516901372055633)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.011127224690377167) - present_state_Q (-0.011127224690377167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.315984433433579 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009328951648165983) - present_state_Q ( -0.009328951648165983)) * f1( 0.008600188618264271)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.009328951648165983) - present_state_Q (-0.009328951648165983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.328928089316072 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010807328883818608) - present_state_Q ( -0.010807328883818608)) * f1( 0.008339948665639333)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.010807328883818608) - present_state_Q (-0.010807328883818608)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.343069782516755 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01182947147989485) - present_state_Q ( -0.01182947147989485)) * f1( 0.009112416738913932)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01182947147989485) - present_state_Q (-0.01182947147989485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.356469174767621 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01023002941579837) - present_state_Q ( -0.01023002941579837)) * f1( 0.008633302953136981)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01023002941579837) - present_state_Q (-0.01023002941579837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.369367478158544 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008281855633557902) - present_state_Q ( -0.008281855633557902)) * f1( 0.008309509890601184)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.008281855633557902) - present_state_Q (-0.008281855633557902)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.382620325214128 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0025033336843634283) - present_state_Q ( -0.060793312761136885)) * f1( 0.008567220040013339)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.0025033336843634283) - present_state_Q (-0.060793312761136885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.398608405828851 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013111502257012518) - present_state_Q ( -0.013111502257012518)) * f1( 0.01030293077456525)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.013111502257012518) - present_state_Q (-0.013111502257012518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.411768155690073 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01088405615074199) - present_state_Q ( -0.01088405615074199)) * f1( 0.008479221606289788)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.01088405615074199) - present_state_Q (-0.01088405615074199)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.424496321405366 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008573202842422209) - present_state_Q ( -0.008573202842422209)) * f1( 0.0082000401422227)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.008573202842422209) - present_state_Q (-0.008573202842422209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.436099253607823 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006889114715823695) - present_state_Q ( -0.006889114715823695)) * f1( 0.007941733086017415)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.006889114715823695) - present_state_Q (-0.006889114715823695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.447483223214887 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009942219663585404) - present_state_Q ( -0.009942219663585404)) * f1( 0.007793327867113047)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.009942219663585404) - present_state_Q (-0.009942219663585404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.458642272869862 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007452711225330234) - present_state_Q ( -0.007452711225330234)) * f1( 0.007638178800405405)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.007452711225330234) - present_state_Q (-0.007452711225330234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.4724583730206335 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013221970750098424) - present_state_Q ( -0.013221970750098424)) * f1( 0.00946024669232774)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.013221970750098424) - present_state_Q (-0.013221970750098424)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.485719955020626 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013498526894722139) - present_state_Q ( -0.013498526894722139)) * f1( 0.009080708309855362)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.013498526894722139) - present_state_Q (-0.013498526894722139)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.498456209458219 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011086716671009291) - present_state_Q ( -0.011086716671009291)) * f1( 0.008719700578436974)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.011086716671009291) - present_state_Q (-0.011086716671009291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.510706355170999 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008933593089711998) - present_state_Q ( -0.008933593089711998)) * f1( 0.008385780420432977)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.008933593089711998) - present_state_Q (-0.008933593089711998)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.52253389049062 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011123300655185785) - present_state_Q ( -0.011123300655185785)) * f1( 0.008097576846927173)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.011123300655185785) - present_state_Q (-0.011123300655185785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.534481725289899 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008745142798381142) - present_state_Q ( -0.008745142798381142)) * f1( 0.008178739929775082)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.008745142798381142) - present_state_Q (-0.008745142798381142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.546058821349758 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006984117713886211) - present_state_Q ( -0.006984117713886211)) * f1( 0.007924095676143333)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.006984117713886211) - present_state_Q (-0.006984117713886211)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.557419111196041 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025151208079246187) - present_state_Q ( -0.025151208079246187)) * f1( 0.00778441152828209)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.025151208079246187) - present_state_Q (-0.025151208079246187)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.579220218393846 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0446394447914891) - present_state_Q ( -0.023587556435120567)) * f1( 0.01493517823092213)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.0446394447914891) - present_state_Q (-0.023587556435120567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.5923936321437475 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01598915014410687) - present_state_Q ( -0.07780368780451195)) * f1( 0.009060075140296811)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.01598915014410687) - present_state_Q (-0.07780368780451195)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.605918461416569 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013342707894677688) - present_state_Q ( -0.013342707894677688)) * f1( 0.00926087477640442)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -0.013342707894677688) - present_state_Q (-0.013342707894677688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.618023693777708 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011036464086784442) - present_state_Q ( -0.011036464086784442)) * f1( 0.0088405654569334)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.011036464086784442) - present_state_Q (-0.011036464086784442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.629633993999935 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01254667142204705) - present_state_Q ( -0.01254667142204705)) * f1( 0.008479953587049767)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.01254667142204705) - present_state_Q (-0.01254667142204705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.641273496959607 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010108431880618732) - present_state_Q ( -0.010108431880618732)) * f1( 0.008499920404517828)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.010108431880618732) - present_state_Q (-0.010108431880618732)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.6524950292255935 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00831346147380657) - present_state_Q ( -0.00831346147380657)) * f1( 0.008193724446941126)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.00831346147380657) - present_state_Q (-0.00831346147380657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.6634156618428575 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019547283553353566) - present_state_Q ( -0.0515297643061197)) * f1( 0.007998597892019694)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.019547283553353566) - present_state_Q (-0.0515297643061197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.6850537016958995 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03807778570549268) - present_state_Q ( -0.05565617152744836)) * f1( 0.015850986745149908)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.03807778570549268) - present_state_Q (-0.05565617152744836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.696202634330992 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016553167955415816) - present_state_Q ( -0.05023059899309153)) * f1( 0.00816521375459921)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.016553167955415816) - present_state_Q (-0.05023059899309153)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.730483164808762 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05470962333633258) - present_state_Q ( -0.11830615513909268)) * f1( 0.02522496203066136)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.05470962333633258) - present_state_Q (-0.11830615513909268)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.744341745727505 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04886500342074129) - present_state_Q ( -0.04886500342074129)) * f1( 0.010146280053201997)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.04886500342074129) - present_state_Q (-0.04886500342074129)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.770788077240518 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019948646602585714) - present_state_Q ( -0.07566214073771502)) * f1( 0.01940432466599601)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.019948646602585714) - present_state_Q (-0.07566214073771502)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.785493241920558 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038380733617662394) - present_state_Q ( -0.05145282759535942)) * f1( 0.01076895617157009)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.038380733617662394) - present_state_Q (-0.05145282759535942)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.796476283001576 ) += alpha ( 0.1 ) * (reward ( -13.702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01564053998720216) - present_state_Q ( -0.05222274046037054)) * f1( 0.00804494624289878)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -13.702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.01564053998720216) - present_state_Q (-0.05222274046037054)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.807849171892256 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013052629542163852) - present_state_Q ( -0.013052629542163852)) * f1( 0.008900719345507065)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.013052629542163852) - present_state_Q (-0.013052629542163852)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.818774017661075 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010543094611427217) - present_state_Q ( -0.010543094611427217)) * f1( 0.008548558053271503)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.010543094611427217) - present_state_Q (-0.010543094611427217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.8292974616885544 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008860548521285812) - present_state_Q ( -0.008860548521285812)) * f1( 0.008233490507062574)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008860548521285812) - present_state_Q (-0.008860548521285812)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.839520866014872 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010678834272828291) - present_state_Q ( -0.010678834272828291)) * f1( 0.007999765185055381)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.010678834272828291) - present_state_Q (-0.010678834272828291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.849490940113865 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008033928951975963) - present_state_Q ( -0.008033928951975963)) * f1( 0.007800082596233715)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008033928951975963) - present_state_Q (-0.008033928951975963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.859214555253376 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006046327539744826) - present_state_Q ( -0.006046327539744826)) * f1( 0.007606201062645872)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.006046327539744826) - present_state_Q (-0.006046327539744826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.868741368266176 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008668083700872957) - present_state_Q ( -0.008668083700872957)) * f1( 0.007453630319899499)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008668083700872957) - present_state_Q (-0.008668083700872957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.878391294211066 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0061176163168303686) - present_state_Q ( -0.0061176163168303686)) * f1( 0.007548596309577048)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.0061176163168303686) - present_state_Q (-0.0061176163168303686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.887842819953523 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010129563841027405) - present_state_Q ( -0.010129563841027405)) * f1( 0.00739548779685649)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.010129563841027405) - present_state_Q (-0.010129563841027405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.897539401075562 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0073473705079581545) - present_state_Q ( -0.0073473705079581545)) * f1( 0.00758574879236564)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.0073473705079581545) - present_state_Q (-0.0073473705079581545)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.907032894867292 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005289779194763164) - present_state_Q ( -0.005289779194763164)) * f1( 0.00742579541681124)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.005289779194763164) - present_state_Q (-0.005289779194763164)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.916369308254277 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008202082233184945) - present_state_Q ( -0.008202082233184945)) * f1( 0.007304424931824942)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.008202082233184945) - present_state_Q (-0.008202082233184945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.925846285880532 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005553444048100744) - present_state_Q ( -0.005553444048100744)) * f1( 0.007413014095230866)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.005553444048100744) - present_state_Q (-0.005553444048100744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.935154051098787 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009768887854094518) - present_state_Q ( -0.009768887854094518)) * f1( 0.007282815243105336)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.009768887854094518) - present_state_Q (-0.009768887854094518)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.944714611807477 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00689173024761577) - present_state_Q ( -0.00689173024761577)) * f1( 0.0074790984974979135)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.00689173024761577) - present_state_Q (-0.00689173024761577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.954092843849059 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004931707816250114) - present_state_Q ( -0.004931707816250114)) * f1( 0.007335452955036743)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.004931707816250114) - present_state_Q (-0.004931707816250114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.963378474860644 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03970431580560695) - present_state_Q ( -0.04437575781724512)) * f1( 0.007283512714951561)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.03970431580560695) - present_state_Q (-0.04437575781724512)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.97885368762447 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01286573141301298) - present_state_Q ( -0.06277765184763481)) * f1( 0.012158639425212392)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.01286573141301298) - present_state_Q (-0.06277765184763481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.989970451993335 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013304915484188568) - present_state_Q ( -0.013304915484188568)) * f1( 0.008700424224785987)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.013304915484188568) - present_state_Q (-0.013304915484188568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.00069359282112 ) += alpha ( 0.1 ) * (reward ( -12.78924146477097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010663828442734908) - present_state_Q ( -0.010663828442734908)) * f1( 0.00839079774968562)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -12.78924146477097) + discount_factor ( 0.1) * next_state_max_Q( -0.010663828442734908) - present_state_Q (-0.010663828442734908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.010311488807949 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00854100472946987) - present_state_Q ( -0.00854100472946987)) * f1( 0.00810403248081243)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.00854100472946987) - present_state_Q (-0.00854100472946987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.019611617660972 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02345523696009233) - present_state_Q ( -0.05529053285447999)) * f1( 0.007866279737871006)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.02345523696009233) - present_state_Q (-0.05529053285447999)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0377413098166635 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043838505544887885) - present_state_Q ( -0.05730686177101776)) * f1( 0.015334514925601204)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.043838505544887885) - present_state_Q (-0.05730686177101776)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.048066084359258 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017574603484498682) - present_state_Q ( -0.05537287178984622)) * f1( 0.008733445709354632)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.017574603484498682) - present_state_Q (-0.05537287178984622)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.059132623932389 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014633712836637254) - present_state_Q ( -0.014633712836637254)) * f1( 0.009328968862066718)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.014633712836637254) - present_state_Q (-0.014633712836637254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0696826648865425 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012341685402051914) - present_state_Q ( -0.012341685402051914)) * f1( 0.008892020046273064)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.012341685402051914) - present_state_Q (-0.012341685402051914)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.079867933648159 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01343679342029087) - present_state_Q ( -0.01343679342029087)) * f1( 0.008585287820405171)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.01343679342029087) - present_state_Q (-0.01343679342029087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.0897171116876745 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010744733691656176) - present_state_Q ( -0.010744733691656176)) * f1( 0.00830029770236713)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.010744733691656176) - present_state_Q (-0.010744733691656176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.099214478841834 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03187920694619002) - present_state_Q ( -0.052610141615891316)) * f1( 0.008030715905172838)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.03187920694619002) - present_state_Q (-0.052610141615891316)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.115437124551037 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02232582900577535) - present_state_Q ( -0.08153250321207092)) * f1( 0.013752172099609772)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.02232582900577535) - present_state_Q (-0.08153250321207092)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.1343519133854825 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0511298746642537) - present_state_Q ( -0.0511298746642537)) * f1( 0.015989228056669056)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.0511298746642537) - present_state_Q (-0.0511298746642537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.146353345466395 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05753817198017239) - present_state_Q ( -0.05825144811293591)) * f1( 0.010150724925415074)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.05753817198017239) - present_state_Q (-0.05825144811293591)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.1643368761262245 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05143888882316262) - present_state_Q ( -0.05769308697336631)) * f1( 0.015210407213486534)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.05143888882316262) - present_state_Q (-0.05769308697336631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.174701047194663 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016583516683097148) - present_state_Q ( -0.08670909808515519)) * f1( 0.008790143407875445)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.016583516683097148) - present_state_Q (-0.08670909808515519)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.18564424437369 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01701706947075763) - present_state_Q ( -0.01701706947075763)) * f1( 0.009226660997130334)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.01701706947075763) - present_state_Q (-0.01701706947075763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.196482016674905 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014097258105992489) - present_state_Q ( -0.014097258105992489)) * f1( 0.009135748787598317)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.014097258105992489) - present_state_Q (-0.014097258105992489)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.206799972740437 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030256541578453516) - present_state_Q ( -0.05769375229408667)) * f1( 0.008728455360952375)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.030256541578453516) - present_state_Q (-0.05769375229408667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.223541141010487 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015848782281899766) - present_state_Q ( -0.07715938089434468)) * f1( 0.014187250734036285)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.015848782281899766) - present_state_Q (-0.07715938089434468)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.23291167633528 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011826426131048458) - present_state_Q ( -0.011826426131048458)) * f1( 0.007897574211231976)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.011826426131048458) - present_state_Q (-0.011826426131048458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.242390233105751 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009069195519569981) - present_state_Q ( -0.009069195519569981)) * f1( 0.007986945269246705)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.009069195519569981) - present_state_Q (-0.009069195519569981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.251602524522403 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0073335308645429585) - present_state_Q ( -0.0073335308645429585)) * f1( 0.007761559644997691)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.0073335308645429585) - present_state_Q (-0.0073335308645429585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.260635128696016 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010782157401230117) - present_state_Q ( -0.010782157401230117)) * f1( 0.007612159733256363)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.010782157401230117) - present_state_Q (-0.010782157401230117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.269466345786122 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04993940325138231) - present_state_Q ( -0.06889788115647368)) * f1( 0.007476592839554666)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.04993940325138231) - present_state_Q (-0.06889788115647368)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.284987795521239 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0271802991930719) - present_state_Q ( -0.0271802991930719)) * f1( 0.013096875086554764)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.0271802991930719) - present_state_Q (-0.0271802991930719)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.297985342344442 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015896316864443923) - present_state_Q ( -0.015896316864443923)) * f1( 0.010957836015820357)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.015896316864443923) - present_state_Q (-0.015896316864443923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.308661617693338 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013460229590221305) - present_state_Q ( -0.013460229590221305)) * f1( 0.008999179288012901)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.013460229590221305) - present_state_Q (-0.013460229590221305)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.318950939579824 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.060887753593357456) - present_state_Q ( -0.014269839449560458)) * f1( 0.008670136483736988)
w2 ( -2.493096647933322 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.060887753593357456) - present_state_Q (-0.014269839449560458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.32888607396132 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06432749232888071) - present_state_Q ( -0.01144722184566614)) * f1( 0.00836945270770652)
w2 ( -2.730510842826813 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.06432749232888071) - present_state_Q (-0.01144722184566614)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.336524826324041 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06998981505752819) - present_state_Q ( -0.5516867807080373)) * f1( 0.006741441895402356)
w2 ( -2.9571315711885138 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.06998981505752819) - present_state_Q (-0.5516867807080373)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.3675655933539765 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.105366043595251) - present_state_Q ( -0.14714090642720945)) * f1( 0.02644215857245988)
w2 ( -2.9571315711885138 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.105366043595251) - present_state_Q (-0.14714090642720945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.39760027661783 ) += alpha ( 0.1 ) * (reward ( -11.875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15120190750755452) - present_state_Q ( -0.7426282217452573)) * f1( 0.026941245811847633)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -11.875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.15120190750755452) - present_state_Q (-0.7426282217452573)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.41581706119983 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03961276451654779) - present_state_Q ( -0.0541727197998322)) * f1( 0.018219557090314826)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.03961276451654779) - present_state_Q (-0.0541727197998322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.4231738669618075 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027079136489396673) - present_state_Q ( -0.027079136489396673)) * f1( 0.007338958516092525)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.027079136489396673) - present_state_Q (-0.027079136489396673)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.438177389378965 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043817387811104204) - present_state_Q ( -0.050770284662026134)) * f1( 0.015000070636243881)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.043817387811104204) - present_state_Q (-0.050770284662026134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.449235706110331 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018810719397977294) - present_state_Q ( -0.018810719397977294)) * f1( 0.011023306626995891)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.018810719397977294) - present_state_Q (-0.018810719397977294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.45838269565275 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01578652834529566) - present_state_Q ( -0.01578652834529566)) * f1( 0.009115557416975433)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.01578652834529566) - present_state_Q (-0.01578652834529566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.467145517351751 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012870171132209824) - present_state_Q ( -0.012870171132209824)) * f1( 0.008730426083823177)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.012870171132209824) - present_state_Q (-0.012870171132209824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.475564973444487 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010807466236741356) - present_state_Q ( -0.010807466236741356)) * f1( 0.008386778684676407)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.010807466236741356) - present_state_Q (-0.010807466236741356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.483733132826137 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012686565896570492) - present_state_Q ( -0.012686565896570492)) * f1( 0.008137828220472692)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.012686565896570492) - present_state_Q (-0.012686565896570492)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.491683333587256 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00972155379268044) - present_state_Q ( -0.00972155379268044)) * f1( 0.007918573721188408)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00972155379268044) - present_state_Q (-0.00972155379268044)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.499422501434741 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00737170101702669) - present_state_Q ( -0.00737170101702669)) * f1( 0.007706756933689827)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00737170101702669) - present_state_Q (-0.00737170101702669)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.506987134344192 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010412792104341156) - present_state_Q ( -0.010412792104341156)) * f1( 0.007535006612649312)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.010412792104341156) - present_state_Q (-0.010412792104341156)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.514654441150417 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007491653149261918) - present_state_Q ( -0.007491653149261918)) * f1( 0.007635278922691343)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.007491653149261918) - present_state_Q (-0.007491653149261918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.522149431643959 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012140090416469376) - present_state_Q ( -0.012140090416469376)) * f1( 0.007466793163937504)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.012140090416469376) - present_state_Q (-0.012140090416469376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.529851164656059 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00904099080252359) - present_state_Q ( -0.00904099080252359)) * f1( 0.0076706264505970725)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00904099080252359) - present_state_Q (-0.00904099080252359)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.537382885305818 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006424963687101534) - present_state_Q ( -0.006424963687101534)) * f1( 0.007499542172917766)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.006424963687101534) - present_state_Q (-0.006424963687101534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.5447717764576945 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010253383271797383) - present_state_Q ( -0.010253383271797383)) * f1( 0.0073598479573144275)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.010253383271797383) - present_state_Q (-0.010253383271797383)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.552303861887265 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00713356358348745) - present_state_Q ( -0.00713356358348745)) * f1( 0.007500381679906337)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00713356358348745) - present_state_Q (-0.00713356358348745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.559688732465126 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005914398089347755) - present_state_Q ( -0.005914398089347755)) * f1( 0.007352983069684788)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.005914398089347755) - present_state_Q (-0.005914398089347755)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.566940293577953 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00960452064939413) - present_state_Q ( -0.00960452064939413)) * f1( 0.007222637588433693)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00960452064939413) - present_state_Q (-0.00960452064939413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.574107801843703 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005992479748410662) - present_state_Q ( -0.005992479748410662)) * f1( 0.007136609252247814)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.005992479748410662) - present_state_Q (-0.005992479748410662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.581186737864611 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004272701877481629) - present_state_Q ( -0.004272701877481629)) * f1( 0.007047332757074734)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.004272701877481629) - present_state_Q (-0.004272701877481629)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.588206560778551 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00983421624709626) - present_state_Q ( -0.00983421624709626)) * f1( 0.006991967664515518)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00983421624709626) - present_state_Q (-0.00983421624709626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.595183700587019 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0059223209950511) - present_state_Q ( -0.0059223209950511)) * f1( 0.006947017793973862)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.0059223209950511) - present_state_Q (-0.0059223209950511)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.602060575707136 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.057318819112273774) - present_state_Q ( -0.07041410897216656)) * f1( 0.006887890582396207)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.057318819112273774) - present_state_Q (-0.07041410897216656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.614677038737156 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026119293306805746) - present_state_Q ( -0.026119293306805746)) * f1( 0.012584771607191209)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.026119293306805746) - present_state_Q (-0.026119293306805746)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.625159425071941 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014710015708938052) - present_state_Q ( -0.014710015708938052)) * f1( 0.01044535680518963)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.014710015708938052) - present_state_Q (-0.014710015708938052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.633735786044566 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011970991590099702) - present_state_Q ( -0.011970991590099702)) * f1( 0.008543965817174485)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011970991590099702) - present_state_Q (-0.011970991590099702)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.641995669097411 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014378959945226422) - present_state_Q ( -0.014378959945226422)) * f1( 0.008230460261632898)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.014378959945226422) - present_state_Q (-0.014378959945226422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.650318516725325 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011402331088691818) - present_state_Q ( -0.011402331088691818)) * f1( 0.008290987333248763)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011402331088691818) - present_state_Q (-0.011402331088691818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.656819807899158 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004012974516786155) - present_state_Q ( -0.004012974516786155)) * f1( 0.006472116139410337)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.004012974516786155) - present_state_Q (-0.004012974516786155)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.663310392743683 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033412362214086146) - present_state_Q ( -0.052637933587328364)) * f1( 0.006490978830742391)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.033412362214086146) - present_state_Q (-0.052637933587328364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.677742691782812 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01863023518430557) - present_state_Q ( -0.08337192561366079)) * f1( 0.01447982062274216)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.01863023518430557) - present_state_Q (-0.08337192561366079)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.685960080764524 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013794401681143088) - present_state_Q ( -0.013794401681143088)) * f1( 0.008187688338109109)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.013794401681143088) - present_state_Q (-0.013794401681143088)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.6942176854901945 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010857012897262963) - present_state_Q ( -0.010857012897262963)) * f1( 0.008225592028777942)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.010857012897262963) - present_state_Q (-0.010857012897262963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.702211737182102 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009068565014186073) - present_state_Q ( -0.009068565014186073)) * f1( 0.007961784162062972)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.009068565014186073) - present_state_Q (-0.009068565014186073)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.710003905036932 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011743383934533537) - present_state_Q ( -0.011743383934533537)) * f1( 0.007762576388282953)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011743383934533537) - present_state_Q (-0.011743383934533537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.7175963651968225 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029480278203249874) - present_state_Q ( -0.05944647227982945)) * f1( 0.007598393534272071)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.029480278203249874) - present_state_Q (-0.05944647227982945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.732507763329662 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052245320151698416) - present_state_Q ( -0.06245341953470024)) * f1( 0.014924142104327909)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.052245320151698416) - present_state_Q (-0.06245341953470024)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.741630540360305 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01672243061970331) - present_state_Q ( -0.0666243522417804)) * f1( 0.00913763700944635)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.01672243061970331) - present_state_Q (-0.0666243522417804)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.750535747062093 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013895773855747886) - present_state_Q ( -0.013895773855747886)) * f1( 0.008873100694951676)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.013895773855747886) - present_state_Q (-0.013895773855747886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.759074225512648 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015369085391541155) - present_state_Q ( -0.015369085391541155)) * f1( 0.008508818796746089)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.015369085391541155) - present_state_Q (-0.015369085391541155)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.780054548330734 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0551551852583549) - present_state_Q ( -0.1234232913242758)) * f1( 0.021126556725069762)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.0551551852583549) - present_state_Q (-0.1234232913242758)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.797487742407775 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04129646869994512) - present_state_Q ( -0.05920278733801302)) * f1( 0.01744432952200496)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.04129646869994512) - present_state_Q (-0.05920278733801302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.804659817163679 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011450197621080212) - present_state_Q ( -0.011450197621080212)) * f1( 0.0071446503410372555)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011450197621080212) - present_state_Q (-0.011450197621080212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.812065001130033 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007952344091567282) - present_state_Q ( -0.007952344091567282)) * f1( 0.007374555510614675)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.007952344091567282) - present_state_Q (-0.007952344091567282)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.81934505370413 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0053275409349308606) - present_state_Q ( -0.0053275409349308606)) * f1( 0.007248236486804203)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.0053275409349308606) - present_state_Q (-0.0053275409349308606)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.826530994910221 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00921376809309022) - present_state_Q ( -0.00921376809309022)) * f1( 0.00715702872024625)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00921376809309022) - present_state_Q (-0.00921376809309022)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.83384586914318 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005920588149574169) - present_state_Q ( -0.005920588149574169)) * f1( 0.0072832930056002014)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.005920588149574169) - present_state_Q (-0.005920588149574169)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.8410482956695535 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011190230497903285) - present_state_Q ( -0.011190230497903285)) * f1( 0.007174718827798571)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011190230497903285) - present_state_Q (-0.011190230497903285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.848461448629242 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007686707578562638) - present_state_Q ( -0.007686707578562638)) * f1( 0.007382315782376175)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.007686707578562638) - present_state_Q (-0.007686707578562638)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.855747126619014 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005405224869675869) - present_state_Q ( -0.005405224869675869)) * f1( 0.0072538878119739495)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.005405224869675869) - present_state_Q (-0.005405224869675869)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.862981558995965 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011609343425507908) - present_state_Q ( -0.011609343425507908)) * f1( 0.007206872350648161)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.011609343425507908) - present_state_Q (-0.011609343425507908)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.870151060925055 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007838368138687815) - present_state_Q ( -0.007838368138687815)) * f1( 0.007139775338104411)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.007838368138687815) - present_state_Q (-0.007838368138687815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.877235134310859 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004461513045805565) - present_state_Q ( -0.004461513045805565)) * f1( 0.0070525664959394585)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.004461513045805565) - present_state_Q (-0.004461513045805565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.884251510755756 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009638287524278085) - present_state_Q ( -0.009638287524278085)) * f1( 0.006988412129581853)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.009638287524278085) - present_state_Q (-0.009638287524278085)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.891453997261932 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0059454227634670636) - present_state_Q ( -0.0059454227634670636)) * f1( 0.0071714064611425946)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.0059454227634670636) - present_state_Q (-0.0059454227634670636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.898558543274398 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012134673206783225) - present_state_Q ( -0.012134673206783225)) * f1( 0.007077814158430782)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.012134673206783225) - present_state_Q (-0.012134673206783225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.905934748862554 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00856853310932705) - present_state_Q ( -0.00856853310932705)) * f1( 0.007346102697286652)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00856853310932705) - present_state_Q (-0.00856853310932705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.913193102743593 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005432511048950569) - present_state_Q ( -0.005432511048950569)) * f1( 0.007226700598272837)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.005432511048950569) - present_state_Q (-0.005432511048950569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.9343069494697485 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05078023757521335) - present_state_Q ( -0.12714436280662744)) * f1( 0.02126991818964157)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.05078023757521335) - present_state_Q (-0.12714436280662744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.962086918323496 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0544960025004592) - present_state_Q ( -0.17271820729845455)) * f1( 0.028113333628514005)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.0544960025004592) - present_state_Q (-0.17271820729845455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.964328596410001 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040629606737532714) - present_state_Q ( -0.040629606737532714)) * f1( 0.00246375997454053)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.040629606737532714) - present_state_Q (-0.040629606737532714)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.97054755441649 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01035871243245534) - present_state_Q ( -0.01035871243245534)) * f1( 0.006814661928638492)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01035871243245534) - present_state_Q (-0.01035871243245534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.976993932927424 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006354939316864433) - present_state_Q ( -0.006354939316864433)) * f1( 0.007061078566413738)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.006354939316864433) - present_state_Q (-0.006354939316864433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.9833360425573385 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035716470124637495) - present_state_Q ( -0.057526178115468146)) * f1( 0.006983765469796881)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.035716470124637495) - present_state_Q (-0.057526178115468146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.996221437150963 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020851923844573877) - present_state_Q ( -0.08577067338846091)) * f1( 0.01423566603189506)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.020851923844573877) - present_state_Q (-0.08577067338846091)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.004076553695251 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017158765942773577) - present_state_Q ( -0.017158765942773577)) * f1( 0.008613321777422042)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.017158765942773577) - present_state_Q (-0.017158765942773577)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.011948573507971 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013929893772675731) - present_state_Q ( -0.013929893772675731)) * f1( 0.008629106963592011)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.013929893772675731) - present_state_Q (-0.013929893772675731)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.019525855577672 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01137154541879106) - present_state_Q ( -0.01137154541879106)) * f1( 0.008303927106114248)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01137154541879106) - present_state_Q (-0.01137154541879106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.026855759352599 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013404297831899517) - present_state_Q ( -0.013404297831899517)) * f1( 0.008034436605822993)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.013404297831899517) - present_state_Q (-0.013404297831899517)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.034215757576574 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0103941718945894) - present_state_Q ( -0.0103941718945894)) * f1( 0.0080650287455762)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0103941718945894) - present_state_Q (-0.0103941718945894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.041355869974283 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014445900852387507) - present_state_Q ( -0.014445900852387507)) * f1( 0.007827207217765366)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.014445900852387507) - present_state_Q (-0.014445900852387507)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.048634729853915 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01118989599675567) - present_state_Q ( -0.01118989599675567)) * f1( 0.0079767439573575)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01118989599675567) - present_state_Q (-0.01118989599675567)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.055715230872737 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008450306323265806) - present_state_Q ( -0.008450306323265806)) * f1( 0.007757270764859416)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.008450306323265806) - present_state_Q (-0.008450306323265806)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.0595997724475215 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05016778008413478) - present_state_Q ( -0.06024379397406604)) * f1( 0.004278155185380534)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.05016778008413478) - present_state_Q (-0.06024379397406604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.068667906022683 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02207885190178736) - present_state_Q ( -0.02207885190178736)) * f1( 0.009948254116530482)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.02207885190178736) - present_state_Q (-0.02207885190178736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.077278403899449 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01844576596888013) - present_state_Q ( -0.01844576596888013)) * f1( 0.009442814676999815)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.01844576596888013) - present_state_Q (-0.01844576596888013)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.085480092469471 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015375246943733562) - present_state_Q ( -0.015375246943733562)) * f1( 0.008991763587775571)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.015375246943733562) - present_state_Q (-0.015375246943733562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.093331058098729 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016766071757020277) - present_state_Q ( -0.016766071757020277)) * f1( 0.008608436590358056)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.016766071757020277) - present_state_Q (-0.016766071757020277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.101168122407245 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013563196988044509) - present_state_Q ( -0.013563196988044509)) * f1( 0.008590478857689132)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.013563196988044509) - present_state_Q (-0.013563196988044509)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.108671625194602 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038095911869961606) - present_state_Q ( -0.06436268294070939)) * f1( 0.008268669429095654)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.038095911869961606) - present_state_Q (-0.06436268294070939)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.121116616354244 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030576669421468926) - present_state_Q ( -0.09281010540385076)) * f1( 0.013758334173090792)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.030576669421468926) - present_state_Q (-0.09281010540385076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.128204393204295 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020853131735140522) - present_state_Q ( -0.020853131735140522)) * f1( 0.00864058775505712)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.020853131735140522) - present_state_Q (-0.020853131735140522)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.135136393275664 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01835348619511466) - present_state_Q ( -0.01835348619511466)) * f1( 0.008448365938949327)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01835348619511466) - present_state_Q (-0.01835348619511466)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.14189874002716 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015422564476063064) - present_state_Q ( -0.015422564476063064)) * f1( 0.00823895247855153)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.015422564476063064) - present_state_Q (-0.015422564476063064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.148481630571073 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012142520138020686) - present_state_Q ( -0.012142520138020686)) * f1( 0.00801742717374671)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.012142520138020686) - present_state_Q (-0.012142520138020686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.156432642285333 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037717812734647076) - present_state_Q ( -0.07331140705047118)) * f1( 0.009753310800355875)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.037717812734647076) - present_state_Q (-0.07331140705047118)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.167861782776284 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02241511656322901) - present_state_Q ( -0.08678678080087311)) * f1( 0.014045699947095149)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.02241511656322901) - present_state_Q (-0.08678678080087311)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.175152874997515 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019336719780407494) - present_state_Q ( -0.019336719780407494)) * f1( 0.008886967467193134)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.019336719780407494) - present_state_Q (-0.019336719780407494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.182445752783845 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01591960654874705) - present_state_Q ( -0.01591960654874705)) * f1( 0.008885812968894633)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01591960654874705) - present_state_Q (-0.01591960654874705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.189443341698042 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012990199332667042) - present_state_Q ( -0.012990199332667042)) * f1( 0.008523288132962683)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.012990199332667042) - present_state_Q (-0.012990199332667042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.1961855317596735 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015352588242049641) - present_state_Q ( -0.015352588242049641)) * f1( 0.0082143314055068)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.015352588242049641) - present_state_Q (-0.015352588242049641)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.202966296520535 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012117243523278573) - present_state_Q ( -0.012117243523278573)) * f1( 0.008258399052286985)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.012117243523278573) - present_state_Q (-0.012117243523278573)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.209527987363765 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009983277921351294) - present_state_Q ( -0.009983277921351294)) * f1( 0.007989716647249564)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.009983277921351294) - present_state_Q (-0.009983277921351294)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.215934482276088 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01329030462666283) - present_state_Q ( -0.01329030462666283)) * f1( 0.0078035733717387295)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01329030462666283) - present_state_Q (-0.01329030462666283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.2222072161276625 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009774486902636978) - present_state_Q ( -0.009774486902636978)) * f1( 0.00763769896802565)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.009774486902636978) - present_state_Q (-0.009774486902636978)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.228344418154268 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006999658993018738) - present_state_Q ( -0.006999658993018738)) * f1( 0.007470403442690825)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.006999658993018738) - present_state_Q (-0.006999658993018738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.23437073731829 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010952018385270185) - present_state_Q ( -0.010952018385270185)) * f1( 0.007338610694695017)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.010952018385270185) - present_state_Q (-0.010952018385270185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.240499423378277 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007509947857391794) - present_state_Q ( -0.007509947857391794)) * f1( 0.0074604545880563)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.007509947857391794) - present_state_Q (-0.007509947857391794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.246509651597026 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013099589150446738) - present_state_Q ( -0.013099589150446738)) * f1( 0.007320738869170801)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.013099589150446738) - present_state_Q (-0.013099589150446738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.252700202164794 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009488423318353163) - present_state_Q ( -0.009488423318353163)) * f1( 0.007537396080021769)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.009488423318353163) - present_state_Q (-0.009488423318353163)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.25876865453149 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006567760087764877) - present_state_Q ( -0.006567760087764877)) * f1( 0.0073863696179010665)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.006567760087764877) - present_state_Q (-0.006567760087764877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.2647369630150775 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010866264035498027) - present_state_Q ( -0.010866264035498027)) * f1( 0.007267899298252912)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.010866264035498027) - present_state_Q (-0.010866264035498027)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.270819135106842 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007315761773304949) - present_state_Q ( -0.007315761773304949)) * f1( 0.007403675587038871)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.007315761773304949) - present_state_Q (-0.007315761773304949)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.291489014640707 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08365396716137011) - present_state_Q ( -0.17233648705513552)) * f1( 0.025652382474160584)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08365396716137011) - present_state_Q (-0.17233648705513552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.304926924301049 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05015634872286935) - present_state_Q ( -0.07575848841645103)) * f1( 0.01648638709885149)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.05015634872286935) - present_state_Q (-0.07575848841645103)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.313006872733277 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023152277490355855) - present_state_Q ( -0.023152277490355855)) * f1( 0.009852612577470278)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.023152277490355855) - present_state_Q (-0.023152277490355855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.32070226485259 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019457750823496815) - present_state_Q ( -0.019457750823496815)) * f1( 0.009379885118170318)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.019457750823496815) - present_state_Q (-0.019457750823496815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.326757882992373 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0583173534076168) - present_state_Q ( -0.08411877501827354)) * f1( 0.007436257332620263)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0583173534076168) - present_state_Q (-0.08411877501827354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.3375841379973785 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035796384778498495) - present_state_Q ( -0.09036431937512232)) * f1( 0.01330845394918081)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.035796384778498495) - present_state_Q (-0.09036431937512232)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.350337462680944 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042130821823723484) - present_state_Q ( -0.10191918398986181)) * f1( 0.01569843002027604)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.042130821823723484) - present_state_Q (-0.10191918398986181)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.359478838475314 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.044966088749656484) - present_state_Q ( -0.10906780430978111)) * f1( 0.011261896215962026)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.044966088749656484) - present_state_Q (-0.10906780430978111)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.365687628695814 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007766858774758291) - present_state_Q ( -0.007766858774758291)) * f1( 0.007558178184187815)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.007766858774758291) - present_state_Q (-0.007766858774758291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.371769709060015 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012203042717566066) - present_state_Q ( -0.012203042717566066)) * f1( 0.007407530104772849)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.012203042717566066) - present_state_Q (-0.012203042717566066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.379585756236178 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019330672379360498) - present_state_Q ( -0.019330672379360498)) * f1( 0.009526818313217387)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.019330672379360498) - present_state_Q (-0.019330672379360498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.386188026844603 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02061997467774816) - present_state_Q ( -0.02061997467774816)) * f1( 0.009057134438495127)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.02061997467774816) - present_state_Q (-0.02061997467774816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.3927658058952845 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017025374084596912) - present_state_Q ( -0.017025374084596912)) * f1( 0.009019533501848417)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.017025374084596912) - present_state_Q (-0.017025374084596912)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.399064700071576 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014050963888027842) - present_state_Q ( -0.014050963888027842)) * f1( 0.008633953788547501)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.014050963888027842) - present_state_Q (-0.014050963888027842)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.405125098528858 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01592105441719653) - present_state_Q ( -0.01592105441719653)) * f1( 0.008308962342738854)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.01592105441719653) - present_state_Q (-0.01592105441719653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.411196072589826 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01263903631310332) - present_state_Q ( -0.01263903631310332)) * f1( 0.008320092331503529)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.01263903631310332) - present_state_Q (-0.01263903631310332)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.416511217615177 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04062970290633991) - present_state_Q ( -0.7021864183823581)) * f1( 0.008041048926290807)
w2 ( -3.1800958949143703 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.04062970290633991) - present_state_Q (-0.7021864183823581)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.426381573665523 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05998738518850002) - present_state_Q ( -0.09751575642081742)) * f1( 0.013677254358188626)
w2 ( -3.3244283141537134 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05998738518850002) - present_state_Q (-0.09751575642081742)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.431982793286903 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0711979359144445) - present_state_Q ( -0.6854506349740053)) * f1( 0.00844854081631563)
w2 ( -3.457024456923445 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0711979359144445) - present_state_Q (-0.6854506349740053)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.444001578026651 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7238319687769375) - present_state_Q ( -0.8119028349904378)) * f1( 0.018297279754167763)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.7238319687769375) - present_state_Q (-0.8119028349904378)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.46084131900771 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05177460078265583) - present_state_Q ( -0.1803944685732349)) * f1( 0.027076134190120477)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.05177460078265583) - present_state_Q (-0.1803944685732349)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.473295462089756 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02312952588124595) - present_state_Q ( -0.11350047533968613)) * f1( 0.023192435728655744)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02312952588124595) - present_state_Q (-0.11350047533968613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.478291992289143 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0196831913011508) - present_state_Q ( -0.0196831913011508)) * f1( 0.009145478298474806)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0196831913011508) - present_state_Q (-0.0196831913011508)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.48320822571952 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016400179433724595) - present_state_Q ( -0.016400179433724595)) * f1( 0.00899364189095342)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.016400179433724595) - present_state_Q (-0.016400179433724595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.487913668125044 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017853695318573174) - present_state_Q ( -0.017853695318573174)) * f1( 0.008610086245604118)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.017853695318573174) - present_state_Q (-0.017853695318573174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.492611338960605 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014435869311541798) - present_state_Q ( -0.014435869311541798)) * f1( 0.008591030177649256)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.014435869311541798) - present_state_Q (-0.014435869311541798)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.496379264678151 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012169049847828427) - present_state_Q ( -0.012169049847828427)) * f1( 0.008269098727286246)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.012169049847828427) - present_state_Q (-0.012169049847828427)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.49992235355459 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06617619434660327) - present_state_Q ( -0.10002163354585625)) * f1( 0.009950638154659498)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06617619434660327) - present_state_Q (-0.10002163354585625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.505392953265712 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04612942160084115) - present_state_Q ( -0.10554154947965738)) * f1( 0.015396520041508323)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.04612942160084115) - present_state_Q (-0.10554154947965738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.511164390273649 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03499372835495659) - present_state_Q ( -0.10118554089510896)) * f1( 0.01622839074701758)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.03499372835495659) - present_state_Q (-0.10118554089510896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5187970280437915 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01689390751112735) - present_state_Q ( -0.11334988311390028)) * f1( 0.02154646415108343)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01689390751112735) - present_state_Q (-0.11334988311390028)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.523219508990006 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056932147266118144) - present_state_Q ( -0.07866168594414255)) * f1( 0.012349502403239985)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.056932147266118144) - present_state_Q (-0.07866168594414255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.526350938851567 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07459611786505134) - present_state_Q ( -0.07600501701269781)) * f1( 0.008733535807480534)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.07459611786505134) - present_state_Q (-0.07600501701269781)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.532001400526105 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06547248019865094) - present_state_Q ( -0.07679391540096817)) * f1( 0.015766578713957577)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.06547248019865094) - present_state_Q (-0.07679391540096817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.534925176269994 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02461355106506746) - present_state_Q ( -0.106043076003014)) * f1( 0.008234857703575432)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02461355106506746) - present_state_Q (-0.106043076003014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.538459589623954 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024621002748635174) - present_state_Q ( -0.024621002748635174)) * f1( 0.00973155521157392)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.024621002748635174) - present_state_Q (-0.024621002748635174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.541947105949957 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020588027334616794) - present_state_Q ( -0.020588027334616794)) * f1( 0.009592843294573345)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.020588027334616794) - present_state_Q (-0.020588027334616794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.545264015394687 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017299297370329404) - present_state_Q ( -0.017299297370329404)) * f1( 0.009116146183992688)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.017299297370329404) - present_state_Q (-0.017299297370329404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5484638313405625 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018663314363346003) - present_state_Q ( -0.018663314363346003)) * f1( 0.008797296269597809)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.018663314363346003) - present_state_Q (-0.018663314363346003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.551552938368163 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01516331433085215) - present_state_Q ( -0.01516331433085215)) * f1( 0.008485573984633949)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01516331433085215) - present_state_Q (-0.01516331433085215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.554537114353205 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01194292706600256) - present_state_Q ( -0.01194292706600256)) * f1( 0.008190814130511034)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01194292706600256) - present_state_Q (-0.01194292706600256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.557424672415815 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015815056197662346) - present_state_Q ( -0.015815056197662346)) * f1( 0.007933210472614602)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015815056197662346) - present_state_Q (-0.015815056197662346)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.560358782839354 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012294065080648649) - present_state_Q ( -0.012294065080648649)) * f1( 0.00805409534729464)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.012294065080648649) - present_state_Q (-0.012294065080648649)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.563209872458508 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009494930901370584) - present_state_Q ( -0.009494930901370584)) * f1( 0.00782079604860347)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009494930901370584) - present_state_Q (-0.009494930901370584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5659890597438535 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012871213034585376) - present_state_Q ( -0.012871213034585376)) * f1( 0.007629921229642804)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.012871213034585376) - present_state_Q (-0.012871213034585376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.568803258762365 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00942423272348224) - present_state_Q ( -0.00942423272348224)) * f1( 0.007719467068508995)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00942423272348224) - present_state_Q (-0.00942423272348224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.571547385508094 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014783322935259741) - present_state_Q ( -0.014783322935259741)) * f1( 0.007537227752914159)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.014783322935259741) - present_state_Q (-0.014783322935259741)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.574365815314425 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011123268056603916) - present_state_Q ( -0.011123268056603916)) * f1( 0.007734316436999653)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.011123268056603916) - present_state_Q (-0.011123268056603916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.577120543488999 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00798898962125152) - present_state_Q ( -0.00798898962125152)) * f1( 0.007553659583053448)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00798898962125152) - present_state_Q (-0.00798898962125152)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.579817710845308 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01252220573834126) - present_state_Q ( -0.01252220573834126)) * f1( 0.007404106986898799)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01252220573834126) - present_state_Q (-0.01252220573834126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5825690658608 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00882367671121888) - present_state_Q ( -0.00882367671121888)) * f1( 0.007545964530832692)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00882367671121888) - present_state_Q (-0.00882367671121888)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.585265051168473 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00713820538369447) - present_state_Q ( -0.00713820538369447)) * f1( 0.00739103065005783)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00713820538369447) - present_state_Q (-0.00713820538369447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.587913087277116 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011817334284234047) - present_state_Q ( -0.011817334284234047)) * f1( 0.007267969044199812)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.011817334284234047) - present_state_Q (-0.011817334284234047)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5905310091924765 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007583658995569261) - present_state_Q ( -0.007583658995569261)) * f1( 0.007177809142914997)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.007583658995569261) - present_state_Q (-0.007583658995569261)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5931154956634055 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005111631530829039) - present_state_Q ( -0.005111631530829039)) * f1( 0.0070818160388629045)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005111631530829039) - present_state_Q (-0.005111631530829039)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.595681970213217 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012329661375946403) - present_state_Q ( -0.012329661375946403)) * f1( 0.007045001559710567)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.012329661375946403) - present_state_Q (-0.012329661375946403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5982340674451105 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007828646874514124) - present_state_Q ( -0.007828646874514124)) * f1( 0.006997754295809142)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.007828646874514124) - present_state_Q (-0.007828646874514124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.600764251967309 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004136425838411105) - present_state_Q ( -0.004136425838411105)) * f1( 0.006931354942631089)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004136425838411105) - present_state_Q (-0.004136425838411105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.603277382857316 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009614393705164651) - present_state_Q ( -0.009614393705164651)) * f1( 0.006893948085663641)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009614393705164651) - present_state_Q (-0.009614393705164651)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6058514326053634 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005441804299027569) - present_state_Q ( -0.005441804299027569)) * f1( 0.007053792463637281)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005441804299027569) - present_state_Q (-0.005441804299027569)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.608395675883383 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012034059506112355) - present_state_Q ( -0.012034059506112355)) * f1( 0.006983466474357352)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.012034059506112355) - present_state_Q (-0.012034059506112355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.611026373767413 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0078383896689265) - present_state_Q ( -0.0078383896689265)) * f1( 0.007213291660517614)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0078383896689265) - present_state_Q (-0.0078383896689265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.613621646579646 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005280449679432716) - present_state_Q ( -0.005280449679432716)) * f1( 0.007111668038726275)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005280449679432716) - present_state_Q (-0.005280449679432716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.616920176759216 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019194803430049735) - present_state_Q ( -0.019194803430049735)) * f1( 0.009069885403834847)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.019194803430049735) - present_state_Q (-0.019194803430049735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6200858459360585 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015612031675729944) - present_state_Q ( -0.015612031675729944)) * f1( 0.008696849950887419)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015612031675729944) - present_state_Q (-0.015612031675729944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.624906431717719 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03573837856305958) - present_state_Q ( -0.15947553552608817)) * f1( 0.018650825829956974)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03573837856305958) - present_state_Q (-0.15947553552608817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.631368358303635 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04049683890180575) - present_state_Q ( -0.17173298501051862)) * f1( 0.025115651006913854)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04049683890180575) - present_state_Q (-0.17173298501051862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6375140221467674 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04962205833104699) - present_state_Q ( -0.08300532290197192)) * f1( 0.023082230737377953)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04962205833104699) - present_state_Q (-0.08300532290197192)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.639554619758558 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015053208575314149) - present_state_Q ( -0.015053208575314149)) * f1( 0.007482928959738977)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.015053208575314149) - present_state_Q (-0.015053208575314149)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.641657029176238 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01134129516419339) - present_state_Q ( -0.01134129516419339)) * f1( 0.00770016149441969)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01134129516419339) - present_state_Q (-0.01134129516419339)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.643714282981404 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008007490375935376) - present_state_Q ( -0.008007490375935376)) * f1( 0.007526506183189487)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.008007490375935376) - present_state_Q (-0.008007490375935376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.645728023769284 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013008148309192119) - present_state_Q ( -0.013008148309192119)) * f1( 0.0073794635606901585)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.013008148309192119) - present_state_Q (-0.013008148309192119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.647788883340432 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009210598200898371) - present_state_Q ( -0.009210598200898371)) * f1( 0.007542685940164269)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.009210598200898371) - present_state_Q (-0.009210598200898371)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6498090040102085 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007032738493565471) - present_state_Q ( -0.007032738493565471)) * f1( 0.007388282533475909)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.007032738493565471) - present_state_Q (-0.007032738493565471)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6559270282789855 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07496297259169601) - present_state_Q ( -0.17190473069376636)) * f1( 0.02374877301027784)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07496297259169601) - present_state_Q (-0.17190473069376636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.661896368363703 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05887137672437121) - present_state_Q ( -0.14345299744197704)) * f1( 0.022932664077946488)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05887137672437121) - present_state_Q (-0.14345299744197704)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.667437910895354 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07253406967468078) - present_state_Q ( -0.17664223386505676)) * f1( 0.021552669798284955)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07253406967468078) - present_state_Q (-0.17664223386505676)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.671013589058282 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06822098133656726) - present_state_Q ( -0.0983810215951065)) * f1( 0.013498255402183384)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06822098133656726) - present_state_Q (-0.0983810215951065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.675466758692986 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05848375678581414) - present_state_Q ( -0.060017092815483335)) * f1( 0.01657682315865482)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05848375678581414) - present_state_Q (-0.060017092815483335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.677856558104747 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016223271052433763) - present_state_Q ( -0.016223271052433763)) * f1( 0.008766847197214305)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.016223271052433763) - present_state_Q (-0.016223271052433763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.679784746021013 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007031852829524325) - present_state_Q ( -0.007031852829524325)) * f1( 0.007052050485433754)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.007031852829524325) - present_state_Q (-0.007031852829524325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.681719105079133 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005434789563706341) - present_state_Q ( -0.005434789563706341)) * f1( 0.007070903375301569)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.005434789563706341) - present_state_Q (-0.005434789563706341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6836279123675295 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01121488676681665) - present_state_Q ( -0.01121488676681665)) * f1( 0.006990794392958885)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01121488676681665) - present_state_Q (-0.01121488676681665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.685526222945095 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006498775266142631) - present_state_Q ( -0.006498775266142631)) * f1( 0.006941560693645456)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.006498775266142631) - present_state_Q (-0.006498775266142631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6873937008169015 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03065431810051412) - present_state_Q ( -0.03065431810051412)) * f1( 0.006883536453758198)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03065431810051412) - present_state_Q (-0.03065431810051412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.691399510032724 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015825320430749162) - present_state_Q ( -0.1446392807285498)) * f1( 0.015421816886799148)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.015825320430749162) - present_state_Q (-0.1446392807285498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.69380055567081 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016102449694779794) - present_state_Q ( -0.016102449694779794)) * f1( 0.008807752012964062)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.016102449694779794) - present_state_Q (-0.016102449694779794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.696106237253182 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013713083256395977) - present_state_Q ( -0.013713083256395977)) * f1( 0.008451261485812921)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.013713083256395977) - present_state_Q (-0.013713083256395977)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.698333613630138 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015122518465592382) - present_state_Q ( -0.015122518465592382)) * f1( 0.008168038796197213)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.015122518465592382) - present_state_Q (-0.015122518465592382)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.700500593053539 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011524618563223015) - present_state_Q ( -0.011524618563223015)) * f1( 0.00793713144262155)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.011524618563223015) - present_state_Q (-0.011524618563223015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.70255271857852 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07021609964194293) - present_state_Q ( -0.08946718990224317)) * f1( 0.00772025419421873)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07021609964194293) - present_state_Q (-0.08946718990224317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.706168100411137 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03652216647311692) - present_state_Q ( -0.03652216647311692)) * f1( 0.013352314302455925)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03652216647311692) - present_state_Q (-0.03652216647311692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7102102970505495 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03495852850675547) - present_state_Q ( -0.11845468511879309)) * f1( 0.015395366980674317)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03495852850675547) - present_state_Q (-0.11845468511879309)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7169379747281885 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06133072449690754) - present_state_Q ( -0.18327830947511106)) * f1( 0.026245067504510167)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06133072449690754) - present_state_Q (-0.18327830947511106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7237714797907175 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04433757027100268) - present_state_Q ( -0.13783040490734086)) * f1( 0.02621058132199256)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04433757027100268) - present_state_Q (-0.13783040490734086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.724621674749171 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05690241484062661) - present_state_Q ( -0.05690241484062661)) * f1( 0.0031613522078956747)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.05690241484062661) - present_state_Q (-0.05690241484062661)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.72692904683685 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015652250716431174) - present_state_Q ( -0.015652250716431174)) * f1( 0.008462871620734757)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.015652250716431174) - present_state_Q (-0.015652250716431174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7291601561717105 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012271196472216448) - present_state_Q ( -0.012271196472216448)) * f1( 0.008174035790255415)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.012271196472216448) - present_state_Q (-0.012271196472216448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.732515604053249 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035904782418356956) - present_state_Q ( -0.10059167574086851)) * f1( 0.012692959050642477)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.035904782418356956) - present_state_Q (-0.10059167574086851)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.736468999903041 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02922497818036569) - present_state_Q ( -0.09881864247133086)) * f1( 0.014948622622484314)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02922497818036569) - present_state_Q (-0.09881864247133086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.738969292795471 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020624709004211185) - present_state_Q ( -0.020624709004211185)) * f1( 0.009185534655581334)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.020624709004211185) - present_state_Q (-0.020624709004211185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.741427870024866 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017231146059150736) - present_state_Q ( -0.017231146059150736)) * f1( 0.00902215703723235)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.017231146059150736) - present_state_Q (-0.017231146059150736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.743512163414843 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04183376261225532) - present_state_Q ( -0.08851156673043588)) * f1( 0.007846829715903775)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04183376261225532) - present_state_Q (-0.08851156673043588)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.748890486239787 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07157431602343423) - present_state_Q ( -0.15954827046452844)) * f1( 0.02078048091015578)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.07157431602343423) - present_state_Q (-0.15954827046452844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.753466935482936 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06285467813377159) - present_state_Q ( -0.07086973043386563)) * f1( 0.017102036095890755)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06285467813377159) - present_state_Q (-0.07086973043386563)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.75600028527234 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022688928361184864) - present_state_Q ( -0.022688928361184864)) * f1( 0.00931333502327902)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.022688928361184864) - present_state_Q (-0.022688928361184864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.758508774773713 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018803412760429673) - present_state_Q ( -0.018803412760429673)) * f1( 0.0092101009513974)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.018803412760429673) - present_state_Q (-0.018803412760429673)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7609054484192885 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01588135941974526) - present_state_Q ( -0.01588135941974526)) * f1( 0.008791072559320197)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.01588135941974526) - present_state_Q (-0.01588135941974526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.763176952749134 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051982539633794414) - present_state_Q ( -0.06875612289693829)) * f1( 0.008485280270221572)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.051982539633794414) - present_state_Q (-0.06875612289693829)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.766452604182454 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02574692351871274) - present_state_Q ( -0.08265373870670717)) * f1( 0.012312291113255654)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02574692351871274) - present_state_Q (-0.08265373870670717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7691146730357605 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02332041305375965) - present_state_Q ( -0.02332041305375965)) * f1( 0.009788589160981276)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.02332041305375965) - present_state_Q (-0.02332041305375965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7716466207437565 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019371327060592546) - present_state_Q ( -0.019371327060592546)) * f1( 0.009297974337414538)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.019371327060592546) - present_state_Q (-0.019371327060592546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.77406310952068 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016376938162749148) - present_state_Q ( -0.016376938162749148)) * f1( 0.008865205433623138)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.016376938162749148) - present_state_Q (-0.016376938162749148)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.776392482981345 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017524430776809805) - present_state_Q ( -0.017524430776809805)) * f1( 0.00854885044049299)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.017524430776809805) - present_state_Q (-0.017524430776809805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.7786279954593684 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03985806349531354) - present_state_Q ( -0.03985806349531354)) * f1( 0.008265350812060354)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03985806349531354) - present_state_Q (-0.03985806349531354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.782333382568433 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005056025499446741) - present_state_Q ( -0.13825532146542488)) * f1( 0.014236146566506507)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.005056025499446741) - present_state_Q (-0.13825532146542488)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.784957196692885 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024706104478436738) - present_state_Q ( -0.024706104478436738)) * f1( 0.009652350516526203)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.024706104478436738) - present_state_Q (-0.024706104478436738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.786675459943373 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020616518381093407) - present_state_Q ( -0.020616518381093407)) * f1( 0.00950114794483212)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.020616518381093407) - present_state_Q (-0.020616518381093407)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.788312186444296 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01751136351955119) - present_state_Q ( -0.01751136351955119)) * f1( 0.009036326138011197)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01751136351955119) - present_state_Q (-0.01751136351955119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.78988518917728 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018039419141577702) - present_state_Q ( -0.018039419141577702)) * f1( 0.00868678808401147)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.018039419141577702) - present_state_Q (-0.018039419141577702)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.791405145842192 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01441247590580769) - present_state_Q ( -0.01441247590580769)) * f1( 0.008378741209600778)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01441247590580769) - present_state_Q (-0.01441247590580769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.792875481906178 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011530571686973022) - present_state_Q ( -0.011530571686973022)) * f1( 0.008093636166924805)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.011530571686973022) - present_state_Q (-0.011530571686973022)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.79364127720102 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05915947855694396) - present_state_Q ( -0.0686065853555576)) * f1( 0.004340397092104451)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05915947855694396) - present_state_Q (-0.0686065853555576)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.795471754818208 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01626881024472859) - present_state_Q ( -0.01626881024472859)) * f1( 0.010099785508846926)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01626881024472859) - present_state_Q (-0.01626881024472859)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.796967493224499 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012826755435740867) - present_state_Q ( -0.012826755435740867)) * f1( 0.008238756833254464)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012826755435740867) - present_state_Q (-0.012826755435740867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.798411849184661 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017142104983217388) - present_state_Q ( -0.017142104983217388)) * f1( 0.007972790338572618)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.017142104983217388) - present_state_Q (-0.017142104983217388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.799884155254505 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013496459382558364) - present_state_Q ( -0.013496459382558364)) * f1( 0.00811238119489707)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013496459382558364) - present_state_Q (-0.013496459382558364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.801584349302524 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07608986640996832) - present_state_Q ( -0.10404982283052154)) * f1( 0.009824339985272662)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07608986640996832) - present_state_Q (-0.10404982283052154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.804310230320136 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05067939750999668) - present_state_Q ( -0.05067939750999668)) * f1( 0.015301705213627597)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05067939750999668) - present_state_Q (-0.05067939750999668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.805642263166521 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00680303420835173) - present_state_Q ( -0.00680303420835173)) * f1( 0.007315197103935255)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00680303420835173) - present_state_Q (-0.00680303420835173)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.806950905328701 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012126166684221529) - present_state_Q ( -0.012126166684221529)) * f1( 0.007205699410151257)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012126166684221529) - present_state_Q (-0.012126166684221529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.808292060377359 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00811129365380672) - present_state_Q ( -0.00811129365380672)) * f1( 0.00737005960730958)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00811129365380672) - present_state_Q (-0.00811129365380672)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8096112049514455 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00658243753314671) - present_state_Q ( -0.00658243753314671)) * f1( 0.0072436280945016055)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00658243753314671) - present_state_Q (-0.00658243753314671)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.810906925929982 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011615865944787293) - present_state_Q ( -0.011615865944787293)) * f1( 0.00713274848749854)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.011615865944787293) - present_state_Q (-0.011615865944787293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.812151907713871 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03675659459104745) - present_state_Q ( -0.0675443971380811)) * f1( 0.0070610592268087104)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03675659459104745) - present_state_Q (-0.0675443971380811)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.814697060315571 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03601538770523769) - present_state_Q ( -0.1132313974168643)) * f1( 0.014819760864000352)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03601538770523769) - present_state_Q (-0.1132313974168643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.816863964331198 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05690441547748452) - present_state_Q ( -0.06961274527072356)) * f1( 0.012290221951771235)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05690441547748452) - present_state_Q (-0.06961274527072356)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.818375422467618 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018843232203339142) - present_state_Q ( -0.018843232203339142)) * f1( 0.008350248730623606)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.018843232203339142) - present_state_Q (-0.018843232203339142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.819854791274852 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03349764420880521) - present_state_Q ( -0.07616000051483689)) * f1( 0.008433179507987302)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03349764420880521) - present_state_Q (-0.07616000051483689)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.822448503591219 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04205225686944844) - present_state_Q ( -0.11213492003358762)) * f1( 0.015087575320773696)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04205225686944844) - present_state_Q (-0.11213492003358762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.825866642980379 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.068947711657356) - present_state_Q ( -0.16066061988491495)) * f1( 0.0204279175183497)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.068947711657356) - present_state_Q (-0.16066061988491495)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.829666440567636 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061044025929022666) - present_state_Q ( -0.1517524769342056)) * f1( 0.022599192789025276)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.061044025929022666) - present_state_Q (-0.1517524769342056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.829791665754027 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04511959026717847) - present_state_Q ( -0.04511959026717847)) * f1( 0.0007009813216183517)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04511959026717847) - present_state_Q (-0.04511959026717847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.830993298339214 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009051264411645036) - present_state_Q ( -0.009051264411645036)) * f1( 0.00660641199095346)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009051264411645036) - present_state_Q (-0.009051264411645036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.832182571433154 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02954545987331869) - present_state_Q ( -0.02954545987331869)) * f1( 0.006605444938327597)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02954545987331869) - present_state_Q (-0.02954545987331869)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.834956401138872 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06787242937396881) - present_state_Q ( -0.06675013376715626)) * f1( 0.0156973248804627)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06787242937396881) - present_state_Q (-0.06675013376715626)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.836810436677617 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016415103883693643) - present_state_Q ( -0.016415103883693643)) * f1( 0.01023051116557252)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.016415103883693643) - present_state_Q (-0.016415103883693643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.838298448221551 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012867869873651228) - present_state_Q ( -0.012867869873651228)) * f1( 0.008196363145606025)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012867869873651228) - present_state_Q (-0.012867869873651228)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.839740922934262 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010746854479822933) - present_state_Q ( -0.010746854479822933)) * f1( 0.007937188375294448)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.010746854479822933) - present_state_Q (-0.010746854479822933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.84347540034959 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05407459814079689) - present_state_Q ( -0.14887517726969154)) * f1( 0.02218193816372836)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05407459814079689) - present_state_Q (-0.14887517726969154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.847481019159762 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07617761191273689) - present_state_Q ( -0.17270298049957203)) * f1( 0.024101931802848135)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07617761191273689) - present_state_Q (-0.17270298049957203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.849046142343686 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07122826474467331) - present_state_Q ( -0.07122826474467331)) * f1( 0.008877970318645886)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07122826474467331) - present_state_Q (-0.07122826474467331)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.85201571515045 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05291587678620369) - present_state_Q ( -0.12706985548762292)) * f1( 0.017414232298173283)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05291587678620369) - present_state_Q (-0.12706985548762292)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8552811939384934 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03445478947771721) - present_state_Q ( -0.12675287108481526)) * f1( 0.019166677472106337)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03445478947771721) - present_state_Q (-0.12675287108481526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.855800143166839 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05759069290656118) - present_state_Q ( -0.05759069290656118)) * f1( 0.0029233235069630065)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05759069290656118) - present_state_Q (-0.05759069290656118)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.85727798718548 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04469877035818445) - present_state_Q ( -0.07235965655358839)) * f1( 0.00840092350368989)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04469877035818445) - present_state_Q (-0.07235965655358839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.859623050362451 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02898997545501982) - present_state_Q ( -0.09031088089792205)) * f1( 0.013480298320532711)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02898997545501982) - present_state_Q (-0.09031088089792205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8611120426719925 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014588754103147628) - present_state_Q ( -0.014588754103147628)) * f1( 0.00820876851065782)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014588754103147628) - present_state_Q (-0.014588754103147628)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.862589884462666 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011926070247706455) - present_state_Q ( -0.011926070247706455)) * f1( 0.008136546557227035)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.011926070247706455) - present_state_Q (-0.011926070247706455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.864031250318927 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01575195532167393) - present_state_Q ( -0.01575195532167393)) * f1( 0.007950794087658848)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01575195532167393) - present_state_Q (-0.01575195532167393)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8654422656442 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011950530366844344) - present_state_Q ( -0.011950530366844344)) * f1( 0.007768714561864242)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.011950530366844344) - present_state_Q (-0.011950530366844344)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8668219688177325 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008594075556985558) - present_state_Q ( -0.008594075556985558)) * f1( 0.007583704198081501)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.008594075556985558) - present_state_Q (-0.008594075556985558)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.868170018764781 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01350058944022144) - present_state_Q ( -0.01350058944022144)) * f1( 0.007427747301969501)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01350058944022144) - present_state_Q (-0.01350058944022144)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.869548249304635 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009623048161720686) - present_state_Q ( -0.009623048161720686)) * f1( 0.007579467854860447)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009623048161720686) - present_state_Q (-0.009623048161720686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.870898743940309 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0074929281357753235) - present_state_Q ( -0.0074929281357753235)) * f1( 0.007419114525121733)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0074929281357753235) - present_state_Q (-0.0074929281357753235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.872226522861599 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013039805238108826) - present_state_Q ( -0.013039805238108826)) * f1( 0.007314382713149712)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013039805238108826) - present_state_Q (-0.013039805238108826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.873486959851276 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0651230106347537) - present_state_Q ( -0.08839555802532598)) * f1( 0.007222508641239303)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0651230106347537) - present_state_Q (-0.08839555802532598)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.875797868764742 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03382319835780474) - present_state_Q ( -0.03382319835780474)) * f1( 0.012862724727024571)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03382319835780474) - present_state_Q (-0.03382319835780474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.877414310795812 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020182777454663434) - present_state_Q ( -0.020182777454663434)) * f1( 0.008936197931900616)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.020182777454663434) - present_state_Q (-0.020182777454663434)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.879016672798122 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016578821260034324) - present_state_Q ( -0.016578821260034324)) * f1( 0.008842503258979176)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.016578821260034324) - present_state_Q (-0.016578821260034324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.8805513933935325 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01926512872815179) - present_state_Q ( -0.01926512872815179)) * f1( 0.008480544234666066)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01926512872815179) - present_state_Q (-0.01926512872815179)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.882097404113601 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015560285269511644) - present_state_Q ( -0.015560285269511644)) * f1( 0.00852721972018817)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015560285269511644) - present_state_Q (-0.015560285269511644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.883589920293646 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01250353256384705) - present_state_Q ( -0.01250353256384705)) * f1( 0.00821969125452049)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01250353256384705) - present_state_Q (-0.01250353256384705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.885033527499538 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015385132186867724) - present_state_Q ( -0.015385132186867724)) * f1( 0.007961707802045923)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015385132186867724) - present_state_Q (-0.015385132186867724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.886490391507868 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01182370354809495) - present_state_Q ( -0.01182370354809495)) * f1( 0.00802064244123799)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01182370354809495) - present_state_Q (-0.01182370354809495)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.887906567815951 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009975921071057622) - present_state_Q ( -0.009975921071057622)) * f1( 0.007789508017212953)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009975921071057622) - present_state_Q (-0.009975921071057622)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.889285822435269 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013180264688164632) - present_state_Q ( -0.013180264688164632)) * f1( 0.007598477853500188)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013180264688164632) - present_state_Q (-0.013180264688164632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.891888562498249 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042075465153822635) - present_state_Q ( -0.11353512936907106)) * f1( 0.015152410720504205)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.042075465153822635) - present_state_Q (-0.11353512936907106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.895461336734116 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029229361186994898) - present_state_Q ( -0.08255611792780437)) * f1( 0.02044621466725505)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.029229361186994898) - present_state_Q (-0.08255611792780437)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.896209441130297 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06873073380873025) - present_state_Q ( -0.06997844974371249)) * f1( 0.004241125044598735)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06873073380873025) - present_state_Q (-0.06997844974371249)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.898050069361981 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01481779225235804) - present_state_Q ( -0.01481779225235804)) * f1( 0.010148479750138988)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01481779225235804) - present_state_Q (-0.01481779225235804)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.899547977229978 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016684695026975307) - present_state_Q ( -0.016684695026975307)) * f1( 0.008266516366072398)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.016684695026975307) - present_state_Q (-0.016684695026975307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.903507001986723 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05392964807996263) - present_state_Q ( -0.08430732743693145)) * f1( 0.022647326596769705)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05392964807996263) - present_state_Q (-0.08430732743693145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.904925072418581 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014234131795330129) - present_state_Q ( -0.014234131795330129)) * f1( 0.007816403082918229)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014234131795330129) - present_state_Q (-0.014234131795330129)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.906356024072835 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010734434827545162) - present_state_Q ( -0.010734434827545162)) * f1( 0.007873734548249429)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.010734434827545162) - present_state_Q (-0.010734434827545162)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.907745901189087 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015788378026272542) - present_state_Q ( -0.015788378026272542)) * f1( 0.007666912972567971)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015788378026272542) - present_state_Q (-0.015788378026272542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.91001265511129 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07482444655261383) - present_state_Q ( -0.0791556012932751)) * f1( 0.012913318031488707)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.07482444655261383) - present_state_Q (-0.0791556012932751)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.91195648542936 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06876810681205774) - present_state_Q ( -0.06876810681205774)) * f1( 0.011012309479207208)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06876810681205774) - present_state_Q (-0.06876810681205774)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.913382088142095 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046034761862292224) - present_state_Q ( -0.09350222608808839)) * f1( 0.008201906650743097)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.046034761862292224) - present_state_Q (-0.09350222608808839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.915067092883657 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04793895250962213) - present_state_Q ( -0.04793895250962213)) * f1( 0.00944567923293507)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04793895250962213) - present_state_Q (-0.04793895250962213)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.916823950720502 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03870091148521613) - present_state_Q ( -0.10530457457375765)) * f1( 0.010181141777321078)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03870091148521613) - present_state_Q (-0.10530457457375765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.921042413311611 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08167250820386993) - present_state_Q ( -0.17933463354696874)) * f1( 0.025475852258458977)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.08167250820386993) - present_state_Q (-0.17933463354696874)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.924567817727367 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05756850587662535) - present_state_Q ( -0.14612793777525854)) * f1( 0.020901647593087085)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05756850587662535) - present_state_Q (-0.14612793777525854)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.925385738411424 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038005031638135595) - present_state_Q ( -0.038005031638135595)) * f1( 0.004562176555185033)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.038005031638135595) - present_state_Q (-0.038005031638135595)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.929524654800361 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04358599371672788) - present_state_Q ( -0.1820745570105078)) * f1( 0.02509470605745792)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04358599371672788) - present_state_Q (-0.1820745570105078)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.930396444673716 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04080203767638128) - present_state_Q ( -0.04080203767638128)) * f1( 0.004869483930470513)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04080203767638128) - present_state_Q (-0.04080203767638128)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.931096631819331 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06323022770059124) - present_state_Q ( -0.06323022770059124)) * f1( 0.003955575113479083)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.06323022770059124) - present_state_Q (-0.06323022770059124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.93274516183838 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022389806451198625) - present_state_Q ( -0.022389806451198625)) * f1( 0.009123609046808019)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.022389806451198625) - present_state_Q (-0.022389806451198625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.934384819143688 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01847008306130732) - present_state_Q ( -0.01847008306130732)) * f1( 0.009056821488208352)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01847008306130732) - present_state_Q (-0.01847008306130732)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.935955521841721 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01543281582598994) - present_state_Q ( -0.01543281582598994)) * f1( 0.008662863323439737)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01543281582598994) - present_state_Q (-0.01543281582598994)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9374763354374664 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017695908132282667) - present_state_Q ( -0.017695908132282667)) * f1( 0.00839714388414053)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.017695908132282667) - present_state_Q (-0.017695908132282667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.9389541047478716 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01398156051523488) - present_state_Q ( -0.01398156051523488)) * f1( 0.00814444279120759)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01398156051523488) - present_state_Q (-0.01398156051523488)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.958301506667197 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0490658150180945) - present_state_Q ( -0.0490658150180945)) * f1( 0.007071067811865476)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0490658150180945) - present_state_Q (-0.0490658150180945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.977655676637474 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059857572866504165) - present_state_Q ( -0.040573521355205894)) * f1( 0.007071067811865476)
w2 ( -3.588396823758573 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.059857572866504165) - present_state_Q (-0.040573521355205894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.993803929753945 ) += alpha ( 0.1 ) * (reward ( -27.405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17566655781625956) - present_state_Q ( -0.7654378314048774)) * f1( 0.006057644011666969)
w2 ( -4.121549748736292 ) += alpha ( 0.1) * (reward ( -27.405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.17566655781625956) - present_state_Q (-0.7654378314048774)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.043225602492222 ) += alpha ( 0.1 ) * (reward ( -25.57848292954194 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.060737990853661016) - present_state_Q ( -0.1495319171821133)) * f1( 0.019430558100355454)
w2 ( -4.630250244965197 ) += alpha ( 0.1) * (reward ( -25.57848292954194) + discount_factor ( 0.1) * next_state_max_Q( -0.060737990853661016) - present_state_Q (-0.1495319171821133)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.09564804259744 ) += alpha ( 0.1 ) * (reward ( -23.751448434574662 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.058627213154043456) - present_state_Q ( -0.15457383430054875)) * f1( 0.02221032221430814)
w2 ( -4.630250244965197 ) += alpha ( 0.1) * (reward ( -23.751448434574662) + discount_factor ( 0.1) * next_state_max_Q( -0.058627213154043456) - present_state_Q (-0.15457383430054875)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.129206548217233 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05172661745527831) - present_state_Q ( -1.017634577073454)) * f1( 0.015375846082361054)
w2 ( -4.630250244965197 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.05172661745527831) - present_state_Q (-1.017634577073454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.149974480797934 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02508017027632735) - present_state_Q ( -0.02508017027632735)) * f1( 0.009102610460740804)
w2 ( -4.630250244965197 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.02508017027632735) - present_state_Q (-0.02508017027632735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.170015503450492 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062960932973099) - present_state_Q ( -0.10210448590489501)) * f1( 0.0088122919894792)
w2 ( -4.630250244965197 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.062960932973099) - present_state_Q (-0.10210448590489501)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.202488452320748 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10933969055095906) - present_state_Q ( -0.10933969055095906)) * f1( 0.014280398647450863)
w2 ( -5.0850407542771 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.10933969055095906) - present_state_Q (-0.10933969055095906)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.214984199584489 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0730945454908052) - present_state_Q ( -1.07447995685466)) * f1( 0.005713448857055032)
w2 ( -5.5224559679728085 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -1.0730945454908052) - present_state_Q (-1.07447995685466)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.242816510323608 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08118707014464042) - present_state_Q ( -1.1856782637392023)) * f1( 0.012849415150430966)
w2 ( -5.955663400580134 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.08118707014464042) - present_state_Q (-1.1856782637392023)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.271973007181305 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.2230029540677578) - present_state_Q ( -1.30071813101934)) * f1( 0.013461288988613287)
w2 ( -6.388853667609703 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -1.2230029540677578) - present_state_Q (-1.30071813101934)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.314971532168202 ) += alpha ( 0.1 ) * (reward ( -22.83793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.327501645665895) - present_state_Q ( -1.430031384512128)) * f1( 0.01996157267885556)
w2 ( -6.819666666952612 ) += alpha ( 0.1) * (reward ( -22.83793118709102) + discount_factor ( 0.1) * next_state_max_Q( -1.327501645665895) - present_state_Q (-1.430031384512128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.331446943467871 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07018545756552574) - present_state_Q ( -0.07018545756552574)) * f1( 0.008223638075451898)
w2 ( -6.819666666952612 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.07018545756552574) - present_state_Q (-0.07018545756552574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.355120768446545 ) += alpha ( 0.1 ) * (reward ( -20.097379444640097 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10237128957770864) - present_state_Q ( -0.0823066551225026)) * f1( 0.011821951857450597)
w2 ( -6.819666666952612 ) += alpha ( 0.1) * (reward ( -20.097379444640097) + discount_factor ( 0.1) * next_state_max_Q( -0.10237128957770864) - present_state_Q (-0.0823066551225026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.372126661635039 ) += alpha ( 0.1 ) * (reward ( -19.183862197156458 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10354038448065174) - present_state_Q ( -0.10354038448065174)) * f1( 0.008907957644408404)
w2 ( -6.819666666952612 ) += alpha ( 0.1) * (reward ( -19.183862197156458) + discount_factor ( 0.1) * next_state_max_Q( -0.10354038448065174) - present_state_Q (-0.10354038448065174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.381628955384745 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4106213420588356) - present_state_Q ( -1.4106213420588356)) * f1( 0.005589326219394049)
w2 ( -7.159682381789009 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -1.4106213420588356) - present_state_Q (-1.4106213420588356)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.404359552214011 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.4694842613148407) - present_state_Q ( -1.5672403694109476)) * f1( 0.013489926008328355)
w2 ( -7.496683441916876 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -1.4694842613148407) - present_state_Q (-1.5672403694109476)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.435270361069443 ) += alpha ( 0.1 ) * (reward ( -18.27034494967282 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.556964030330958) - present_state_Q ( -1.6615053502805044)) * f1( 0.018438213172711687)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -18.27034494967282) + discount_factor ( 0.1) * next_state_max_Q( -1.556964030330958) - present_state_Q (-1.6615053502805044)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.4729116588304505 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07840184894812299) - present_state_Q ( -0.17981743404002215)) * f1( 0.02190376801000353)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.07840184894812299) - present_state_Q (-0.17981743404002215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.49519870169453 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06330416325490758) - present_state_Q ( -0.12250268954663796)) * f1( 0.012927027603969712)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.06330416325490758) - present_state_Q (-0.12250268954663796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.540847581869069 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10060785169937268) - present_state_Q ( -0.21472264641220742)) * f1( 0.02661406180294874)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.10060785169937268) - present_state_Q (-0.21472264641220742)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.551169098092909 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08830042054934052) - present_state_Q ( -0.08830042054934052)) * f1( 0.00597401328829647)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08830042054934052) - present_state_Q (-0.08830042054934052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.572350793282553 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054327332613777236) - present_state_Q ( -0.10331228474740858)) * f1( 0.01227287723718045)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.054327332613777236) - present_state_Q (-0.10331228474740858)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.589024873594686 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04191070761707641) - present_state_Q ( -0.04191070761707641)) * f1( 0.009627562949103835)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.04191070761707641) - present_state_Q (-0.04191070761707641)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.618729365767299 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05203334785806653) - present_state_Q ( -0.15071802376511903)) * f1( 0.017258701965280135)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.05203334785806653) - present_state_Q (-0.15071802376511903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.6661994407937915 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07792487681329315) - present_state_Q ( -0.18246240812540818)) * f1( 0.027627539933852776)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.07792487681329315) - present_state_Q (-0.18246240812540818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.710621868764739 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05330289678501698) - present_state_Q ( -0.16169320464358536)) * f1( 0.025826295114968593)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.05330289678501698) - present_state_Q (-0.16169320464358536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.722626475675333 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08378329795521876) - present_state_Q ( -0.09978213652166389)) * f1( 0.006952975486311868)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.08378329795521876) - present_state_Q (-0.09978213652166389)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.738090529364769 ) += alpha ( 0.1 ) * (reward ( -17.356827702189175 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0812468590157993) - present_state_Q ( -0.0812468590157993)) * f1( 0.008947186506633053)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -17.356827702189175) + discount_factor ( 0.1) * next_state_max_Q( -0.0812468590157993) - present_state_Q (-0.0812468590157993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.765075224149772 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09240203554148815) - present_state_Q ( -1.658796867934565)) * f1( 0.0182406001664369)
w2 ( -7.831974161965385 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.09240203554148815) - present_state_Q (-1.658796867934565)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.782728304402502 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10396542135239312) - present_state_Q ( -1.67036025374547)) * f1( 0.011941193397885146)
w2 ( -8.127641096827292 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -0.10396542135239312) - present_state_Q (-1.67036025374547)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.800303479420128 ) += alpha ( 0.1 ) * (reward ( -16.443310454705536 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.6893129598437158) - present_state_Q ( -1.7288927224818582)) * f1( 0.011808615778825486)
w2 ( -8.425308077391453 ) += alpha ( 0.1) * (reward ( -16.443310454705536) + discount_factor ( 0.1) * next_state_max_Q( -1.6893129598437158) - present_state_Q (-1.7288927224818582)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -7.827339079763553 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13373792744892468) - present_state_Q ( -0.1448973389353511)) * f1( 0.01755755740000106)
w2 ( -8.425308077391453 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.13373792744892468) - present_state_Q (-0.1448973389353511)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.879140190176818 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14792122698104657) - present_state_Q ( -0.27790296679209764)) * f1( 0.03393082346323928)
w2 ( -8.425308077391453 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.14792122698104657) - present_state_Q (-0.27790296679209764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.906058667925673 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12756492494466146) - present_state_Q ( -0.14548135571289428)) * f1( 0.01748285916996594)
w2 ( -8.425308077391453 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.12756492494466146) - present_state_Q (-0.14548135571289428)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -7.95774802094141 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14522066780757975) - present_state_Q ( -0.27953193961686645)) * f1( 0.03386183208988007)
w2 ( -8.425308077391453 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -0.14522066780757975) - present_state_Q (-0.27953193961686645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.01019332810594 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.8575696069329994) - present_state_Q ( -1.9952678400270618)) * f1( 0.03822465595910379)
w2 ( -8.699713723949216 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -1.8575696069329994) - present_state_Q (-1.9952678400270618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.062463131592793 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.6951626872786094) - present_state_Q ( -3.852029419566854)) * f1( 0.04338722370711371)
w2 ( -9.181604926204532 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -3.6951626872786094) - present_state_Q (-3.852029419566854)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.11757436171653 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.907043170350962) - present_state_Q ( -4.058108478811918)) * f1( 0.04645879503075157)
w2 ( -9.656100488022334 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -3.907043170350962) - present_state_Q (-4.058108478811918)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.146761242583452 ) += alpha ( 0.1 ) * (reward ( -15.529793207221894 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.152898160445498) - present_state_Q ( -4.084118258049965)) * f1( 0.02502955926424176)
w2 ( -10.122539078630993 ) += alpha ( 0.1) * (reward ( -15.529793207221894) + discount_factor ( 0.1) * next_state_max_Q( -2.152898160445498) - present_state_Q (-4.084118258049965)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.203472557008084 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.2591285339082834) - present_state_Q ( -2.4075570762742413)) * f1( 0.0456075544694624)
w2 ( -10.37123171336809 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -2.2591285339082834) - present_state_Q (-2.4075570762742413)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -8.25275219008926 ) += alpha ( 0.1 ) * (reward ( -14.616275959738253 ) + discount_factor ( 0.1 ) * next_state_max_Q(-4.462160758471623) - present_state_Q ( -4.53126750049029)) * f1( 0.0467938300213361)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -14.616275959738253) + discount_factor ( 0.1) * next_state_max_Q( -4.462160758471623) - present_state_Q (-4.53126750049029)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -8.32079945988111 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5287342317586259) - present_state_Q ( -0.5287342317586259)) * f1( 0.06489130603326043)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.5287342317586259) - present_state_Q (-0.5287342317586259)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.375272729895405 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4710664629697868) - present_state_Q ( -0.4710664629697868)) * f1( 0.05659719342600375)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.4710664629697868) - present_state_Q (-0.4710664629697868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.424600479944294 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4417173704606929) - present_state_Q ( -0.4417173704606929)) * f1( 0.05111077981479337)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.4417173704606929) - present_state_Q (-0.4417173704606929)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.473950016892815 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4449375753859639) - present_state_Q ( -0.4449375753859639)) * f1( 0.051148713908342414)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.4449375753859639) - present_state_Q (-0.4449375753859639)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.53644488020419 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46584362066727736) - present_state_Q ( -0.5759965415106995)) * f1( 0.06565084695754558)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.46584362066727736) - present_state_Q (-0.5759965415106995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.581614989445091 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3954434311597634) - present_state_Q ( -0.3954434311597634)) * f1( 0.04660175892087019)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.3954434311597634) - present_state_Q (-0.3954434311597634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.619864620521424 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3297883214848217) - present_state_Q ( -0.3297883214848217)) * f1( 0.0392228269446558)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.3297883214848217) - present_state_Q (-0.3297883214848217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.667310581948747 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3676362531509971) - present_state_Q ( -0.46330343608595076)) * f1( 0.04930910645070547)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.3676362531509971) - present_state_Q (-0.46330343608595076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.72707415540239 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37474244347990426) - present_state_Q ( -0.546455377719863)) * f1( 0.06264716882744016)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.37474244347990426) - present_state_Q (-0.546455377719863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.774274997682062 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4367986465155817) - present_state_Q ( -0.4367986465155817)) * f1( 0.04888456888852102)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.4367986465155817) - present_state_Q (-0.4367986465155817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.826230455614503 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4502413484431037) - present_state_Q ( -0.5283112065505764)) * f1( 0.06005102415439421)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.4502413484431037) - present_state_Q (-0.5283112065505764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.889614976907156 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48858739257868833) - present_state_Q ( -0.66199557362095)) * f1( 0.07437720747332263)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.48858739257868833) - present_state_Q (-0.66199557362095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.943113327739587 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4676560356938289) - present_state_Q ( -0.5462066342280802)) * f1( 0.061949993672511056)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.4676560356938289) - present_state_Q (-0.5462066342280802)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -8.984606512453336 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4240847899353696) - present_state_Q ( -0.4240847899353696)) * f1( 0.04740184257450718)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.4240847899353696) - present_state_Q (-0.4240847899353696)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.021801601236197 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3984194634669142) - present_state_Q ( -0.3984194634669142)) * f1( 0.04237986253445468)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.3984194634669142) - present_state_Q (-0.3984194634669142)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.071216740641983 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4028083730944362) - present_state_Q ( -0.5758809379008385)) * f1( 0.06429193987138175)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.4028083730944362) - present_state_Q (-0.5758809379008385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.10109038750707 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3602460423988336) - present_state_Q ( -0.3602460423988336)) * f1( 0.037827030479143826)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.3602460423988336) - present_state_Q (-0.3602460423988336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.14440345406606 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38184088964372953) - present_state_Q ( -0.502077206716084)) * f1( 0.055831908670395815)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.38184088964372953) - present_state_Q (-0.502077206716084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.198166382847624 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5378804288164276) - present_state_Q ( -0.6529177853844905)) * f1( 0.0705316484522355)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.5378804288164276) - present_state_Q (-0.6529177853844905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.253404195019407 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.566034748642254) - present_state_Q ( -0.6826292059828633)) * f1( 0.07272315243808147)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.566034748642254) - present_state_Q (-0.6826292059828633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.335940781243204 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.855794035694749) - present_state_Q ( -1.0429381130680522)) * f1( 0.1136195170735933)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.855794035694749) - present_state_Q (-1.0429381130680522)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.41350068985719 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.907522383115638) - present_state_Q ( -0.9922176775053372)) * f1( 0.10595341221814193)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.907522383115638) - present_state_Q (-0.9922176775053372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.530150221144261 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.127748791112794) - present_state_Q ( -1.6544079950750008)) * f1( 0.17462446881454494)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -1.127748791112794) - present_state_Q (-1.6544079950750008)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.621812707622626 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.577770723258287) - present_state_Q ( -1.577770723258287)) * f1( 0.13476484451407558)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -1.577770723258287) - present_state_Q (-1.577770723258287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.723983406231353 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.5956014582839257) - present_state_Q ( -1.7251865191127036)) * f1( 0.1779198817666649)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -1.5956014582839257) - present_state_Q (-1.7251865191127036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.830193826539789 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.711238109531044) - present_state_Q ( -1.8471519941710104)) * f1( 0.1885801664796859)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -1.711238109531044) - present_state_Q (-1.8471519941710104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -9.940108835562235 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.9628273760093735) - present_state_Q ( -1.9628273760093735)) * f1( 0.19834549785544994)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -1.9628273760093735) - present_state_Q (-1.9628273760093735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -10.038377458970714 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-2.25080710829995) - present_state_Q ( -2.25080710829995)) * f1( 0.22492790137571286)
w2 ( -10.792480694771895 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -2.25080710829995) - present_state_Q (-2.25080710829995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9935365756297232 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0158113883008419) - present_state_Q ( 0.007071067811865476)) * f1( 0.007071067811865476)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.0158113883008419) - present_state_Q (0.007071067811865476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9870731943389083 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015697301855231084) - present_state_Q ( 0.0069987356080533575)) * f1( 0.007071067811865476)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.015697301855231084) - present_state_Q (0.0069987356080533575)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9620014214568305 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23095772045071672) - present_state_Q ( 0.030957720450716698)) * f1( 0.027421719881469537)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.23095772045071672) - present_state_Q (0.030957720450716698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.9333174704856144 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23397572073402284) - present_state_Q ( 0.033975720734022816)) * f1( 0.03136314574062627)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.23397572073402284) - present_state_Q (0.033975720734022816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.900301847839571 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23666214642047267) - present_state_Q ( 0.23666214642047267)) * f1( 0.03531774483510726)
w2 ( 0.8130366318677033 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.23666214642047267) - present_state_Q (0.23666214642047267)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8637049471379825 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20154718163837282) - present_state_Q ( 0.20154718163837282)) * f1( 0.03928153879022215)
w2 ( 0.6267053331014844 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.20154718163837282) - present_state_Q (0.20154718163837282)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8235468020820474 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16613164549028864) - present_state_Q ( 0.16613164549028864)) * f1( 0.043251999713512776)
w2 ( 0.44101151398593097 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.16613164549028864) - present_state_Q (0.16613164549028864)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.7798495671043401 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1303734307655737) - present_state_Q ( 0.1303734307655737)) * f1( 0.047227446137894126)
w2 ( 0.2559613427354225 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.1303734307655737) - present_state_Q (0.1303734307655737)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.7326370728680176 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09423137448361299) - present_state_Q ( 0.09423137448361299)) * f1( 0.0512067169246152)
w2 ( 0.07156172849798925 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.09423137448361299) - present_state_Q (0.09423137448361299)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.681934560039415 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05766515160603668) - present_state_Q ( 0.05766515160603668)) * f1( 0.05518898484016222)
w2 ( -0.1121796937276476 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.05766515160603668) - present_state_Q (0.05766515160603668)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.6277685194548971 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0206352144830372) - present_state_Q ( 0.0206352144830372)) * f1( 0.05917364478530655)
w2 ( -0.2952545770850705 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.0206352144830372) - present_state_Q (0.0206352144830372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.5702092755370102 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024405008113686705) - present_state_Q ( -0.024405008113686705)) * f1( 0.063160244035846)
w2 ( -0.4775187364357524 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.024405008113686705) - present_state_Q (-0.024405008113686705)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.5489469418141807 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008995259663131309) - present_state_Q ( -0.08650848762401918)) * f1( 0.023500096420393714)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.008995259663131309) - present_state_Q (-0.08650848762401918)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.5436369605617836 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009591940605791126) - present_state_Q ( 0.0028032541336795473)) * f1( 0.005811504479727482)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.009591940605791126) - present_state_Q (0.0028032541336795473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5389704587076934 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008589148783163716) - present_state_Q ( 0.0038587198792412957)) * f1( 0.005106603061518562)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.008589148783163716) - present_state_Q (0.0038587198792412957)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5324829625580882 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008101166869052392) - present_state_Q ( 0.005566941722633146)) * f1( 0.007097971917240086)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.008101166869052392) - present_state_Q (0.005566941722633146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.5230455379541287 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009304259777043796) - present_state_Q ( 0.0027191791268056076)) * f1( 0.010328843877605423)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( 0.009304259777043796) - present_state_Q (0.0027191791268056076)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.511554071595332 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012048065425224031) - present_state_Q ( 0.008995343243723345)) * f1( 0.013963839957474631)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( 0.012048065425224031) - present_state_Q (0.008995343243723345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.49611924692160386 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011980428868107114) - present_state_Q ( 0.010376214659789707)) * f1( 0.018752444528117324)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( 0.011980428868107114) - present_state_Q (0.010376214659789707)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.479423206751149 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011787902949731418) - present_state_Q ( 0.010778995439664422)) * f1( 0.02028371043442281)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( 0.011787902949731418) - present_state_Q (0.010778995439664422)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.4635236035024823 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0115558378751151) - present_state_Q ( 0.011045739503530827)) * f1( 0.021726622191232395)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.0115558378751151) - present_state_Q (0.011045739503530827)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.44876738565072954 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011303794686322116) - present_state_Q ( 0.011217344330985394)) * f1( 0.02303964294591243)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( 0.011303794686322116) - present_state_Q (0.011217344330985394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.4332711130130949 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010121368572133435) - present_state_Q ( 0.009768105686305413)) * f1( 0.02420015776159999)
w2 ( -0.6584740256606739 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( 0.010121368572133435) - present_state_Q (0.009768105686305413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.4193294214486257 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010791087995687383) - present_state_Q ( 0.011565646810631422)) * f1( 0.021766523144593704)
w2 ( -0.7865761710686049 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( 0.010791087995687383) - present_state_Q (0.011565646810631422)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.4077726789698969 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1456303621945598) - present_state_Q ( -0.15028858638944842)) * f1( 0.018464508738805962)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1456303621945598) - present_state_Q (-0.15028858638944842)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.3872270857128437 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01643975741822692) - present_state_Q ( 0.01643975741822692)) * f1( 0.0373834967864047)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.01643975741822692) - present_state_Q (0.01643975741822692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.36506804305877066 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01693118985465179) - present_state_Q ( 0.01693118985465179)) * f1( 0.040315985513685086)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.01693118985465179) - present_state_Q (0.01693118985465179)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.34103405141401766 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017359265017834618) - present_state_Q ( 0.017359265017834618)) * f1( 0.04372418789735041)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.017359265017834618) - present_state_Q (0.017359265017834618)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.3148955090532191 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017636249498397864) - present_state_Q ( 0.017636249498397864)) * f1( 0.047550765803513584)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.017636249498397864) - present_state_Q (0.017636249498397864)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2864688527350194 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01754398816506225) - present_state_Q ( 0.01754398816506225)) * f1( 0.051714042704161925)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.01754398816506225) - present_state_Q (0.01754398816506225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2558458336906964 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017106020335501324) - present_state_Q ( 0.017106020335501324)) * f1( 0.055713681715597964)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.017106020335501324) - present_state_Q (0.017106020335501324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2230287147144025 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016300730089649794) - present_state_Q ( 0.016300730089649794)) * f1( 0.05971336908771792)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.016300730089649794) - present_state_Q (0.016300730089649794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.18802031056514706 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015101910938860093) - present_state_Q ( 0.015101910938860093)) * f1( 0.06371309571277398)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.015101910938860093) - present_state_Q (0.015101910938860093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.15082402405790857 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013483432928845586) - present_state_Q ( 0.013483432928845586)) * f1( 0.06771285463487836)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.013483432928845586) - present_state_Q (0.013483432928845586)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.11144726501497437 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010895134048854761) - present_state_Q ( 0.010895134048854761)) * f1( 0.07171264044994606)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.010895134048854761) - present_state_Q (0.010895134048854761)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07657655059611519 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008936364696608115) - present_state_Q ( 0.008936364696608115)) * f1( 0.07620966359089934)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( 0.008936364696608115) - present_state_Q (0.008936364696608115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.039904990723286064 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006444842391424478) - present_state_Q ( 0.006444842391424478)) * f1( 0.08018469269216612)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( 0.006444842391424478) - present_state_Q (0.006444842391424478)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0014365892297905636 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003517286906742087) - present_state_Q ( 0.003517286906742087)) * f1( 0.08416208801851453)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( 0.003517286906742087) - present_state_Q (0.003517286906742087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.03882383190267267 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00012827114563517165) - present_state_Q ( 0.00012827114563517165)) * f1( 0.08814152924209596)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( 0.00012827114563517165) - present_state_Q (0.00012827114563517165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0688949921275053 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00271892144698982) - present_state_Q ( -0.0024073162267116946)) * f1( 0.065866792687792)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00271892144698982) - present_state_Q (-0.0024073162267116946)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0971948088578953 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004550642293726106) - present_state_Q ( -0.004007231566562322)) * f1( 0.062006146965260606)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.004550642293726106) - present_state_Q (-0.004007231566562322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12372881952598856 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007193855435252815) - present_state_Q ( -0.006401910325372559)) * f1( 0.05816433738966195)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.007193855435252815) - present_state_Q (-0.006401910325372559)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.15799898793738734 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00915776522741877) - present_state_Q ( -0.00915776522741877)) * f1( 0.07516468716102871)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00915776522741877) - present_state_Q (-0.00915776522741877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.20142872559050218 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01349253312089678) - present_state_Q ( -0.015119647107230133)) * f1( 0.09536999111180938)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.01349253312089678) - present_state_Q (-0.015119647107230133)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.24030599363466326 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0191789079657007) - present_state_Q ( -0.017335271569485246)) * f1( 0.08540383555282174)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0191789079657007) - present_state_Q (-0.017335271569485246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2867042527135437 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02407673317136905) - present_state_Q ( -0.02493940603146119)) * f1( 0.10208515438233104)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.02407673317136905) - present_state_Q (-0.02493940603146119)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.32637248962435894 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027751688988598682) - present_state_Q ( -0.025200892646717262)) * f1( 0.08727576681066872)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.027751688988598682) - present_state_Q (-0.025200892646717262)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4120344625217785 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033756075954547646) - present_state_Q ( -0.08932683479180425)) * f1( 0.19114000292759006)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.033756075954547646) - present_state_Q (-0.08932683479180425)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46712787157839425 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034266288950743067) - present_state_Q ( -0.03096856999793586)) * f1( 0.07567156420060304)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.034266288950743067) - present_state_Q (-0.03096856999793586)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5184375370820999 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03750876717024922) - present_state_Q ( -0.03370870809574065)) * f1( 0.07049793280308941)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.03750876717024922) - present_state_Q (-0.03370870809574065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6557058481344724 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1416021924253511) - present_state_Q ( -0.1416021924253511)) * f1( 0.19116296097837177)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.1416021924253511) - present_state_Q (-0.1416021924253511)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8424726083030772 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11916436664787364) - present_state_Q ( -0.16937884676410506)) * f1( 0.2611875736023878)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.11916436664787364) - present_state_Q (-0.16937884676410506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0262424734444375 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21552101044570868) - present_state_Q ( -0.21552101044570868)) * f1( 0.2583152906840763)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.21552101044570868) - present_state_Q (-0.21552101044570868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2072053068955193 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26032511612026976) - present_state_Q ( -0.26032511612026976)) * f1( 0.255819605672183)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.26032511612026976) - present_state_Q (-0.26032511612026976)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.385205637839813 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22893447673074002) - present_state_Q ( -0.3139789577449137)) * f1( 0.2536682342200527)
w2 ( -0.9117540747129147 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.22893447673074002) - present_state_Q (-0.3139789577449137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5099717408332534 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38558660814891854) - present_state_Q ( -0.25033646090640416)) * f1( 0.17581703835860027)
w2 ( -1.1956084819040194 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.38558660814891854) - present_state_Q (-0.25033646090640416)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.5805303148742451 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4023433241202621) - present_state_Q ( -0.14952856543546422)) * f1( 0.09801375960119753)
w2 ( -1.339585356840933 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.4023433241202621) - present_state_Q (-0.14952856543546422)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.646624433827544 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4283929360161471) - present_state_Q ( -0.4196247995928971)) * f1( 0.09535529927584477)
w2 ( -1.47821240631849 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.4283929360161471) - present_state_Q (-0.4196247995928971)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7097044464030273 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44766895873599954) - present_state_Q ( -0.44635529662091417)) * f1( 0.09133361290299197)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.44766895873599954) - present_state_Q (-0.44635529662091417)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7810110829366208 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17039848938114063) - present_state_Q ( -0.17039848938114063)) * f1( 0.09966294297947306)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.17039848938114063) - present_state_Q (-0.17039848938114063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8341924635308828 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1685500662482319) - present_state_Q ( -0.15250935288418435)) * f1( 0.08496831156042955)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1685500662482319) - present_state_Q (-0.15250935288418435)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9008403197003578 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17832779863777823) - present_state_Q ( -0.19793056205673484)) * f1( 0.10724532953476358)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17832779863777823) - present_state_Q (-0.19793056205673484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.95397231659072 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17967517213451306) - present_state_Q ( -0.1629683154862042)) * f1( 0.08501636491580548)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.17967517213451306) - present_state_Q (-0.1629683154862042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.020332854667313 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18922017294894522) - present_state_Q ( -0.21017259809566302)) * f1( 0.10697497723820443)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.18922017294894522) - present_state_Q (-0.21017259809566302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.082103968757421 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19807527697461944) - present_state_Q ( -0.20299060146609782)) * f1( 0.09944737029504844)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19807527697461944) - present_state_Q (-0.20299060146609782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.135083270279701 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19690095407792071) - present_state_Q ( -0.17831158717193807)) * f1( 0.0849571968790064)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.19690095407792071) - present_state_Q (-0.17831158717193807)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2015608353153433 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2083323054075113) - present_state_Q ( -0.23107040409432117)) * f1( 0.10749262946459004)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.2083323054075113) - present_state_Q (-0.23107040409432117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.251141766548945 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19720632510176914) - present_state_Q ( -0.20205336730341752)) * f1( 0.09357062906068445)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.19720632510176914) - present_state_Q (-0.20205336730341752)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2996936057192077 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20364188852873136) - present_state_Q ( -0.21128968630912093)) * f1( 0.09177732636876063)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.20364188852873136) - present_state_Q (-0.21128968630912093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.4760910044555375 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20938114890727666) - present_state_Q ( -0.21206734220573706)) * f1( 0.28435180187181647)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.20938114890727666) - present_state_Q (-0.21206734220573706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5158873254210206 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20426429649384922) - present_state_Q ( -0.20487891169861894)) * f1( 0.09079421981301834)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.20426429649384922) - present_state_Q (-0.20487891169861894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5395601596708794 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21783731729875763) - present_state_Q ( -0.19153850993490815)) * f1( 0.06794115803381595)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.21783731729875763) - present_state_Q (-0.19153850993490815)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5669181937207526 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22047759269158465) - present_state_Q ( -0.22117199301251647)) * f1( 0.07918515629185648)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.22047759269158465) - present_state_Q (-0.22117199301251647)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.5922000916084875 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21215565860932936) - present_state_Q ( -0.18635041583374692)) * f1( 0.07246309868151389)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.21215565860932936) - present_state_Q (-0.18635041583374692)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6211744957999654 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21440583320363157) - present_state_Q ( -0.21440583320363157)) * f1( 0.08371434769836839)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.21440583320363157) - present_state_Q (-0.21440583320363157)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6553186845614807 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24050803291278794) - present_state_Q ( -0.2405954257542277)) * f1( 0.09932784501722441)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.24050803291278794) - present_state_Q (-0.2405954257542277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.668740352078446 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21665048904859918) - present_state_Q ( -0.21665048904859918)) * f1( 0.08223813786293656)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21665048904859918) - present_state_Q (-0.21665048904859918)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.681677556725385 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23655564903833026) - present_state_Q ( -0.23655564903833026)) * f1( 0.08014948792439165)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23655564903833026) - present_state_Q (-0.23655564903833026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6965875959295578 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21905176077151944) - present_state_Q ( -0.2222681521626159)) * f1( 0.09165980366104998)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.21905176077151944) - present_state_Q (-0.2222681521626159)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6949172657820877 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2228089106403732) - present_state_Q ( -0.2228089106403732)) * f1( 0.08329659620630202)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.2228089106403732) - present_state_Q (-0.2228089106403732)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.6932677799725746 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2213879559476094) - present_state_Q ( -0.2213879559476094)) * f1( 0.08278508208476748)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.2213879559476094) - present_state_Q (-0.2213879559476094)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7161926007144253 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24199850224322067) - present_state_Q ( -0.25090347501207366)) * f1( 0.0911941361165494)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.24199850224322067) - present_state_Q (-0.25090347501207366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.742624514410348 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2383815823965372) - present_state_Q ( -0.26544272620820347)) * f1( 0.10577219865160803)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.2383815823965372) - present_state_Q (-0.26544272620820347)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7589761088743234 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23243938864367047) - present_state_Q ( -0.25912629286009564)) * f1( 0.10276575094818732)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23243938864367047) - present_state_Q (-0.25912629286009564)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.77276286966176 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23791566417667234) - present_state_Q ( -0.2092441502903889)) * f1( 0.0839846046929448)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.23791566417667234) - present_state_Q (-0.2092441502903889)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7885040370631438 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25096389490302556) - present_state_Q ( -0.2805515877534066)) * f1( 0.10016145818664723)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.25096389490302556) - present_state_Q (-0.2805515877534066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.7864814549623844 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25039385629337807) - present_state_Q ( -0.25039385629337807)) * f1( 0.08975114160369616)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.25039385629337807) - present_state_Q (-0.25039385629337807)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.784490956062054 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24844325663123404) - present_state_Q ( -0.24844325663123404)) * f1( 0.08902094887986643)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.24844325663123404) - present_state_Q (-0.24844325663123404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.78286898196179 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22314682262135202) - present_state_Q ( -0.22314682262135202)) * f1( 0.08076267560375826)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.22314682262135202) - present_state_Q (-0.22314682262135202)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.78054640384686 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25776721735669794) - present_state_Q ( -0.25776721735669794)) * f1( 0.10011522707911163)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.25776721735669794) - present_state_Q (-0.25776721735669794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.8252469239190248 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23675217531119747) - present_state_Q ( -0.264185015308395)) * f1( 0.10330420711466287)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.23675217531119747) - present_state_Q (-0.264185015308395)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.86771903604305 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2637411269062683) - present_state_Q ( -0.2908141842250512)) * f1( 0.09870013819366096)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2637411269062683) - present_state_Q (-0.2908141842250512)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.912184790040187 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.244878024377731) - present_state_Q ( -0.27224572629158705)) * f1( 0.10293407693429146)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.244878024377731) - present_state_Q (-0.27224572629158705)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.95306191962672 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24586621526128116) - present_state_Q ( -0.24923950032302936)) * f1( 0.09412331760833148)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.24586621526128116) - present_state_Q (-0.24923950032302936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.9953252579221457 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2612243596424026) - present_state_Q ( -0.2920220895333414)) * f1( 0.09824829743316406)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2612243596424026) - present_state_Q (-0.2920220895333414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.0314328548489726 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2613148842609076) - present_state_Q ( -0.2616870090168749)) * f1( 0.08335027908867318)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2613148842609076) - present_state_Q (-0.2616870090168749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.073029731862771 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26342634922683916) - present_state_Q ( -0.29354375262976834)) * f1( 0.09672826004149288)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.26342634922683916) - present_state_Q (-0.29354375262976834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.111245746293243 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2425102590592713) - present_state_Q ( -0.2460224345212933)) * f1( 0.08793751214890054)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2425102590592713) - present_state_Q (-0.2460224345212933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1460499587923674 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2727361459818582) - present_state_Q ( -0.27538419353957205)) * f1( 0.08057508654047423)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2727361459818582) - present_state_Q (-0.27538419353957205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.1900468486569884 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2637817305283368) - present_state_Q ( -0.29530604440031377)) * f1( 0.10235028261814902)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2637817305283368) - present_state_Q (-0.29530604440031377)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.2294378813407345 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28983696722452) - present_state_Q ( -0.28983696722452)) * f1( 0.09146383810267321)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.28983696722452) - present_state_Q (-0.28983696722452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.265650005151397 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2640837473490092) - present_state_Q ( -0.27412890774452725)) * f1( 0.08382696659691746)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2640837473490092) - present_state_Q (-0.27412890774452725)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.3047402430325357 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2929208100832866) - present_state_Q ( -0.30368407502075706)) * f1( 0.09105164049164634)
w2 ( -1.6163433979009263 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.2929208100832866) - present_state_Q (-0.30368407502075706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.335387400786864 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6547907388293726) - present_state_Q ( -0.6570684727552759)) * f1( 0.10007301875177069)
w2 ( -1.6775929897221709 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.6547907388293726) - present_state_Q (-0.6570684727552759)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.365870155936358 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3373859891658966) - present_state_Q ( -0.6699031526971111)) * f1( 0.10100636317146222)
w2 ( -1.7379510784452517 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.3373859891658966) - present_state_Q (-0.6699031526971111)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.3975816501569507 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29693258276853185) - present_state_Q ( -0.6753715209679643)) * f1( 0.10541015810685428)
w2 ( -1.7981188929901206 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.29693258276853185) - present_state_Q (-0.6753715209679643)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.4280131069156754 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3013588745876578) - present_state_Q ( -0.31056561660520154)) * f1( 0.09020364777120751)
w2 ( -1.7981188929901206 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.3013588745876578) - present_state_Q (-0.31056561660520154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.523958973971117 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3159613661327394) - present_state_Q ( -0.3159613661327394)) * f1( 0.28473086620416355)
w2 ( -1.7981188929901206 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.3159613661327394) - present_state_Q (-0.3159613661327394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.5570444939880184 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6495990682077961) - present_state_Q ( -0.6495990682077961)) * f1( 0.08306793784935072)
w2 ( -1.8777778345107443 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.6495990682077961) - present_state_Q (-0.6495990682077961)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.5893676342317025 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6626149672355327) - present_state_Q ( -0.6626149672355327)) * f1( 0.08139321500629508)
w2 ( -1.957202489848869 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.6626149672355327) - present_state_Q (-0.6626149672355327)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.6239780575340457 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34782195392696225) - present_state_Q ( -0.7392624518967361)) * f1( 0.0895922179545101)
w2 ( -2.034464609467152 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.34782195392696225) - present_state_Q (-0.7392624518967361)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.6941371520116593 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3995778182398554) - present_state_Q ( -0.8064707401332858)) * f1( 0.18457706370254717)
w2 ( -2.11048607504933 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3995778182398554) - present_state_Q (-0.8064707401332858)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.724030099103221 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.738987633206419) - present_state_Q ( -0.705639450384325)) * f1( 0.07595050849713723)
w2 ( -2.1892029860564204 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.738987633206419) - present_state_Q (-0.705639450384325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.7619943952042902 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3768136778183108) - present_state_Q ( -0.8146542750295949)) * f1( 0.1001534390197534)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3768136778183108) - present_state_Q (-0.8146542750295949)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.775723170832219 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3371955966536671) - present_state_Q ( -0.3481528071445434)) * f1( 0.09076268878349428)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3371955966536671) - present_state_Q (-0.3481528071445434)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.7914000420121967 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3268551448227081) - present_state_Q ( -0.3650055448237933)) * f1( 0.10488204637734967)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3268551448227081) - present_state_Q (-0.3650055448237933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8030699916270585 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31700367831127213) - present_state_Q ( -0.28326187661728114)) * f1( 0.07407267352231414)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.31700367831127213) - present_state_Q (-0.28326187661728114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.816055066859574 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30743779750029826) - present_state_Q ( -0.30770272352582584)) * f1( 0.08377059571408854)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.30743779750029826) - present_state_Q (-0.30770272352582584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8286027118775 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.305405008190086) - present_state_Q ( -0.305405008190086)) * f1( 0.08083937402602677)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.305405008190086) - present_state_Q (-0.305405008190086)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8413154365108784 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30640680339330506) - present_state_Q ( -0.32004738176299646)) * f1( 0.08267751589011263)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.30640680339330506) - present_state_Q (-0.32004738176299646)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.853967920897811 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3124267347255068) - present_state_Q ( -0.3124267347255068)) * f1( 0.08184805017993432)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3124267347255068) - present_state_Q (-0.3124267347255068)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.8617708637029002 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3881776867951554) - present_state_Q ( -1.1088357243801032)) * f1( 0.10307493166064684)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.3881776867951554) - present_state_Q (-1.1088357243801032)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.90593717836508 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3347558309124746) - present_state_Q ( -0.3720072956069805)) * f1( 0.10443543445020559)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3347558309124746) - present_state_Q (-0.3720072956069805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.949573147583421 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3314044809159783) - present_state_Q ( -0.37026645538666764)) * f1( 0.10314709711163293)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3314044809159783) - present_state_Q (-0.37026645538666764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -3.9891693983366237 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35128039235805125) - present_state_Q ( -0.3645490449535073)) * f1( 0.09342781271723898)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.35128039235805125) - present_state_Q (-0.3645490449535073)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.02787120239378 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35731899227113434) - present_state_Q ( -0.3704599669592861)) * f1( 0.09143184700664529)
w2 ( -2.265015252659829 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.35731899227113434) - present_state_Q (-0.3704599669592861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.062756210659975 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8169674242973931) - present_state_Q ( -0.8169674242973931)) * f1( 0.09102853812669219)
w2 ( -2.34166156377084 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.8169674242973931) - present_state_Q (-0.8169674242973931)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -4.106878431841253 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43091817471258886) - present_state_Q ( -0.43091817471258886)) * f1( 0.10556161704535642)
w2 ( -2.4252567613743774 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.43091817471258886) - present_state_Q (-0.43091817471258886)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -4.145781246376068 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8959355425328315) - present_state_Q ( -1.111738975070835)) * f1( 0.10972631203618831)
w2 ( -2.4961655777063902 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.8959355425328315) - present_state_Q (-1.111738975070835)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -4.178524260733524 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8972982673434005) - present_state_Q ( -0.8602884118508607)) * f1( 0.0862332786478599)
w2 ( -2.572106130752424 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.8972982673434005) - present_state_Q (-0.8602884118508607)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -4.205861633445045 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4581680769254498) - present_state_Q ( -0.9279948107482201)) * f1( 0.09862354919773829)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.4581680769254498) - present_state_Q (-0.9279948107482201)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -4.222623333190219 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3645785449448161) - present_state_Q ( -0.32149229810876484)) * f1( 0.06826137919335637)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3645785449448161) - present_state_Q (-0.32149229810876484)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.246145455616446 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3753763274751883) - present_state_Q ( -0.38015144269188067)) * f1( 0.09809312454405243)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3753763274751883) - present_state_Q (-0.38015144269188067)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.264888874496777 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3671108411757599) - present_state_Q ( -0.32867241110181183)) * f1( 0.07654779158627972)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3671108411757599) - present_state_Q (-0.32867241110181183)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.283276386929119 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3615336022687016) - present_state_Q ( -0.3218767607924966)) * f1( 0.07490345503433378)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3615336022687016) - present_state_Q (-0.3218767607924966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.305001846831374 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3480712701352034) - present_state_Q ( -0.3496749612785061)) * f1( 0.08956426609805944)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3480712701352034) - present_state_Q (-0.3496749612785061)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.319602060421671 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33258390592339837) - present_state_Q ( -0.2881897046853511)) * f1( 0.05873870935290975)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.33258390592339837) - present_state_Q (-0.2881897046853511)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.341289097750536 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34916474563476824) - present_state_Q ( -0.3934198306933894)) * f1( 0.09104364736419367)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.34916474563476824) - present_state_Q (-0.3934198306933894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.363342331124594 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35837657115475674) - present_state_Q ( -0.4053239581849785)) * f1( 0.09300980474503169)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.35837657115475674) - present_state_Q (-0.4053239581849785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.381694430794213 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36980405655605253) - present_state_Q ( -0.330111038465102)) * f1( 0.07498545958160967)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.36980405655605253) - present_state_Q (-0.330111038465102)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.404344139902578 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3774514666670038) - present_state_Q ( -0.4249076232536007)) * f1( 0.09624293539916524)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3774514666670038) - present_state_Q (-0.4249076232536007)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.4282764179061695 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36519210368976884) - present_state_Q ( -0.40784032982278834)) * f1( 0.10101286793818944)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.36519210368976884) - present_state_Q (-0.40784032982278834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.444946881907413 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3831873360887125) - present_state_Q ( -0.33863383652903817)) * f1( 0.0683149483586508)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3831873360887125) - present_state_Q (-0.33863383652903817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.465894910892543 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3888112908108265) - present_state_Q ( -0.3888112908108265)) * f1( 0.08762586749012245)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.3888112908108265) - present_state_Q (-0.3888112908108265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.490340018278591 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38418070506471846) - present_state_Q ( -0.42777138195736913)) * f1( 0.10396871256693949)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.38418070506471846) - present_state_Q (-0.42777138195736913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.527536218063752 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42550069582420286) - present_state_Q ( -0.43059912626071944)) * f1( 0.08899597772827336)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.42550069582420286) - present_state_Q (-0.43059912626071944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.565683711648655 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4439133915317014) - present_state_Q ( -0.4496731581164574)) * f1( 0.09164993654900115)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.4439133915317014) - present_state_Q (-0.4496731581164574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.59408566850527 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3656350991145307) - present_state_Q ( -0.32038925105250765)) * f1( 0.06630145845291359)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.3656350991145307) - present_state_Q (-0.32038925105250765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.631539825547777 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40734851232676345) - present_state_Q ( -0.45253641664160266)) * f1( 0.09012535537458127)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.40734851232676345) - present_state_Q (-0.45253641664160266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.667950534936855 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4061397868644139) - present_state_Q ( -0.35954396922007825)) * f1( 0.08569935356803074)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.4061397868644139) - present_state_Q (-0.35954396922007825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.7200013491936605 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47983902754837715) - present_state_Q ( -0.8871354430287276)) * f1( 0.18491062415731022)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.47983902754837715) - present_state_Q (-0.8871354430287276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.748447899864526 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4032662891113445) - present_state_Q ( -0.41564595773322255)) * f1( 0.08676036175696628)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.4032662891113445) - present_state_Q (-0.41564595773322255)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.765919007953746 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38606926602290675) - present_state_Q ( -0.341789696386975)) * f1( 0.0716801940336065)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.38606926602290675) - present_state_Q (-0.341789696386975)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.787125925995372 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42570239353543377) - present_state_Q ( -0.4396564220118844)) * f1( 0.09049383274701324)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.42570239353543377) - present_state_Q (-0.4396564220118844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.809925659141014 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4679863163292216) - present_state_Q ( -0.4679863163292216)) * f1( 0.09830165760055488)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4679863163292216) - present_state_Q (-0.4679863163292216)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.8308105508934815 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42637529063334884) - present_state_Q ( -0.42637529063334884)) * f1( 0.08861493491590966)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.42637529063334884) - present_state_Q (-0.42637529063334884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.8615477740836885 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.409966754459978) - present_state_Q ( -0.4620075433695082)) * f1( 0.09507166908303916)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.409966754459978) - present_state_Q (-0.4620075433695082)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.889677451597041 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41066730872024015) - present_state_Q ( -0.424139178797404)) * f1( 0.08599727071577495)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.41066730872024015) - present_state_Q (-0.424139178797404)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.915189087917655 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40486962351379324) - present_state_Q ( -0.3540983461170778)) * f1( 0.07637168115380769)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.40486962351379324) - present_state_Q (-0.3540983461170778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.939357019196442 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4041764191564938) - present_state_Q ( -0.3571824641950603)) * f1( 0.07241752643651864)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.4041764191564938) - present_state_Q (-0.3571824641950603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.966927336800941 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40264461310274974) - present_state_Q ( -0.4210133250565716)) * f1( 0.08422737936299748)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.40264461310274974) - present_state_Q (-0.4210133250565716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -4.994225601788301 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40827824528831635) - present_state_Q ( -0.40827824528831635)) * f1( 0.08305881687454822)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.40827824528831635) - present_state_Q (-0.40827824528831635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.017749894130267 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4740688247715479) - present_state_Q ( -0.5172248791433856)) * f1( 0.10359775726958588)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4740688247715479) - present_state_Q (-0.5172248791433856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.040052638992447 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4275258938840904) - present_state_Q ( -0.43375225077755447)) * f1( 0.09492339004505448)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4275258938840904) - present_state_Q (-0.43375225077755447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.060362311272197 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4291579646309339) - present_state_Q ( -0.44442531664902907)) * f1( 0.08682901097904737)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4291579646309339) - present_state_Q (-0.44442531664902907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.084806474022324 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45519324804968564) - present_state_Q ( -0.5015644686125046)) * f1( 0.10699974674900281)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.45519324804968564) - present_state_Q (-0.5015644686125046)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.102891613118686 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43454636013547654) - present_state_Q ( -0.38520855089683553)) * f1( 0.07539251073401462)
w2 ( -2.6275439504900016 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.43454636013547654) - present_state_Q (-0.38520855089683553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.123097624563537 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4497102433183138) - present_state_Q ( -0.43839480187338586)) * f1( 0.0860882395290778)
w2 ( -2.674486509788189 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.4497102433183138) - present_state_Q (-0.43839480187338586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.146397508081614 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42745600912807546) - present_state_Q ( -0.471713473253283)) * f1( 0.10079618489881234)
w2 ( -2.674486509788189 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.42745600912807546) - present_state_Q (-0.471713473253283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.1597792229519355 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5197719941487663) - present_state_Q ( -1.48560371536585)) * f1( 0.1023908223591258)
w2 ( -2.674486509788189 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.5197719941487663) - present_state_Q (-1.48560371536585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.174488290208559 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9858823027676196) - present_state_Q ( -0.9398068350933513)) * f1( 0.07744332452885921)
w2 ( -2.7124731725408755 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.9858823027676196) - present_state_Q (-0.9398068350933513)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.214383423740806 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47447038817690623) - present_state_Q ( -0.4832282410213351)) * f1( 0.28675679504052276)
w2 ( -2.7124731725408755 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.47447038817690623) - present_state_Q (-0.4832282410213351)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.225496135881851 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4734684013037375) - present_state_Q ( -0.41936228304382767)) * f1( 0.07637502839127117)
w2 ( -2.7124731725408755 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.4734684013037375) - present_state_Q (-0.41936228304382767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.239444566620414 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.432164749450862) - present_state_Q ( -0.4812564781980405)) * f1( 0.1004210658988861)
w2 ( -2.7124731725408755 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.432164749450862) - present_state_Q (-0.4812564781980405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.251191282825476 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42132764798591266) - present_state_Q ( -0.42132764798591266)) * f1( 0.08113271739140132)
w2 ( -2.7124731725408755 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.42132764798591266) - present_state_Q (-0.42132764798591266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.256597638993725 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037131529653935805) - present_state_Q ( -0.579626164162111)) * f1( 0.007071067811865476)
w2 ( -2.8653880168639967 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.037131529653935805) - present_state_Q (-0.579626164162111)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.26199152307113 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06369749750188941) - present_state_Q ( -0.599920960969256)) * f1( 0.007071067811865476)
w2 ( -3.0179500971866706 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06369749750188941) - present_state_Q (-0.599920960969256)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.267821635873317 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6641611330478858) - present_state_Q ( -0.6299713110962739)) * f1( 0.007613001631843749)
w2 ( -3.1711120977778964 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.6641611330478858) - present_state_Q (-0.6299713110962739)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.269596643141249 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09144307862783115) - present_state_Q ( -0.6552128821223655)) * f1( 0.00308774665186261)
w2 ( -3.2860831409404145 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.09144307862783115) - present_state_Q (-0.6552128821223655)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.279084495386521 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7430078999271047) - present_state_Q ( -0.7907871039626796)) * f1( 0.024636733488798416)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.7430078999271047) - present_state_Q (-0.7907871039626796)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.285868070025897 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10271558925410985) - present_state_Q ( -0.14299071075369052)) * f1( 0.026012308703281174)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.10271558925410985) - present_state_Q (-0.14299071075369052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.287051994612331 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04930564161111823) - present_state_Q ( -0.03708235316919489)) * f1( 0.00437130633135982)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.04930564161111823) - present_state_Q (-0.03708235316919489)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.288975345077767 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06902402876923841) - present_state_Q ( -0.005832877030474446)) * f1( 0.007015376221641721)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06902402876923841) - present_state_Q (-0.005832877030474446)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.293286174947303 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08299715611245109) - present_state_Q ( -0.0891925832136041)) * f1( 0.01620820591054716)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.08299715611245109) - present_state_Q (-0.0891925832136041)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.295382049507474 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06362512242569938) - present_state_Q ( -0.011237559881112748)) * f1( 0.00766126554407957)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06362512242569938) - present_state_Q (-0.011237559881112748)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.297477879807887 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03604738258749114) - present_state_Q ( -0.05248236103382406)) * f1( 0.007786344881398541)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.03604738258749114) - present_state_Q (-0.05248236103382406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.301029676807554 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047848257261039036) - present_state_Q ( -0.062002505648995114)) * f1( 0.013236506960751073)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.047848257261039036) - present_state_Q (-0.062002505648995114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.304415565861089 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06879599745141972) - present_state_Q ( -0.09998546132653212)) * f1( 0.012789266356686757)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.06879599745141972) - present_state_Q (-0.09998546132653212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.311143899010727 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06399549625417796) - present_state_Q ( -0.011291969430401897)) * f1( 0.007369251664882014)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06399549625417796) - present_state_Q (-0.011291969430401897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.318067775436287 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06686717232329774) - present_state_Q ( -0.008254678362364811)) * f1( 0.007580661517771571)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06686717232329774) - present_state_Q (-0.008254678362364811)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.324849775984954 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07066542050813338) - present_state_Q ( -0.005773059138205188)) * f1( 0.007423002044806203)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07066542050813338) - present_state_Q (-0.005773059138205188)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.331514389024425 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06595630926658791) - present_state_Q ( -0.009415942311636621)) * f1( 0.007297805560043741)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06595630926658791) - present_state_Q (-0.009415942311636621)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.338305615618696 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06930480656400433) - present_state_Q ( -0.0064074908803503205)) * f1( 0.007433726989452126)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06930480656400433) - present_state_Q (-0.0064074908803503205)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.347703386389281 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08370081033535129) - present_state_Q ( -0.08370081033535129)) * f1( 0.01037299660914788)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.08370081033535129) - present_state_Q (-0.08370081033535129)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.364692986430702 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07019497113326859) - present_state_Q ( -0.10673523100056967)) * f1( 0.018803255362203702)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07019497113326859) - present_state_Q (-0.10673523100056967)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.387044293968998 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10189859856275835) - present_state_Q ( -0.14109668803255226)) * f1( 0.024823021016252125)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.10189859856275835) - present_state_Q (-0.14109668803255226)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.399980055142623 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04237562996459165) - present_state_Q ( -0.06972188646279326)) * f1( 0.015860090876052634)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.04237562996459165) - present_state_Q (-0.06972188646279326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.406295911389716 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06359246204347596) - present_state_Q ( -0.014235964423324657)) * f1( 0.007689343266402459)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06359246204347596) - present_state_Q (-0.014235964423324657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.412537049766297 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06552618603982323) - present_state_Q ( -0.011462578511601807)) * f1( 0.007595633275025174)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06552618603982323) - present_state_Q (-0.011462578511601807)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.418676337009775 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06826059524064507) - present_state_Q ( -0.008293376407122346)) * f1( 0.007468548542131284)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06826059524064507) - present_state_Q (-0.008293376407122346)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.424704172859497 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07195562861537265) - present_state_Q ( -0.0055072059291757635)) * f1( 0.0073301520092635495)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07195562861537265) - present_state_Q (-0.0055072059291757635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.430598748003457 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02506795676568801) - present_state_Q ( -0.05728687057628828)) * f1( 0.007217662856264117)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.02506795676568801) - present_state_Q (-0.05728687057628828)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.443215956208452 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04685587286789022) - present_state_Q ( -0.09328109830336234)) * f1( 0.015513482803590127)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.04685587286789022) - present_state_Q (-0.09328109830336234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.464482162805934 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07453048141598909) - present_state_Q ( -0.14505858302596175)) * f1( 0.02630637787555277)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07453048141598909) - present_state_Q (-0.14505858302596175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.480489246024588 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11321416453324296) - present_state_Q ( -0.11839968094676917)) * f1( 0.019726331134596747)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11321416453324296) - present_state_Q (-0.11839968094676917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.497629598637056 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09687832058759815) - present_state_Q ( -0.13831922609551903)) * f1( 0.023873998035313693)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.09687832058759815) - present_state_Q (-0.13831922609551903)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.516137958518548 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07051696801923134) - present_state_Q ( -0.14411996996987456)) * f1( 0.025809761497919376)
w2 ( -3.363105139409379 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07051696801923134) - present_state_Q (-0.14411996996987456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.524138258277943 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0889840547393215) - present_state_Q ( -0.0889840547393215)) * f1( 0.011068403206658526)
w2 ( -3.5076661860214537 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0889840547393215) - present_state_Q (-0.0889840547393215)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.528233700592092 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7366870913036863) - present_state_Q ( -0.7648170134003963)) * f1( 0.006189283216280829)
w2 ( -3.6400059795334356 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.7366870913036863) - present_state_Q (-0.7648170134003963)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.535831736294459 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7807330178664235) - present_state_Q ( -0.7537689347985689)) * f1( 0.011455863926156171)
w2 ( -3.7726548264705797 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.7807330178664235) - present_state_Q (-0.7537689347985689)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.540410934348331 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8236915652282908) - present_state_Q ( -0.7664763423907703)) * f1( 0.006913008402706034)
w2 ( -3.9051354423506033 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.8236915652282908) - present_state_Q (-0.7664763423907703)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.554270959466314 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8696290055519187) - present_state_Q ( -0.9086905691821259)) * f1( 0.021367789660116196)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.8696290055519187) - present_state_Q (-0.9086905691821259)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -5.576936902006299 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06969231820088527) - present_state_Q ( -0.11368791574619905)) * f1( 0.020879707980958954)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06969231820088527) - present_state_Q (-0.11368791574619905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.580329489952035 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05519776754319477) - present_state_Q ( -0.05084922977591653)) * f1( 0.0031076541259356097)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.05519776754319477) - present_state_Q (-0.05084922977591653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.590313191904773 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06115571438725406) - present_state_Q ( -0.018603915370978528)) * f1( 0.009117770322562472)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06115571438725406) - present_state_Q (-0.018603915370978528)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.600270309475625 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06516468603228447) - present_state_Q ( -0.015441159408799773)) * f1( 0.00909053315456273)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06516468603228447) - present_state_Q (-0.015441159408799773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.60979916959689 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06976208989200035) - present_state_Q ( -0.012682434010636533)) * f1( 0.008696992172252874)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06976208989200035) - present_state_Q (-0.012682434010636533)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.618955957105227 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06487886156509762) - present_state_Q ( -0.01472763620825447)) * f1( 0.008359334407925755)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.06487886156509762) - present_state_Q (-0.01472763620825447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.7664442491837 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04299451183443889) - present_state_Q ( -0.7875154505778449)) * f1( 0.1448948058871723)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.04299451183443889) - present_state_Q (-0.7875154505778449)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.774088457465361 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027327657637523633) - present_state_Q ( -0.06120098563087115)) * f1( 0.007651690485324395)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.027327657637523633) - present_state_Q (-0.06120098563087115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.788058657075904 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055450879121808815) - present_state_Q ( -0.04269946834780667)) * f1( 0.015355211943512262)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.055450879121808815) - present_state_Q (-0.04269946834780667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.794693790170107 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07052920438173572) - present_state_Q ( -0.011665527192685349)) * f1( 0.007266950975697648)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07052920438173572) - present_state_Q (-0.011665527192685349)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.801522477666587 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07369525219163015) - present_state_Q ( -0.008268947980453224)) * f1( 0.007475895955320254)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07369525219163015) - present_state_Q (-0.008268947980453224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.808223093096816 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07792396648912327) - present_state_Q ( -0.005770695406363458)) * f1( 0.007333340302400014)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07792396648912327) - present_state_Q (-0.005770695406363458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.814824552534091 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07280342901417662) - present_state_Q ( -0.009385031646925775)) * f1( 0.007228085449369785)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07280342901417662) - present_state_Q (-0.009385031646925775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.820863196812422 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07660240383132282) - present_state_Q ( -0.006188895677341655)) * f1( 0.007343489395930642)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07660240383132282) - present_state_Q (-0.006188895677341655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.826758586071348 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02190537688911134) - present_state_Q ( -0.06431549168497151)) * f1( 0.007225157651753644)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.02190537688911134) - present_state_Q (-0.06431549168497151)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.833310795648172 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01431862183720042) - present_state_Q ( -0.01431862183720042)) * f1( 0.007981964167471151)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.01431862183720042) - present_state_Q (-0.01431862183720042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.839919450175709 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06952609548584797) - present_state_Q ( -0.06952609548584797)) * f1( 0.008099752811632674)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06952609548584797) - present_state_Q (-0.06952609548584797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.846539544501174 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06626163091743069) - present_state_Q ( -0.016957966750764594)) * f1( 0.009071348519840719)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.06626163091743069) - present_state_Q (-0.016957966750764594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.857518285188356 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05661729297642596) - present_state_Q ( -0.06346542816093756)) * f1( 0.015142392421745499)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05661729297642596) - present_state_Q (-0.06346542816093756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.863134990499851 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07141316498276003) - present_state_Q ( -0.01146444876176503)) * f1( 0.007690098159232362)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07141316498276003) - present_state_Q (-0.01146444876176503)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.868760793576492 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03268842719630697) - present_state_Q ( -0.06085460046509632)) * f1( 0.00775913737918443)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.03268842719630697) - present_state_Q (-0.06085460046509632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.879209740104848 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05275660092908886) - present_state_Q ( -0.09423385210668066)) * f1( 0.014473869380471866)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.05275660092908886) - present_state_Q (-0.09423385210668066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.892608338815552 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13225062393926124) - present_state_Q ( -0.14510236746813923)) * f1( 0.02139413639759521)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.13225062393926124) - present_state_Q (-0.14510236746813923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.9081459127117055 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10238005966922702) - present_state_Q ( -0.15098339682036035)) * f1( 0.024844713154047866)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.10238005966922702) - present_state_Q (-0.15098339682036035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.922573689030884 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08327327946914534) - present_state_Q ( -0.12496036530383263)) * f1( 0.022981530101761082)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08327327946914534) - present_state_Q (-0.12496036530383263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.937816852086257 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09004130658486033) - present_state_Q ( -0.15218566422435773)) * f1( 0.024383446068310156)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.09004130658486033) - present_state_Q (-0.15218566422435773)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.9579980832922566 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12191364792832297) - present_state_Q ( -0.12191364792832297)) * f1( 0.020305158789869954)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.12191364792832297) - present_state_Q (-0.12191364792832297)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.972268824710907 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061727910737158054) - present_state_Q ( -0.06308329748844001)) * f1( 0.014282482739344729)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.061727910737158054) - present_state_Q (-0.06308329748844001)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.979190440133018 ) += alpha ( 0.1 ) * (reward ( -10.048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08092494692964522) - present_state_Q ( -0.0037979343348870224)) * f1( 0.00688513502805048)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -10.048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.08092494692964522) - present_state_Q (-0.0037979343348870224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.985407933062314 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027709746150668155) - present_state_Q ( -0.06034552148656921)) * f1( 0.006849271116251592)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.027709746150668155) - present_state_Q (-0.06034552148656921)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -5.999643647283872 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0542732085642215) - present_state_Q ( -0.10553668583186795)) * f1( 0.01575607613190966)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0542732085642215) - present_state_Q (-0.10553668583186795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.007693906396814 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06414123874273273) - present_state_Q ( -0.03982121888784867)) * f1( 0.008844722729052882)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.06414123874273273) - present_state_Q (-0.03982121888784867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.012151899747342 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09523392525192256) - present_state_Q ( -0.025559692117217633)) * f1( 0.0048886136415304425)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09523392525192256) - present_state_Q (-0.025559692117217633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.014191680497 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0538048721248237) - present_state_Q ( -0.048976529484073536)) * f1( 0.0022435941278041013)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0538048721248237) - present_state_Q (-0.048976529484073536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.02162982469402 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07368670640400546) - present_state_Q ( -0.011789490391922737)) * f1( 0.008146256165971415)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07368670640400546) - present_state_Q (-0.011789490391922737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.028844674263561 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07828068530449839) - present_state_Q ( -0.009165214972163837)) * f1( 0.007899036177181958)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07828068530449839) - present_state_Q (-0.009165214972163837)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.0358693827214145 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07297529334800731) - present_state_Q ( -0.012294933010247963)) * f1( 0.007693947162133187)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07297529334800731) - present_state_Q (-0.012294933010247963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.042980194856473 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07695401923383395) - present_state_Q ( -0.009120198354517798)) * f1( 0.007785207546153853)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07695401923383395) - present_state_Q (-0.009120198354517798)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.04991569847682 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07878223448660385) - present_state_Q ( -0.007732720689421881)) * f1( 0.007591967392220237)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07878223448660385) - present_state_Q (-0.007732720689421881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.056693384735558 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07505670620098692) - present_state_Q ( -0.010880035430660084)) * f1( 0.007422071948017364)
w2 ( -4.034863648575447 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.07505670620098692) - present_state_Q (-0.010880035430660084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.060326530177052 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09363471241839606) - present_state_Q ( -0.0317225329634123)) * f1( 0.003986853759417054)
w2 ( -4.217119916837744 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.09363471241839606) - present_state_Q (-0.0317225329634123)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.076391467552922 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11649973356127644) - present_state_Q ( -0.9599237169288252)) * f1( 0.01962273860127496)
w2 ( -4.380857891463018 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.11649973356127644) - present_state_Q (-0.9599237169288252)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.097243082539107 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9868206911868683) - present_state_Q ( -1.0356274514714143)) * f1( 0.02543429786825733)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.9868206911868683) - present_state_Q (-1.0356274514714143)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.101048562238556 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09405909410627702) - present_state_Q ( -0.013230132905628115)) * f1( 0.004630759257111238)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.09405909410627702) - present_state_Q (-0.013230132905628115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.108217227348254 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06777045583525095) - present_state_Q ( -0.018555509126223804)) * f1( 0.008731757267414738)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06777045583525095) - present_state_Q (-0.018555509126223804)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.115166785807417 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07091284102096473) - present_state_Q ( -0.015492257948108901)) * f1( 0.00846139452726156)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07091284102096473) - present_state_Q (-0.015492257948108901)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.121898513337257 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07471308354838489) - present_state_Q ( -0.012235051814067734)) * f1( 0.00819254768428625)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07471308354838489) - present_state_Q (-0.012235051814067734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.12842378585022 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07938074808058022) - present_state_Q ( -0.009544843727108494)) * f1( 0.007938240931254186)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07938074808058022) - present_state_Q (-0.009544843727108494)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.143895394900636 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08489893236035709) - present_state_Q ( -0.11316716664979135)) * f1( 0.0190607996451532)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08489893236035709) - present_state_Q (-0.11316716664979135)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.163130939825291 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08963717329146521) - present_state_Q ( -0.11203981241097394)) * f1( 0.0236932406659615)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08963717329146521) - present_state_Q (-0.11203981241097394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.182284514872486 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11025334856711956) - present_state_Q ( -0.15372982815606884)) * f1( 0.023707998325155143)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11025334856711956) - present_state_Q (-0.15372982815606884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.199349925507081 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11467210226591959) - present_state_Q ( -0.13267827978198188)) * f1( 0.021067252982872927)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.11467210226591959) - present_state_Q (-0.13267827978198188)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.202876196140999 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10160622362323073) - present_state_Q ( -0.02942708201523776)) * f1( 0.004299077663575445)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.10160622362323073) - present_state_Q (-0.02942708201523776)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.2177747085755595 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09751841990553084) - present_state_Q ( -0.11790451709077861)) * f1( 0.018362615083450322)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.09751841990553084) - present_state_Q (-0.11790451709077861)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.233846010046936 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08825701540833787) - present_state_Q ( -0.1302046733497911)) * f1( 0.019840436257192836)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08825701540833787) - present_state_Q (-0.1302046733497911)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.245945777277444 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054607358397558474) - present_state_Q ( -0.054607358397558474)) * f1( 0.016668661849722)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.054607358397558474) - present_state_Q (-0.054607358397558474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.252148910184949 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07456722912394383) - present_state_Q ( -0.014364033748984794)) * f1( 0.008496010040902603)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07456722912394383) - present_state_Q (-0.014364033748984794)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.258137138153011 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07925987508903644) - present_state_Q ( -0.011364278383156136)) * f1( 0.008197774341201305)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07925987508903644) - present_state_Q (-0.011364278383156136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.263933747454725 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07371436342181045) - present_state_Q ( -0.014802227955276111)) * f1( 0.007939791519154223)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.07371436342181045) - present_state_Q (-0.014802227955276111)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.270544829238908 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07764802716769349) - present_state_Q ( -0.01145976977574007)) * f1( 0.008044674803460314)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07764802716769349) - present_state_Q (-0.01145976977574007)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.276966512492921 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08255352030380364) - present_state_Q ( -0.009004338838837389)) * f1( 0.007811405538652252)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08255352030380364) - present_state_Q (-0.009004338838837389)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.283233012870087 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07704576165299473) - present_state_Q ( -0.01176255837258767)) * f1( 0.00762570881758504)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07704576165299473) - present_state_Q (-0.01176255837258767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.289536443962812 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06716304057829489) - present_state_Q ( -0.03347660920163389)) * f1( 0.007691899838327135)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06716304057829489) - present_state_Q (-0.03347660920163389)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.301497419078428 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041913541502365585) - present_state_Q ( -0.08460725604431447)) * f1( 0.014691835772733336)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.041913541502365585) - present_state_Q (-0.08460725604431447)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.313085671196493 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0690133624141835) - present_state_Q ( -0.10464239401819324)) * f1( 0.014264370445299704)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0690133624141835) - present_state_Q (-0.10464239401819324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.329426696885391 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13007788842014176) - present_state_Q ( -0.14108693662092475)) * f1( 0.020190118098893477)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.13007788842014176) - present_state_Q (-0.14108693662092475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.345627930132019 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0781217291384875) - present_state_Q ( -0.12344050013681131)) * f1( 0.01998665122412618)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0781217291384875) - present_state_Q (-0.12344050013681131)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.353841882340364 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06644776116085835) - present_state_Q ( -0.023948264310120548)) * f1( 0.010011701679172647)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.06644776116085835) - present_state_Q (-0.023948264310120548)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.360557248773914 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05654708817659842) - present_state_Q ( -0.059840007345138876)) * f1( 0.008222088960314601)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.05654708817659842) - present_state_Q (-0.059840007345138876)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.369644926288335 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0734974366481089) - present_state_Q ( -0.0734974366481089)) * f1( 0.01114299444408865)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0734974366481089) - present_state_Q (-0.0734974366481089)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.377209295715307 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07818865195721258) - present_state_Q ( -0.01741588846521756)) * f1( 0.009211295437739271)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07818865195721258) - present_state_Q (-0.01741588846521756)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.384489381488016 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0720732215812568) - present_state_Q ( -0.01827186833429822)) * f1( 0.008866701624036384)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.0720732215812568) - present_state_Q (-0.01827186833429822)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.391504233225592 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07615034984737785) - present_state_Q ( -0.014850609156698629)) * f1( 0.008539680261964588)
w2 ( -4.544822433312691 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.07615034984737785) - present_state_Q (-0.014850609156698629)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.397522426813686 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08100869046558437) - present_state_Q ( -0.9207698960017442)) * f1( 0.008233964907153368)
w2 ( -4.691002157320643 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.08100869046558437) - present_state_Q (-0.9207698960017442)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.412921926954466 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0424782594417898) - present_state_Q ( -1.0547154931212916)) * f1( 0.02117879646665016)
w2 ( -4.836425908524156 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -1.0424782594417898) - present_state_Q (-1.0547154931212916)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.430242733851371 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.082795972947833) - present_state_Q ( -1.1295598319353224)) * f1( 0.02405542338341552)
w2 ( -4.9804334083784 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -1.082795972947833) - present_state_Q (-1.1295598319353224)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.4474009389484666 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1094277362884681) - present_state_Q ( -1.1577973019843066)) * f1( 0.023914538977359585)
w2 ( -5.123929422358346 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -1.1094277362884681) - present_state_Q (-1.1577973019843066)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.45139690382304 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1246878021030924) - present_state_Q ( -1.071627358750068)) * f1( 0.007351656145275701)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -1.1246878021030924) - present_state_Q (-1.071627358750068)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -6.465208284943892 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1387610223375289) - present_state_Q ( -0.14412383797481712)) * f1( 0.0220475075929393)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.1387610223375289) - present_state_Q (-0.14412383797481712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.480892042976402 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0954662892536695) - present_state_Q ( -0.1260170934424814)) * f1( 0.024981495127281806)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0954662892536695) - present_state_Q (-0.1260170934424814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.485752385484105 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08596277624336686) - present_state_Q ( -0.008074094582922853)) * f1( 0.0076000529964361865)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08596277624336686) - present_state_Q (-0.008074094582922853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.490516051812424 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08022777673775716) - present_state_Q ( -0.011494616542888959)) * f1( 0.0074535367704424505)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08022777673775716) - present_state_Q (-0.011494616542888959)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.495341300076863 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08454482549963371) - present_state_Q ( -0.008102139891463724)) * f1( 0.007545376982538541)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08454482549963371) - present_state_Q (-0.008102139891463724)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.50006484803704 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07869981977460051) - present_state_Q ( -0.013369227442306321)) * f1( 0.00739311025756328)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.07869981977460051) - present_state_Q (-0.013369227442306321)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.50491038598608 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08234628223347315) - present_state_Q ( -0.009651325168451637)) * f1( 0.0075792011676456775)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08234628223347315) - present_state_Q (-0.009651325168451637)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.509656463008037 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0871465334198263) - present_state_Q ( -0.006989053575966564)) * f1( 0.007419981307298293)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0871465334198263) - present_state_Q (-0.006989053575966564)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.51432323615426 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08138405524429725) - present_state_Q ( -0.0106926022324995)) * f1( 0.0073008832435881915)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08138405524429725) - present_state_Q (-0.0106926022324995)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.519058070859653 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08568561387616601) - present_state_Q ( -0.007221356436631438)) * f1( 0.007402843090806399)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08568561387616601) - present_state_Q (-0.007221356436631438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.523706895280383 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07981603320669818) - present_state_Q ( -0.012651270729182805)) * f1( 0.007275211302024091)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.07981603320669818) - present_state_Q (-0.012651270729182805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.528478453078311 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08350047791471608) - present_state_Q ( -0.008829408222406153)) * f1( 0.00746239021060104)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08350047791471608) - present_state_Q (-0.008829408222406153)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.53316163747356 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08740289840411976) - present_state_Q ( -0.006457365408511563)) * f1( 0.0073210175523876295)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08740289840411976) - present_state_Q (-0.006457365408511563)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.537796886190942 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08086163000404013) - present_state_Q ( -0.012816652121388604)) * f1( 0.007254034953523426)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08086163000404013) - present_state_Q (-0.012816652121388604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.542386275252312 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08394721307741387) - present_state_Q ( -0.00863954748985229)) * f1( 0.007177227502245443)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08394721307741387) - present_state_Q (-0.00863954748985229)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.546918479679622 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08832008033681496) - present_state_Q ( -0.005081062952156953)) * f1( 0.00708337160729596)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08832008033681496) - present_state_Q (-0.005081062952156953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.551403385995758 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08244901215887519) - present_state_Q ( -0.010490822400563454)) * f1( 0.007016025262282108)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.08244901215887519) - present_state_Q (-0.010490822400563454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.555342026376193 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08636659755392663) - present_state_Q ( -0.0064549114739889885)) * f1( 0.007182993794669177)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08636659755392663) - present_state_Q (-0.0064549114739889885)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.559223728840672 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08038503076480183) - present_state_Q ( -0.013093358437703585)) * f1( 0.007088509849124623)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08038503076480183) - present_state_Q (-0.013093358437703585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.563245269245344 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08368238558557134) - present_state_Q ( -0.009094051489805242)) * f1( 0.007338071653783472)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08368238558557134) - present_state_Q (-0.009094051489805242)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.567203839933993 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08821505491190129) - present_state_Q ( -0.005869995971934858)) * f1( 0.007218327848053652)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08821505491190129) - present_state_Q (-0.005869995971934858)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.57110971405029 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08234151334718635) - present_state_Q ( -0.010718314445599893)) * f1( 0.00712930372942973)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08234151334718635) - present_state_Q (-0.010718314445599893)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5750986680694545 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0863883632140557) - present_state_Q ( -0.006861272529520261)) * f1( 0.007275288272239942)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0863883632140557) - present_state_Q (-0.006861272529520261)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.586429421911977 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12277110725450892) - present_state_Q ( -0.1353319050481786)) * f1( 0.021147164755044013)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.12277110725450892) - present_state_Q (-0.1353319050481786)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.5938959903219585 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06154923447338551) - present_state_Q ( -0.9186461968734057)) * f1( 0.016343187086937154)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06154923447338551) - present_state_Q (-0.9186461968734057)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.599618872462535 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0794773946892929) - present_state_Q ( -0.015247878309690604)) * f1( 0.010455037893705131)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0794773946892929) - present_state_Q (-0.015247878309690604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.604165507858302 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08489058540520163) - present_state_Q ( -0.01246531300038749)) * f1( 0.008301131605080945)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08489058540520163) - present_state_Q (-0.01246531300038749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.608562915997323 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07908436071933012) - present_state_Q ( -0.014606739118088664)) * f1( 0.00803266837755435)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07908436071933012) - present_state_Q (-0.014606739118088664)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.61297788520258 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08378070370431837) - present_state_Q ( -0.011322904271111028)) * f1( 0.008059221148058066)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08378070370431837) - present_state_Q (-0.011322904271111028)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6172593369005615 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07786496437373573) - present_state_Q ( -0.01566745671233338)) * f1( 0.007822542713578627)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07786496437373573) - present_state_Q (-0.01566745671233338)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6216227443023 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08174727239380558) - present_state_Q ( -0.012090354754356474)) * f1( 0.007966510397947922)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08174727239380558) - present_state_Q (-0.012090354754356474)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.625869043275868 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08664085970796838) - present_state_Q ( -0.009163177259176945)) * f1( 0.007747866236279252)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08664085970796838) - present_state_Q (-0.009163177259176945)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.63001333192501 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08074959317478902) - present_state_Q ( -0.012955039181547223)) * f1( 0.007567785940452978)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08074959317478902) - present_state_Q (-0.012955039181547223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.634219429460736 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08499585704543022) - present_state_Q ( -0.00939548229256169)) * f1( 0.00767506972672996)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08499585704543022) - present_state_Q (-0.00939548229256169)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.638324631169966 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07899182907259725) - present_state_Q ( -0.015124733051716927)) * f1( 0.007499622311932368)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.07899182907259725) - present_state_Q (-0.015124733051716927)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.642551051230048 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08245989378460311) - present_state_Q ( -0.011428488269885316)) * f1( 0.007715372439426683)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.08245989378460311) - present_state_Q (-0.011428488269885316)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.647698865349788 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04124025439041633) - present_state_Q ( -0.07758336180033552)) * f1( 0.009519513448623397)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.04124025439041633) - present_state_Q (-0.07758336180033552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.655189558813039 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06174027980247447) - present_state_Q ( -0.10325964850046607)) * f1( 0.013912831740039794)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06174027980247447) - present_state_Q (-0.10325964850046607)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -6.6605972459334595 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06403503694214456) - present_state_Q ( -0.030513526343120326)) * f1( 0.009909645591159078)
w2 ( -5.23263866543526 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.06403503694214456) - present_state_Q (-0.030513526343120326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.004517179682986 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007071067811865476) - present_state_Q ( -0.007071067811865476)) * f1( 0.007071067811865476)
w2 ( -1.0 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007071067811865476) - present_state_Q (-0.007071067811865476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0090368741510718 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014813470604397543) - present_state_Q ( -0.004288864896231681)) * f1( 0.007071067811865476)
w2 ( -1.0 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.014813470604397543) - present_state_Q (-0.004288864896231681)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0099743232761667 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009803459236713686) - present_state_Q ( -0.009526947064282323)) * f1( 0.0014679584136179369)
w2 ( -1.0 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.009803459236713686) - present_state_Q (-0.009526947064282323)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0154987533632656 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002920058240464207) - present_state_Q ( -0.002920058240464207)) * f1( 0.008642735315406502)
w2 ( -1.0 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.002920058240464207) - present_state_Q (-0.002920058240464207)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0241768873307653 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006343164187655706) - present_state_Q ( -0.006343164187655706)) * f1( 0.01358311731986051)
w2 ( -1.0 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.006343164187655706) - present_state_Q (-0.006343164187655706)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0305675782084642 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009821504787410078) - present_state_Q ( -0.023931700256540483)) * f1( 0.023515917432392515)
w2 ( -1.0 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.009821504787410078) - present_state_Q (-0.023931700256540483)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0333085415645407 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20559534115663822) - present_state_Q ( -0.21741900629463515)) * f1( 0.010775530468484992)
w2 ( -1.050873845405439 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.20559534115663822) - present_state_Q (-0.21741900629463515)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0381468724452467 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006454009512340953) - present_state_Q ( -0.020943771564098844)) * f1( 0.017786324357854752)
w2 ( -1.050873845405439 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.006454009512340953) - present_state_Q (-0.020943771564098844)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0390353310751792 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21311460386949477) - present_state_Q ( -0.21311460386949477)) * f1( 0.003485862161109102)
w2 ( -1.1018488173848064 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.21311460386949477) - present_state_Q (-0.21311460386949477)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.041840834039501 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22748884385498525) - present_state_Q ( -0.242164117860794)) * f1( 0.01745128981421854)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.22748884385498525) - present_state_Q (-0.242164117860794)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0417891178427476 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002138616157000527) - present_state_Q ( -0.021741583240191247)) * f1( 0.024023070186114173)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.002138616157000527) - present_state_Q (-0.021741583240191247)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0417687355226988 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009807223124682999) - present_state_Q ( -0.010747781631392738)) * f1( 0.02086843069482428)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.009807223124682999) - present_state_Q (-0.010747781631392738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0417658069468614 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033483067484488004) - present_state_Q ( -0.0033483067484488004)) * f1( 0.009718264774533107)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0033483067484488004) - present_state_Q (-0.0033483067484488004)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.041762907618213 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003515750961770929) - present_state_Q ( -0.003515750961770929)) * f1( 0.009162981993824102)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.003515750961770929) - present_state_Q (-0.003515750961770929)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0417621943102437 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015376194610829338) - present_state_Q ( -0.0015376194610829338)) * f1( 0.0051544899791619156)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0015376194610829338) - present_state_Q (-0.0015376194610829338)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0417594086671318 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0034125174458397825) - present_state_Q ( -0.0034125174458397825)) * f1( 0.009070016673552143)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0034125174458397825) - present_state_Q (-0.0034125174458397825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.041728829938411 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0055757391239116515) - present_state_Q ( -0.019628418713430674)) * f1( 0.016034281144712146)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0055757391239116515) - present_state_Q (-0.019628418713430674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0416842054574276 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01015487410848361) - present_state_Q ( -0.02345505181014733)) * f1( 0.019886518378594946)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.01015487410848361) - present_state_Q (-0.02345505181014733)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.051045031566186 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00708444024558018) - present_state_Q ( -0.027707909410116256)) * f1( 0.025808234568986333)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00708444024558018) - present_state_Q (-0.027707909410116256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.05623667733229 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006100271730447547) - present_state_Q ( -0.020953070595988336)) * f1( 0.019085468581727816)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.006100271730447547) - present_state_Q (-0.020953070595988336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0571543242540935 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033743538826554818) - present_state_Q ( -0.0033743538826554818)) * f1( 0.003352116721973737)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0033743538826554818) - present_state_Q (-0.0033743538826554818)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0646280362169807 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005313019909341859) - present_state_Q ( -0.02626204388776332)) * f1( 0.027529299211114605)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.005313019909341859) - present_state_Q (-0.02626204388776332)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.071368947518352 ) += alpha ( 0.1 ) * (reward ( -2.7405517424509225 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0064124006104514086) - present_state_Q ( -0.02770138904975852)) * f1( 0.024842204477849798)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -2.7405517424509225) + discount_factor ( 0.1) * next_state_max_Q( -0.0064124006104514086) - present_state_Q (-0.02770138904975852)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0749179555865436 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005337453152497774) - present_state_Q ( -0.020357365938321496)) * f1( 0.01963804068785999)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005337453152497774) - present_state_Q (-0.020357365938321496)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0782071682273753 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0036587038975680556) - present_state_Q ( -0.018857146921055223)) * f1( 0.018187083434346347)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0036587038975680556) - present_state_Q (-0.018857146921055223)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.080540371661614 ) += alpha ( 0.1 ) * (reward ( -1.8270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008178270318468803) - present_state_Q ( -0.00848832536438033)) * f1( 0.012824280564826167)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -1.8270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.008178270318468803) - present_state_Q (-0.00848832536438033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1077025251171078 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006940280294970725) - present_state_Q ( -0.02826452568644947)) * f1( 0.024840472314450208)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.006940280294970725) - present_state_Q (-0.02826452568644947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1274388723312512 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003709470034070292) - present_state_Q ( -0.019288458052419688)) * f1( 0.018035113928521987)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.003709470034070292) - present_state_Q (-0.019288458052419688)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1528640571462854 ) += alpha ( 0.1 ) * (reward ( -10.96220696980369 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004118984569065879) - present_state_Q ( -0.02006146079475246)) * f1( 0.02323513952751782)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -10.96220696980369) + discount_factor ( 0.1) * next_state_max_Q( -0.004118984569065879) - present_state_Q (-0.02006146079475246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1750780037712447 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0063920109911131655) - present_state_Q ( -0.029405583180193463)) * f1( 0.024393757975093848)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.0063920109911131655) - present_state_Q (-0.029405583180193463)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1883660790819883 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009380126742293486) - present_state_Q ( -0.01419907330326057)) * f1( 0.014567204950102363)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.009380126742293486) - present_state_Q (-0.01419907330326057)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1969779882721254 ) += alpha ( 0.1 ) * (reward ( -9.13517247483641 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003967953583819136) - present_state_Q ( -0.003967953583819136)) * f1( 0.00943088609713111)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -9.13517247483641) + discount_factor ( 0.1) * next_state_max_Q( -0.003967953583819136) - present_state_Q (-0.003967953583819136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2093913671662824 ) += alpha ( 0.1 ) * (reward ( -8.221655227352768 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003662347517061953) - present_state_Q ( -0.02052554775203895)) * f1( 0.015135505795270674)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -8.221655227352768) + discount_factor ( 0.1) * next_state_max_Q( -0.003662347517061953) - present_state_Q (-0.02052554775203895)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2222247245150575 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0050773423299315945) - present_state_Q ( -0.022058742017513018)) * f1( 0.017612302437259787)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0050773423299315945) - present_state_Q (-0.022058742017513018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2390048544414862 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011311989397817425) - present_state_Q ( -0.02782317888489353)) * f1( 0.023045051665017836)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.011311989397817425) - present_state_Q (-0.02782317888489353)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2442700594675336 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004278732222688917) - present_state_Q ( -0.004278732222688917)) * f1( 0.007208376312322672)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.004278732222688917) - present_state_Q (-0.004278732222688917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2503492666656655 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002930082240096537) - present_state_Q ( -0.002930082240096537)) * f1( 0.008321410504872126)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.002930082240096537) - present_state_Q (-0.002930082240096537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.256243413195815 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015537033892230235) - present_state_Q ( -0.0022788737435581385)) * f1( 0.008065982699752151)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.015537033892230235) - present_state_Q (-0.0022788737435581385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2713664014859691 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01123564647414625) - present_state_Q ( -0.025660877048288948)) * f1( 0.02076306620940957)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.01123564647414625) - present_state_Q (-0.025660877048288948)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.285681538682454 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028091902844490358) - present_state_Q ( -0.028091902844490358)) * f1( 0.01965594079665703)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.028091902844490358) - present_state_Q (-0.028091902844490358)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2886201192865394 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007737598657758139) - present_state_Q ( -0.007737598657758139)) * f1( 0.0040248055275831415)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.007737598657758139) - present_state_Q (-0.007737598657758139)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3066678670321703 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008070865927047187) - present_state_Q ( -0.033592416984708176)) * f1( 0.02480669819754779)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.008070865927047187) - present_state_Q (-0.033592416984708176)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3217480775724901 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006758203629730594) - present_state_Q ( -0.023525185142782738)) * f1( 0.02069953757094227)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.006758203629730594) - present_state_Q (-0.023525185142782738)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3348561893211732 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00973305971478694) - present_state_Q ( -0.02842536752986318)) * f1( 0.018003951682239956)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.00973305971478694) - present_state_Q (-0.02842536752986318)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.346747107730437 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007502607427425037) - present_state_Q ( -0.02553839870355729)) * f1( 0.016326166760305266)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.007502607427425037) - present_state_Q (-0.02553839870355729)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3661463869499815 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012599521202393788) - present_state_Q ( -0.03701889305629891)) * f1( 0.02667528619684088)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.012599521202393788) - present_state_Q (-0.03701889305629891)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3768148064540338 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011150780752513197) - present_state_Q ( -0.011286598714563497)) * f1( 0.016710007629827092)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.011150780752513197) - present_state_Q (-0.011286598714563497)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.382031821192924 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003283817921753973) - present_state_Q ( -0.003283817921753973)) * f1( 0.008162215161589019)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.003283817921753973) - present_state_Q (-0.003283817921753973)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3871100190553445 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0025454330714553414) - present_state_Q ( -0.0025454330714553414)) * f1( 0.007944204988549669)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0025454330714553414) - present_state_Q (-0.0025454330714553414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3920520506854512 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019097817232804418) - present_state_Q ( -0.0019097817232804418)) * f1( 0.007730498113565984)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0019097817232804418) - present_state_Q (-0.0019097817232804418)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3968789169620148 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002793164884395734) - present_state_Q ( -0.002793164884395734)) * f1( 0.007551291560304763)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.002793164884395734) - present_state_Q (-0.002793164884395734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4102508085725478 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012222622675322474) - present_state_Q ( -0.03142953543666273)) * f1( 0.021010406844591475)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.012222622675322474) - present_state_Q (-0.03142953543666273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.422428689625301 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005123665407711763) - present_state_Q ( -0.02275867734942005)) * f1( 0.01671433176047121)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.005123665407711763) - present_state_Q (-0.02275867734942005)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4249294258640626 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005562272812354824) - present_state_Q ( -0.005562272812354824)) * f1( 0.00342419699006635)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.005562272812354824) - present_state_Q (-0.005562272812354824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4357931694786032 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010661119784900868) - present_state_Q ( -0.01490017106543185)) * f1( 0.01489346171001837)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.010661119784900868) - present_state_Q (-0.01490017106543185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.44125893214825 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0030926270073875907) - present_state_Q ( -0.0030926270073875907)) * f1( 0.00748185811268237)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0030926270073875907) - present_state_Q (-0.0030926270073875907)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4468616932428628 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022910015259634074) - present_state_Q ( -0.0022910015259634074)) * f1( 0.007668632292378755)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0022910015259634074) - present_state_Q (-0.0022910015259634074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4523389792104437 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016611317426272717) - present_state_Q ( -0.0016611317426272717)) * f1( 0.0074963098602067015)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0016611317426272717) - present_state_Q (-0.0016611317426272717)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4577049320598465 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013406254955906758) - present_state_Q ( -0.019214109695969418)) * f1( 0.0073604364458804755)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.013406254955906758) - present_state_Q (-0.019214109695969418)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4673641772270332 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007771034472961634) - present_state_Q ( -0.007771034472961634)) * f1( 0.013229769338295295)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.007771034472961634) - present_state_Q (-0.007771034472961634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.4858113486087403 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015481697211320188) - present_state_Q ( -0.03950262748716412)) * f1( 0.025373735512485163)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.015481697211320188) - present_state_Q (-0.03950262748716412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5052005014353809 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021952153717837097) - present_state_Q ( -0.042935773681058574)) * f1( 0.026679637429927183)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.021952153717837097) - present_state_Q (-0.042935773681058574)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5194867841569302 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009840448100155713) - present_state_Q ( -0.031033138091588915)) * f1( 0.019629167689716932)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.009840448100155713) - present_state_Q (-0.031033138091588915)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5316583370273154 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01398917995822319) - present_state_Q ( -0.01640215523168728)) * f1( 0.016689055255258618)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.01398917995822319) - present_state_Q (-0.01640215523168728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5390612869639815 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002965436628165712) - present_state_Q ( -0.002965436628165712)) * f1( 0.010133435442136332)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.002965436628165712) - present_state_Q (-0.002965436628165712)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5448837673253637 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0024480135056906107) - present_state_Q ( -0.0024480135056906107)) * f1( 0.00796952141854956)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0024480135056906107) - present_state_Q (-0.0024480135056906107)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5505723576575676 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032967477303646373) - present_state_Q ( -0.0032967477303646373)) * f1( 0.0077870735739003155)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0032967477303646373) - present_state_Q (-0.0032967477303646373)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5561423825455525 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002425988405229018) - present_state_Q ( -0.002425988405229018)) * f1( 0.007623952242157026)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.002425988405229018) - present_state_Q (-0.002425988405229018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5615922686175925 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017344806222139263) - present_state_Q ( -0.0017344806222139263)) * f1( 0.0074588772709047735)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0017344806222139263) - present_state_Q (-0.0017344806222139263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5669465482334286 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0027400274974581666) - present_state_Q ( -0.0027400274974581666)) * f1( 0.007328935173557544)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0027400274974581666) - present_state_Q (-0.0027400274974581666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5723916972642966 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018787676022082077) - present_state_Q ( -0.0018787676022082077)) * f1( 0.007452526466722309)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0018787676022082077) - present_state_Q (-0.0018787676022082077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5777347481630448 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032991198724885372) - present_state_Q ( -0.0032991198724885372)) * f1( 0.007314069072899882)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0032991198724885372) - present_state_Q (-0.0032991198724885372)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.583237800608413 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023939861207565236) - present_state_Q ( -0.0023939861207565236)) * f1( 0.00753225424947235)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0023939861207565236) - present_state_Q (-0.0023939861207565236)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5886316326417795 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016573963436364025) - present_state_Q ( -0.0016573963436364025)) * f1( 0.007382089942234815)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0016573963436364025) - present_state_Q (-0.0016573963436364025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5939386272523572 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0027591715260235623) - present_state_Q ( -0.0027591715260235623)) * f1( 0.00726422861466058)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0027591715260235623) - present_state_Q (-0.0027591715260235623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.5993462755010703 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018606416961901504) - present_state_Q ( -0.0018606416961901504)) * f1( 0.007401184336294946)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0018606416961901504) - present_state_Q (-0.0018606416961901504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6046576404251511 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003361328404314913) - present_state_Q ( -0.003361328404314913)) * f1( 0.007270750063560951)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.003361328404314913) - present_state_Q (-0.003361328404314913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6088980968826925 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011623032546046266) - present_state_Q ( -0.011623032546046266)) * f1( 0.005810693336986903)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.011623032546046266) - present_state_Q (-0.011623032546046266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.61352903294883 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013589381066297379) - present_state_Q ( -0.0013589381066297379)) * f1( 0.0072433098832014806)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0013589381066297379) - present_state_Q (-0.0013589381066297379)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6180643069443117 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002985796483373062) - present_state_Q ( -0.002985796483373062)) * f1( 0.007095308460965667)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.002985796483373062) - present_state_Q (-0.002985796483373062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6225638734300223 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001901479017870925) - present_state_Q ( -0.001901479017870925)) * f1( 0.007038370487179334)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.001901479017870925) - present_state_Q (-0.001901479017870925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6270171871428125 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010630694896022965) - present_state_Q ( -0.0010630694896022965)) * f1( 0.006965198255325588)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0010630694896022965) - present_state_Q (-0.0010630694896022965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6314434790310919 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002321254162915814) - present_state_Q ( -0.002321254162915814)) * f1( 0.006924161134783941)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.002321254162915814) - present_state_Q (-0.002321254162915814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6359634296140104 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001328071184131051) - present_state_Q ( -0.001328071184131051)) * f1( 0.00706968526545817)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.001328071184131051) - present_state_Q (-0.001328071184131051)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.640436771912807 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002897191731842322) - present_state_Q ( -0.002897191731842322)) * f1( 0.006998330755837421)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.002897191731842322) - present_state_Q (-0.002897191731842322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6450472507107954 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018661494981492169) - present_state_Q ( -0.0018661494981492169)) * f1( 0.007211827319087891)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.0018661494981492169) - present_state_Q (-0.0018661494981492169)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6495932326839484 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001341050362627309) - present_state_Q ( -0.001341050362627309)) * f1( 0.007110414130170157)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.001341050362627309) - present_state_Q (-0.001341050362627309)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.654091141470332 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007458187536038174) - present_state_Q ( -0.01694725740047504)) * f1( 0.007051760913239237)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.007458187536038174) - present_state_Q (-0.01694725740047504)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6641454066918715 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010206099432645988) - present_state_Q ( -0.029127070583457322)) * f1( 0.01579241762821532)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010206099432645988) - present_state_Q (-0.029127070583457322)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6808997008788904 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01124785533787149) - present_state_Q ( -0.03430708308528112)) * f1( 0.02633727317909054)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01124785533787149) - present_state_Q (-0.03430708308528112)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.6929877292297453 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010702793587908863) - present_state_Q ( -0.03392443220246314)) * f1( 0.01900105607415333)
w2 ( -1.1340012026146462 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.010702793587908863) - present_state_Q (-0.03392443220246314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7060954642585122 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022051949329764734) - present_state_Q ( -0.037450915365844106)) * f1( 0.020611671750966922)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.022051949329764734) - present_state_Q (-0.037450915365844106)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.7112445821823747 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02254913652874415) - present_state_Q ( -0.02254913652874415)) * f1( 0.008077901118150845)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.02254913652874415) - present_state_Q (-0.02254913652874415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7241276608939726 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02740745581114273) - present_state_Q ( -0.043922075221668896)) * f1( 0.020277329954594105)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.02740745581114273) - present_state_Q (-0.043922075221668896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.73534575197089 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012867511462692348) - present_state_Q ( -0.03599267020905824)) * f1( 0.02059726808648626)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.012867511462692348) - present_state_Q (-0.03599267020905824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7386339391252272 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0072210508702385415) - present_state_Q ( -0.0072210508702385415)) * f1( 0.004503354848119887)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0072210508702385415) - present_state_Q (-0.0072210508702385415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7590725269236156 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016249844368240077) - present_state_Q ( -0.0505047204431535)) * f1( 0.028155201415609864)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.016249844368240077) - present_state_Q (-0.0505047204431535)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7609280838334105 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010407653721849453) - present_state_Q ( -0.017788622093347337)) * f1( 0.002544860278294573)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.010407653721849453) - present_state_Q (-0.017788622093347337)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7702293490340066 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018600607490740235) - present_state_Q ( -0.019600378189188132)) * f1( 0.012758241225345206)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.018600607490740235) - present_state_Q (-0.019600378189188132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7845953689055791 ) += alpha ( 0.1 ) * (reward ( -7.308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008900110850882043) - present_state_Q ( -0.03386204639900833)) * f1( 0.01974665592655532)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -7.308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.008900110850882043) - present_state_Q (-0.03386204639900833)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.7996549075671622 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017307709336275404) - present_state_Q ( -0.04223894824349986)) * f1( 0.023700459397046233)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.017307709336275404) - present_state_Q (-0.04223894824349986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8135029006356767 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018434303716533346) - present_state_Q ( -0.03900225138231665)) * f1( 0.02178226666798839)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.018434303716533346) - present_state_Q (-0.03900225138231665)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8260105368816213 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01409680726340138) - present_state_Q ( -0.03973991745674204)) * f1( 0.019677571776328166)
w2 ( -1.2611887028536986 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.01409680726340138) - present_state_Q (-0.03973991745674204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.841192777790245 ) += alpha ( 0.1 ) * (reward ( -6.394620732385485 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26156256675582074) - present_state_Q ( -0.2869590885382104)) * f1( 0.024751698134581973)
w2 ( -1.3838650608641556 ) += alpha ( 0.1) * (reward ( -6.394620732385485) + discount_factor ( 0.1) * next_state_max_Q( -0.26156256675582074) - present_state_Q (-0.2869590885382104)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.8431026319749466 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0056866350419489025) - present_state_Q ( -0.0056866350419489025)) * f1( 0.0034876903527971403)
w2 ( -1.3838650608641556 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0056866350419489025) - present_state_Q (-0.0056866350419489025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.847205644397321 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002442564396174878) - present_state_Q ( -0.002442564396174878)) * f1( 0.00748874508659049)
w2 ( -1.3838650608641556 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.002442564396174878) - present_state_Q (-0.002442564396174878)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.8557031707032998 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017068600978506498) - present_state_Q ( -0.017068600978506498)) * f1( 0.015546885864620255)
w2 ( -1.3838650608641556 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.017068600978506498) - present_state_Q (-0.017068600978506498)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.860500939149518 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021730924005082922) - present_state_Q ( -0.005038150776697343)) * f1( 0.008757867169467049)
w2 ( -1.3838650608641556 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.021730924005082922) - present_state_Q (-0.005038150776697343)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.865006821286491 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02328741726757008) - present_state_Q ( -0.2809176865986263)) * f1( 0.00866096981046983)
w2 ( -1.4879153516647552 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02328741726757008) - present_state_Q (-0.2809176865986263)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.873947783457331 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30992353229052494) - present_state_Q ( -0.33629396307175985)) * f1( 0.017274544866031596)
w2 ( -1.591431389165938 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.30992353229052494) - present_state_Q (-0.33629396307175985)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.8826731916415989 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3244812950218795) - present_state_Q ( -0.34880448095270983)) * f1( 0.016894163371047793)
w2 ( -1.6947263318349644 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.3244812950218795) - present_state_Q (-0.34880448095270983)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.89025061774243 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3462843273371572) - present_state_Q ( -0.37290393359549157)) * f1( 0.014733967623157077)
w2 ( -1.7975828915157657 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.3462843273371572) - present_state_Q (-0.37290393359549157)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.9012226215417352 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.376384568848977) - present_state_Q ( -0.3804567390571144)) * f1( 0.02135343478691361)
w2 ( -1.9003485955703583 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.376384568848977) - present_state_Q (-0.3804567390571144)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.9058047628095847 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3847687545485693) - present_state_Q ( -0.3847687545485693)) * f1( 0.008923679425096358)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.3847687545485693) - present_state_Q (-0.3847687545485693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.9089828264839475 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004341554952627235) - present_state_Q ( -0.004341554952627235)) * f1( 0.008706638310626806)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004341554952627235) - present_state_Q (-0.004341554952627235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9120363186135614 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0050850287938933) - present_state_Q ( -0.0050850287938933)) * f1( 0.008366895286722941)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0050850287938933) - present_state_Q (-0.0050850287938933)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9151071999438354 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004072654026988293) - present_state_Q ( -0.004072654026988293)) * f1( 0.008412443295781693)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004072654026988293) - present_state_Q (-0.004072654026988293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9180722567225608 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032981073114175543) - present_state_Q ( -0.0032981073114175543)) * f1( 0.008120994263791209)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0032981073114175543) - present_state_Q (-0.0032981073114175543)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.920949896060055 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003992480568559868) - present_state_Q ( -0.003992480568559868)) * f1( 0.007882915895668939)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.003992480568559868) - present_state_Q (-0.003992480568559868)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9305712172264582 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025285906380412556) - present_state_Q ( -0.05500432605139198)) * f1( 0.026714069264554028)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.025285906380412556) - present_state_Q (-0.05500432605139198)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9355735283177713 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020384382277981446) - present_state_Q ( -0.029006483791783808)) * f1( 0.013791485729331327)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.020384382277981446) - present_state_Q (-0.029006483791783808)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9443994940655567 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011097034150285977) - present_state_Q ( -0.04980640473580014)) * f1( 0.024480039899510406)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.011097034150285977) - present_state_Q (-0.04980640473580014)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9505436655325372 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01841502205202011) - present_state_Q ( -0.020189311558863072)) * f1( 0.01689945725155669)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01841502205202011) - present_state_Q (-0.020189311558863072)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.953998475030985 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006906355533303635) - present_state_Q ( -0.006906355533303635)) * f1( 0.009470801709331877)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.006906355533303635) - present_state_Q (-0.006906355533303635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.957415678997456 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005755594303285617) - present_state_Q ( -0.005755594303285617)) * f1( 0.009365053331455334)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005755594303285617) - present_state_Q (-0.005755594303285617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9606725705563697 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00482672600549643) - present_state_Q ( -0.00482672600549643)) * f1( 0.008923663058319662)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00482672600549643) - present_state_Q (-0.00482672600549643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9638224188076319 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005367040037911056) - present_state_Q ( -0.005367040037911056)) * f1( 0.008631521667939146)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005367040037911056) - present_state_Q (-0.005367040037911056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9668687500653135 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004327759475271379) - present_state_Q ( -0.004327759475271379)) * f1( 0.008345715169563561)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004327759475271379) - present_state_Q (-0.004327759475271379)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9698160770976136 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033711326470659293) - present_state_Q ( -0.0033711326470659293)) * f1( 0.008072579572839238)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0033711326470659293) - present_state_Q (-0.0033711326470659293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9726755340370747 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004596175992490958) - present_state_Q ( -0.004596175992490958)) * f1( 0.00783427362860388)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004596175992490958) - present_state_Q (-0.004596175992490958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9755832082753515 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00353553575938543) - present_state_Q ( -0.00353553575938543)) * f1( 0.00796429534001858)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00353553575938543) - present_state_Q (-0.00353553575938543)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9784112697693959 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002717069946465921) - present_state_Q ( -0.002717069946465921)) * f1( 0.007744668626709356)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.002717069946465921) - present_state_Q (-0.002717069946465921)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9811740550511998 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003730695011471346) - present_state_Q ( -0.003730695011471346)) * f1( 0.007567799823144016)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.003730695011471346) - present_state_Q (-0.003730695011471346)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9846522731253262 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019942043920597535) - present_state_Q ( -0.02631694786190414)) * f1( 0.0076557700841244905)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.019942043920597535) - present_state_Q (-0.02631694786190414)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9906975956760475 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018432015247718442) - present_state_Q ( -0.018432015247718442)) * f1( 0.01328351125677548)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.018432015247718442) - present_state_Q (-0.018432015247718442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.9955652160624384 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005588520424623801) - present_state_Q ( -0.03424159764138964)) * f1( 0.010736047635715519)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.005588520424623801) - present_state_Q (-0.03424159764138964)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.999605358628825 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006019557652794464) - present_state_Q ( -0.006019557652794464)) * f1( 0.008855750168727706)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.006019557652794464) - present_state_Q (-0.006019557652794464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0036330555990167 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004932313300600552) - present_state_Q ( -0.004932313300600552)) * f1( 0.008826576993870421)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.004932313300600552) - present_state_Q (-0.004932313300600552)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0074982174118565 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004098230492132248) - present_state_Q ( -0.004098230492132248)) * f1( 0.008468992858371565)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.004098230492132248) - present_state_Q (-0.004098230492132248)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.011256257531218 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00494682460766062) - present_state_Q ( -0.00494682460766062)) * f1( 0.008235655671955897)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00494682460766062) - present_state_Q (-0.00494682460766062)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0149119443667245 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038710180048442257) - present_state_Q ( -0.0038710180048442257)) * f1( 0.008009651357978933)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0038710180048442257) - present_state_Q (-0.0038710180048442257)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0184668960734102 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029062198672676993) - present_state_Q ( -0.0029062198672676993)) * f1( 0.0077874579869053944)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0029062198672676993) - present_state_Q (-0.0029062198672676993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.021933968221184 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00425516961716841) - present_state_Q ( -0.00425516961716841)) * f1( 0.007596969915856752)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00425516961716841) - present_state_Q (-0.00425516961716841)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.025453826354806 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013855920767872603) - present_state_Q ( -0.01991791355760991)) * f1( 0.0077375608174057855)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.013855920767872603) - present_state_Q (-0.01991791355760991)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.031447779670173 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007094471372784198) - present_state_Q ( -0.026750103816685845)) * f1( 0.013198047248028784)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.007094471372784198) - present_state_Q (-0.026750103816685845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0356469568799187 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00558198071984405) - present_state_Q ( -0.00558198071984405)) * f1( 0.0092035500913646)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00558198071984405) - present_state_Q (-0.00558198071984405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0396543566873597 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006264392580058012) - present_state_Q ( -0.006264392580058012)) * f1( 0.008784404289307171)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.006264392580058012) - present_state_Q (-0.006264392580058012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0436718706735606 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005151306663108375) - present_state_Q ( -0.005151306663108375)) * f1( 0.008804641585205475)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.005151306663108375) - present_state_Q (-0.005151306663108375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0475310897167778 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004177785227325108) - present_state_Q ( -0.004177785227325108)) * f1( 0.008456104260160727)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.004177785227325108) - present_state_Q (-0.004177785227325108)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.051253241672477 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00506550848620208) - present_state_Q ( -0.00506550848620208)) * f1( 0.00815719856024671)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.00506550848620208) - present_state_Q (-0.00506550848620208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.054986137925004 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02040852263492133) - present_state_Q ( -0.028303364859434653)) * f1( 0.008219841787809797)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.02040852263492133) - present_state_Q (-0.028303364859434653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.061273655060784 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011979758813814526) - present_state_Q ( -0.011979758813814526)) * f1( 0.013798084158715416)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.011979758813814526) - present_state_Q (-0.011979758813814526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0656665237946146 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0072909361926803825) - present_state_Q ( -0.0072909361926803825)) * f1( 0.009631320408625159)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0072909361926803825) - present_state_Q (-0.0072909361926803825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0692490731681943 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0145118361946065) - present_state_Q ( -0.020882954057780408)) * f1( 0.007876929861528563)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0145118361946065) - present_state_Q (-0.020882954057780408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.075147584743618 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007750935307217207) - present_state_Q ( -0.026667182820782117)) * f1( 0.012987469836238705)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.007750935307217207) - present_state_Q (-0.026667182820782117)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0787612583378237 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0046527143931609) - present_state_Q ( -0.0046527143931609)) * f1( 0.007918820504114818)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.0046527143931609) - present_state_Q (-0.0046527143931609)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.082410288269596 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003568662099608439) - present_state_Q ( -0.003568662099608439)) * f1( 0.00799458935395888)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.003568662099608439) - present_state_Q (-0.003568662099608439)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.085246596864229 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002945813682410242) - present_state_Q ( -0.002945813682410242)) * f1( 0.007767691299395775)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.002945813682410242) - present_state_Q (-0.002945813682410242)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.088020462341932 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0041304964691861075) - present_state_Q ( -0.0041304964691861075)) * f1( 0.007598899571568828)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0041304964691861075) - present_state_Q (-0.0041304964691861075)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0907446236113674 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029040723645128485) - present_state_Q ( -0.0029040723645128485)) * f1( 0.007460480896284407)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0029040723645128485) - present_state_Q (-0.0029040723645128485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0934180148043273 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020541936897290882) - present_state_Q ( -0.0020541936897290882)) * f1( 0.007319906892609405)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0020541936897290882) - present_state_Q (-0.0020541936897290882)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.0960732611184247 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004289787550364366) - present_state_Q ( -0.004289787550364366)) * f1( 0.007274232543275229)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004289787550364366) - present_state_Q (-0.004289787550364366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.098702388798346 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029753333740001955) - present_state_Q ( -0.0029753333740001955)) * f1( 0.007200345137477816)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0029753333740001955) - present_state_Q (-0.0029753333740001955)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.101297518966148 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017425074538312355) - present_state_Q ( -0.0017425074538312355)) * f1( 0.00710507774630922)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0017425074538312355) - present_state_Q (-0.0017425074538312355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.103863649440975 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003668484061073603) - present_state_Q ( -0.003668484061073603)) * f1( 0.007029016712935229)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.003668484061073603) - present_state_Q (-0.003668484061073603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.10650358398376 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023551617903605107) - present_state_Q ( -0.0023551617903605107)) * f1( 0.007228836694815628)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0023551617903605107) - present_state_Q (-0.0023551617903605107)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.109105916394556 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017780395154002018) - present_state_Q ( -0.0017780395154002018)) * f1( 0.0071248587895018246)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0017780395154002018) - present_state_Q (-0.0017780395154002018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1116791866247007 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0036758015904980175) - present_state_Q ( -0.0036758015904980175)) * f1( 0.007048586290139061)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0036758015904980175) - present_state_Q (-0.0036758015904980175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1142331308964493 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022217324573020314) - present_state_Q ( -0.0022217324573020314)) * f1( 0.006993142710441507)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0022217324573020314) - present_state_Q (-0.0022217324573020314)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1149973246086 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016621943620028137) - present_state_Q ( -0.016621943620028137)) * f1( 0.0020999471618179037)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.016621943620028137) - present_state_Q (-0.016621943620028137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1178677668399297 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033348203570184856) - present_state_Q ( -0.0033348203570184856)) * f1( 0.007861925620747567)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0033348203570184856) - present_state_Q (-0.0033348203570184856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.120663490651804 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002827602301638963) - present_state_Q ( -0.002827602301638963)) * f1( 0.007656320206756491)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.002827602301638963) - present_state_Q (-0.002827602301638963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1233935921489495 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038832358748224025) - present_state_Q ( -0.0038832358748224025)) * f1( 0.007478553890638721)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0038832358748224025) - present_state_Q (-0.0038832358748224025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.126078589295661 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002609665204778081) - present_state_Q ( -0.002609665204778081)) * f1( 0.007352691149825884)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.002609665204778081) - present_state_Q (-0.002609665204778081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.128718726604624 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001960917533926485) - present_state_Q ( -0.001960917533926485)) * f1( 0.007228689589643902)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.001960917533926485) - present_state_Q (-0.001960917533926485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1313287127939557 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038853518778288318) - present_state_Q ( -0.0038853518778288318)) * f1( 0.007149526335790929)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0038853518778288318) - present_state_Q (-0.0038853518778288318)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1383843486174734 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015397174252820516) - present_state_Q ( -0.04449640132160209)) * f1( 0.01953867735356696)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015397174252820516) - present_state_Q (-0.04449640132160209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.14553153505581 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01077971896754117) - present_state_Q ( -0.040986363679410845)) * f1( 0.019775508708692793)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01077971896754117) - present_state_Q (-0.040986363679410845)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1514847252571503 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01092689310563397) - present_state_Q ( -0.021867263022561877)) * f1( 0.016385102662639626)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01092689310563397) - present_state_Q (-0.021867263022561877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.155911340632478 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029662528620441214) - present_state_Q ( -0.03334399068159979)) * f1( 0.012224769234541626)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0029662528620441214) - present_state_Q (-0.03334399068159979)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.159210268945129 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006775979577469256) - present_state_Q ( -0.006775979577469256)) * f1( 0.0090431877524778)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.006775979577469256) - present_state_Q (-0.006775979577469256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.162486932237155 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0055814129215453956) - present_state_Q ( -0.0055814129215453956)) * f1( 0.00897950736980005)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0055814129215453956) - present_state_Q (-0.0055814129215453956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.165624709111611 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00468755273081165) - present_state_Q ( -0.00468755273081165)) * f1( 0.008597001819207743)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00468755273081165) - present_state_Q (-0.00468755273081165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.168663585030618 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005339805119424585) - present_state_Q ( -0.005339805119424585)) * f1( 0.008327368516145054)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005339805119424585) - present_state_Q (-0.005339805119424585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1716135170798276 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0041807553937115065) - present_state_Q ( -0.0041807553937115065)) * f1( 0.008081327424669863)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0041807553937115065) - present_state_Q (-0.0041807553937115065)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1780502353669924 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01567576878444428) - present_state_Q ( -0.04637371038587194)) * f1( 0.017833886151302842)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.01567576878444428) - present_state_Q (-0.04637371038587194)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1855706335187675 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02284175919658034) - present_state_Q ( -0.053093201610031296)) * f1( 0.020871095137185387)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.02284175919658034) - present_state_Q (-0.053093201610031296)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.192451151684574 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013435792621808293) - present_state_Q ( -0.04397560026968066)) * f1( 0.01905202340510629)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.013435792621808293) - present_state_Q (-0.04397560026968066)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.195311116611064 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015476004835024657) - present_state_Q ( -0.015476004835024657)) * f1( 0.007856743054971716)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.015476004835024657) - present_state_Q (-0.015476004835024657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.1982979565358973 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005106865296188735) - present_state_Q ( -0.027047023298467848)) * f1( 0.008233805758375937)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005106865296188735) - present_state_Q (-0.027047023298467848)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2008540836385215 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019257327221880446) - present_state_Q ( -0.02781317927172966)) * f1( 0.007045201768595129)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.019257327221880446) - present_state_Q (-0.02781317927172966)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2054651653329476 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0106249080678869) - present_state_Q ( -0.0106249080678869)) * f1( 0.012652142621993781)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0106249080678869) - present_state_Q (-0.0106249080678869)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2086688524707165 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005147331605631793) - present_state_Q ( -0.005147331605631793)) * f1( 0.008778580564495082)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005147331605631793) - present_state_Q (-0.005147331605631793)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2117668529382715 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005728005692899635) - present_state_Q ( -0.005728005692899635)) * f1( 0.008490199139400809)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.005728005692899635) - present_state_Q (-0.005728005692899635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2147674565183397 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004551993423485307) - present_state_Q ( -0.004551993423485307)) * f1( 0.008220894349488863)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.004551993423485307) - present_state_Q (-0.004551993423485307)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2176687312145154 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012352951290131212) - present_state_Q ( -0.012352951290131212)) * f1( 0.007964077557880044)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.012352951290131212) - present_state_Q (-0.012352951290131212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.222876196491393 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014151698135208223) - present_state_Q ( -0.04572930226587305)) * f1( 0.014431183042236471)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.0014151698135208223) - present_state_Q (-0.04572930226587305)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.226531926637134 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00905478499492991) - present_state_Q ( -0.00905478499492991)) * f1( 0.010026908059294067)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.00905478499492991) - present_state_Q (-0.00905478499492991)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.2334437953563007 ) += alpha ( 0.1 ) * (reward ( -3.6540689899345633 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009801427442566978) - present_state_Q ( -0.04123469496005167)) * f1( 0.019126241367085052)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -3.6540689899345633) + discount_factor ( 0.1) * next_state_max_Q( -0.009801427442566978) - present_state_Q (-0.04123469496005167)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.245746615030095 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021255998827518024) - present_state_Q ( -0.052834671893195836)) * f1( 0.02265548087002348)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.021255998827518024) - present_state_Q (-0.052834671893195836)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.256167992575153 ) += alpha ( 0.1 ) * (reward ( -5.481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015023121395657377) - present_state_Q ( -0.045855552249109074)) * f1( 0.019168394860588734)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -5.481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.015023121395657377) - present_state_Q (-0.045855552249109074)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -2.267466661856352 ) += alpha ( 0.1 ) * (reward ( -4.567586237418205 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03136978333573499) - present_state_Q ( -0.06181584952785364)) * f1( 0.025058552690251916)
w2 ( -2.003044827686521 ) += alpha ( 0.1) * (reward ( -4.567586237418205) + discount_factor ( 0.1) * next_state_max_Q( -0.03136978333573499) - present_state_Q (-0.06181584952785364)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0000471763392342 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007071067811865476) - present_state_Q ( -0.007071067811865476)) * f1( 0.007071067811865476)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.007071067811865476) - present_state_Q (-0.007071067811865476)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0000881954126908 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007044598009230276) - present_state_Q ( -0.01577610960130511)) * f1( 0.007071067811865476)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.007044598009230276) - present_state_Q (-0.01577610960130511)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0001356883911539 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1637822370737696) - present_state_Q ( -0.04325581434147719)) * f1( 0.035317744835107245)
w2 ( 1.0 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.1637822370737696) - present_state_Q (-0.04325581434147719)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0009042579803789 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16775206881174498) - present_state_Q ( 0.15483983249910102)) * f1( 0.038045959121735334)
w2 ( 0.9959597833411644 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.16775206881174498) - present_state_Q (0.15483983249910102)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0046761614035038 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24422022650809583) - present_state_Q ( 0.22424833579374004)) * f1( 0.16599362042306154)
w2 ( 0.9868705101184668 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.24422022650809583) - present_state_Q (0.22424833579374004)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9946204184020973 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3022841234786863) - present_state_Q ( -0.3614664859597987)) * f1( 0.30354782901539346)
w2 ( 0.9868705101184668 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.3022841234786863) - present_state_Q (-0.3614664859597987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9825302067967367 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3072141243052305) - present_state_Q ( -0.3595261955858485)) * f1( 0.36769682840196316)
w2 ( 0.9868705101184668 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.3072141243052305) - present_state_Q (-0.3595261955858485)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9801564537161219 ) += alpha ( 0.1 ) * (reward ( 0.0001427370704510559 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09504711012398681) - present_state_Q ( -0.0895095583111386)) * f1( 0.29617275421308137)
w2 ( 0.9884734618058506 ) += alpha ( 0.1) * (reward ( 0.0001427370704510559) + discount_factor ( 0.1) * next_state_max_Q( -0.09504711012398681) - present_state_Q (-0.0895095583111386)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9769137264271779 ) += alpha ( 0.1 ) * (reward ( 1.7842133806381986e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10187883184572519) - present_state_Q ( -0.11390249609205508)) * f1( 0.31260489184925844)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 1.7842133806381986e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.10187883184572519) - present_state_Q (-0.11390249609205508)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9718497541635528 ) += alpha ( 0.1 ) * (reward ( 4.355989698823727e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15698590386739833) - present_state_Q ( -0.15698590386739833)) * f1( 0.3584166180774874)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 4.355989698823727e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.15698590386739833) - present_state_Q (-0.15698590386739833)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.955245897539082 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4421321029022739) - present_state_Q ( -0.4421321029022739)) * f1( 0.448131099614602)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.4421321029022739) - present_state_Q (-0.4421321029022739)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9320658735120346 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4952195836451408) - present_state_Q ( -0.4952195836451408)) * f1( 0.5200840819479879)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.4952195836451408) - present_state_Q (-0.4952195836451408)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9093656073276227 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48652572766236757) - present_state_Q ( -0.48652572766236757)) * f1( 0.5184210525488071)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.48652572766236757) - present_state_Q (-0.48652572766236757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8832870597146883 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5623214357056664) - present_state_Q ( -0.5542746949860704)) * f1( 0.5236208741364687)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.5623214357056664) - present_state_Q (-0.5542746949860704)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8531728848554473 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3289020161374785) - present_state_Q ( -0.5291862277403246)) * f1( 0.5958115554119038)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.3289020161374785) - present_state_Q (-0.5291862277403246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8264111669427284 ) += alpha ( 0.1 ) * (reward ( 0.006851379356127306 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5012114944696653) - present_state_Q ( -0.5012114944696653)) * f1( 0.5843913425669984)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 0.006851379356127306) + discount_factor ( 0.1) * next_state_max_Q( -0.5012114944696653) - present_state_Q (-0.5012114944696653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8112502245086679 ) += alpha ( 0.1 ) * (reward ( 0.0017128448390318265 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2997977709982558) - present_state_Q ( -0.28852611501102776)) * f1( 0.5825324691275214)
w2 ( 0.9905481109066764 ) += alpha ( 0.1) * (reward ( 0.0017128448390318265) + discount_factor ( 0.1) * next_state_max_Q( -0.2997977709982558) - present_state_Q (-0.28852611501102776)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.797196073234384 ) += alpha ( 0.1 ) * (reward ( 1.0454375238231363e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06253161586814626) - present_state_Q ( -0.2546068897175555)) * f1( 0.5658922643929598)
w2 ( 1.0004822642136562 ) += alpha ( 0.1) * (reward ( 1.0454375238231363e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.06253161586814626) - present_state_Q (-0.2546068897175555)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7846788791285327 ) += alpha ( 0.1 ) * (reward ( 1.3067969047789203e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0382320971695444) - present_state_Q ( -0.2316399355468341)) * f1( 0.5494413696907451)
w2 ( 1.005038598991613 ) += alpha ( 0.1) * (reward ( 1.3067969047789203e-08) + discount_factor ( 0.1) * next_state_max_Q( -0.0382320971695444) - present_state_Q (-0.2316399355468341)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7741006793595979 ) += alpha ( 0.1 ) * (reward ( 3.266992261947301e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21725662889984895) - present_state_Q ( -0.21725662889984895)) * f1( 0.5409986872185417)
w2 ( 1.0089492183771502 ) += alpha ( 0.1) * (reward ( 3.266992261947301e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.21725662889984895) - present_state_Q (-0.21725662889984895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.767778488554499 ) += alpha ( 0.1 ) * (reward ( 7.976055327019778e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1533635921344279) - present_state_Q ( -0.1533635921344279)) * f1( 0.45803937898845903)
w2 ( 1.0089492183771502 ) += alpha ( 0.1) * (reward ( 7.976055327019778e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.1533635921344279) - present_state_Q (-0.1533635921344279)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7619447719392192 ) += alpha ( 0.1 ) * (reward ( 2.4925172896936805e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08415881484576682) - present_state_Q ( -0.12570654340730264)) * f1( 0.4349546036005168)
w2 ( 1.0116316668749883 ) += alpha ( 0.1) * (reward ( 2.4925172896936805e-14) + discount_factor ( 0.1) * next_state_max_Q( 0.08415881484576682) - present_state_Q (-0.12570654340730264)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7652851825740843 ) += alpha ( 0.1 ) * (reward ( 6.231293224234201e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09009819825502147) - present_state_Q ( 0.09009819825502147)) * f1( 0.4119469028189848)
w2 ( 1.0083881317378078 ) += alpha ( 0.1) * (reward ( 6.231293224234201e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.09009819825502147) - present_state_Q (0.09009819825502147)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7685922017439465 ) += alpha ( 0.1 ) * (reward ( 3.1156466121171006e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08900652331942338) - present_state_Q ( 0.08900652331942338)) * f1( 0.4128310608318815)
w2 ( 1.0051838968983087 ) += alpha ( 0.1) * (reward ( 3.1156466121171006e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.08900652331942338) - present_state_Q (0.08900652331942338)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7717077594971214 ) += alpha ( 0.1 ) * (reward ( 0.009135172474837966 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09442638135867104) - present_state_Q ( 0.09442638135867104)) * f1( 0.4107602453746306)
w2 ( 1.00214995406839 ) += alpha ( 0.1) * (reward ( 0.009135172474837966) + discount_factor ( 0.1) * next_state_max_Q( 0.09442638135867104) - present_state_Q (0.09442638135867104)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7749491079588553 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418983 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09505100667615773) - present_state_Q ( 0.09505100667615773)) * f1( 0.4002736128503473)
w2 ( 0.9989108212775452 ) += alpha ( 0.1) * (reward ( 0.004567586237418983) + discount_factor ( 0.1) * next_state_max_Q( 0.09505100667615773) - present_state_Q (0.09505100667615773)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7782670551373746 ) += alpha ( 0.1 ) * (reward ( 0.0022837931187094916 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0955689568753374) - present_state_Q ( 0.0955689568753374)) * f1( 0.39627562530986704)
w2 ( 0.9955616905547814 ) += alpha ( 0.1) * (reward ( 0.0022837931187094916) + discount_factor ( 0.1) * next_state_max_Q( 0.0955689568753374) - present_state_Q (0.0955689568753374)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7808318410255876 ) += alpha ( 0.1 ) * (reward ( 0.0011418965593547458 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29515122555790313) - present_state_Q ( 0.09603888744694683)) * f1( 0.3922778522016453)
w2 ( 0.9929464158215093 ) += alpha ( 0.1) * (reward ( 0.0011418965593547458) + discount_factor ( 0.1) * next_state_max_Q( 0.29515122555790313) - present_state_Q (0.09603888744694683)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7834324028474374 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796773729 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29570736696442323) - present_state_Q ( 0.09711808380012127)) * f1( 0.38828032971487647)
w2 ( 0.9902673598685493 ) += alpha ( 0.1) * (reward ( 0.0005709482796773729) + discount_factor ( 0.1) * next_state_max_Q( 0.29570736696442323) - present_state_Q (0.09711808380012127)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7936667535120537 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983868645 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29623186315850625) - present_state_Q ( 0.29623186315850625)) * f1( 0.3842831026644181)
w2 ( 0.9796144317604366 ) += alpha ( 0.1) * (reward ( 0.00028547413983868645) + discount_factor ( 0.1) * next_state_max_Q( 0.29623186315850625) - present_state_Q (0.29623186315850625)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8035566764284899 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991935135 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28911997514469595) - present_state_Q ( 0.28911997514469595)) * f1( 0.3802862272223898)
w2 ( 0.9692118221380244 ) += alpha ( 0.1) * (reward ( 0.00014273706991935135) + discount_factor ( 0.1) * next_state_max_Q( 0.28911997514469595) - present_state_Q (0.28911997514469595)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8131166845378109 ) += alpha ( 0.1 ) * (reward ( 7.136853495967568e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2823678946919806) - present_state_Q ( 0.2823678946919806)) * f1( 0.3762897747574989)
w2 ( 0.953968237936755 ) += alpha ( 0.1) * (reward ( 7.136853495967568e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.2823678946919806) - present_state_Q (0.2823678946919806)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8225629545962673 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28196361804332953) - present_state_Q ( 0.28196361804332953)) * f1( 0.37229383734385146)
w2 ( 0.938744343618464 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.28196361804332953) - present_state_Q (0.28196361804332953)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8313180498075627 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27238320519126635) - present_state_Q ( 0.27238320519126635)) * f1( 0.3571656199427289)
w2 ( 0.9240367210661601 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.27238320519126635) - present_state_Q (0.27238320519126635)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8397001843808706 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2633956099286006) - present_state_Q ( 0.2633956099286006)) * f1( 0.35360624904701005)
w2 ( 0.9098138933940279 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.2633956099286006) - present_state_Q (0.2633956099286006)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8477299395208402 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2548607913908062) - present_state_Q ( 0.2548607913908062)) * f1( 0.3500783157282145)
w2 ( 0.8960516782909305 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.2548607913908062) - present_state_Q (0.2548607913908062)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8554266278131308 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24674928557147352) - present_state_Q ( 0.24674928557147352)) * f1( 0.3465850669786284)
w2 ( 0.882727350686074 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.24674928557147352) - present_state_Q (0.24674928557147352)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8628083313766037 ) += alpha ( 0.1 ) * (reward ( 1.1151333587446788e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23903276342128282) - present_state_Q ( 0.23903276342128282)) * f1( 0.3431301737054362)
w2 ( 0.8698196483693262 ) += alpha ( 0.1) * (reward ( 1.1151333587446788e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.23903276342128282) - present_state_Q (0.23903276342128282)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8698919560598708 ) += alpha ( 0.1 ) * (reward ( 5.575666793723394e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23168392342199562) - present_state_Q ( 0.23168392342199562)) * f1( 0.33971779407110575)
w2 ( 0.8573087499585392 ) += alpha ( 0.1) * (reward ( 5.575666793723394e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.23168392342199562) - present_state_Q (0.23168392342199562)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8766932903932619 ) += alpha ( 0.1 ) * (reward ( 2.787833396861697e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2246763522688966) - present_state_Q ( 0.2246763522688966)) * f1( 0.33635264640592416)
w2 ( 0.8451762436630192 ) += alpha ( 0.1) * (reward ( 2.787833396861697e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.2246763522688966) - present_state_Q (0.2246763522688966)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8832270638183131 ) += alpha ( 0.1 ) * (reward ( 1.3939166984308485e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21798436785151004) - present_state_Q ( 0.21798436785151004)) * f1( 0.3330400927242137)
w2 ( 0.8334050961625379 ) += alpha ( 0.1) * (reward ( 1.3939166984308485e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.21798436785151004) - present_state_Q (0.21798436785151004)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.889507003868811 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21158285138211685) - present_state_Q ( 0.21158285138211685)) * f1( 0.32978623369708826)
w2 ( 0.8219796221879035 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.21158285138211685) - present_state_Q (0.21158285138211685)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8955458783258723 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2054470691356029) - present_state_Q ( 0.2054470691356029)) * f1( 0.32659801554127255)
w2 ( 0.810885480454581 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.2054470691356029) - present_state_Q (0.2054470691356029)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9010600428301191 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1995525048173557) - present_state_Q ( 0.1995525048173557)) * f1( 0.3234833485578454)
w2 ( 0.800657755542934 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.1995525048173557) - present_state_Q (0.1995525048173557)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9065173285658145 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1942973592836189) - present_state_Q ( 0.1942973592836189)) * f1( 0.3204512358338015)
w2 ( 0.7904397533158636 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( 0.1942973592836189) - present_state_Q (0.1942973592836189)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9118457996427075 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18900373253943797) - present_state_Q ( 0.18900373253943797)) * f1( 0.31751190868873175)
w2 ( 0.7803705793458565 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( 0.18900373253943797) - present_state_Q (0.18900373253943797)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9170142166586202 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18376340358197868) - present_state_Q ( 0.18376340358197868)) * f1( 0.3146769625478482)
w2 ( 0.7705158693459909 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( 0.18376340358197868) - present_state_Q (0.18376340358197868)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.922051366358353 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1657001019943376) - present_state_Q ( 0.17860903295317937)) * f1( 0.31195948277328917)
w2 ( 0.7608277848775469 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( 0.1657001019943376) - present_state_Q (0.17860903295317937)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9268889960191701 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17405970447036262) - present_state_Q ( 0.17405970447036262)) * f1( 0.3093741443705767)
w2 ( 0.7514456892845376 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( 0.17405970447036262) - present_state_Q (0.17405970447036262)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9320056187249732 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16968144118658612) - present_state_Q ( 0.21539220716962273)) * f1( 0.2580486427800205)
w2 ( 0.7395488097256749 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( 0.16968144118658612) - present_state_Q (0.21539220716962273)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9368457426250768 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2009119035173416) - present_state_Q ( 0.21068187728719215)) * f1( 0.25404898257766106)
w2 ( 0.728117650621645 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.2009119035173416) - present_state_Q (0.21068187728719215)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9415134788656795 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1965162251471748) - present_state_Q ( 0.20635989971377278)) * f1( 0.2500493600747091)
w2 ( 0.7169172950457505 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.1965162251471748) - present_state_Q (0.20635989971377278)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9460163154021565 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19233854226979016) - present_state_Q ( 0.20225679807080174)) * f1( 0.24604978191320456)
w2 ( 0.7059369889431456 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.19233854226979016) - present_state_Q (0.20225679807080174)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9503615177307547 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18836802414862636) - present_state_Q ( 0.19836225832679696)) * f1( 0.24205025639272976)
w2 ( 0.6951659968524417 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.18836802414862636) - present_state_Q (0.19836225832679696)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9545560273335865 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1845939181478255) - present_state_Q ( 0.19466614648747252)) * f1( 0.23805079402183119)
w2 ( 0.6845938592040864 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1845939181478255) - present_state_Q (0.19466614648747252)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9586064246098422 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1810056905112081) - present_state_Q ( 0.1911586807408335)) * f1( 0.23405140830524437)
w2 ( 0.6742105063187067 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1810056905112081) - present_state_Q (0.1911586807408335)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9625189227998343 ) += alpha ( 0.1 ) * (reward ( 1.1151333587446788e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1775930500824947) - present_state_Q ( 0.18783050003580376)) * f1( 0.23005211689358085)
w2 ( 0.664006301525055 ) += alpha ( 0.1) * (reward ( 1.1151333587446788e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1775930500824947) - present_state_Q (0.18783050003580376)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9662993764727376 ) += alpha ( 0.1 ) * (reward ( 5.575666793723394e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17434590137727554) - present_state_Q ( 0.18467268142588278)) * f1( 0.22605294330633827)
w2 ( 0.6539720495017665 ) += alpha ( 0.1) * (reward ( 5.575666793723394e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.17434590137727554) - present_state_Q (0.18467268142588278)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.969863249854668 ) += alpha ( 0.1 ) * (reward ( 2.787833396861697e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16815240042200336) - present_state_Q ( 0.17731137444830897)) * f1( 0.22205391959197657)
w2 ( 0.6443422981644004 ) += alpha ( 0.1) * (reward ( 2.787833396861697e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.16815240042200336) - present_state_Q (0.17731137444830897)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9731835749550831 ) += alpha ( 0.1 ) * (reward ( 0.009135181186815774 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15982723685228337) - present_state_Q ( 0.16848821306690764)) * f1( 0.2315908462652005)
w2 ( 0.6357400796727085 ) += alpha ( 0.1) * (reward ( 0.009135181186815774) + discount_factor ( 0.1) * next_state_max_Q( 0.15982723685228337) - present_state_Q (0.16848821306690764)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9764682415647784 ) += alpha ( 0.1 ) * (reward ( 0.004567590593407887 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1576582093274346) - present_state_Q ( 0.1663869062440212)) * f1( 0.22489476311677656)
w2 ( 0.6269768699896363 ) += alpha ( 0.1) * (reward ( 0.004567590593407887) + discount_factor ( 0.1) * next_state_max_Q( 0.1576582093274346) - present_state_Q (0.1663869062440212)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9797032742945394 ) += alpha ( 0.1 ) * (reward ( 0.0022837952967039435 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15540601089797726) - present_state_Q ( 0.16421715015482952)) * f1( 0.22098311880112634)
w2 ( 0.6181933047635366 ) += alpha ( 0.1) * (reward ( 0.0022837952967039435) + discount_factor ( 0.1) * next_state_max_Q( 0.15540601089797726) - present_state_Q (0.16421715015482952)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9828618644153188 ) += alpha ( 0.1 ) * (reward ( 0.0011418976483519717 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1541798457511784) - present_state_Q ( 0.1620652792777434)) * f1( 0.2170771795908842)
w2 ( 0.6094629809402802 ) += alpha ( 0.1) * (reward ( 0.0011418976483519717) + discount_factor ( 0.1) * next_state_max_Q( 0.1541798457511784) - present_state_Q (0.1620652792777434)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.985935777718698 ) += alpha ( 0.1 ) * (reward ( 0.0005709488241759859 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1521383392678859) - present_state_Q ( 0.1599797927074038)) * f1( 0.21317750900727261)
w2 ( 0.6008112803428938 ) += alpha ( 0.1) * (reward ( 0.0005709488241759859) + discount_factor ( 0.1) * next_state_max_Q( 0.1521383392678859) - present_state_Q (0.1599797927074038)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9889216848893668 ) += alpha ( 0.1 ) * (reward ( 0.00028547441208799293 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15018468424150974) - present_state_Q ( 0.15797593516708536)) * f1( 0.20928474621317128)
w2 ( 0.592250960803043 ) += alpha ( 0.1) * (reward ( 0.00028547441208799293) + discount_factor ( 0.1) * next_state_max_Q( 0.15018468424150974) - present_state_Q (0.15797593516708536)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9918195617123361 ) += alpha ( 0.1 ) * (reward ( 0.00014273720604399647 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14832595210458754) - present_state_Q ( 0.15606015224151917)) * f1( 0.20539961893585962)
w2 ( 0.583785871613542 ) += alpha ( 0.1) * (reward ( 0.00014273720604399647) + discount_factor ( 0.1) * next_state_max_Q( 0.14832595210458754) - present_state_Q (0.15606015224151917)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9946309084194513 ) += alpha ( 0.1 ) * (reward ( 7.136860302199823e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14656310012859872) - present_state_Q ( 0.15423271168541958)) * f1( 0.20152295908305598)
w2 ( 0.5754155696293698 ) += alpha ( 0.1) * (reward ( 7.136860302199823e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.14656310012859872) - present_state_Q (0.15423271168541958)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9973578805670402 ) += alpha ( 0.1 ) * (reward ( 3.5684301510999116e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14489415832206579) - present_state_Q ( 0.15249085816532135)) * f1( 0.19765572171641044)
w2 ( 0.5671376241474736 ) += alpha ( 0.1) * (reward ( 3.5684301510999116e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.14489415832206579) - present_state_Q (0.15249085816532135)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.000002867615555 ) += alpha ( 0.1 ) * (reward ( 1.7842150755499558e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14331580568365085) - present_state_Q ( 0.15083036101666664)) * f1( 0.19379900823573779)
w2 ( 0.5589487678496208 ) += alpha ( 0.1) * (reward ( 1.7842150755499558e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.14331580568365085) - present_state_Q (0.15083036101666664)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.002568291190096 ) += alpha ( 0.1 ) * (reward ( 8.921075377749779e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1418241448059403) - present_state_Q ( 0.14924625902043254)) * f1( 0.18995409487726306)
w2 ( 0.5508454724417531 ) += alpha ( 0.1) * (reward ( 8.921075377749779e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1418241448059403) - present_state_Q (0.14924625902043254)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.005056510962174 ) += alpha ( 0.1 ) * (reward ( 4.4605376888748896e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14041507708908418) - present_state_Q ( 0.14773320113837263)) * f1( 0.1861224679616557)
w2 ( 0.5428242384682466 ) += alpha ( 0.1) * (reward ( 4.4605376888748896e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.14041507708908418) - present_state_Q (0.14773320113837263)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0074697831028538 ) += alpha ( 0.1 ) * (reward ( 2.2302688444374448e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1390844783911194) - present_state_Q ( 0.14628558025264976)) * f1( 0.18230586777257615)
w2 ( 0.5348817443395649 ) += alpha ( 0.1) * (reward ( 2.2302688444374448e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1390844783911194) - present_state_Q (0.14628558025264976)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0098102442757817 ) += alpha ( 0.1 ) * (reward ( 1.1151344222187224e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13782827411059306) - present_state_Q ( 0.14489755601398085)) * f1( 0.17850634354534356)
w2 ( 0.527014927531455 ) += alpha ( 0.1) * (reward ( 1.1151344222187224e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.13782827411059306) - present_state_Q (0.14489755601398085)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0102596461857234 ) += alpha ( 0.1 ) * (reward ( 5.575672111093612e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12439136556967895) - present_state_Q ( 0.038160026504374445)) * f1( 0.1747263228556671)
w2 ( 0.5254717075886433 ) += alpha ( 0.1) * (reward ( 5.575672111093612e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.12439136556967895) - present_state_Q (0.038160026504374445)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0107470875895674 ) += alpha ( 0.1 ) * (reward ( 2.787836055546806e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12725100592303) - present_state_Q ( 0.041235943263635666)) * f1( 0.1709686997996601)
w2 ( 0.5237610737553796 ) += alpha ( 0.1) * (reward ( 2.787836055546806e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.12725100592303) - present_state_Q (0.041235943263635666)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0112344293565163 ) += alpha ( 0.1 ) * (reward ( 1.393918027773403e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12723684552135844) - present_state_Q ( 0.04186461927606605)) * f1( 0.16723694785762216)
w2 ( 0.5225954419420945 ) += alpha ( 0.1) * (reward ( 1.393918027773403e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.12723684552135844) - present_state_Q (0.04186461927606605)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0134542726253193 ) += alpha ( 0.1 ) * (reward ( 6.969590138867015e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1424422033818133) - present_state_Q ( 0.1424422033818133)) * f1( 0.17315751955976574)
w2 ( 0.5149035671412306 ) += alpha ( 0.1) * (reward ( 6.969590138867015e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.1424422033818133) - present_state_Q (0.1424422033818133)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0139458776493928 ) += alpha ( 0.1 ) * (reward ( 3.4847950694335075e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12631857250244055) - present_state_Q ( 0.0429628903721557)) * f1( 0.16208006727411742)
w2 ( 0.5136903272102722 ) += alpha ( 0.1) * (reward ( 3.4847950694335075e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.12631857250244055) - present_state_Q (0.0429628903721557)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0158802538740226 ) += alpha ( 0.1 ) * (reward ( 0.009135189898811754 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13760612225973473) - present_state_Q ( 0.13760612225973473)) * f1( 0.16863140320366263)
w2 ( 0.5068077080021752 ) += alpha ( 0.1) * (reward ( 0.009135189898811754) + discount_factor ( 0.1) * next_state_max_Q( 0.13760612225973473) - present_state_Q (0.13760612225973473)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0178243037363788 ) += alpha ( 0.1 ) * (reward ( 0.004567594949405877 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13138041242344775) - present_state_Q ( 0.13324303702504034)) * f1( 0.16826151950234797)
w2 ( 0.49987546395217786 ) += alpha ( 0.1) * (reward ( 0.004567594949405877) + discount_factor ( 0.1) * next_state_max_Q( 0.13138041242344775) - present_state_Q (0.13324303702504034)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.019744606834473 ) += alpha ( 0.1 ) * (reward ( 0.0011418987373514693 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12828813781745033) - present_state_Q ( 0.12796834467430798)) * f1( 0.16845113900960038)
w2 ( 0.49303560602286517 ) += alpha ( 0.1) * (reward ( 0.0011418987373514693) + discount_factor ( 0.1) * next_state_max_Q( 0.12828813781745033) - present_state_Q (0.12796834467430798)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.021674106761576 ) += alpha ( 0.1 ) * (reward ( 0.0005709493686757346 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12790862429736108) - present_state_Q ( 0.1275701706961884)) * f1( 0.1689455960775883)
w2 ( 0.4861831044889986 ) += alpha ( 0.1) * (reward ( 0.0005709493686757346) + discount_factor ( 0.1) * next_state_max_Q( 0.12790862429736108) - present_state_Q (0.1275701706961884)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0220500060931788 ) += alpha ( 0.1 ) * (reward ( 0.0002854746843378673 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12285072805251429) - present_state_Q ( 0.03696149312411612)) * f1( 0.15411429193241324)
w2 ( 0.4852074666636175 ) += alpha ( 0.1) * (reward ( 0.0002854746843378673) + discount_factor ( 0.1) * next_state_max_Q( 0.12285072805251429) - present_state_Q (0.03696149312411612)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0239237102553347 ) += alpha ( 0.1 ) * (reward ( 0.00014273734216893366 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1270921994265651) - present_state_Q ( 0.12621953021934856)) * f1( 0.1652769053491251)
w2 ( 0.47840541228754613 ) += alpha ( 0.1) * (reward ( 0.00014273734216893366) + discount_factor ( 0.1) * next_state_max_Q( 0.1270921994265651) - present_state_Q (0.12621953021934856)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.024304164211562 ) += alpha ( 0.1 ) * (reward ( 7.136867108446683e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12250346481964375) - present_state_Q ( 0.037655740508974106)) * f1( 0.15017509096250276)
w2 ( 0.47739205127330914 ) += alpha ( 0.1) * (reward ( 7.136867108446683e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.12250346481964375) - present_state_Q (0.037655740508974106)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0260724829196959 ) += alpha ( 0.1 ) * (reward ( 3.5684335542233415e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12204117269155035) - present_state_Q ( 0.12204117269155035)) * f1( 0.16104705165614017)
w2 ( 0.47080396900809796 ) += alpha ( 0.1) * (reward ( 3.5684335542233415e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.12204117269155035) - present_state_Q (0.12204117269155035)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.027827058301161 ) += alpha ( 0.1 ) * (reward ( 1.7842167771116708e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12188294100152178) - present_state_Q ( 0.1209514335875421)) * f1( 0.16134724210723975)
w2 ( 0.4642792511689208 ) += alpha ( 0.1) * (reward ( 1.7842167771116708e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.12188294100152178) - present_state_Q (0.1209514335875421)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0283916359606795 ) += alpha ( 0.1 ) * (reward ( 8.921083885542128e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11189277609028114) - present_state_Q ( 0.049820103939048926)) * f1( 0.14618068578450774)
w2 ( 0.4619619368541527 ) += alpha ( 0.1) * (reward ( 8.921083885542128e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.11189277609028114) - present_state_Q (0.049820103939048926)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0289083561584755 ) += alpha ( 0.1 ) * (reward ( 4.460541942771064e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12006277101990762) - present_state_Q ( 0.05109328685469153)) * f1( 0.1322125112692861)
w2 ( 0.4603986348857224 ) += alpha ( 0.1) * (reward ( 4.460541942771064e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.12006277101990762) - present_state_Q (0.05109328685469153)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.030552724831594 ) += alpha ( 0.1 ) * (reward ( 2.230270971385532e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11928200003147849) - present_state_Q ( 0.11928200003147849)) * f1( 0.15317602498362848)
w2 ( 0.4539575407002809 ) += alpha ( 0.1) * (reward ( 2.230270971385532e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.11928200003147849) - present_state_Q (0.11928200003147849)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.032183952308748 ) += alpha ( 0.1 ) * (reward ( 1.115135485692766e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11925479621875307) - present_state_Q ( 0.11818420474980501)) * f1( 0.15351629659937138)
w2 ( 0.44758208410073425 ) += alpha ( 0.1) * (reward ( 1.115135485692766e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.11925479621875307) - present_state_Q (0.11818420474980501)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.032678533783355 ) += alpha ( 0.1 ) * (reward ( 5.57567742846383e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10941719631243937) - present_state_Q ( 0.049303420049192276)) * f1( 0.12892772161097762)
w2 ( 0.4452804155297219 ) += alpha ( 0.1) * (reward ( 5.57567742846383e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.10941719631243937) - present_state_Q (0.049303420049192276)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0331803111557658 ) += alpha ( 0.1 ) * (reward ( 2.787838714231915e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10801772829000866) - present_state_Q ( 0.05072565390914313)) * f1( 0.12568439307831503)
w2 ( 0.44368347143787107 ) += alpha ( 0.1) * (reward ( 2.787838714231915e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.10801772829000866) - present_state_Q (0.05072565390914313)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0347080908190478 ) += alpha ( 0.1 ) * (reward ( 1.3939193571159575e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11679325776719399) - present_state_Q ( 0.11679325776719399)) * f1( 0.145345308690079)
w2 ( 0.43737664388195874 ) += alpha ( 0.1) * (reward ( 1.3939193571159575e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.11679325776719399) - present_state_Q (0.11679325776719399)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0362802193054188 ) += alpha ( 0.1 ) * (reward ( 6.969596785579788e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13295408299927955) - present_state_Q ( 0.12116934634573329)) * f1( 0.14573765736040833)
w2 ( 0.4309042117809685 ) += alpha ( 0.1) * (reward ( 6.969596785579788e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.13295408299927955) - present_state_Q (0.12116934634573329)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0367205539962194 ) += alpha ( 0.1 ) * (reward ( 3.484798392789894e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1195988078333273) - present_state_Q ( 0.047909766263266165)) * f1( 0.12248581928995311)
w2 ( 0.4294662177556905 ) += alpha ( 0.1) * (reward ( 3.484798392789894e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.1195988078333273) - present_state_Q (0.047909766263266165)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.037860666016449 ) += alpha ( 0.1 ) * (reward ( 1.742399196394947e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10338824618894518) - present_state_Q ( 0.10294447698250286)) * f1( 0.12311475656671819)
w2 ( 0.4239098796593135 ) += alpha ( 0.1) * (reward ( 1.742399196394947e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.10338824618894518) - present_state_Q (0.10294447698250286)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0393045069345028 ) += alpha ( 0.1 ) * (reward ( 4.355997990987367e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12656996454880098) - present_state_Q ( 0.12970420583431824)) * f1( 0.12335543747195772)
w2 ( 0.41688704735790705 ) += alpha ( 0.1) * (reward ( 4.355997990987367e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.12656996454880098) - present_state_Q (0.12970420583431824)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0407126146817707 ) += alpha ( 0.1 ) * (reward ( 2.1779989954936836e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11408466388489275) - present_state_Q ( 0.12865810387372068)) * f1( 0.12009485092028184)
w2 ( 0.4098520692394731 ) += alpha ( 0.1) * (reward ( 2.1779989954936836e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.11408466388489275) - present_state_Q (0.12865810387372068)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0411135376655072 ) += alpha ( 0.1 ) * (reward ( 1.0889994977468418e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11293734446357254) - present_state_Q ( 0.04559577850173534)) * f1( 0.11688020568612706)
w2 ( 0.4077939466614904 ) += alpha ( 0.1) * (reward ( 1.0889994977468418e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.11293734446357254) - present_state_Q (0.04559577850173534)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0415306965054876 ) += alpha ( 0.1 ) * (reward ( 5.444997488734209e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11281057501551453) - present_state_Q ( 0.04796552182803296)) * f1( 0.11371539801143035)
w2 ( 0.40559287883457146 ) += alpha ( 0.1) * (reward ( 5.444997488734209e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.11281057501551453) - present_state_Q (0.04796552182803296)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0419513827418565 ) += alpha ( 0.1 ) * (reward ( 2.7224987443671045e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11613591012287183) - present_state_Q ( 0.0496487031032429)) * f1( 0.110604706086878)
w2 ( 0.4040714743618232 ) += alpha ( 0.1) * (reward ( 2.7224987443671045e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.11613591012287183) - present_state_Q (0.0496487031032429)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0432509496192928 ) += alpha ( 0.1 ) * (reward ( 1.3612493721835523e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11773963228552989) - present_state_Q ( 0.12914359192865613)) * f1( 0.11072428986413008)
w2 ( 0.39702929664798453 ) += alpha ( 0.1) * (reward ( 1.3612493721835523e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.11773963228552989) - present_state_Q (0.12914359192865613)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0445030380380191 ) += alpha ( 0.1 ) * (reward ( 6.806246860917761e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11584163352281317) - present_state_Q ( 0.12673186013143828)) * f1( 0.10873760001190741)
w2 ( 0.3901204348453189 ) += alpha ( 0.1) * (reward ( 6.806246860917761e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.11584163352281317) - present_state_Q (0.12673186013143828)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.044930306264657 ) += alpha ( 0.1 ) * (reward ( 3.4031234304588807e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11822988091791391) - present_state_Q ( 0.05180550712009148)) * f1( 0.10686375880897714)
w2 ( 0.38772148370566273 ) += alpha ( 0.1) * (reward ( 3.4031234304588807e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.11822988091791391) - present_state_Q (0.05180550712009148)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0459722705805417 ) += alpha ( 0.1 ) * (reward ( 0.004567586245925997 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12171123917982071) - present_state_Q ( 0.10860576732041897)) * f1( 0.11342088754508366)
w2 ( 0.38220946027627206 ) += alpha ( 0.1) * (reward ( 0.004567586245925997) + discount_factor ( 0.1) * next_state_max_Q( 0.12171123917982071) - present_state_Q (0.10860576732041897)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0463194043485609 ) += alpha ( 0.1 ) * (reward ( 0.0005709482807407496 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10502122446236548) - present_state_Q ( 0.043409688474575214)) * f1( 0.10735005458171537)
w2 ( 0.3802692632114162 ) += alpha ( 0.1) * (reward ( 0.0005709482807407496) + discount_factor ( 0.1) * next_state_max_Q( 0.10502122446236548) - present_state_Q (0.043409688474575214)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0466832207360688 ) += alpha ( 0.1 ) * (reward ( 0.0002854741403703748 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10653983491726832) - present_state_Q ( 0.04570036163458932)) * f1( 0.10466252186128479)
w2 ( 0.3788788270513165 ) += alpha ( 0.1) * (reward ( 0.0002854741403703748) + discount_factor ( 0.1) * next_state_max_Q( 0.10653983491726832) - present_state_Q (0.04570036163458932)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0478182936965885 ) += alpha ( 0.1 ) * (reward ( 0.0001427370701851874 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12361096997539571) - present_state_Q ( 0.12361096997539571)) * f1( 0.10216022141575043)
w2 ( 0.3722123988968562 ) += alpha ( 0.1) * (reward ( 0.0001427370701851874) + discount_factor ( 0.1) * next_state_max_Q( 0.12361096997539571) - present_state_Q (0.12361096997539571)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0481889512058638 ) += alpha ( 0.1 ) * (reward ( 7.13685350925937e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10743380493134852) - present_state_Q ( 0.048248313591304895)) * f1( 0.09901742289346979)
w2 ( 0.3707150563143331 ) += alpha ( 0.1) * (reward ( 7.13685350925937e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.10743380493134852) - present_state_Q (0.048248313591304895)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0492643348947737 ) += alpha ( 0.1 ) * (reward ( 3.568426754629685e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12381615101504298) - present_state_Q ( 0.12381615101504298)) * f1( 0.09653453990057376)
w2 ( 0.36403112521557357 ) += alpha ( 0.1) * (reward ( 3.568426754629685e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.12381615101504298) - present_state_Q (0.12381615101504298)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0503304519123262 ) += alpha ( 0.1 ) * (reward ( 1.7842133773148425e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1116670620431031) - present_state_Q ( 0.12450565274765042)) * f1( 0.09407929997746123)
w2 ( 0.35723185895099957 ) += alpha ( 0.1) * (reward ( 1.7842133773148425e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.1116670620431031) - present_state_Q (0.12450565274765042)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0507168900589523 ) += alpha ( 0.1 ) * (reward ( 4.460533443287106e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11160198066787269) - present_state_Q ( 0.056799944385672735)) * f1( 0.08467968151726578)
w2 ( 0.3554064475195819 ) += alpha ( 0.1) * (reward ( 4.460533443287106e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.11160198066787269) - present_state_Q (0.056799944385672735)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0516881289885027 ) += alpha ( 0.1 ) * (reward ( 2.230266721643553e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11505311497233116) - present_state_Q ( 0.12715486202439263)) * f1( 0.08398283050253066)
w2 ( 0.3484676083039556 ) += alpha ( 0.1) * (reward ( 2.230266721643553e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.11505311497233116) - present_state_Q (0.12715486202439263)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.052618599133849 ) += alpha ( 0.1 ) * (reward ( 1.1151333608217766e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1134687291222097) - present_state_Q ( 0.12491193044681179)) * f1( 0.08193358962995863)
w2 ( 0.3416537717598818 ) += alpha ( 0.1) * (reward ( 1.1151333608217766e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1134687291222097) - present_state_Q (0.12491193044681179)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0529632765302392 ) += alpha ( 0.1 ) * (reward ( 5.575666804108883e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1118541256296206) - present_state_Q ( 0.05425345341654732)) * f1( 0.0800319336270475)
w2 ( 0.3390697227626675 ) += alpha ( 0.1) * (reward ( 5.575666804108883e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.1118541256296206) - present_state_Q (0.05425345341654732)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0533044523281725 ) += alpha ( 0.1 ) * (reward ( 2.7878334020544414e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11270890686200274) - present_state_Q ( 0.054850403226051114)) * f1( 0.07828861788611297)
w2 ( 0.33645496873727687 ) += alpha ( 0.1) * (reward ( 2.7878334020544414e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.11270890686200274) - present_state_Q (0.054850403226051114)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0536412717354824 ) += alpha ( 0.1 ) * (reward ( 1.3939167010272207e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11341349195492838) - present_state_Q ( 0.0552470970448531)) * f1( 0.07671443789112631)
w2 ( 0.33382063222981545 ) += alpha ( 0.1) * (reward ( 1.3939167010272207e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.11341349195492838) - present_state_Q (0.0552470970448531)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.054458589448246 ) += alpha ( 0.1 ) * (reward ( 6.969583505136104e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1095010302870275) - present_state_Q ( 0.1194628814378103)) * f1( 0.07531999914621049)
w2 ( 0.32948012388128456 ) += alpha ( 0.1) * (reward ( 6.969583505136104e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.1095010302870275) - present_state_Q (0.1194628814378103)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0553048626261075 ) += alpha ( 0.1 ) * (reward ( 3.484791752568052e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12069588490016972) - present_state_Q ( 0.11028034018253871)) * f1( 0.08616912746908328)
w2 ( 0.3235874808706083 ) += alpha ( 0.1) * (reward ( 3.484791752568052e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.12069588490016972) - present_state_Q (0.11028034018253871)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0556664756493113 ) += alpha ( 0.1 ) * (reward ( 1.742395876284026e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11056184173542527) - present_state_Q ( 0.06348677413828098)) * f1( 0.06896987877110239)
w2 ( 0.3214902579689771 ) += alpha ( 0.1) * (reward ( 1.742395876284026e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.11056184173542527) - present_state_Q (0.06348677413828098)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0564558000345388 ) += alpha ( 0.1 ) * (reward ( 4.3559896825964014e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12351081982025934) - present_state_Q ( 0.11208396298302307)) * f1( 0.07914385023076267)
w2 ( 0.3155062853702767 ) += alpha ( 0.1) * (reward ( 4.3559896825964014e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.12351081982025934) - present_state_Q (0.11208396298302307)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0568040402106404 ) += alpha ( 0.1 ) * (reward ( 2.1779948412982007e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11289758265625843) - present_state_Q ( 0.0642744547640427)) * f1( 0.06572467399653967)
w2 ( 0.3123272037110514 ) += alpha ( 0.1) * (reward ( 2.1779948412982007e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.11289758265625843) - present_state_Q (0.0642744547640427)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0571222301074221 ) += alpha ( 0.1 ) * (reward ( 2.722493551622751e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11244549629173023) - present_state_Q ( 0.06481861488018666)) * f1( 0.059392524518667326)
w2 ( 0.3101842411119008 ) += alpha ( 0.1) * (reward ( 2.722493551622751e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.11244549629173023) - present_state_Q (0.06481861488018666)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0574269084347225 ) += alpha ( 0.1 ) * (reward ( 1.3612467758113754e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1137154201194098) - present_state_Q ( 0.06180267686882355)) * f1( 0.06041472772471738)
w2 ( 0.30715837302865534 ) += alpha ( 0.1) * (reward ( 1.3612467758113754e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.1137154201194098) - present_state_Q (0.06180267686882355)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0577243111213834 ) += alpha ( 0.1 ) * (reward ( 6.806233879056877e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11429729635521303) - present_state_Q ( 0.061917258814879694)) * f1( 0.05890616789849264)
w2 ( 0.30412912128197755 ) += alpha ( 0.1) * (reward ( 6.806233879056877e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.11429729635521303) - present_state_Q (0.061917258814879694)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0580141763631152 ) += alpha ( 0.1 ) * (reward ( 3.4031169395284386e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1147009885621235) - present_state_Q ( 0.061762300478988216)) * f1( 0.05763622044269624)
w2 ( 0.3011115891866529 ) += alpha ( 0.1) * (reward ( 3.4031169395284386e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.1147009885621235) - present_state_Q (0.061762300478988216)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0586292155826658 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11063035109792654) - present_state_Q ( 0.11968701521162953)) * f1( 0.056620943098404375)
w2 ( 0.29676662998326003 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.11063035109792654) - present_state_Q (0.11968701521162953)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0592869777933265 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12108897190739631) - present_state_Q ( 0.1114743317731656)) * f1( 0.0661962797651856)
w2 ( 0.2908047039083145 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12108897190739631) - present_state_Q (0.1114743317731656)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0595653056383212 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11125868667714926) - present_state_Q ( 0.06791950939363384)) * f1( 0.04900686792346717)
w2 ( 0.28739708546475934 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11125868667714926) - present_state_Q (0.06791950939363384)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.060060134168166 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10368584022576595) - present_state_Q ( 0.11866191643767607)) * f1( 0.045693351456582855)
w2 ( 0.28306535216815537 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10368584022576595) - present_state_Q (0.11866191643767607)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0603216544826928 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10695709168494563) - present_state_Q ( 0.06529657589096127)) * f1( 0.04789673318853914)
w2 ( 0.27978930016480735 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10695709168494563) - present_state_Q (0.06529657589096127)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0605883260836382 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11106186140199395) - present_state_Q ( 0.07008605237032751)) * f1( 0.04521400572611047)
w2 ( 0.27625050819099967 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11106186140199395) - present_state_Q (0.07008605237032751)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0608209285838168 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11432785639468432) - present_state_Q ( 0.06793418689662734)) * f1( 0.041167563105184275)
w2 ( 0.27286042411557015 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11432785639468432) - present_state_Q (0.06793418689662734)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0610448624117454 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1146565679436644) - present_state_Q ( 0.06726171757612662)) * f1( 0.040134343677865225)
w2 ( 0.26951266046866457 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1146565679436644) - present_state_Q (0.06726171757612662)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0614716418473924 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1102630126733776) - present_state_Q ( 0.11912325711553255)) * f1( 0.039481170611909054)
w2 ( 0.2651887822347368 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1102630126733776) - present_state_Q (0.11912325711553255)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.061951413796833 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1206508505338634) - present_state_Q ( 0.11101807998167426)) * f1( 0.048484833610982374)
w2 ( 0.2592516025390395 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1206508505338634) - present_state_Q (0.11101807998167426)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0621476572241937 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11065668927784314) - present_state_Q ( 0.07380121078793271)) * f1( 0.03128106039126678)
w2 ( 0.2554874700274306 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11065668927784314) - present_state_Q (0.07380121078793271)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0623272295398718 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11113719552638794) - present_state_Q ( 0.07489322072073293)) * f1( 0.028155177194766935)
w2 ( 0.25293628998070683 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11113719552638794) - present_state_Q (0.07489322072073293)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0625063889511768 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11208143871424472) - present_state_Q ( 0.0680003200214413)) * f1( 0.03154649521293866)
w2 ( 0.24952875941170582 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11208143871424472) - present_state_Q (0.0680003200214413)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0626787942087286 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11219423682496688) - present_state_Q ( 0.06642823711507889)) * f1( 0.031227850560895686)
w2 ( 0.2462162306057509 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11219423682496688) - present_state_Q (0.06642823711507889)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0630047415523525 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10803627135025248) - present_state_Q ( 0.11454456378366758)) * f1( 0.03141935615329034)
w2 ( 0.24206659313980522 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10803627135025248) - present_state_Q (0.11454456378366758)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.063368492375255 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1162959956544331) - present_state_Q ( 0.10901345513696167)) * f1( 0.037352271664321555)
w2 ( 0.23622356180551413 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1162959956544331) - present_state_Q (0.10901345513696167)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0635009316558703 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1087935500244864) - present_state_Q ( 0.07653369560640086)) * f1( 0.020172204822545132)
w2 ( 0.232284301369277 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1087935500244864) - present_state_Q (0.07653369560640086)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.063612944017058 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1090727847127903) - present_state_Q ( 0.07724288360060651)) * f1( 0.01688570730142374)
w2 ( 0.2296308771641039 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1090727847127903) - present_state_Q (0.07724288360060651)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.063741935265216 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10972644704205686) - present_state_Q ( 0.06589514425841417)) * f1( 0.023486048378145552)
w2 ( 0.2263355271908514 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10972644704205686) - present_state_Q (0.06589514425841417)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0639866381571361 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10549665431779814) - present_state_Q ( 0.11081822176100983)) * f1( 0.024404748694757422)
w2 ( 0.2223247849376822 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10549665431779814) - present_state_Q (0.11081822176100983)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0642577917555307 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11260478626481982) - present_state_Q ( 0.10643971327418057)) * f1( 0.02848873490087335)
w2 ( 0.21661403085882028 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11260478626481982) - present_state_Q (0.10643971327418057)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.064333579958255 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10607526119576882) - present_state_Q ( 0.07777051087341108)) * f1( 0.011284221956770821)
w2 ( 0.21258425177359022 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10607526119576882) - present_state_Q (0.07777051087341108)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0643904629431045 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10658496699270982) - present_state_Q ( 0.0788697279154566)) * f1( 0.008339240303307759)
w2 ( 0.2098558025249428 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10658496699270982) - present_state_Q (0.0788697279154566)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0644802377683271 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1073545548763932) - present_state_Q ( 0.06363483389987099)) * f1( 0.016970865805478175)
w2 ( 0.20668183982020888 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1073545548763932) - present_state_Q (0.06363483389987099)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0645758355764978 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06053804723413696) - present_state_Q ( 0.0652953220864677)) * f1( 0.019078982588734108)
w2 ( 0.20567971292244452 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.06053804723413696) - present_state_Q (0.0652953220864677)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0646831131266183 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06878145915217672) - present_state_Q ( 0.06303662355317453)) * f1( 0.020793893497122844)
w2 ( 0.20361607726642297 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( 0.06878145915217672) - present_state_Q (0.06303662355317453)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0646946625896305 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06452655756676075) - present_state_Q ( 0.03541907043467825)) * f1( 0.004328458875964269)
w2 ( 0.20254877240405125 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( 0.06452655756676075) - present_state_Q (0.03541907043467825)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.06470676800923 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06624807487958545) - present_state_Q ( 0.03206550390960698)) * f1( 0.004981900204117873)
w2 ( 0.2015768204095595 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( 0.06624807487958545) - present_state_Q (0.03206550390960698)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0647270842918026 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06928973355255733) - present_state_Q ( 0.03311573849916085)) * f1( 0.007931147650035678)
w2 ( 0.20106450407227494 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( 0.06928973355255733) - present_state_Q (0.03311573849916085)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0647452112259292 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07302748811590819) - present_state_Q ( 0.02506549419541073)) * f1( 0.010371718716061686)
w2 ( 0.2003654132225157 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( 0.07302748811590819) - present_state_Q (0.02506549419541073)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0648364940033699 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06696709099620321) - present_state_Q ( 0.07100306698903863)) * f1( 0.014226562696223206)
w2 ( 0.19779886838973573 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( 0.06696709099620321) - present_state_Q (0.07100306698903863)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0649097626012094 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07053142094131375) - present_state_Q ( 0.06631864441248593)) * f1( 0.012377678860493685)
w2 ( 0.19543110303839994 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07053142094131375) - present_state_Q (0.06631864441248593)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.064977351735771 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06984718265561508) - present_state_Q ( 0.06795013189006409)) * f1( 0.011092964842416224)
w2 ( 0.19299391386411904 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.06984718265561508) - present_state_Q (0.06795013189006409)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0650545776766391 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07019355392609125) - present_state_Q ( 0.07019355392609125)) * f1( 0.012227738468522016)
w2 ( 0.19046765960812936 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07019355392609125) - present_state_Q (0.07019355392609125)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0651294244920846 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10422780731656521) - present_state_Q ( 0.06613427539493932)) * f1( 0.013436868462787821)
w2 ( 0.18823955666427286 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10422780731656521) - present_state_Q (0.06613427539493932)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0651959447813548 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06990922321801443) - present_state_Q ( 0.06990922321801443)) * f1( 0.010573236172929934)
w2 ( 0.18572300304976175 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.06990922321801443) - present_state_Q (0.06990922321801443)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0653177971950276 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09709742327867138) - present_state_Q ( 0.09125558630779416)) * f1( 0.01494321972303968)
w2 ( 0.18083038622696918 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.09709742327867138) - present_state_Q (0.09125558630779416)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.065381224221107 ) += alpha ( 0.1 ) * (reward ( 0.009136287608195154 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09074841861994068) - present_state_Q ( 0.051693877682176634)) * f1( 0.018943195964010874)
w2 ( 0.17949107629848968 ) += alpha ( 0.1) * (reward ( 0.009136287608195154) + discount_factor ( 0.1) * next_state_max_Q( 0.09074841861994068) - present_state_Q (0.051693877682176634)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0654431772428286 ) += alpha ( 0.1 ) * (reward ( 0.004568143804097577 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08576298981958326) - present_state_Q ( 0.05032773942015401)) * f1( 0.016661519372881923)
w2 ( 0.17800374443312575 ) += alpha ( 0.1) * (reward ( 0.004568143804097577) + discount_factor ( 0.1) * next_state_max_Q( 0.08576298981958326) - present_state_Q (0.05032773942015401)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0655893411512265 ) += alpha ( 0.1 ) * (reward ( 0.0022840719020487885 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0831308181837971) - present_state_Q ( 0.0831308181837971)) * f1( 0.02015118214133863)
w2 ( 0.175102397854591 ) += alpha ( 0.1) * (reward ( 0.0022840719020487885) + discount_factor ( 0.1) * next_state_max_Q( 0.0831308181837971) - present_state_Q (0.0831308181837971)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0657525499917664 ) += alpha ( 0.1 ) * (reward ( 0.0011420359510243942 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09183537641628917) - present_state_Q ( 0.07744270173880577)) * f1( 0.02431701788319779)
w2 ( 0.17107537016582186 ) += alpha ( 0.1) * (reward ( 0.0011420359510243942) + discount_factor ( 0.1) * next_state_max_Q( 0.09183537641628917) - present_state_Q (0.07744270173880577)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.065915681832043 ) += alpha ( 0.1 ) * (reward ( 0.0005710179755121971 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07279948484788779) - present_state_Q ( 0.07163671572408635)) * f1( 0.025574966533971594)
w2 ( 0.16724822520999474 ) += alpha ( 0.1) * (reward ( 0.0005710179755121971) + discount_factor ( 0.1) * next_state_max_Q( 0.07279948484788779) - present_state_Q (0.07163671572408635)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.06610623886909 ) += alpha ( 0.1 ) * (reward ( 0.00028550898775609856 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07308817723510685) - present_state_Q ( 0.07308817723510685)) * f1( 0.02909540903809833)
w2 ( 0.16331859417856434 ) += alpha ( 0.1) * (reward ( 0.00028550898775609856) + discount_factor ( 0.1) * next_state_max_Q( 0.07308817723510685) - present_state_Q (0.07308817723510685)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.06616364313807 ) += alpha ( 0.1 ) * (reward ( 0.00014275449387804928 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07430499649380277) - present_state_Q ( 0.0513742520086421)) * f1( 0.013105698905870919)
w2 ( 0.16069053430664132 ) += alpha ( 0.1) * (reward ( 0.00014275449387804928) + discount_factor ( 0.1) * next_state_max_Q( 0.07430499649380277) - present_state_Q (0.0513742520086421)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0662178490561969 ) += alpha ( 0.1 ) * (reward ( 7.137724693902464e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07604107751450356) - present_state_Q ( 0.04909202520307204)) * f1( 0.013087988001631967)
w2 ( 0.15820554189436037 ) += alpha ( 0.1) * (reward ( 7.137724693902464e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07604107751450356) - present_state_Q (0.04909202520307204)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.066272111248524 ) += alpha ( 0.1 ) * (reward ( 3.568862346951232e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07744622339826482) - present_state_Q ( 0.04588071822684081)) * f1( 0.014241892993923921)
w2 ( 0.15591951745854768 ) += alpha ( 0.1) * (reward ( 3.568862346951232e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07744622339826482) - present_state_Q (0.04588071822684081)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.066327983868497 ) += alpha ( 0.1 ) * (reward ( 1.784431173475616e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0783139672806346) - present_state_Q ( 0.04208329541325209)) * f1( 0.016320772107038847)
w2 ( 0.15386547419614044 ) += alpha ( 0.1) * (reward ( 1.784431173475616e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.0783139672806346) - present_state_Q (0.04208329541325209)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0664589066708794 ) += alpha ( 0.1 ) * (reward ( 8.92215586737808e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07371111431150693) - present_state_Q ( 0.07620068644038133)) * f1( 0.019023766406508887)
w2 ( 0.15111264808200592 ) += alpha ( 0.1) * (reward ( 8.92215586737808e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.07371111431150693) - present_state_Q (0.07620068644038133)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0665742155082865 ) += alpha ( 0.1 ) * (reward ( 4.46107793368904e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08824902013454444) - present_state_Q ( 0.07490627379896976)) * f1( 0.017450700429589558)
w2 ( 0.14714803343955102 ) += alpha ( 0.1) * (reward ( 4.46107793368904e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.08824902013454444) - present_state_Q (0.07490627379896976)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0665845236114535 ) += alpha ( 0.1 ) * (reward ( 2.23053896684452e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07461679772286048) - present_state_Q ( 0.05291710900140715)) * f1( 0.002267849890446384)
w2 ( 0.14442084151814177 ) += alpha ( 0.1) * (reward ( 2.23053896684452e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.07461679772286048) - present_state_Q (0.05291710900140715)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0665873528972005 ) += alpha ( 0.1 ) * (reward ( 0.00913628774431983 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04611092527318198) - present_state_Q ( 0.01882578858602039)) * f1( 0.005571205723908762)
w2 ( 0.14421770518556648 ) += alpha ( 0.1) * (reward ( 0.00913628774431983) + discount_factor ( 0.1) * next_state_max_Q( 0.04611092527318198) - present_state_Q (0.01882578858602039)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.066596076203839 ) += alpha ( 0.1 ) * (reward ( 0.004568143872159915 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049220922518053156) - present_state_Q ( 0.018740377912535047)) * f1( 0.009430457216414792)
w2 ( 0.1440327023497951 ) += alpha ( 0.1) * (reward ( 0.004568143872159915) + discount_factor ( 0.1) * next_state_max_Q( 0.049220922518053156) - present_state_Q (0.018740377912535047)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.066597572488363 ) += alpha ( 0.1 ) * (reward ( 0.0022840719360799574 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05590123970435486) - present_state_Q ( 0.00910842900116176)) * f1( 0.012123192373515352)
w2 ( 0.14398333302600924 ) += alpha ( 0.1) * (reward ( 0.0022840719360799574) + discount_factor ( 0.1) * next_state_max_Q( 0.05590123970435486) - present_state_Q (0.00910842900116176)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0666266688058026 ) += alpha ( 0.1 ) * (reward ( 0.0005710179840199894 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04800663479320519) - present_state_Q ( 0.04305800612674421)) * f1( 0.007720656683689098)
w2 ( 0.1424758800394731 ) += alpha ( 0.1) * (reward ( 0.0005710179840199894) + discount_factor ( 0.1) * next_state_max_Q( 0.04800663479320519) - present_state_Q (0.04305800612674421)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0666833216026763 ) += alpha ( 0.1 ) * (reward ( 0.0002855089920099947 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04815310831699713) - present_state_Q ( 0.04667245785595085)) * f1( 0.01362775188935476)
w2 ( 0.14081301451818345 ) += alpha ( 0.1) * (reward ( 0.0002855089920099947) + discount_factor ( 0.1) * next_state_max_Q( 0.04815310831699713) - present_state_Q (0.04667245785595085)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0666907328157187 ) += alpha ( 0.1 ) * (reward ( 0.009277926970841406 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020058687788467644) - present_state_Q ( 0.018259736748525414)) * f1( 0.01062396176175573)
w2 ( 0.1406734956982067 ) += alpha ( 0.1) * (reward ( 0.009277926970841406) + discount_factor ( 0.1) * next_state_max_Q( 0.020058687788467644) - present_state_Q (0.018259736748525414)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0667084015915118 ) += alpha ( 0.1 ) * (reward ( 0.004638963485420703 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0208345628389362) - present_state_Q ( 0.0208345628389362)) * f1( 0.012520264077440554)
w2 ( 0.14039125283681425 ) += alpha ( 0.1) * (reward ( 0.004638963485420703) + discount_factor ( 0.1) * next_state_max_Q( 0.0208345628389362) - present_state_Q (0.0208345628389362)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0667270290369844 ) += alpha ( 0.1 ) * (reward ( 0.0023194817427103515 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018232792638527583) - present_state_Q ( 0.018232792638527583)) * f1( 0.013220300677140718)
w2 ( 0.14010945220417495 ) += alpha ( 0.1) * (reward ( 0.0023194817427103515) + discount_factor ( 0.1) * next_state_max_Q( 0.018232792638527583) - present_state_Q (0.018232792638527583)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0667475861612217 ) += alpha ( 0.1 ) * (reward ( 0.0011597408713551758 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022382945625794595) - present_state_Q ( 0.022382945625794595)) * f1( 0.010828138784687656)
w2 ( 0.13972975400033774 ) += alpha ( 0.1) * (reward ( 0.0011597408713551758) + discount_factor ( 0.1) * next_state_max_Q( 0.022382945625794595) - present_state_Q (0.022382945625794595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0667683702091466 ) += alpha ( 0.1 ) * (reward ( 0.0005798704356775879 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018050346695266357) - present_state_Q ( 0.016480924388114306)) * f1( 0.014744622228298983)
w2 ( 0.13944783361467955 ) += alpha ( 0.1) * (reward ( 0.0005798704356775879) + discount_factor ( 0.1) * next_state_max_Q( 0.018050346695266357) - present_state_Q (0.016480924388114306)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0667789198054816 ) += alpha ( 0.1 ) * (reward ( 0.00028993521783879394 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020684342427627934) - present_state_Q ( 0.013730849913656038)) * f1( 0.009276425119846143)
w2 ( 0.13922038400561845 ) += alpha ( 0.1) * (reward ( 0.00028993521783879394) + discount_factor ( 0.1) * next_state_max_Q( 0.020684342427627934) - present_state_Q (0.013730849913656038)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0668074706007953 ) += alpha ( 0.1 ) * (reward ( 0.00014496760891939697 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024062397846227865) - present_state_Q ( 0.024062397846227865)) * f1( 0.01327253151169449)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 0.00014496760891939697) + discount_factor ( 0.1) * next_state_max_Q( 0.024062397846227865) - present_state_Q (0.024062397846227865)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0667942622617277 ) += alpha ( 0.1 ) * (reward ( 0.009171414377066258 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007047775786654909) - present_state_Q ( -0.009042598853637463)) * f1( 0.007543641156046573)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 0.009171414377066258) + discount_factor ( 0.1) * next_state_max_Q( -0.007047775786654909) - present_state_Q (-0.009042598853637463)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0667823341566753 ) += alpha ( 0.1 ) * (reward ( 0.004585707188533129 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010439530905674167) - present_state_Q ( -0.004927657317059668)) * f1( 0.014083747344219442)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 0.004585707188533129) + discount_factor ( 0.1) * next_state_max_Q( -0.010439530905674167) - present_state_Q (-0.004927657317059668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.066412159541592 ) += alpha ( 0.1 ) * (reward ( 8.991039208649584e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06625129310131406) - present_state_Q ( -0.06625129310131406)) * f1( 0.06207322109519872)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 8.991039208649584e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.06625129310131406) - present_state_Q (-0.06625129310131406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.065914628112384 ) += alpha ( 0.1 ) * (reward ( 1.123879901081198e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06665537028945313) - present_state_Q ( -0.07261240897871417)) * f1( 0.07544299458216316)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 1.123879901081198e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.06665537028945313) - present_state_Q (-0.07261240897871417)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0654150942360132 ) += alpha ( 0.1 ) * (reward ( 4.3901558633448775e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08235650068274068) - present_state_Q ( -0.0801582751376666)) * f1( 0.06945434004906333)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 4.3901558633448775e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.08235650068274068) - present_state_Q (-0.0801582751376666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0634319024248222 ) += alpha ( 0.1 ) * (reward ( 0.013702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12839089509939666) - present_state_Q ( -0.15438270415411562)) * f1( 0.1277448077088305)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 0.013702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.12839089509939666) - present_state_Q (-0.15438270415411562)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0570911123523374 ) += alpha ( 0.1 ) * (reward ( 4.181750095292545e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2674395807342838) - present_state_Q ( -0.2774581813204531)) * f1( 0.2529086469190716)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 4.181750095292545e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.2674395807342838) - present_state_Q (-0.2774581813204531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0413566971975439 ) += alpha ( 0.1 ) * (reward ( 1.5952110654039555e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40958451852150113) - present_state_Q ( -0.40958451852150113)) * f1( 0.42683946083603713)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 1.5952110654039555e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.40958451852150113) - present_state_Q (-0.40958451852150113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0245946274270126 ) += alpha ( 0.1 ) * (reward ( 7.976055327019778e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41172805346842234) - present_state_Q ( -0.44083260905571814)) * f1( 0.4194084472580053)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 7.976055327019778e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.41172805346842234) - present_state_Q (-0.44083260905571814)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0072498135022028 ) += alpha ( 0.1 ) * (reward ( 1.9940138317549444e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4264332135795903) - present_state_Q ( -0.4435379672514948)) * f1( 0.43265267078192154)
w2 ( 0.13879016019656473 ) += alpha ( 0.1) * (reward ( 1.9940138317549444e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.4264332135795903) - present_state_Q (-0.4435379672514948)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9860459178130443 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4736317820356017) - present_state_Q ( -0.4736317820356017)) * f1( 0.4974303877541331)
w2 ( 0.1558409043498464 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.4736317820356017) - present_state_Q (-0.4736317820356017)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9625673379509587 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46768756933639816) - present_state_Q ( -0.4797226320841104)) * f1( 0.5422882484635514)
w2 ( 0.17315905935586523 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.46768756933639816) - present_state_Q (-0.4797226320841104)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9412923220760295 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44008215111164795) - present_state_Q ( -0.44008215111164795)) * f1( 0.5371475863764791)
w2 ( 0.18900201679588455 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.44008215111164795) - present_state_Q (-0.44008215111164795)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.921133128668105 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4337754286737095) - present_state_Q ( -0.4264458708249494)) * f1( 0.5262067940985647)
w2 ( 0.2043261772848869 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.4337754286737095) - present_state_Q (-0.4264458708249494)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.901483010345589 ) += alpha ( 0.1 ) * (reward ( 1.3939166984308485e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4204197989879371) - present_state_Q ( -0.41299572037766386)) * f1( 0.529718635895487)
w2 ( 0.21916433247970848 ) += alpha ( 0.1) * (reward ( 1.3939166984308485e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.4204197989879371) - present_state_Q (-0.41299572037766386)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8807351416911663 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4001678669539694) - present_state_Q ( -0.41034320749722847)) * f1( 0.5602454399769013)
w2 ( 0.23397774615445655 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.4001678669539694) - present_state_Q (-0.41034320749722847)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8611031356393294 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39716481218737654) - present_state_Q ( -0.39716481218737654)) * f1( 0.549226401413098)
w2 ( 0.24827567939388273 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( -0.39716481218737654) - present_state_Q (-0.39716481218737654)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8433511272528897 ) += alpha ( 0.1 ) * (reward ( 3.323356386258241e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3770678577187759) - present_state_Q ( -0.3689368249638369)) * f1( 0.5359419824887909)
w2 ( 0.26152488096156246 ) += alpha ( 0.1) * (reward ( 3.323356386258241e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.3770678577187759) - present_state_Q (-0.3689368249638369)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8245397477108949 ) += alpha ( 0.1 ) * (reward ( 2.0770977414114005e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3585807523641593) - present_state_Q ( -0.3673189654770815)) * f1( 0.5675293856942286)
w2 ( 0.27478331657118915 ) += alpha ( 0.1) * (reward ( 2.0770977414114005e-15) + discount_factor ( 0.1) * next_state_max_Q( -0.3585807523641593) - present_state_Q (-0.3673189654770815)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8070023202708899 ) += alpha ( 0.1 ) * (reward ( 2.5963721767642507e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3428767371099015) - present_state_Q ( -0.3467812094082737)) * f1( 0.5612092871256603)
w2 ( 0.2872830579990805 ) += alpha ( 0.1) * (reward ( 2.5963721767642507e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.3428767371099015) - present_state_Q (-0.3467812094082737)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7895680243581285 ) += alpha ( 0.1 ) * (reward ( 5.575666793723394e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3348895269696381) - present_state_Q ( -0.3392844971107802)) * f1( 0.5701281278553844)
w2 ( 0.29951490207830034 ) += alpha ( 0.1) * (reward ( 5.575666793723394e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.3348895269696381) - present_state_Q (-0.3392844971107802)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7719803372875622 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32657037097339126) - present_state_Q ( -0.335558039084612)) * f1( 0.5806414291860705)
w2 ( 0.31163094215847187 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( -0.32657037097339126) - present_state_Q (-0.335558039084612)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.756388495853954 ) += alpha ( 0.1 ) * (reward ( 8.507792348821097e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31793758655513443) - present_state_Q ( -0.31155039708226595)) * f1( 0.5573358873918723)
w2 ( 0.3228212076958823 ) += alpha ( 0.1) * (reward ( 8.507792348821097e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.31793758655513443) - present_state_Q (-0.31155039708226595)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7404718875835908 ) += alpha ( 0.1 ) * (reward ( 6.646712772516482e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3048334908825626) - present_state_Q ( -0.3088606071885772)) * f1( 0.5717639572634692)
w2 ( 0.3339562980198978 ) += alpha ( 0.1) * (reward ( 6.646712772516482e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.3048334908825626) - present_state_Q (-0.3088606071885772)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7261295806158019 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28881480244994484) - present_state_Q ( -0.2839115377845375)) * f1( 0.5623771215894847)
w2 ( 0.3441575003214795 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.28881480244994484) - present_state_Q (-0.2839115377845375)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.711216115750617 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28225823885266066) - present_state_Q ( -0.28663438784943374)) * f1( 0.5771273457969774)
w2 ( 0.3544938428800462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.28225823885266066) - present_state_Q (-0.28663438784943374)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6983637872818191 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2606138677135968) - present_state_Q ( -0.25550114716885947)) * f1( 0.560161344595703)
w2 ( 0.3636714332959462 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.2606138677135968) - present_state_Q (-0.25550114716885947)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6853681708579196 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2552029457922924) - present_state_Q ( -0.2552029457922924)) * f1( 0.5658074893886657)
w2 ( 0.3728587393444687 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.2552029457922924) - present_state_Q (-0.2552029457922924)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6731704938063564 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24679102519757765) - present_state_Q ( -0.241502706829498)) * f1( 0.5623771215894847)
w2 ( 0.38153453825825673 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.24679102519757765) - present_state_Q (-0.241502706829498)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6610154001183993 ) += alpha ( 0.1 ) * (reward ( 2.1779948412982007e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23064186846461754) - present_state_Q ( -0.23367790409241157)) * f1( 0.5771273457969774)
w2 ( 0.38995908703521454 ) += alpha ( 0.1) * (reward ( 2.1779948412982007e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.23064186846461754) - present_state_Q (-0.23367790409241157)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6500938900340207 ) += alpha ( 0.1 ) * (reward ( 5.317370218013185e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2185004521251956) - present_state_Q ( -0.21429164051470334)) * f1( 0.5675233603836422)
w2 ( 0.39765675084732316 ) += alpha ( 0.1) * (reward ( 5.317370218013185e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.2185004521251956) - present_state_Q (-0.21429164051470334)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.63878180734548 ) += alpha ( 0.1 ) * (reward ( 6.646712772516482e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21907524220412378) - present_state_Q ( -0.21907524220412378)) * f1( 0.5737289452967914)
w2 ( 0.40554345956667426 ) += alpha ( 0.1) * (reward ( 6.646712772516482e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.21907524220412378) - present_state_Q (-0.21907524220412378)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6270009209513996 ) += alpha ( 0.1 ) * (reward ( 1.0385488707057003e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20680437648788985) - present_state_Q ( -0.21422223099475346)) * f1( 0.6086998673729054)
w2 ( 0.41328513130051286 ) += alpha ( 0.1) * (reward ( 1.0385488707057003e-15) + discount_factor ( 0.1) * next_state_max_Q( -0.20680437648788985) - present_state_Q (-0.21422223099475346)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6154665181424999 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2071228822764954) - present_state_Q ( -0.21390254382678642)) * f1( 0.5970488921984372)
w2 ( 0.4210127415244783 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.2071228822764954) - present_state_Q (-0.21390254382678642)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6053227212161532 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18890820683045825) - present_state_Q ( -0.19366404769418197)) * f1( 0.5803976444115435)
w2 ( 0.4280036706049238 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.18890820683045825) - present_state_Q (-0.19366404769418197)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5962138745452966 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17788047206222207) - present_state_Q ( -0.17788047206222207)) * f1( 0.5689742459113709)
w2 ( 0.43440736759916376 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.17788047206222207) - present_state_Q (-0.17788047206222207)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5876455950270473 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1602343178494742) - present_state_Q ( -0.164827817345888)) * f1( 0.5758082657275131)
w2 ( 0.4403595430216014 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.1602343178494742) - present_state_Q (-0.164827817345888)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5797882295985424 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15477661752516647) - present_state_Q ( -0.15477661752516647)) * f1( 0.5640649195769225)
w2 ( 0.4459315012525074 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.15477661752516647) - present_state_Q (-0.15477661752516647)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5719121516971319 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14765901695003777) - present_state_Q ( -0.15262602470365785)) * f1( 0.571309362673063)
w2 ( 0.4514459061728536 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.14765901695003777) - present_state_Q (-0.15262602470365785)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5643609250331928 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14201721938296602) - present_state_Q ( -0.14647152626378407)) * f1( 0.570895731073829)
w2 ( 0.45673669834587305 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.14201721938296602) - present_state_Q (-0.14647152626378407)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5576834092325758 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1324437847698741) - present_state_Q ( -0.1324437847698741)) * f1( 0.5601970687848605)
w2 ( 0.46150467459758854 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.1324437847698741) - present_state_Q (-0.1324437847698741)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5501985233266117 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13056311603134324) - present_state_Q ( -0.14132729604498973)) * f1( 0.583521358203729)
w2 ( 0.46663551397526276 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.13056311603134324) - present_state_Q (-0.14132729604498973)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5434070773187374 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12317868700693865) - present_state_Q ( -0.1298711030449912)) * f1( 0.5777336579258266)
w2 ( 0.47133764334903466 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.12317868700693865) - present_state_Q (-0.1298711030449912)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5421660474316515 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019515980535038613) - present_state_Q ( -0.02365348811662188)) * f1( 0.571853365525483)
w2 ( 0.47263975675282177 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.019515980535038613) - present_state_Q (-0.02365348811662188)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5410173264042768 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017499674244360885) - present_state_Q ( -0.02185677999267599)) * f1( 0.571309362673063)
w2 ( 0.47384616550691616 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.017499674244360885) - present_state_Q (-0.02185677999267599)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5394330192893949 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018244434410350785) - present_state_Q ( -0.028830155873163177)) * f1( 0.5866562931319022)
w2 ( 0.47546650825284387 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.018244434410350785) - present_state_Q (-0.028830155873163177)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5378259425050259 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018490617219570238) - present_state_Q ( -0.02906029934780585)) * f1( 0.5905930507337148)
w2 ( 0.4770991825103948 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.018490617219570238) - present_state_Q (-0.02906029934780585)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.536884372336677 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018707731782657666) - present_state_Q ( -0.018707731782657666)) * f1( 0.5592281780055552)
w2 ( 0.4781094000266583 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.018707731782657666) - present_state_Q (-0.018707731782657666)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5359409031204256 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01389541164341107) - present_state_Q ( -0.018031810005226745)) * f1( 0.5669114141057016)
w2 ( 0.47910793615711145 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.01389541164341107) - present_state_Q (-0.018031810005226745)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5354499592515215 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009700609162099139) - present_state_Q ( -0.009700609162099139)) * f1( 0.5623287966311284)
w2 ( 0.4796317690518648 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.009700609162099139) - present_state_Q (-0.009700609162099139)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5338724341075517 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017204703213388073) - present_state_Q ( -0.029366578761290674)) * f1( 0.5706138161890615)
w2 ( 0.4812905355582619 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.017204703213388073) - present_state_Q (-0.029366578761290674)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5329029507649455 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0114375220912738) - present_state_Q ( -0.017801053162027236)) * f1( 0.5820170658784817)
w2 ( 0.4822899736154359 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0114375220912738) - present_state_Q (-0.017801053162027236)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5322803028926902 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012389549680446865) - present_state_Q ( -0.012389549680446865)) * f1( 0.5583988014224913)
w2 ( 0.48295900929818003 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.012389549680446865) - present_state_Q (-0.012389549680446865)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5313900688026818 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005360257117662082) - present_state_Q ( -0.015831039523949708)) * f1( 0.5820420307821756)
w2 ( 0.483876710126911 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.005360257117662082) - present_state_Q (-0.015831039523949708)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5308583203639847 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010580817814208221) - present_state_Q ( -0.010580817814208221)) * f1( 0.5583988014224913)
w2 ( 0.48444807428887826 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.010580817814208221) - present_state_Q (-0.010580817814208221)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5299480033613058 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00576180524307901) - present_state_Q ( -0.01621623664629601)) * f1( 0.5820420307821756)
w2 ( 0.4853864776561975 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00576180524307901) - present_state_Q (-0.01621623664629601)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5295028181669241 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008858368154077534) - present_state_Q ( -0.008858368154077534)) * f1( 0.5583988014224913)
w2 ( 0.4858648295365177 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.008858368154077534) - present_state_Q (-0.008858368154077534)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5281520359532781 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010405031518848806) - present_state_Q ( -0.024248143531923894)) * f1( 0.5820420307821756)
w2 ( 0.48725728795932005 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.010405031518848806) - present_state_Q (-0.024248143531923894)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5288345859623871 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006718891034271757) - present_state_Q ( -0.006718891034271757)) * f1( 0.5583988014224913)
w2 ( 0.48652388737819036 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006718891034271757) - present_state_Q (-0.006718891034271757)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5294573937646769 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003023153702574888) - present_state_Q ( -0.0077338344740005005)) * f1( 0.5746081827891676)
w2 ( 0.4858735578274346 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003023153702574888) - present_state_Q (-0.0077338344740005005)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5296074232122286 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005854302816762524) - present_state_Q ( -0.0163443726102937)) * f1( 0.5973930515717583)
w2 ( 0.48572287367017125 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005854302816762524) - present_state_Q (-0.0163443726102937)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.530235914762281 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008124849449264337) - present_state_Q ( -0.008124849449264337)) * f1( 0.5735468804563838)
w2 ( 0.4850653948434512 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.008124849449264337) - present_state_Q (-0.008124849449264337)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5299798048221961 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013393431550254298) - present_state_Q ( -0.02389388208164822)) * f1( 0.597801923682403)
w2 ( 0.4853224464820682 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013393431550254298) - present_state_Q (-0.02389388208164822)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5300189450271054 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01277479587141278) - present_state_Q ( -0.018880889491702746)) * f1( 0.5868668200340269)
w2 ( 0.4852824303793615 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01277479587141278) - present_state_Q (-0.018880889491702746)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5305111929942585 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010720887255339207) - present_state_Q ( -0.010720887255339207)) * f1( 0.5709508981111542)
w2 ( 0.48476513759416945 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.010720887255339207) - present_state_Q (-0.010720887255339207)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5308268026946771 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014116000111321025) - present_state_Q ( -0.014116000111321025)) * f1( 0.5670370601099161)
w2 ( 0.4844311809032004 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014116000111321025) - present_state_Q (-0.014116000111321025)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5311082065942335 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009929178264400262) - present_state_Q ( -0.014503563154773969)) * f1( 0.5912219718548807)
w2 ( 0.4841455989259201 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009929178264400262) - present_state_Q (-0.014503563154773969)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5322334387927544 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00817007933949998) - present_state_Q ( -0.00817007933949998)) * f1( 0.5611446092197311)
w2 ( 0.48294245216478254 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.00817007933949998) - present_state_Q (-0.00817007933949998)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5332382407761684 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010694653519746522) - present_state_Q ( -0.010694653519746522)) * f1( 0.565120009255862)
w2 ( 0.4818756324093783 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.010694653519746522) - present_state_Q (-0.010694653519746522)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5338553163696077 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01202302192872673) - present_state_Q ( -0.017917607195288565)) * f1( 0.5772341737231722)
w2 ( 0.4812342196640527 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.01202302192872673) - present_state_Q (-0.017917607195288565)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.534048753581826 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01082950108566777) - present_state_Q ( -0.025216647109836066)) * f1( 0.5912219718548807)
w2 ( 0.48103791043865834 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.01082950108566777) - present_state_Q (-0.025216647109836066)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5349963077229342 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011688246680827186) - present_state_Q ( -0.011688246680827186)) * f1( 0.5611446092197311)
w2 ( 0.48002474471395246 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.011688246680827186) - present_state_Q (-0.011688246680827186)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5349085291595905 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014322271543865062) - present_state_Q ( -0.030294773312089074)) * f1( 0.6024490893276354)
w2 ( 0.4801121664379441 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.014322271543865062) - present_state_Q (-0.030294773312089074)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5353856116558189 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014489705583107804) - present_state_Q ( -0.020700182984147164)) * f1( 0.5850682508269747)
w2 ( 0.4796229081380237 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.014489705583107804) - present_state_Q (-0.020700182984147164)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.53507060934058 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02566694697860128) - present_state_Q ( -0.03517815974510108)) * f1( 0.6050816067825245)
w2 ( 0.47993526499538763 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.02566694697860128) - present_state_Q (-0.03517815974510108)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5351848492427562 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02135901383744998) - present_state_Q ( -0.02762986593539557)) * f1( 0.597628785466751)
w2 ( 0.47982057182301613 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.02135901383744998) - present_state_Q (-0.02762986593539557)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5349834734792128 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0211833426274613) - present_state_Q ( -0.03284398869765481)) * f1( 0.6065284743147831)
w2 ( 0.4800197800436401 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0211833426274613) - present_state_Q (-0.03284398869765481)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5350745545881851 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02163362623920123) - present_state_Q ( -0.028024640173813176)) * f1( 0.589811922806902)
w2 ( 0.47992712565116313 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.02163362623920123) - present_state_Q (-0.028024640173813176)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5349392721695645 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025948579353210377) - present_state_Q ( -0.03229042275424365)) * f1( 0.5907406936230847)
w2 ( 0.48006452849482795 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.025948579353210377) - present_state_Q (-0.03229042275424365)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5350619425458397 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012139571740980792) - present_state_Q ( -0.02655592985670191)) * f1( 0.5944643398520715)
w2 ( 0.4799407158103136 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.012139571740980792) - present_state_Q (-0.02655592985670191)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5358892754677413 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014409780437776243) - present_state_Q ( -0.014409780437776243)) * f1( 0.5730756062948893)
w2 ( 0.47907451290848296 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.014409780437776243) - present_state_Q (-0.014409780437776243)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.536769413333123 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013266670322300722) - present_state_Q ( -0.013266670322300722)) * f1( 0.5690970618440793)
w2 ( 0.47814658206041666 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.013266670322300722) - present_state_Q (-0.013266670322300722)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5326760484133396 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11208050300691141) - present_state_Q ( -0.11208050300691141)) * f1( 0.55717104627916)
w2 ( 0.4810852594716851 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.11208050300691141) - present_state_Q (-0.11208050300691141)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5283055521047908 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1088601573701303) - present_state_Q ( -0.11504471487932447)) * f1( 0.5694221673594969)
w2 ( 0.4841553867403972 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.1088601573701303) - present_state_Q (-0.11504471487932447)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.523775871269448 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10306933203856267) - present_state_Q ( -0.11353658054459159)) * f1( 0.5973930515717585)
w2 ( 0.48718835193704624 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.10306933203856267) - present_state_Q (-0.11353658054459159)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5194100468936631 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10584832151252843) - present_state_Q ( -0.11201096880990835)) * f1( 0.589811922806902)
w2 ( 0.4901491767064121 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.10584832151252843) - present_state_Q (-0.11201096880990835)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5154927995340922 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10625880782008795) - present_state_Q ( -0.10625880782008795)) * f1( 0.574145696246944)
w2 ( 0.4928782730909549 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.10625880782008795) - present_state_Q (-0.10625880782008795)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5107851902868842 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10031664700115006) - present_state_Q ( -0.11535700721303066)) * f1( 0.6041606538344898)
w2 ( 0.4959950660944911 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.10031664700115006) - present_state_Q (-0.11535700721303066)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5063094857918011 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09890042301024166) - present_state_Q ( -0.11084942741043857)) * f1( 0.6084934261045739)
w2 ( 0.49893722080188735 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.09890042301024166) - present_state_Q (-0.11084942741043857)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5021970202962278 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09040543833478112) - present_state_Q ( -0.1042294536761256)) * f1( 0.6067069452948899)
w2 ( 0.5016485564986128 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.09040543833478112) - present_state_Q (-0.1042294536761256)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49814442041321455 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09895992029091771) - present_state_Q ( -0.10399992893936016)) * f1( 0.6076005869792076)
w2 ( 0.5043164932780432 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.09895992029091771) - present_state_Q (-0.10399992893936016)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49544410169412123 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08357732411769425) - present_state_Q ( -0.08357732411769425)) * f1( 0.5647539473838366)
w2 ( 0.5062290562492998 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.08357732411769425) - present_state_Q (-0.08357732411769425)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49314654222862847 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07533627029704856) - present_state_Q ( -0.07533627029704856)) * f1( 0.5687432998157954)
w2 ( 0.5078449412830132 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07533627029704856) - present_state_Q (-0.07533627029704856)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.48975629512111357 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07536847984910247) - present_state_Q ( -0.09108169616434306)) * f1( 0.6038987394265547)
w2 ( 0.5100905145132101 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07536847984910247) - present_state_Q (-0.09108169616434306)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.48753269515526765 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07450940558745667) - present_state_Q ( -0.07450940558745667)) * f1( 0.5607653655513588)
w2 ( 0.5116766324173782 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07450940558745667) - present_state_Q (-0.07450940558745667)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4852909323641843 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07455557953249861) - present_state_Q ( -0.07455557953249861)) * f1( 0.5647539473838367)
w2 ( 0.5132644125835677 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.07455557953249861) - present_state_Q (-0.07455557953249861)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4831580095120677 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06477494433496325) - present_state_Q ( -0.06964543819030686)) * f1( 0.5964144692805926)
w2 ( 0.5146949096368597 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.06477494433496325) - present_state_Q (-0.06964543819030686)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4814264774128558 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059281654688990326) - present_state_Q ( -0.06469453213429183)) * f1( 0.5521317633690225)
w2 ( 0.515949343606495 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.059281654688990326) - present_state_Q (-0.06469453213429183)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4801962870335129 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05591137131809015) - present_state_Q ( -0.05591137131809015)) * f1( 0.5368560266890225)
w2 ( 0.5168659322769659 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.05591137131809015) - present_state_Q (-0.05591137131809015)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4798730457305959 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03740504340486245) - present_state_Q ( -0.03740504340486245)) * f1( 0.5164406220584297)
w2 ( 0.5171162931425606 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03740504340486245) - present_state_Q (-0.03740504340486245)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47890983268762644 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04481830935711112) - present_state_Q ( -0.04987390110130713)) * f1( 0.5355184269241416)
w2 ( 0.517835755252204 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.04481830935711112) - present_state_Q (-0.04987390110130713)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4779684774631548 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.044089571486648216) - present_state_Q ( -0.04940693634857729)) * f1( 0.535090106484004)
w2 ( 0.5185394537232202 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.044089571486648216) - present_state_Q (-0.04940693634857729)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47724513536874685 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03949836622352704) - present_state_Q ( -0.04485866616551265)) * f1( 0.5356775345575133)
w2 ( 0.5190795862079662 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03949836622352704) - present_state_Q (-0.04485866616551265)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47693807647772013 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03700432463224168) - present_state_Q ( -0.03700432463224168)) * f1( 0.5205822015846067)
w2 ( 0.5193155211977466 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03700432463224168) - present_state_Q (-0.03700432463224168)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47620592516478005 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04073500031576899) - present_state_Q ( -0.045267117079863056)) * f1( 0.5310023374631918)
w2 ( 0.5198670451826977 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.04073500031576899) - present_state_Q (-0.045267117079863056)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4754659685578196 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039957510856836576) - present_state_Q ( -0.04527018396625837)) * f1( 0.5335360283250024)
w2 ( 0.5204218018009403 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.039957510856836576) - present_state_Q (-0.04527018396625837)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4749747722737246 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03575295949253332) - present_state_Q ( -0.040240749792704644)) * f1( 0.530453193058869)
w2 ( 0.520792199257698 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03575295949253332) - present_state_Q (-0.040240749792704644)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47412773697004307 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03723624780821172) - present_state_Q ( -0.046676189317315875)) * f1( 0.5448206965479295)
w2 ( 0.5214140811421775 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03723624780821172) - present_state_Q (-0.046676189317315875)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4730453790106219 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04219983543191763) - present_state_Q ( -0.051638125735904095)) * f1( 0.5408375822550187)
w2 ( 0.5222145861329056 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.04219983543191763) - present_state_Q (-0.051638125735904095)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4725622230494243 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034109061034163946) - present_state_Q ( -0.03987215614104239)) * f1( 0.5335360283250024)
w2 ( 0.5225768154374303 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.034109061034163946) - present_state_Q (-0.03987215614104239)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47201255836699496 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03715087886015178) - present_state_Q ( -0.04164141396059243)) * f1( 0.5224547852847489)
w2 ( 0.522997647783433 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.03715087886015178) - present_state_Q (-0.04164141396059243)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47215455213364366 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02735128858363778) - present_state_Q ( -0.02735128858363778)) * f1( 0.509055424077082)
w2 ( 0.5228860734754637 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.02735128858363778) - present_state_Q (-0.02735128858363778)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47123369930844394 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034756297625889554) - present_state_Q ( -0.039500367383055845)) * f1( 0.5186619684910586)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.034756297625889554) - present_state_Q (-0.039500367383055845)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4712638764688184 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033321254430462325) - present_state_Q ( -0.0033321254430462325)) * f1( 0.007071067811865476)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0033321254430462325) - present_state_Q (-0.0033321254430462325)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.47129406332130785 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003345017760785387) - present_state_Q ( -0.003319707954325769)) * f1( 0.007071067811865476)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.003345017760785387) - present_state_Q (-0.003319707954325769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4712710797015039 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055092307920038594) - present_state_Q ( -0.05327671733767962)) * f1( 0.10988408011912552)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.055092307920038594) - present_state_Q (-0.05327671733767962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.47092529133482286 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055776752607153986) - present_state_Q ( -0.06058614754722035)) * f1( 0.12527222842982988)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.055776752607153986) - present_state_Q (-0.06058614754722035)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.47057340214978893 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05470472596877555) - present_state_Q ( -0.05470472596877555)) * f1( 0.11364495083616755)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.05470472596877555) - present_state_Q (-0.05470472596877555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.47017084530058373 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053704487029582124) - present_state_Q ( -0.051976908255973134)) * f1( 0.10743075047443684)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.053704487029582124) - present_state_Q (-0.051976908255973134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46947693398344414 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05352497003189981) - present_state_Q ( -0.0598538040576866)) * f1( 0.12470729075111477)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( -0.05352497003189981) - present_state_Q (-0.0598538040576866)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46876983842824205 ) += alpha ( 0.1 ) * (reward ( 0.01885021429622005 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05204084748002384) - present_state_Q ( -0.05204084748002384)) * f1( 0.10764623174820004)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 0.01885021429622005) + discount_factor ( 0.1) * next_state_max_Q( -0.05204084748002384) - present_state_Q (-0.05204084748002384)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4680284533772848 ) += alpha ( 0.1 ) * (reward ( 0.0028728623154659783 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05422650814613523) - present_state_Q ( -0.060459690236764396)) * f1( 0.12802388343125626)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 0.0028728623154659783) + discount_factor ( 0.1) * next_state_max_Q( -0.05422650814613523) - present_state_Q (-0.060459690236764396)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46732759445109345 ) += alpha ( 0.1 ) * (reward ( 0.0014364311577329892 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05534331418039443) - present_state_Q ( -0.060106864407569925)) * f1( 0.12513334907401785)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 0.0014364311577329892) + discount_factor ( 0.1) * next_state_max_Q( -0.05534331418039443) - present_state_Q (-0.060106864407569925)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4665559742690016 ) += alpha ( 0.1 ) * (reward ( 1.1222118419788978e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05646008431643137) - present_state_Q ( -0.062437122372845225)) * f1( 0.13584303668108919)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 1.1222118419788978e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.05646008431643137) - present_state_Q (-0.062437122372845225)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46592905324386497 ) += alpha ( 0.1 ) * (reward ( 5.611059209894489e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.057650524859390886) - present_state_Q ( -0.057650524859390886)) * f1( 0.12081478814181174)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 5.611059209894489e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.057650524859390886) - present_state_Q (-0.057650524859390886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46542606504753237 ) += alpha ( 0.1 ) * (reward ( 7.137127473466426e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05338985096656268) - present_state_Q ( -0.051354111302308623)) * f1( 0.10914003533174971)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 7.137127473466426e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.05338985096656268) - present_state_Q (-0.051354111302308623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4647591935167965 ) += alpha ( 0.1 ) * (reward ( 3.568563736733213e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05450002738635805) - present_state_Q ( -0.059211962240468555)) * f1( 0.1239592418981086)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 3.568563736733213e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.05450002738635805) - present_state_Q (-0.059211962240468555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46402360256394 ) += alpha ( 0.1 ) * (reward ( 2.787940419322823e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05579105471777508) - present_state_Q ( -0.06198698760770279)) * f1( 0.1304050585463623)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 2.787940419322823e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.05579105471777508) - present_state_Q (-0.06198698760770279)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4629873891640715 ) += alpha ( 0.1 ) * (reward ( 2.1780784444822925e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06526236372651024) - present_state_Q ( -0.07104214615012973)) * f1( 0.16061361739446575)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 2.1780784444822925e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.06526236372651024) - present_state_Q (-0.07104214615012973)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45993252983412264 ) += alpha ( 0.1 ) * (reward ( 0.013702758712254612 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01422410216069378) - present_state_Q ( -0.11392223466839628)) * f1( 0.2420599684528662)
w2 ( 0.5235962491822954 ) += alpha ( 0.1) * (reward ( 0.013702758712254612) + discount_factor ( 0.1) * next_state_max_Q( -0.01422410216069378) - present_state_Q (-0.11392223466839628)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4594060180674862 ) += alpha ( 0.1 ) * (reward ( 0.006851379356127306 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005378807357751425) - present_state_Q ( -0.014180989226685578)) * f1( 0.25690408590157565)
w2 ( 0.5240061389392362 ) += alpha ( 0.1) * (reward ( 0.006851379356127306) + discount_factor ( 0.1) * next_state_max_Q( -0.005378807357751425) - present_state_Q (-0.014180989226685578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.46125622073939004 ) += alpha ( 0.1 ) * (reward ( 5.352640121974458e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0782789601936699) - present_state_Q ( 0.06922483362893439)) * f1( 0.3016139199725631)
w2 ( 0.5215524024909023 ) += alpha ( 0.1) * (reward ( 5.352640121974458e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.0782789601936699) - present_state_Q (0.06922483362893439)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.46303865251106924 ) += alpha ( 0.1 ) * (reward ( 2.676320060987229e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07495440795303196) - present_state_Q ( 0.06585485498031832)) * f1( 0.30556330658720016)
w2 ( 0.5192190964515261 ) += alpha ( 0.1) * (reward ( 2.676320060987229e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07495440795303196) - present_state_Q (0.06585485498031832)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4647847289260476 ) += alpha ( 0.1 ) * (reward ( 1.3381600304936144e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07271161104336604) - present_state_Q ( 0.0636976950115759)) * f1( 0.30951583869631)
w2 ( 0.5169625703592488 ) += alpha ( 0.1) * (reward ( 1.3381600304936144e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07271161104336604) - present_state_Q (0.0636976950115759)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.46597387432924653 ) += alpha ( 0.1 ) * (reward ( 3.345400076234036e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06809271431164271) - present_state_Q ( 0.04660906712886376)) * f1( 0.29880690220000294)
w2 ( 0.5153707123473438 ) += alpha ( 0.1) * (reward ( 3.345400076234036e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.06809271431164271) - present_state_Q (0.04660906712886376)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4678656075949659 ) += alpha ( 0.1 ) * (reward ( 8.36350019058509e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06508658519985552) - present_state_Q ( 0.07074825617204672)) * f1( 0.2944847047757816)
w2 ( 0.512801161895262 ) += alpha ( 0.1) * (reward ( 8.36350019058509e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.06508658519985552) - present_state_Q (0.07074825617204672)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.46974890852794093 ) += alpha ( 0.1 ) * (reward ( 4.181750095292545e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06183659462560176) - present_state_Q ( 0.07099713862865065)) * f1( 0.29057429230725546)
w2 ( 0.5102086394556188 ) += alpha ( 0.1) * (reward ( 4.181750095292545e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.06183659462560176) - present_state_Q (0.07099713862865065)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4687353495442414 ) += alpha ( 0.1 ) * (reward ( 2.0908750476462726e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06390982487496033) - present_state_Q ( -0.02896502895924194)) * f1( 0.2866706249662308)
w2 ( 0.5116228882769885 ) += alpha ( 0.1) * (reward ( 2.0908750476462726e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.06390982487496033) - present_state_Q (-0.02896502895924194)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.46939827756775127 ) += alpha ( 0.1 ) * (reward ( 0.009135224746712599 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06955811510737811) - present_state_Q ( 0.03923295361632906)) * f1( 0.2864620131639825)
w2 ( 0.5106972115826334 ) += alpha ( 0.1) * (reward ( 0.009135224746712599) + discount_factor ( 0.1) * next_state_max_Q( 0.06955811510737811) - present_state_Q (0.03923295361632906)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47116410023228433 ) += alpha ( 0.1 ) * (reward ( 0.0022838061866781497 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0704099738771555) - present_state_Q ( 0.0704099738771555)) * f1( 0.2890755081439478)
w2 ( 0.5082538047705228 ) += alpha ( 0.1) * (reward ( 0.0022838061866781497) + discount_factor ( 0.1) * next_state_max_Q( 0.0704099738771555) - present_state_Q (0.0704099738771555)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4702198455612137 ) += alpha ( 0.1 ) * (reward ( 0.0011419030933390749 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06542670441274936) - present_state_Q ( -0.026860861915463108)) * f1( 0.2733370295578462)
w2 ( 0.5089447134795244 ) += alpha ( 0.1) * (reward ( 0.0011419030933390749) + discount_factor ( 0.1) * next_state_max_Q( 0.06542670441274936) - present_state_Q (-0.026860861915463108)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4720064211468371 ) += alpha ( 0.1 ) * (reward ( 0.0005709515466695374 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07025593008846642) - present_state_Q ( 0.07093815029157705)) * f1( 0.282054040920263)
w2 ( 0.506411049250082 ) += alpha ( 0.1) * (reward ( 0.0005709515466695374) + discount_factor ( 0.1) * next_state_max_Q( 0.07025593008846642) - present_state_Q (0.07093815029157705)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4738106927490139 ) += alpha ( 0.1 ) * (reward ( 0.0002854757733347687 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07059529909701281) - present_state_Q ( 0.07130806422540484)) * f1( 0.2820802574632413)
w2 ( 0.5038525269083872 ) += alpha ( 0.1) * (reward ( 0.0002854757733347687) + discount_factor ( 0.1) * next_state_max_Q( 0.07059529909701281) - present_state_Q (0.07130806422540484)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4729159403208201 ) += alpha ( 0.1 ) * (reward ( 0.00014273788666738436 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06936511106281268) - present_state_Q ( -0.02626271073957137)) * f1( 0.2683562800062621)
w2 ( 0.5045193661030376 ) += alpha ( 0.1) * (reward ( 0.00014273788666738436) + discount_factor ( 0.1) * next_state_max_Q( 0.06936511106281268) - present_state_Q (-0.02626271073957137)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4752062839090031 ) += alpha ( 0.1 ) * (reward ( 7.136894333369218e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06921243891775286) - present_state_Q ( 0.07031138251759453)) * f1( 0.3617163756762825)
w2 ( 0.5019866153157382 ) += alpha ( 0.1) * (reward ( 7.136894333369218e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.06921243891775286) - present_state_Q (0.07031138251759453)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47440324469053585 ) += alpha ( 0.1 ) * (reward ( 1.7842235833423045e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06740602078180649) - present_state_Q ( -0.023565342445377835)) * f1( 0.2648215491155741)
w2 ( 0.5031995667861139 ) += alpha ( 0.1) * (reward ( 1.7842235833423045e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.06740602078180649) - present_state_Q (-0.023565342445377835)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4736277722185513 ) += alpha ( 0.1 ) * (reward ( 8.921117916711522e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06829584081956755) - present_state_Q ( -0.02288894460719547)) * f1( 0.2608607455457449)
w2 ( 0.5037941157822553 ) += alpha ( 0.1) * (reward ( 8.921117916711522e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.06829584081956755) - present_state_Q (-0.02288894460719547)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.47546487348263067 ) += alpha ( 0.1 ) * (reward ( 4.460558958355761e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0746766393845881) - present_state_Q ( 0.07549149148361495)) * f1( 0.2700850280556338)
w2 ( 0.5010733411028074 ) += alpha ( 0.1) * (reward ( 4.460558958355761e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.0746766393845881) - present_state_Q (0.07549149148361495)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47473546856318166 ) += alpha ( 0.1 ) * (reward ( 2.2302794791778806e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06917850494540445) - present_state_Q ( -0.02152715900708957)) * f1( 0.2564062190432167)
w2 ( 0.5016422858984296 ) += alpha ( 0.1) * (reward ( 2.2302794791778806e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.06917850494540445) - present_state_Q (-0.02152715900708957)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.47655595209882606 ) += alpha ( 0.1 ) * (reward ( 5.575698697944701e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07573315382307064) - present_state_Q ( 0.07573315382307064)) * f1( 0.26709253859246773)
w2 ( 0.49891591466359386 ) += alpha ( 0.1) * (reward ( 5.575698697944701e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.07573315382307064) - present_state_Q (0.07573315382307064)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47585567418114355 ) += alpha ( 0.1 ) * (reward ( 2.7878493489723507e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07442960260253931) - present_state_Q ( -0.020331867776385126)) * f1( 0.25212429323172963)
w2 ( 0.4994714168000253 ) += alpha ( 0.1) * (reward ( 2.7878493489723507e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.07442960260253931) - present_state_Q (-0.020331867776385126)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.47766789392458214 ) += alpha ( 0.1 ) * (reward ( 1.3939246744861754e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07672574542371653) - present_state_Q ( 0.0766444626426732)) * f1( 0.2627481218600374)
w2 ( 0.49671254685171196 ) += alpha ( 0.1) * (reward ( 1.3939246744861754e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.07672574542371653) - present_state_Q (0.0766444626426732)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47700639449858095 ) += alpha ( 0.1 ) * (reward ( 6.969623372430877e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07487321315823152) - present_state_Q ( -0.019180774267524334)) * f1( 0.24804834493344935)
w2 ( 0.49724591015730357 ) += alpha ( 0.1) * (reward ( 6.969623372430877e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.07487321315823152) - present_state_Q (-0.019180774267524334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.47880582873906785 ) += alpha ( 0.1 ) * (reward ( 3.4848116862154384e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07713610106586634) - present_state_Q ( 0.07713610106586634)) * f1( 0.2592006018348841)
w2 ( 0.4944690119128571 ) += alpha ( 0.1) * (reward ( 3.4848116862154384e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.07713610106586634) - present_state_Q (0.07713610106586634)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4805120776838471 ) += alpha ( 0.1 ) * (reward ( 1.7424058431077192e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07179275362453547) - present_state_Q ( 0.07706902359907278)) * f1( 0.24413442789689485)
w2 ( 0.49307121729660586 ) += alpha ( 0.1) * (reward ( 1.7424058431077192e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.07179275362453547) - present_state_Q (0.07706902359907278)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.48219673055608925 ) += alpha ( 0.1 ) * (reward ( 0.004567590593432812 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07457135139385639) - present_state_Q ( 0.07991894573574805)) * f1( 0.24812905607714988)
w2 ( 0.49035544849648866 ) += alpha ( 0.1) * (reward ( 0.004567590593432812) + discount_factor ( 0.1) * next_state_max_Q( 0.07457135139385639) - present_state_Q (0.07991894573574805)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4839195480432814 ) += alpha ( 0.1 ) * (reward ( 0.002283795296716406 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0749500850495159) - present_state_Q ( 0.08034719933823621)) * f1( 0.24413442789689485)
w2 ( 0.4875327126750259 ) += alpha ( 0.1) * (reward ( 0.002283795296716406) + discount_factor ( 0.1) * next_state_max_Q( 0.0749500850495159) - present_state_Q (0.08034719933823621)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4833187423184824 ) += alpha ( 0.1 ) * (reward ( 0.001141897648358203 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07107159038577934) - present_state_Q ( -0.01676986520816466)) * f1( 0.24014053336035623)
w2 ( 0.48853346955082994 ) += alpha ( 0.1) * (reward ( 0.001141897648358203) + discount_factor ( 0.1) * next_state_max_Q( 0.07107159038577934) - present_state_Q (-0.01676986520816466)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4834007001186221 ) += alpha ( 0.1 ) * (reward ( 0.0005709488241791015 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0735569915076362) - present_state_Q ( 0.011397266485401097)) * f1( 0.2361475336246368)
w2 ( 0.4883946448104116 ) += alpha ( 0.1) * (reward ( 0.0005709488241791015) + discount_factor ( 0.1) * next_state_max_Q( 0.0735569915076362) - present_state_Q (0.011397266485401097)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4835003472690529 ) += alpha ( 0.1 ) * (reward ( 0.00028547441208955077 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07351760107322593) - present_state_Q ( 0.01321731271699099)) * f1( 0.17857662008044248)
w2 ( 0.48828304324646005 ) += alpha ( 0.1) * (reward ( 0.00028547441208955077) + discount_factor ( 0.1) * next_state_max_Q( 0.07351760107322593) - present_state_Q (0.01321731271699099)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.48531472481591337 ) += alpha ( 0.1 ) * (reward ( 0.00014273720604477538 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07817043891447274) - present_state_Q ( 0.1151523728986021)) * f1( 0.16926333400231464)
w2 ( 0.48399533957441565 ) += alpha ( 0.1) * (reward ( 0.00014273720604477538) + discount_factor ( 0.1) * next_state_max_Q( 0.07817043891447274) - present_state_Q (0.1151523728986021)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4870903368501584 ) += alpha ( 0.1 ) * (reward ( 7.136860302238769e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07612286885111483) - present_state_Q ( 0.11478195785741069)) * f1( 0.1657927338682446)
w2 ( 0.4797114074796446 ) += alpha ( 0.1) * (reward ( 7.136860302238769e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07612286885111483) - present_state_Q (0.11478195785741069)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.48882726749996763 ) += alpha ( 0.1 ) * (reward ( 3.5684301511193846e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07395827890284591) - present_state_Q ( 0.11438392247387624)) * f1( 0.1624021978773705)
w2 ( 0.47543331106836134 ) += alpha ( 0.1) * (reward ( 3.5684301511193846e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.07395827890284591) - present_state_Q (0.11438392247387624)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4904808641172889 ) += alpha ( 0.1 ) * (reward ( 1.7842150755596923e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10706156924123243) - present_state_Q ( 0.11465229282342097)) * f1( 0.15910937798345776)
w2 ( 0.47127617931841964 ) += alpha ( 0.1) * (reward ( 1.7842150755596923e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.10706156924123243) - present_state_Q (0.11465229282342097)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4920735750438958 ) += alpha ( 0.1 ) * (reward ( 8.921075377798461e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10608953644512346) - present_state_Q ( 0.11370975702544844)) * f1( 0.15449431041391032)
w2 ( 0.4671525040261973 ) += alpha ( 0.1) * (reward ( 8.921075377798461e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10608953644512346) - present_state_Q (0.11370975702544844)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49363458569793334 ) += alpha ( 0.1 ) * (reward ( 4.4605376889073445e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.105273852897513) - present_state_Q ( 0.11288993926808748)) * f1( 0.1525048583425108)
w2 ( 0.4630581802885714 ) += alpha ( 0.1) * (reward ( 4.4605376889073445e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.105273852897513) - present_state_Q (0.11288993926808748)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4951648556865559 ) += alpha ( 0.1 ) * (reward ( 2.2302688444536722e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1046856482598931) - present_state_Q ( 0.11226809261077669)) * f1( 0.15032520763951365)
w2 ( 0.45898628838793365 ) += alpha ( 0.1) * (reward ( 2.2302688444536722e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.1046856482598931) - present_state_Q (0.11226809261077669)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4966647401116836 ) += alpha ( 0.1 ) * (reward ( 1.1151344222187224e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10439269364599917) - present_state_Q ( 0.11192664384617333)) * f1( 0.1477918720008303)
w2 ( 0.4549268380140476 ) += alpha ( 0.1) * (reward ( 1.1151344222187224e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10439269364599917) - present_state_Q (0.11192664384617333)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4968152614737839 ) += alpha ( 0.1 ) * (reward ( 5.575672111093612e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.105271080097816) - present_state_Q ( 0.020927428674853796)) * f1( 0.14473537587725446)
w2 ( 0.45451084749013315 ) += alpha ( 0.1) * (reward ( 5.575672111093612e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.105271080097816) - present_state_Q (0.020927428674853796)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4969875704767805 ) += alpha ( 0.1 ) * (reward ( 2.787836055546806e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10720475893231159) - present_state_Q ( 0.022936330456222703)) * f1( 0.14105680003014104)
w2 ( 0.45402222445895773 ) += alpha ( 0.1) * (reward ( 2.787836055546806e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.10720475893231159) - present_state_Q (0.022936330456222703)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.497149275767074 ) += alpha ( 0.1 ) * (reward ( 1.393918027773403e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10662512074804224) - present_state_Q ( 0.022482950783362918)) * f1( 0.13680304191981907)
w2 ( 0.4537858184726226 ) += alpha ( 0.1) * (reward ( 1.393918027773403e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.10662512074804224) - present_state_Q (0.022482950783362918)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.49862664772274007 ) += alpha ( 0.1 ) * (reward ( 6.969590138867015e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10879925245261073) - present_state_Q ( 0.10879925245261073)) * f1( 0.15087654800623257)
w2 ( 0.44986904817216467 ) += alpha ( 0.1) * (reward ( 6.969590138867015e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.10879925245261073) - present_state_Q (0.10879925245261073)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4999925119441298 ) += alpha ( 0.1 ) * (reward ( 1.7423975347167537e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10524454686194175) - present_state_Q ( 0.10286435754738314)) * f1( 0.14791703660044667)
w2 ( 0.4461754527546761 ) += alpha ( 0.1) * (reward ( 1.7423975347167537e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.10524454686194175) - present_state_Q (0.10286435754738314)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5013854687398276 ) += alpha ( 0.1 ) * (reward ( 4.355993836791884e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10515774264136703) - present_state_Q ( 0.11170697454611783)) * f1( 0.13765592776634067)
w2 ( 0.44212780491763665 ) += alpha ( 0.1) * (reward ( 4.355993836791884e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.10515774264136703) - present_state_Q (0.11170697454611783)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5026910874744791 ) += alpha ( 0.1 ) * (reward ( 2.177996918395942e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10536843677848204) - present_state_Q ( 0.10831519181269818)) * f1( 0.1335284128479366)
w2 ( 0.4382166710793625 ) += alpha ( 0.1) * (reward ( 2.177996918395942e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.10536843677848204) - present_state_Q (0.10831519181269818)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5040179916046337 ) += alpha ( 0.1 ) * (reward ( 5.444992295989855e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10421865569602998) - present_state_Q ( 0.1109264765397223)) * f1( 0.1320242051121238)
w2 ( 0.43419648666233773 ) += alpha ( 0.1) * (reward ( 5.444992295989855e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.10421865569602998) - present_state_Q (0.1109264765397223)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.505309171648809 ) += alpha ( 0.1 ) * (reward ( 2.7224961479949277e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10458024287072311) - present_state_Q ( 0.11130680723864503)) * f1( 0.1280312969449457)
w2 ( 0.4301625353551648 ) += alpha ( 0.1) * (reward ( 2.7224961479949277e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.10458024287072311) - present_state_Q (0.11130680723864503)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5065626281456663 ) += alpha ( 0.1 ) * (reward ( 1.3612480739974639e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10504448667117931) - present_state_Q ( 0.11179457566651654)) * f1( 0.12374912893033453)
w2 ( 0.42611093028063385 ) += alpha ( 0.1) * (reward ( 1.3612480739974639e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.10504448667117931) - present_state_Q (0.11179457566651654)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.506755022009923 ) += alpha ( 0.1 ) * (reward ( 6.806240369987319e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10505875796401763) - present_state_Q ( 0.026636235396733432)) * f1( 0.1192743806309486)
w2 ( 0.42546571589934307 ) += alpha ( 0.1) * (reward ( 6.806240369987319e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.10505875796401763) - present_state_Q (0.026636235396733432)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5069591527527945 ) += alpha ( 0.1 ) * (reward ( 3.4031201849936596e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10656413264695933) - present_state_Q ( 0.02830655030776464)) * f1( 0.11565391405570974)
w2 ( 0.4247597104189816 ) += alpha ( 0.1) * (reward ( 3.4031201849936596e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.10656413264695933) - present_state_Q (0.02830655030776464)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.507164184663386 ) += alpha ( 0.1 ) * (reward ( 1.7015600924968298e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10719882935558805) - present_state_Q ( 0.02901662257183163)) * f1( 0.11205926020599362)
w2 ( 0.4243937756265964 ) += alpha ( 0.1) * (reward ( 1.7015600924968298e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.10719882935558805) - present_state_Q (0.02901662257183163)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5083203197969925 ) += alpha ( 0.1 ) * (reward ( 8.507792348821097e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10608398692512569) - present_state_Q ( 0.11025487626998187)) * f1( 0.11602368310575716)
w2 ( 0.42240084607521716 ) += alpha ( 0.1) * (reward ( 8.507792348821097e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.10608398692512569) - present_state_Q (0.11025487626998187)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.509523042538841 ) += alpha ( 0.1 ) * (reward ( 4.253896174410548e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11122978702813369) - present_state_Q ( 0.10692070857781971)) * f1( 0.12554814644053414)
w2 ( 0.41856893688038704 ) += alpha ( 0.1) * (reward ( 4.253896174410548e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.11122978702813369) - present_state_Q (0.10692070857781971)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5105854758389681 ) += alpha ( 0.1 ) * (reward ( 2.126948087205274e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10572789232964834) - present_state_Q ( 0.10849924058096398)) * f1( 0.10849298483968296)
w2 ( 0.41465187882655213 ) += alpha ( 0.1) * (reward ( 2.126948087205274e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.10572789232964834) - present_state_Q (0.10849924058096398)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5117073161415149 ) += alpha ( 0.1 ) * (reward ( 5.317370218013185e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10354466323880536) - present_state_Q ( 0.11046580924106138)) * f1( 0.11205926020599362)
w2 ( 0.41064742510988617 ) += alpha ( 0.1) * (reward ( 5.317370218013185e-13) + discount_factor ( 0.1) * next_state_max_Q( 0.10354466323880536) - present_state_Q (0.11046580924106138)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5127519424205929 ) += alpha ( 0.1 ) * (reward ( 2.6586851090065927e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1045811582787797) - present_state_Q ( 0.10674326095725778)) * f1( 0.10849298483968296)
w2 ( 0.4067960193047216 ) += alpha ( 0.1) * (reward ( 2.6586851090065927e-13) + discount_factor ( 0.1) * next_state_max_Q( 0.1045811582787797) - present_state_Q (0.10674326095725778)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5138260315718534 ) += alpha ( 0.1 ) * (reward ( 6.646712772516482e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10269150637295905) - present_state_Q ( 0.10976156699634462)) * f1( 0.10795688662189651)
w2 ( 0.4028163226503623 ) += alpha ( 0.1) * (reward ( 6.646712772516482e-14) + discount_factor ( 0.1) * next_state_max_Q( 0.10269150637295905) - present_state_Q (0.10976156699634462)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5140253856642387 ) += alpha ( 0.1 ) * (reward ( 3.323356386258241e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1030819387329388) - present_state_Q ( 0.02961055343625226)) * f1( 0.10327964917216315)
w2 ( 0.4020442282678453 ) += alpha ( 0.1) * (reward ( 3.323356386258241e-14) + discount_factor ( 0.1) * next_state_max_Q( 0.1030819387329388) - present_state_Q (0.02961055343625226)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5142393834722264 ) += alpha ( 0.1 ) * (reward ( 1.6616781931291204e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10290020397627979) - present_state_Q ( 0.031870352843726926)) * f1( 0.09916335094574706)
w2 ( 0.40161262161892364 ) += alpha ( 0.1) * (reward ( 1.6616781931291204e-14) + discount_factor ( 0.1) * next_state_max_Q( 0.10290020397627979) - present_state_Q (0.031870352843726926)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5152749819710173 ) += alpha ( 0.1 ) * (reward ( 4.154195482822801e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10931898351360192) - present_state_Q ( 0.1051399916920911)) * f1( 0.10992670184347317)
w2 ( 0.3978442978852946 ) += alpha ( 0.1) * (reward ( 4.154195482822801e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.10931898351360192) - present_state_Q (0.1051399916920911)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5162407122241703 ) += alpha ( 0.1 ) * (reward ( 2.0770977414114005e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10249523984358881) - present_state_Q ( 0.10700670353546943)) * f1( 0.09980967382818047)
w2 ( 0.3959091542942724 ) += alpha ( 0.1) * (reward ( 2.0770977414114005e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.10249523984358881) - present_state_Q (0.10700670353546943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5172515850318565 ) += alpha ( 0.1 ) * (reward ( 1.0385488707057003e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10683784461378698) - present_state_Q ( 0.10264258889504205)) * f1( 0.10992670184347317)
w2 ( 0.39223080211692596 ) += alpha ( 0.1) * (reward ( 1.0385488707057003e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.10683784461378698) - present_state_Q (0.10264258889504205)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5174237183892454 ) += alpha ( 0.1 ) * (reward ( 5.192744353528501e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1020329755904404) - present_state_Q ( 0.02744945725346437)) * f1( 0.09980967382818047)
w2 ( 0.39154095572914915 ) += alpha ( 0.1) * (reward ( 5.192744353528501e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.1020329755904404) - present_state_Q (0.02744945725346437)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5183593113188393 ) += alpha ( 0.1 ) * (reward ( 2.5963721767642507e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10076762025132174) - present_state_Q ( 0.10497248972826478)) * f1( 0.09859168080998736)
w2 ( 0.3896430411750865 ) += alpha ( 0.1) * (reward ( 2.5963721767642507e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.10076762025132174) - present_state_Q (0.10497248972826478)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5193351115142636 ) += alpha ( 0.1 ) * (reward ( 1.2981860883821253e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10475130070360275) - present_state_Q ( 0.10088038653006975)) * f1( 0.10793622336099454)
w2 ( 0.38602683091669815 ) += alpha ( 0.1) * (reward ( 1.2981860883821253e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.10475130070360275) - present_state_Q (0.10088038653006975)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5194981242624451 ) += alpha ( 0.1 ) * (reward ( 6.490930441910627e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10024299630524047) - present_state_Q ( 0.0265584277478568)) * f1( 0.09859168080998736)
w2 ( 0.38536546579200487 ) += alpha ( 0.1) * (reward ( 6.490930441910627e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.10024299630524047) - present_state_Q (0.0265584277478568)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5196620087727858 ) += alpha ( 0.1 ) * (reward ( 3.2454652209553133e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10080894701002518) - present_state_Q ( 0.02688565784806425)) * f1( 0.09752265408708422)
w2 ( 0.3846932752661224 ) += alpha ( 0.1) * (reward ( 3.2454652209553133e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.10080894701002518) - present_state_Q (0.02688565784806425)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5205626899786715 ) += alpha ( 0.1 ) * (reward ( 1.6227326104776567e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09967547797866257) - present_state_Q ( 0.10319849178270127)) * f1( 0.09660753901968384)
w2 ( 0.3828286563864257 ) += alpha ( 0.1) * (reward ( 1.6227326104776567e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.09967547797866257) - present_state_Q (0.10319849178270127)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5215042908684057 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10437915519367516) - present_state_Q ( 0.10071423840599826)) * f1( 0.1043020871504296)
w2 ( 0.3792176034709605 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10437915519367516) - present_state_Q (0.10071423840599826)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5216880255144727 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10005677815832474) - present_state_Q ( 0.031018520012232965)) * f1( 0.08743921662271154)
w2 ( 0.3787973466270325 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10005677815832474) - present_state_Q (0.031018520012232965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.521852497898222 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10068216918713895) - present_state_Q ( 0.02781069355674822)) * f1( 0.0926997888201788)
w2 ( 0.37808764756151114 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10068216918713895) - present_state_Q (0.02781069355674822)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5220166997064155 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10112035633103525) - present_state_Q ( 0.027977375410644684)) * f1( 0.09191082298922364)
w2 ( 0.3773730339704095 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10112035633103525) - present_state_Q (0.027977375410644684)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5228653557838642 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10008041591355282) - present_state_Q ( 0.10297022910402875)) * f1( 0.0912904590732627)
w2 ( 0.375513790220156 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10008041591355282) - present_state_Q (0.10297022910402875)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5237498698454398 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10420574399105265) - present_state_Q ( 0.10118959831454646)) * f1( 0.09744668648190727)
w2 ( 0.3718830292635384 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10420574399105265) - present_state_Q (0.10118959831454646)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5239327872001023 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10042486260762112) - present_state_Q ( 0.03267053670601496)) * f1( 0.08083655068074583)
w2 ( 0.37143046825463333 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10042486260762112) - present_state_Q (0.03267053670601496)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5240967465085276 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10152565860728083) - present_state_Q ( 0.028927675863235185)) * f1( 0.08732801480437458)
w2 ( 0.37067946385453304 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10152565860728083) - present_state_Q (0.028927675863235185)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5249345088157449 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10483568883081554) - present_state_Q ( 0.10259104205523042)) * f1( 0.09095486808670795)
w2 ( 0.3669951649276471 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10483568883081554) - present_state_Q (0.10259104205523042)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.525117314843366 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10168004169317144) - present_state_Q ( 0.03463260073134076)) * f1( 0.07472268228809849)
w2 ( 0.36650587299640663 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10168004169317144) - present_state_Q (0.03463260073134076)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5253561958385992 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10201460320477296) - present_state_Q ( 0.029994236668343784)) * f1( 0.08257792375714215)
w2 ( 0.3653487550434985 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10201460320477296) - present_state_Q (0.029994236668343784)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5261956953521962 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10098523381762214) - present_state_Q ( 0.10275667813209734)) * f1( 0.08247097687848154)
w2 ( 0.36331288849899507 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10098523381762214) - present_state_Q (0.10275667813209734)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5270625158463649 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1039749186764997) - present_state_Q ( 0.10211425110314548)) * f1( 0.08594981568210086)
w2 ( 0.3592788112305818 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.1039749186764997) - present_state_Q (0.10211425110314548)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5273139224651017 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10243933453743452) - present_state_Q ( 0.03706046856762797)) * f1( 0.06992897850994802)
w2 ( 0.357840742927033 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10243933453743452) - present_state_Q (0.03706046856762797)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5275518421416027 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10244697039958983) - present_state_Q ( 0.037148462596416625)) * f1( 0.06601739382398232)
w2 ( 0.3571199641664071 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10244697039958983) - present_state_Q (0.037148462596416625)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5283142349923582 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10083719776449857) - present_state_Q ( 0.10332300223403251)) * f1( 0.07447100463283045)
w2 ( 0.35302498596911036 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10083719776449857) - present_state_Q (0.10332300223403251)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5284970748222799 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10306161171784002) - present_state_Q ( 0.039258478082838034)) * f1( 0.06315205462949035)
w2 ( 0.3518668932926682 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10306161171784002) - present_state_Q (0.039258478082838034)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5286803858623391 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10125603830493261) - present_state_Q ( 0.04102084663294016)) * f1( 0.059333095788035035)
w2 ( 0.3506310835805703 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10125603830493261) - present_state_Q (0.04102084663294016)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5288520251843357 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10483265431137792) - present_state_Q ( 0.04138720227739257)) * f1( 0.05553963006410962)
w2 ( 0.3500130048436452 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10483265431137792) - present_state_Q (0.04138720227739257)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5290175494778556 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10528216733421503) - present_state_Q ( 0.03696465458777588)) * f1( 0.06261217734091687)
w2 ( 0.34895554732947104 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10528216733421503) - present_state_Q (0.03696465458777588)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5291808370324154 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10544013307759995) - present_state_Q ( 0.03668212519902652)) * f1( 0.06247105959259118)
w2 ( 0.3479100228538204 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10544013307759995) - present_state_Q (0.03668212519902652)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5293418038019044 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10548054653654342) - present_state_Q ( 0.03626743198522844)) * f1( 0.06258579568777353)
w2 ( 0.3468812477605574 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10548054653654342) - present_state_Q (0.03626743198522844)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5299409103533149 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10458948630917742) - present_state_Q ( 0.10562322112247947)) * f1( 0.06295498675341285)
w2 ( 0.34497796231072614 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10458948630917742) - present_state_Q (0.10562322112247947)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5305549501857518 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10694001155638014) - present_state_Q ( 0.10583694257545737)) * f1( 0.06453866395904444)
w2 ( 0.34117224465393337 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10694001155638014) - present_state_Q (0.10583694257545737)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5307113893959612 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10477818879823214) - present_state_Q ( 0.04249143888595108)) * f1( 0.048866454396406946)
w2 ( 0.3405319722538108 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10477818879823214) - present_state_Q (0.04249143888595108)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5308645373533026 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10487757109621579) - present_state_Q ( 0.03645039141546557)) * f1( 0.05898783441514357)
w2 ( 0.33949346688157706 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10487757109621579) - present_state_Q (0.03645039141546557)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5313712138657073 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1037737955832483) - present_state_Q ( 0.10445662901368709)) * f1( 0.05964824510611398)
w2 ( 0.33609570380235604 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.1037737955832483) - present_state_Q (0.10445662901368709)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5319102843851682 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1051806464181149) - present_state_Q ( 0.1044489644441366)) * f1( 0.060323470332074754)
w2 ( 0.3325211712597598 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( 0.1051806464181149) - present_state_Q (0.1044489644441366)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5320537325021961 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10504655263777012) - present_state_Q ( 0.04475161023822326)) * f1( 0.04487920114894029)
w2 ( 0.3312426447855303 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( 0.10504655263777012) - present_state_Q (0.04475161023822326)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5321960827695629 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10657470001296901) - present_state_Q ( 0.04660784352857167)) * f1( 0.04089528751803026)
w2 ( 0.32985030570681345 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( 0.10657470001296901) - present_state_Q (0.04660784352857167)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5323260456909353 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10653334284955804) - present_state_Q ( 0.04643041508721431)) * f1( 0.0369148531975637)
w2 ( 0.32914618305636184 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( 0.10653334284955804) - present_state_Q (0.04643041508721431)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5327839674992833 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10542600015595749) - present_state_Q ( 0.10661313264229372)) * f1( 0.047807227513542494)
w2 ( 0.32531478071688746 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( 0.10542600015595749) - present_state_Q (0.10661313264229372)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5332253488230903 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1008144388761544) - present_state_Q ( 0.09979208946070109)) * f1( 0.049278958410796284)
w2 ( 0.32173206437676083 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( 0.1008144388761544) - present_state_Q (0.09979208946070109)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5335931001744564 ) += alpha ( 0.1 ) * (reward ( 0.009170856742316238 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10360517397149542) - present_state_Q ( 0.10885143187981407)) * f1( 0.041172314558419874)
w2 ( 0.3181592620671469 ) += alpha ( 0.1) * (reward ( 0.009170856742316238) + discount_factor ( 0.1) * next_state_max_Q( 0.10360517397149542) - present_state_Q (0.10885143187981407)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5339452339179135 ) += alpha ( 0.1 ) * (reward ( 0.004585428371158119 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10298755653763644) - present_state_Q ( 0.10951797810668404)) * f1( 0.03721014748207912)
w2 ( 0.3143739103038764 ) += alpha ( 0.1) * (reward ( 0.004585428371158119) + discount_factor ( 0.1) * next_state_max_Q( 0.10298755653763644) - present_state_Q (0.10951797810668404)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5342693496076557 ) += alpha ( 0.1 ) * (reward ( 0.0022927141855790594 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10345591557867251) - present_state_Q ( 0.11009607596151633)) * f1( 0.03325703933272901)
w2 ( 0.3104755994951536 ) += alpha ( 0.1) * (reward ( 0.0022927141855790594) + discount_factor ( 0.1) * next_state_max_Q( 0.10345591557867251) - present_state_Q (0.11009607596151633)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5345598503148294 ) += alpha ( 0.1 ) * (reward ( 0.0011463570927895297 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1038548330238997) - present_state_Q ( 0.11062251364282019)) * f1( 0.029316654903301813)
w2 ( 0.3065119725652479 ) += alpha ( 0.1) * (reward ( 0.0011463570927895297) + discount_factor ( 0.1) * next_state_max_Q( 0.1038548330238997) - present_state_Q (0.11062251364282019)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5346584223944895 ) += alpha ( 0.1 ) * (reward ( 0.0005731785463947649 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10419420242984274) - present_state_Q ( 0.04980827083850435)) * f1( 0.025394917685629518)
w2 ( 0.3049593456832829 ) += alpha ( 0.1) * (reward ( 0.0005731785463947649) + discount_factor ( 0.1) * next_state_max_Q( 0.10419420242984274) - present_state_Q (0.04980827083850435)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5347467008477419 ) += alpha ( 0.1 ) * (reward ( 0.00028658927319738243 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10208901952347843) - present_state_Q ( 0.05155135428462463)) * f1( 0.021502033248055878)
w2 ( 0.30331711116091975 ) += alpha ( 0.1) * (reward ( 0.00028658927319738243) + discount_factor ( 0.1) * next_state_max_Q( 0.10208901952347843) - present_state_Q (0.05155135428462463)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5348196833320866 ) += alpha ( 0.1 ) * (reward ( 0.00014329463659869122 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10162354210153855) - present_state_Q ( 0.05163888178348892)) * f1( 0.01765709555224479)
w2 ( 0.30249044650218504 ) += alpha ( 0.1) * (reward ( 0.00014329463659869122) + discount_factor ( 0.1) * next_state_max_Q( 0.10162354210153855) - present_state_Q (0.05163888178348892)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.535085814228609 ) += alpha ( 0.1 ) * (reward ( 7.164731829934561e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10612024839919805) - present_state_Q ( 0.1051412626197332)) * f1( 0.028174643797510616)
w2 ( 0.2987121428837245 ) += alpha ( 0.1) * (reward ( 7.164731829934561e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.10612024839919805) - present_state_Q (0.1051412626197332)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5353651120951755 ) += alpha ( 0.1 ) * (reward ( 3.5823659149672804e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10344518184083844) - present_state_Q ( 0.10459339016847169)) * f1( 0.029645348657252022)
w2 ( 0.29494362095071497 ) += alpha ( 0.1) * (reward ( 3.5823659149672804e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.10344518184083844) - present_state_Q (0.10459339016847169)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5356447644888114 ) += alpha ( 0.1 ) * (reward ( 1.7911829574836402e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1051418447502292) - present_state_Q ( 0.10382452637743136)) * f1( 0.02997589337286861)
w2 ( 0.29121192374780164 ) += alpha ( 0.1) * (reward ( 1.7911829574836402e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.1051418447502292) - present_state_Q (0.10382452637743136)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5357021590203558 ) += alpha ( 0.1 ) * (reward ( 8.955914787418201e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10088307695487087) - present_state_Q ( 0.05316298533591715)) * f1( 0.013327196026110441)
w2 ( 0.28948929487877595 ) += alpha ( 0.1) * (reward ( 8.955914787418201e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10088307695487087) - present_state_Q (0.05316298533591715)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5357425706231203 ) += alpha ( 0.1 ) * (reward ( 4.4779573937091005e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10452859477264866) - present_state_Q ( 0.053073130528752885)) * f1( 0.009482776180013017)
w2 ( 0.28863697901689406 ) += alpha ( 0.1) * (reward ( 4.4779573937091005e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10452859477264866) - present_state_Q (0.053073130528752885)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5359487869813361 ) += alpha ( 0.1 ) * (reward ( 2.2389786968545502e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10476858877703175) - present_state_Q ( 0.10476858877703175)) * f1( 0.02187055590208076)
w2 ( 0.2848653993800688 ) += alpha ( 0.1) * (reward ( 2.2389786968545502e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10476858877703175) - present_state_Q (0.10476858877703175)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5361713360946119 ) += alpha ( 0.1 ) * (reward ( 1.1194893484272751e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10530204770322722) - present_state_Q ( 0.1026244760701108)) * f1( 0.0241656528092201)
w2 ( 0.2811816733076512 ) += alpha ( 0.1) * (reward ( 1.1194893484272751e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.10530204770322722) - present_state_Q (0.1026244760701108)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5362014195613795 ) += alpha ( 0.1 ) * (reward ( 5.597446742136376e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10291053026401308) - present_state_Q ( 0.053329249963255346)) * f1( 0.0069900367980915415)
w2 ( 0.279460167819964 ) += alpha ( 0.1) * (reward ( 5.597446742136376e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.10291053026401308) - present_state_Q (0.053329249963255346)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5363167936323492 ) += alpha ( 0.1 ) * (reward ( 1.399361685534094e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1014522227937221) - present_state_Q ( 0.10784775114145581)) * f1( 0.011808725686045046)
w2 ( 0.27555207226292744 ) += alpha ( 0.1) * (reward ( 1.399361685534094e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.1014522227937221) - present_state_Q (0.10784775114145581)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5364008214254822 ) += alpha ( 0.1 ) * (reward ( 3.498404213835235e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10332001548875067) - present_state_Q ( 0.10388761100834734)) * f1( 0.008981591273869695)
w2 ( 0.2718098492839102 ) += alpha ( 0.1) * (reward ( 3.498404213835235e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.10332001548875067) - present_state_Q (0.10388761100834734)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5365108150695939 ) += alpha ( 0.1 ) * (reward ( 1.7492021069176174e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10275494648321362) - present_state_Q ( 0.1034215865230178)) * f1( 0.011808725686045043)
w2 ( 0.2680840063086032 ) += alpha ( 0.1) * (reward ( 1.7492021069176174e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.10275494648321362) - present_state_Q (0.1034215865230178)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5365377792259882 ) += alpha ( 0.1 ) * (reward ( 8.746010534588087e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09991897218876629) - present_state_Q ( 0.049558432624541016)) * f1( 0.006814890937067884)
w2 ( 0.2672926757754101 ) += alpha ( 0.1) * (reward ( 8.746010534588087e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.09991897218876629) - present_state_Q (0.049558432624541016)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5366644839905022 ) += alpha ( 0.1 ) * (reward ( 4.3730052672940434e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10254268806355053) - present_state_Q ( 0.10254268806355053)) * f1( 0.013729216681528109)
w2 ( 0.2636011391800425 ) += alpha ( 0.1) * (reward ( 4.3730052672940434e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.10254268806355053) - present_state_Q (0.10254268806355053)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.536778071323136 ) += alpha ( 0.1 ) * (reward ( 2.1865026336470217e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10135610940834092) - present_state_Q ( 0.09874214575205062)) * f1( 0.012819295515715706)
w2 ( 0.26005687787505394 ) += alpha ( 0.1) * (reward ( 2.1865026336470217e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.10135610940834092) - present_state_Q (0.09874214575205062)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5368777253324576 ) += alpha ( 0.1 ) * (reward ( 1.0932513168235109e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09956518314008594) - present_state_Q ( 0.09891950631539814)) * f1( 0.011201738249237252)
w2 ( 0.2564983583987284 ) += alpha ( 0.1) * (reward ( 1.0932513168235109e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.09956518314008594) - present_state_Q (0.09891950631539814)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.536982648870985 ) += alpha ( 0.1 ) * (reward ( 5.466256584117554e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0993099903698678) - present_state_Q ( 0.0993099903698678)) * f1( 0.011739172439114575)
w2 ( 0.2529231987672782 ) += alpha ( 0.1) * (reward ( 5.466256584117554e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.0993099903698678) - present_state_Q (0.0993099903698678)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5371024613633151 ) += alpha ( 0.1 ) * (reward ( 2.733128292058777e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0958588478108019) - present_state_Q ( 0.0958588478108019)) * f1( 0.013887606093741399)
w2 ( 0.24947228025702184 ) += alpha ( 0.1) * (reward ( 2.733128292058777e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.0958588478108019) - present_state_Q (0.0958588478108019)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5371885543080844 ) += alpha ( 0.1 ) * (reward ( 1.3665641460293886e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09644566320031729) - present_state_Q ( 0.094692574666328)) * f1( 0.010122864318831384)
w2 ( 0.24607035992863624 ) += alpha ( 0.1) * (reward ( 1.3665641460293886e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.09644566320031729) - present_state_Q (0.094692574666328)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.537307039760577 ) += alpha ( 0.1 ) * (reward ( 6.832820730146943e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09351582309240841) - present_state_Q ( 0.09402866739416281)) * f1( 0.013992622971066638)
w2 ( 0.2426832765279725 ) += alpha ( 0.1) * (reward ( 6.832820730146943e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.09351582309240841) - present_state_Q (0.09402866739416281)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5374066337149215 ) += alpha ( 0.1 ) * (reward ( 3.4164103650734714e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09280304333891173) - present_state_Q ( 0.09280304333891173)) * f1( 0.011924172455501715)
w2 ( 0.23934236696913824 ) += alpha ( 0.1) * (reward ( 3.4164103650734714e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.09280304333891173) - present_state_Q (0.09280304333891173)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5375078558790877 ) += alpha ( 0.1 ) * (reward ( 1.7082051825367357e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09042552337616154) - present_state_Q ( 0.09223244798468176)) * f1( 0.012167603216671896)
w2 ( 0.2360147711439389 ) += alpha ( 0.1) * (reward ( 1.7082051825367357e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.09042552337616154) - present_state_Q (0.09223244798468176)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.537616663945247 ) += alpha ( 0.1 ) * (reward ( 8.541025912683679e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08943606776698064) - present_state_Q ( 0.0892701260604544)) * f1( 0.013545721530213897)
w2 ( 0.2328017103729303 ) += alpha ( 0.1) * (reward ( 8.541025912683679e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.08943606776698064) - present_state_Q (0.0892701260604544)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5376996360010058 ) += alpha ( 0.1 ) * (reward ( 4.270512956341839e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08957934803107552) - present_state_Q ( 0.08794926317531417)) * f1( 0.01050394485991042)
w2 ( 0.22964205723821285 ) += alpha ( 0.1) * (reward ( 4.270512956341839e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.08957934803107552) - present_state_Q (0.08794926317531417)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5378064427500268 ) += alpha ( 0.1 ) * (reward ( 2.1352564781709196e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08767859770720346) - present_state_Q ( 0.08767859770720346)) * f1( 0.013535134991371375)
w2 ( 0.22648562772083894 ) += alpha ( 0.1) * (reward ( 2.1352564781709196e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.08767859770720346) - present_state_Q (0.08767859770720346)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5379016627216917 ) += alpha ( 0.1 ) * (reward ( 1.0676282390854598e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08613411153824616) - present_state_Q ( 0.08613411153824616)) * f1( 0.012283167101704904)
w2 ( 0.2233847997055048 ) += alpha ( 0.1) * (reward ( 1.0676282390854598e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.08613411153824616) - present_state_Q (0.08613411153824616)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5379883793743583 ) += alpha ( 0.1 ) * (reward ( 5.338141195427299e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08805115902728307) - present_state_Q ( 0.08254776222877061)) * f1( 0.011759362728023737)
w2 ( 0.22043509385248444 ) += alpha ( 0.1) * (reward ( 5.338141195427299e-13) + discount_factor ( 0.1) * next_state_max_Q( 0.08805115902728307) - present_state_Q (0.08254776222877061)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5380141143450252 ) += alpha ( 0.1 ) * (reward ( 0.004567586237551657 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0399127148654493) - present_state_Q ( 0.038589241203830726)) * f1( 0.00856964436843881)
w2 ( 0.21983448618288975 ) += alpha ( 0.1) * (reward ( 0.004567586237551657) + discount_factor ( 0.1) * next_state_max_Q( 0.0399127148654493) - present_state_Q (0.038589241203830726)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5380557827001411 ) += alpha ( 0.1 ) * (reward ( 0.0022837931187758287 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04035977609434307) - present_state_Q ( 0.04035977609434307)) * f1( 0.01224099546038951)
w2 ( 0.2191536860755671 ) += alpha ( 0.1) * (reward ( 0.0022837931187758287) + discount_factor ( 0.1) * next_state_max_Q( 0.04035977609434307) - present_state_Q (0.04035977609434307)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5381017255594093 ) += alpha ( 0.1 ) * (reward ( 0.0011418965593879143 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.038597531675166746) - present_state_Q ( 0.038597531675166746)) * f1( 0.013675146060746313)
w2 ( 0.21848176843660186 ) += alpha ( 0.1) * (reward ( 0.0011418965593879143) + discount_factor ( 0.1) * next_state_max_Q( 0.038597531675166746) - present_state_Q (0.038597531675166746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5381380152727686 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796939572 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03954546413133497) - present_state_Q ( 0.038733892594346314)) * f1( 0.010608422371558582)
w2 ( 0.21779760047857147 ) += alpha ( 0.1) * (reward ( 0.0005709482796939572) + discount_factor ( 0.1) * next_state_max_Q( 0.03954546413133497) - present_state_Q (0.038733892594346314)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5381824632997813 ) += alpha ( 0.1 ) * (reward ( 0.0002854741398469786 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03829791871372733) - present_state_Q ( 0.039930380648861466)) * f1( 0.012410410370694399)
w2 ( 0.21708129818581864 ) += alpha ( 0.1) * (reward ( 0.0002854741398469786) + discount_factor ( 0.1) * next_state_max_Q( 0.03829791871372733) - present_state_Q (0.039930380648861466)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5382283136526004 ) += alpha ( 0.1 ) * (reward ( 0.00014273706992347308 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03844949162928516) - present_state_Q ( 0.03839094558921216)) * f1( 0.013327328188350408)
w2 ( 0.21639323299869145 ) += alpha ( 0.1) * (reward ( 0.00014273706992347308) + discount_factor ( 0.1) * next_state_max_Q( 0.03844949162928516) - present_state_Q (0.03839094558921216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5382647396044685 ) += alpha ( 0.1 ) * (reward ( 7.136853496173654e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039733940902182345) - present_state_Q ( 0.03800440140922593)) * f1( 0.010726248326663624)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 7.136853496173654e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.039733940902182345) - present_state_Q (0.03800440140922593)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.538242714584618 ) += alpha ( 0.1 ) * (reward ( 0.009170856742317277 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0063864162178433595) - present_state_Q ( -0.007791005037552659)) * f1( 0.013493060583108485)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.009170856742317277) + discount_factor ( 0.1) * next_state_max_Q( -0.0063864162178433595) - present_state_Q (-0.007791005037552659)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5350715318269978 ) += alpha ( 0.1 ) * (reward ( 2.8079038107495875e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13848394848040618) - present_state_Q ( -0.1397692232726248)) * f1( 0.2518394129250897)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 2.8079038107495875e-10) + discount_factor ( 0.1) * next_state_max_Q( -0.13848394848040618) - present_state_Q (-0.1397692232726248)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5313463377584404 ) += alpha ( 0.1 ) * (reward ( 0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1357247048168369) - present_state_Q ( -0.13002796937906208)) * f1( 0.243482801621078)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.1357247048168369) - present_state_Q (-0.13002796937906208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5226847881111296 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13657046782663854) - present_state_Q ( -0.2148051879937594)) * f1( 0.41189904683769596)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.13657046782663854) - present_state_Q (-0.2148051879937594)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5147066687119293 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13655855922551843) - present_state_Q ( -0.20808308621026037)) * f1( 0.40557561345797416)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.13655855922551843) - present_state_Q (-0.20808308621026037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5111294038085775 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1441944596014438) - present_state_Q ( -0.1441944596014438)) * f1( 0.2756323354963457)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.1441944596014438) - present_state_Q (-0.1441944596014438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5057906624016552 ) += alpha ( 0.1 ) * (reward ( 1.1151333587446788e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17308573835182126) - present_state_Q ( -0.17308573835182126)) * f1( 0.3427141074428012)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 1.1151333587446788e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.17308573835182126) - present_state_Q (-0.17308573835182126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.49894741241881185 ) += alpha ( 0.1 ) * (reward ( 5.444987103245502e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19792357586915438) - present_state_Q ( -0.19811257989179987)) * f1( 0.3837618568154989)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 5.444987103245502e-10) + discount_factor ( 0.1) * next_state_max_Q( -0.19792357586915438) - present_state_Q (-0.19811257989179987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.491605041255131 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20917114308304205) - present_state_Q ( -0.20404268793508315)) * f1( 0.400947339991019)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( -0.20917114308304205) - present_state_Q (-0.20404268793508315)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4841831107375598 ) += alpha ( 0.1 ) * (reward ( 6.646712772516482e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16500729236564235) - present_state_Q ( -0.19864146999853763)) * f1( 0.40748327290859304)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 6.646712772516482e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.16500729236564235) - present_state_Q (-0.19864146999853763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.479375828714746 ) += alpha ( 0.1 ) * (reward ( 3.323356386258241e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15913667566973452) - present_state_Q ( -0.15913667566973452)) * f1( 0.3356501225950765)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 3.323356386258241e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.15913667566973452) - present_state_Q (-0.15913667566973452)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4750793921888758 ) += alpha ( 0.1 ) * (reward ( 2.5963721767642507e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14170262866603506) - present_state_Q ( -0.14988965872469096)) * f1( 0.3165676135460147)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 2.5963721767642507e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.14170262866603506) - present_state_Q (-0.14988965872469096)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4704950370687533 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1574626687866846) - present_state_Q ( -0.1574626687866846)) * f1( 0.3234879702278967)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.1574626687866846) - present_state_Q (-0.1574626687866846)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4629860423401276 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20089252216922762) - present_state_Q ( -0.20089252216922762)) * f1( 0.4153129935430113)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.20089252216922762) - present_state_Q (-0.20089252216922762)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45493127288767565 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19240702911015306) - present_state_Q ( -0.2023745273463122)) * f1( 0.43982969706930236)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.19240702911015306) - present_state_Q (-0.2023745273463122)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.44561284918652194 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17639025667259725) - present_state_Q ( -0.2148674331635063)) * f1( 0.47246863773065023)
w2 ( 0.21571404022301052 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.17639025667259725) - present_state_Q (-0.2148674331635063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4376201994686859 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17805998013378907) - present_state_Q ( -0.17805998013378907)) * f1( 0.49874889922114807)
w2 ( 0.21891911986541873 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.17805998013378907) - present_state_Q (-0.17805998013378907)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.42888307363110456 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1823869667468333) - present_state_Q ( -0.18643108836885305)) * f1( 0.5194721205622883)
w2 ( 0.22228296769930211 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.1823869667468333) - present_state_Q (-0.18643108836885305)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.42232432375081747 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1030632285170131) - present_state_Q ( -0.15347332598977662)) * f1( 0.4580046645153016)
w2 ( 0.22801107519552433 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.1030632285170131) - present_state_Q (-0.15347332598977662)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.418071897369355 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09543955455442492) - present_state_Q ( -0.10127413500718478)) * f1( 0.46353477991256)
w2 ( 0.23168063922026882 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.09543955455442492) - present_state_Q (-0.10127413500718478)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4154554301883899 ) += alpha ( 0.1 ) * (reward ( 6.806233879056877e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06164022384558329) - present_state_Q ( -0.06164022384558329)) * f1( 0.4716377669789241)
w2 ( 0.23500921131201408 ) += alpha ( 0.1) * (reward ( 6.806233879056877e-11) + discount_factor ( 0.1) * next_state_max_Q( -0.06164022384558329) - present_state_Q (-0.06164022384558329)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.41264139225362334 ) += alpha ( 0.1 ) * (reward ( 5.192744353528501e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05835284549245526) - present_state_Q ( -0.06294415197209247)) * f1( 0.4927497360314904)
w2 ( 0.23843574335738493 ) += alpha ( 0.1) * (reward ( 5.192744353528501e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.05835284549245526) - present_state_Q (-0.06294415197209247)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40996071909836457 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05518117027390859) - present_state_Q ( -0.05982062645124456)) * f1( 0.49365548364164663)
w2 ( 0.24169389392281615 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.05518117027390859) - present_state_Q (-0.05982062645124456)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40676659950699146 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.048758431242450384) - present_state_Q ( -0.06618890617948084)) * f1( 0.5161461777399456)
w2 ( 0.24540693460291094 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( -0.048758431242450384) - present_state_Q (-0.06618890617948084)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4043917003505623 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04841907018670091) - present_state_Q ( -0.052910376041104656)) * f1( 0.4938825830735795)
w2 ( 0.2482921132722814 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.04841907018670091) - present_state_Q (-0.052910376041104656)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4025123709662281 ) += alpha ( 0.1 ) * (reward ( 6.969583492154242e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03308790229231498) - present_state_Q ( -0.04238592187471482)) * f1( 0.48092732330528526)
w2 ( 0.2506367453527605 ) += alpha ( 0.1) * (reward ( 6.969583492154242e-08) + discount_factor ( 0.1) * next_state_max_Q( -0.03308790229231498) - present_state_Q (-0.04238592187471482)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4006491421749166 ) += alpha ( 0.1 ) * (reward ( 4.253896174410548e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03534738579863814) - present_state_Q ( -0.03952042397630576)) * f1( 0.5177694326409755)
w2 ( 0.25279588647680223 ) += alpha ( 0.1) * (reward ( 4.253896174410548e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.03534738579863814) - present_state_Q (-0.03952042397630576)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39821501455270186 ) += alpha ( 0.1 ) * (reward ( 1.063474043602637e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03656055171375064) - present_state_Q ( -0.055187631202129356)) * f1( 0.47235652577592696)
w2 ( 0.2558877810387113 ) += alpha ( 0.1) * (reward ( 1.063474043602637e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.03656055171375064) - present_state_Q (-0.055187631202129356)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3970845638499137 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030713088817947237) - present_state_Q ( -0.035629847199003445)) * f1( 0.482616678745283)
w2 ( 0.25729318298925363 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.030713088817947237) - present_state_Q (-0.035629847199003445)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39541843263539794 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03388368171057121) - present_state_Q ( -0.0467926048777218)) * f1( 0.4861910448573848)
w2 ( 0.2593493268431633 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.03388368171057121) - present_state_Q (-0.0467926048777218)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3944109854922453 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028433227089472157) - present_state_Q ( -0.03299217529859988)) * f1( 0.47942442144739844)
w2 ( 0.2606101476500523 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.028433227089472157) - present_state_Q (-0.03299217529859988)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3931675784964228 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029339913367279757) - present_state_Q ( -0.037027007513656895)) * f1( 0.49820289391356626)
w2 ( 0.2621076182721779 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.029339913367279757) - present_state_Q (-0.037027007513656895)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39186436561641486 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03412982289299285) - present_state_Q ( -0.03901254374842866)) * f1( 0.4924401922830884)
w2 ( 0.26369548161123546 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.03412982289299285) - present_state_Q (-0.03901254374842866)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3908353970523113 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03385372326745545) - present_state_Q ( -0.03385372326745545)) * f1( 0.4823325158740805)
w2 ( 0.2649754723191879 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.03385372326745545) - present_state_Q (-0.03385372326745545)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3900479404778963 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028437612790989303) - present_state_Q ( -0.028437612790989303)) * f1( 0.47844457786808536)
w2 ( 0.26596299306141113 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.028437612790989303) - present_state_Q (-0.028437612790989303)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3889185362098787 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0302203537028696) - present_state_Q ( -0.035093037563323604)) * f1( 0.492419189492595)
w2 ( 0.26733914284450316 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0302203537028696) - present_state_Q (-0.035093037563323604)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3880415638768194 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030318328237244868) - present_state_Q ( -0.030318328237244868)) * f1( 0.4831451327386968)
w2 ( 0.2684282222208242 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.030318328237244868) - present_state_Q (-0.030318328237244868)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38700138466194534 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029808366691432636) - present_state_Q ( -0.03267793722121062)) * f1( 0.5058763025369604)
w2 ( 0.26966193790545806 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.029808366691432636) - present_state_Q (-0.03267793722121062)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38571012413893485 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024433586319221934) - present_state_Q ( -0.02793698523220367)) * f1( 0.5065032697216294)
w2 ( 0.27119155550147495 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.024433586319221934) - present_state_Q (-0.02793698523220367)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38430926032584795 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026222040614043124) - present_state_Q ( -0.03095008919515682)) * f1( 0.494517612759516)
w2 ( 0.2728912286095001 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.026222040614043124) - present_state_Q (-0.03095008919515682)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38291219749033656 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02423221769245293) - present_state_Q ( -0.030919159773642763)) * f1( 0.49026736206955873)
w2 ( 0.274600984889764 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.02423221769245293) - present_state_Q (-0.030919159773642763)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38181081602228445 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0198216425533434) - present_state_Q ( -0.024596234865563144)) * f1( 0.48703370880690655)
w2 ( 0.27595782912637773 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0198216425533434) - present_state_Q (-0.024596234865563144)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.380813161099799 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014060303608427166) - present_state_Q ( -0.021452232004653404)) * f1( 0.4976777846557595)
w2 ( 0.27716060122500635 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.014060303608427166) - present_state_Q (-0.021452232004653404)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3803256647633213 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011419865927460499) - present_state_Q ( -0.011419865927460499)) * f1( 0.47431607299682255)
w2 ( 0.2777772739850892 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.011419865927460499) - present_state_Q (-0.011419865927460499)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3795142390070365 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013351968029244543) - present_state_Q ( -0.01794146110303882)) * f1( 0.4886263048813427)
w2 ( 0.2787736498430961 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.013351968029244543) - present_state_Q (-0.01794146110303882)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3788160446432119 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012833457916788471) - present_state_Q ( -0.01598612289037238)) * f1( 0.47487244017774133)
w2 ( 0.2796558164690177 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.012833457916788471) - present_state_Q (-0.01598612289037238)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3781224160520191 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007623245899689546) - present_state_Q ( -0.014914232517404397)) * f1( 0.490130797027119)
w2 ( 0.28050493094466383 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.007623245899689546) - present_state_Q (-0.014914232517404397)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3777822602773575 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008228688868912953) - present_state_Q ( -0.008228688868912953)) * f1( 0.4593087267680327)
w2 ( 0.2809492801435851 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.008228688868912953) - present_state_Q (-0.008228688868912953)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.37793011425544365 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003731037602211912) - present_state_Q ( 0.003731037602211912)) * f1( 0.44031236183766104)
w2 ( 0.28074780411306566 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.003731037602211912) - present_state_Q (0.003731037602211912)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.37812838151650036 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005048836701057047) - present_state_Q ( 0.005048836701057047)) * f1( 0.43633210930264216)
w2 ( 0.28047516693120855 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.005048836701057047) - present_state_Q (0.005048836701057047)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3783736129038806 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006302217716079139) - present_state_Q ( 0.006302217716079139)) * f1( 0.43235465924353444)
w2 ( 0.2801348471745403 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.006302217716079139) - present_state_Q (0.006302217716079139)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3786625774217605 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007495009061995489) - present_state_Q ( 0.007495009061995489)) * f1( 0.42838065154751576)
w2 ( 0.27973011668519254 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.007495009061995489) - present_state_Q (0.007495009061995489)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3789922422912321 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00863065176385433) - present_state_Q ( 0.00863065176385433)) * f1( 0.4244109360858703)
w2 ( 0.2792640614899444 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00863065176385433) - present_state_Q (0.00863065176385433)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3793597526674896 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00971216792638499) - present_state_Q ( 0.00971216792638499)) * f1( 0.4204466660826993)
w2 ( 0.27873960442191964 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00971216792638499) - present_state_Q (0.00971216792638499)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3797624098357643 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010742093421881066) - present_state_Q ( 0.010742093421881066)) * f1( 0.416489445834848)
w2 ( 0.27815953137713806 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.010742093421881066) - present_state_Q (0.010742093421881066)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38011157325762124 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009404127551753044) - present_state_Q ( 0.009404127551753044)) * f1( 0.4125415733504157)
w2 ( 0.2776517084893434 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.009404127551753044) - present_state_Q (0.009404127551753044)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3805455093757081 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01173056903979186) - present_state_Q ( 0.01173056903979186)) * f1( 0.4110211880457056)
w2 ( 0.27701825776119465 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.01173056903979186) - present_state_Q (0.01173056903979186)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38100414848610914 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012508324394062603) - present_state_Q ( 0.012508324394062603)) * f1( 0.40740789533618665)
w2 ( 0.27634280824391527 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.012508324394062603) - present_state_Q (0.012508324394062603)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38234703576163853 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011888109266457864) - present_state_Q ( 0.01616641504767008)) * f1( 0.4039007857819849)
w2 ( 0.27434793129967344 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.011888109266457864) - present_state_Q (0.01616641504767008)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38364593283389775 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011651262729943324) - present_state_Q ( 0.015966659669313543)) * f1( 0.39274971281352)
w2 ( 0.2723636185989139 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.011651262729943324) - present_state_Q (0.015966659669313543)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3849259734766079 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011446036141885413) - present_state_Q ( 0.015800310457611932)) * f1( 0.38876226361842775)
w2 ( 0.2703880554913281 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.011446036141885413) - present_state_Q (0.015800310457611932)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38618805368024406 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011260513766666175) - present_state_Q ( 0.01565606911572706)) * f1( 0.38477629519312184)
w2 ( 0.2684200337300041 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.011260513766666175) - present_state_Q (0.01565606911572706)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38743299875197 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011092851987264812) - present_state_Q ( 0.015532505040371847)) * f1( 0.3807920854371168)
w2 ( 0.266458419842525 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.011092851987264812) - present_state_Q (0.015532505040371847)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3868373842049851 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03786345127564816) - present_state_Q ( -0.03786345127564816)) * f1( 0.3768099862512005)
w2 ( 0.2674068255144296 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03786345127564816) - present_state_Q (-0.03786345127564816)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38631985504733296 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035723812281609424) - present_state_Q ( -0.035723812281609424)) * f1( 0.37283044985316627)
w2 ( 0.2682396906806562 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.035723812281609424) - present_state_Q (-0.035723812281609424)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38581752936727526 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03543210322373566) - present_state_Q ( -0.03543210322373566)) * f1( 0.36885406714406815)
w2 ( 0.26878443259872375 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03543210322373566) - present_state_Q (-0.03543210322373566)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3870419335901796 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01677232985398852) - present_state_Q ( 0.015489152730160938)) * f1( 0.3816451969853277)
w2 ( 0.26685949671705766 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01677232985398852) - present_state_Q (0.015489152730160938)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38657543108976644 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034480691011032516) - present_state_Q ( -0.034480691011032516)) * f1( 0.3655323433787786)
w2 ( 0.26736998779546794 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.034480691011032516) - present_state_Q (-0.034480691011032516)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.387837393089358 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01712342412514356) - present_state_Q ( 0.01712342412514356)) * f1( 0.3746759340820866)
w2 ( 0.2653491021957298 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01712342412514356) - present_state_Q (0.01712342412514356)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3874021333214595 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03370029858164626) - present_state_Q ( -0.03370029858164626)) * f1( 0.36091419486728477)
w2 ( 0.2658314991466822 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03370029858164626) - present_state_Q (-0.03370029858164626)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.38817836590843247 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0169658316808787) - present_state_Q ( 0.002622362457668642)) * f1( 0.4043694327554324)
w2 ( 0.264679731692327 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0169658316808787) - present_state_Q (0.002622362457668642)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3877807814766746 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03268964532547641) - present_state_Q ( -0.03268964532547641)) * f1( 0.3565672257291722)
w2 ( 0.26512574512605724 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03268964532547641) - present_state_Q (-0.03268964532547641)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3890311299032336 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018421274030847395) - present_state_Q ( 0.01745685653131812)) * f1( 0.3689968107150207)
w2 ( 0.2630926406813829 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.018421274030847395) - present_state_Q (0.01745685653131812)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38866183288370226 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031924429942194146) - present_state_Q ( -0.031924429942194146)) * f1( 0.3530010103492887)
w2 ( 0.263511106361315 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.031924429942194146) - present_state_Q (-0.031924429942194146)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.38992875016356954 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01845180480138739) - present_state_Q ( 0.01845180480138739)) * f1( 0.36325326034667466)
w2 ( 0.2614184882050597 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01845180480138739) - present_state_Q (0.01845180480138739)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3895817374930971 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03136159862059976) - present_state_Q ( -0.03136159862059976)) * f1( 0.3485780015085016)
w2 ( 0.26181669195741436 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.03136159862059976) - present_state_Q (-0.03136159862059976)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.39079458333141215 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01710461393013951) - present_state_Q ( 0.01710461393013951)) * f1( 0.3602744519773892)
w2 ( 0.2597968221082065 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01710461393013951) - present_state_Q (0.01710461393013951)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.391990037552155 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0169875096595897) - present_state_Q ( 0.016618876871732552)) * f1( 0.36017995223670957)
w2 ( 0.25780539385687967 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0169875096595897) - present_state_Q (0.016618876871732552)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39177183142758726 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02741399912323665) - present_state_Q ( -0.02741399912323665)) * f1( 0.3408270207086443)
w2 ( 0.2581895291125541 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02741399912323665) - present_state_Q (-0.02741399912323665)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3929844516015312 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01872332444575775) - present_state_Q ( 0.01845708192786427)) * f1( 0.3479032817645262)
w2 ( 0.2560982234465764 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01872332444575775) - present_state_Q (0.01845708192786427)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39417154349140265 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017568314954362535) - present_state_Q ( 0.017568314954362535)) * f1( 0.34830639824826215)
w2 ( 0.25405331374206047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.017568314954362535) - present_state_Q (0.017568314954362535)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3952858230506329 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015819609309294186) - present_state_Q ( 0.015194005251372461)) * f1( 0.34949688048464694)
w2 ( 0.2521403703858535 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.015819609309294186) - present_state_Q (0.015194005251372461)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39642785630781485 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018899481219564696) - present_state_Q ( 0.016571010300109934)) * f1( 0.34658102846767724)
w2 ( 0.25016328595818393 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.018899481219564696) - present_state_Q (0.016571010300109934)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3961674790705918 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.029132486445501718) - present_state_Q ( -0.029132486445501718)) * f1( 0.327564155278742)
w2 ( 0.25048124167223507 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.029132486445501718) - present_state_Q (-0.029132486445501718)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.39721263092095116 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017636082893623978) - present_state_Q ( 0.013503703459413069)) * f1( 0.3482627532917936)
w2 ( 0.24868061526505167 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.017636082893623978) - present_state_Q (0.013503703459413069)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39707484621151984 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028676208345627202) - present_state_Q ( -0.02541646756587948)) * f1( 0.3220396214918814)
w2 ( 0.24885175533631743 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.028676208345627202) - present_state_Q (-0.02541646756587948)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.398273564453753 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020448822830075347) - present_state_Q ( 0.020910717292375675)) * f1( 0.32278986248862696)
w2 ( 0.24662358453877498 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.020448822830075347) - present_state_Q (0.020910717292375675)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.39941799956319957 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019005842070026174) - present_state_Q ( 0.019021610460840277)) * f1( 0.3233655748644843)
w2 ( 0.24450010226656435 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.019005842070026174) - present_state_Q (0.019021610460840277)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40050946337287313 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017593410261493303) - present_state_Q ( 0.01719916425667048)) * f1( 0.32377880876750614)
w2 ( 0.2424774921757527 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.017593410261493303) - present_state_Q (0.01719916425667048)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4015758828290696 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016245663499698193) - present_state_Q ( 0.016245663499698193)) * f1( 0.32422398901624194)
w2 ( 0.24050400564978863 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.016245663499698193) - present_state_Q (0.016245663499698193)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4026081744824875 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0164342771422972) - present_state_Q ( 0.015157245579960399)) * f1( 0.32478176613090465)
w2 ( 0.2385969558808644 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0164342771422972) - present_state_Q (0.015157245579960399)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40240857865313356 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027481503796588633) - present_state_Q ( -0.027481503796588633)) * f1( 0.3088280486790274)
w2 ( 0.2388554762195547 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.027481503796588633) - present_state_Q (-0.027481503796588633)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4034730940834296 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016863227436031913) - present_state_Q ( 0.016863227436031913)) * f1( 0.3182669551866667)
w2 ( 0.2368486412410286 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.016863227436031913) - present_state_Q (0.016863227436031913)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40316159408571794 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014406410171313916) - present_state_Q ( -0.027168358300883974)) * f1( 0.3012964614287442)
w2 ( 0.23726218741576233 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.014406410171313916) - present_state_Q (-0.027168358300883974)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4042203312824157 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01701685678321363) - present_state_Q ( 0.01701685678321363)) * f1( 0.3152362449865801)
w2 ( 0.23524705645248842 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01701685678321363) - present_state_Q (0.01701685678321363)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40405292366840245 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026546098569329546) - present_state_Q ( -0.026546098569329546)) * f1( 0.29781770593268797)
w2 ( 0.23547190220299738 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.026546098569329546) - present_state_Q (-0.026546098569329546)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4051057568624437 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01723030280049906) - present_state_Q ( 0.01723030280049906)) * f1( 0.31169551700102377)
w2 ( 0.23344524515479007 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01723030280049906) - present_state_Q (0.01723030280049906)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40493472608369724 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026768895389073455) - present_state_Q ( -0.026768895389073455)) * f1( 0.293783478065501)
w2 ( 0.23367811159080978 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.026768895389073455) - present_state_Q (-0.026768895389073455)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40484203478516484 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023823962951706912) - present_state_Q ( -0.023823962951706912)) * f1( 0.2922889255323648)
w2 ( 0.23380496045908433 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.023823962951706912) - present_state_Q (-0.023823962951706912)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40585447306723177 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01724859586722667) - present_state_Q ( 0.016373675820192238)) * f1( 0.3075528797451259)
w2 ( 0.23182981078809578 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01724859586722667) - present_state_Q (0.016373675820192238)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40688151994752186 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020354372871205936) - present_state_Q ( 0.017559853399910036)) * f1( 0.3039071287978134)
w2 ( 0.22980212512434806 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.020354372871205936) - present_state_Q (0.017559853399910036)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4067920008728089 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023777221651005265) - present_state_Q ( -0.023777221651005265)) * f1( 0.28608070862732166)
w2 ( 0.22992729130579734 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.023777221651005265) - present_state_Q (-0.023777221651005265)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4077128180271897 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018497559282897702) - present_state_Q ( 0.013449567961093173)) * f1( 0.3082732892636057)
w2 ( 0.22813508188684875 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.018497559282897702) - present_state_Q (0.013449567961093173)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4076257312352924 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02374913688799138) - present_state_Q ( -0.02374913688799138)) * f1( 0.2805741233916262)
w2 ( 0.22825923701682954 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02374913688799138) - present_state_Q (-0.02374913688799138)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4086425396015977 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01793675975772234) - present_state_Q ( 0.01793675975772234)) * f1( 0.2954684853505815)
w2 ( 0.22619443129293215 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01793675975772234) - present_state_Q (0.01793675975772234)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4085480019122091 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024072866412401764) - present_state_Q ( -0.024072866412401764)) * f1( 0.2784422708859063)
w2 ( 0.2263302406857917 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.024072866412401764) - present_state_Q (-0.024072866412401764)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40953835750102446 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020956437045668047) - present_state_Q ( 0.01750100443754518)) * f1( 0.29408606849939384)
w2 ( 0.2243096983448326 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.020956437045668047) - present_state_Q (0.01750100443754518)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40949110473515665 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02220442234028075) - present_state_Q ( -0.02220442234028075)) * f1( 0.27574577754419444)
w2 ( 0.2243782437510958 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02220442234028075) - present_state_Q (-0.02220442234028075)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41054598335587983 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021544893044695368) - present_state_Q ( 0.021544893044695368)) * f1( 0.2801002787833847)
w2 ( 0.2221185988297019 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.021544893044695368) - present_state_Q (0.021544893044695368)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.410523979914469 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021206918531818678) - present_state_Q ( -0.021206918531818678)) * f1( 0.26968910602742585)
w2 ( 0.22215123409886045 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.021206918531818678) - present_state_Q (-0.021206918531818678)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41156757191189597 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021765356420466744) - present_state_Q ( 0.021765356420466744)) * f1( 0.27565108141107814)
w2 ( 0.21987968415517486 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.021765356420466744) - present_state_Q (0.021765356420466744)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4115680787029632 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020279083448954335) - present_state_Q ( -0.020279083448954335)) * f1( 0.26436888298712746)
w2 ( 0.21987891736135032 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.020279083448954335) - present_state_Q (-0.020279083448954335)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4125940048206968 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021686008118531747) - present_state_Q ( 0.021686008118531747)) * f1( 0.2714969947865019)
w2 ( 0.21761165222596923 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.021686008118531747) - present_state_Q (0.021686008118531747)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4125033189172182 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01819301370321827) - present_state_Q ( -0.01993813854501117)) * f1( 0.26006146770206207)
w2 ( 0.21775113602459564 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01819301370321827) - present_state_Q (-0.01993813854501117)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4135171717055662 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021745617994848185) - present_state_Q ( 0.021745617994848185)) * f1( 0.26792157733857613)
w2 ( 0.21548065195589347 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.021745617994848185) - present_state_Q (0.021745617994848185)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4134428260832312 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01828678039811471) - present_state_Q ( -0.019342816530905943)) * f1( 0.25626262704868497)
w2 ( 0.21559669794073524 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.01828678039811471) - present_state_Q (-0.019342816530905943)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4144481284853446 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022030553266992714) - present_state_Q ( 0.022030553266992714)) * f1( 0.2638738379537523)
w2 ( 0.21331082736733728 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.022030553266992714) - present_state_Q (0.022030553266992714)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4145854829458188 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013862211987693107) - present_state_Q ( -0.013862211987693107)) * f1( 0.2370487834600244)
w2 ( 0.2130790532009073 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013862211987693107) - present_state_Q (-0.013862211987693107)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41556903725523436 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02175445527490351) - present_state_Q ( 0.02175445527490351)) * f1( 0.25986025846070343)
w2 ( 0.21080809191908215 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.02175445527490351) - present_state_Q (0.02175445527490351)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4156597536096627 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016032073944921343) - present_state_Q ( -0.016032073944921343)) * f1( 0.23614958877855002)
w2 ( 0.2106544327831124 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.016032073944921343) - present_state_Q (-0.016032073944921343)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41679225222754346 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02855124384929361) - present_state_Q ( 0.02855124384929361)) * f1( 0.25758237169492254)
w2 ( 0.20801644491827018 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.02855124384929361) - present_state_Q (0.02855124384929361)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.41685251006933 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.027471179428668765) - present_state_Q ( -0.012941693689767608)) * f1( 0.23341880342513566)
w2 ( 0.20791318358558863 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.027471179428668765) - present_state_Q (-0.012941693689767608)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41788636845923904 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02888688437112963) - present_state_Q ( 0.02888688437112963)) * f1( 0.23354245910778662)
w2 ( 0.20525707113256728 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.02888688437112963) - present_state_Q (0.02888688437112963)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4189927559613024 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01193920904356853) - present_state_Q ( 0.02911220518294494)) * f1( 0.22776201696933343)
w2 ( 0.20331401229108828 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01193920904356853) - present_state_Q (0.02911220518294494)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4199825453091017 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.027882651362372987) - present_state_Q ( 0.027882651362372987)) * f1( 0.2282475460960294)
w2 ( 0.20071212842053976 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.027882651362372987) - present_state_Q (0.027882651362372987)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4201515645717754 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011844969410020348) - present_state_Q ( -0.011844969410020348)) * f1( 0.22210524959963515)
w2 ( 0.2002555360717005 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.011844969410020348) - present_state_Q (-0.011844969410020348)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.42036060199160263 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009712417187545841) - present_state_Q ( -0.009712417187545841)) * f1( 0.21936583271676185)
w2 ( 0.19987436929246521 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009712417187545841) - present_state_Q (-0.009712417187545841)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.42146281732992413 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02888141914688712) - present_state_Q ( 0.0454851908724308)) * f1( 0.18108469371927904)
w2 ( 0.1962223256580203 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.02888141914688712) - present_state_Q (0.0454851908724308)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4225150446000301 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03635538149100974) - present_state_Q ( 0.044784627834573404)) * f1( 0.17708469906638727)
w2 ( 0.19265715957991159 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03635538149100974) - present_state_Q (0.044784627834573404)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.42353370847221067 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03570317157629384) - present_state_Q ( 0.044153460736999314)) * f1( 0.173084705365916)
w2 ( 0.18912595026816903 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03570317157629384) - present_state_Q (0.044153460736999314)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.42390599751391717 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03678009828242072) - present_state_Q ( 0.007425565570324491)) * f1( 0.16908471289720925)
w2 ( 0.18780487622666373 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03678009828242072) - present_state_Q (0.007425565570324491)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4242919172968356 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039311281053457145) - present_state_Q ( 0.010228399544095643)) * f1( 0.15708474799380828)
w2 ( 0.18633081924335837 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.039311281053457145) - present_state_Q (0.010228399544095643)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4251832581626951 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03496571965475187) - present_state_Q ( 0.04345154114265669)) * f1( 0.1530847672058222)
w2 ( 0.18516631296082128 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03496571965475187) - present_state_Q (0.04345154114265669)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.42610839483657953 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03573979724589631) - present_state_Q ( 0.04419774675585254)) * f1( 0.15708474799380828)
w2 ( 0.18281054848158385 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03573979724589631) - present_state_Q (0.04419774675585254)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4270074714820026 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04434541651134416) - present_state_Q ( 0.039207591178776646)) * f1( 0.16949832383135846)
w2 ( 0.17962794481294495 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.04434541651134416) - present_state_Q (0.039207591178776646)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4273565103723771 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03549649228646337) - present_state_Q ( 0.008041214917485329)) * f1( 0.15334340597224236)
w2 ( 0.17826223017463425 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03549649228646337) - present_state_Q (0.008041214917485329)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.42771140837405597 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03631879077079429) - present_state_Q ( 0.009110818908005809)) * f1( 0.14943523771663583)
w2 ( 0.17683727308779829 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03631879077079429) - present_state_Q (0.009110818908005809)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.42855858202367053 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03513476067107528) - present_state_Q ( 0.04345529767494023)) * f1( 0.145532059655895)
w2 ( 0.17450878642549805 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03513476067107528) - present_state_Q (0.04345529767494023)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.42938750819767757 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04359355182494362) - present_state_Q ( 0.0382889925428727)) * f1( 0.15879817144064656)
w2 ( 0.17137678748689497 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.04359355182494362) - present_state_Q (0.0382889925428727)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.42972630420142893 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03491891649825106) - present_state_Q ( 0.008980317503993453)) * f1( 0.1425982878274967)
w2 ( 0.1699512612386645 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03491891649825106) - present_state_Q (0.008980317503993453)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4300692764418596 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03569224603465995) - present_state_Q ( 0.010020553824650301)) * f1( 0.13873341993763838)
w2 ( 0.16846796078841306 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03569224603465995) - present_state_Q (0.010020553824650301)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43040075645842457 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.035132488819435204) - present_state_Q ( 0.009819474613655697)) * f1( 0.1348764320548725)
w2 ( 0.16748489796115765 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.035132488819435204) - present_state_Q (0.009819474613655697)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.43069932830612373 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.034603139169046865) - present_state_Q ( 0.006264852779395716)) * f1( 0.14167188315734722)
w2 ( 0.16622040493242782 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.034603139169046865) - present_state_Q (0.006264852779395716)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43145584273090704 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03368734100703864) - present_state_Q ( 0.038714258043723616)) * f1( 0.14109897692741066)
w2 ( 0.16407577017672012 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03368734100703864) - present_state_Q (0.038714258043723616)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.43220225677306223 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04236559438407545) - present_state_Q ( 0.03703667086550135)) * f1( 0.146153783441567)
w2 ( 0.16101154279411412 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.04236559438407545) - present_state_Q (0.03703667086550135)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4325246076349052 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.033707641060358715) - present_state_Q ( 0.00990079198196523)) * f1( 0.1299782322264966)
w2 ( 0.15952352042457799 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.033707641060358715) - present_state_Q (0.00990079198196523)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43283553435346117 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03296135183494095) - present_state_Q ( 0.00968153304020461)) * f1( 0.1261072201302709)
w2 ( 0.15853729071232264 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.03296135183494095) - present_state_Q (0.00968153304020461)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.43311578698140824 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032114282830638094) - present_state_Q ( 0.005985778997128384)) * f1( 0.13317019757618823)
w2 ( 0.1572746089724984 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.032114282830638094) - present_state_Q (0.005985778997128384)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43380313359625555 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0315262338069974) - present_state_Q ( 0.036686650457818666)) * f1( 0.13268119812201698)
w2 ( 0.15520243409142673 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0315262338069974) - present_state_Q (0.036686650457818666)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.43448193695983084 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04052254894135808) - present_state_Q ( 0.03520427822086515)) * f1( 0.13734739698813528)
w2 ( 0.1522370919948426 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.04052254894135808) - present_state_Q (0.03520427822086515)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43489564138718845 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03187858137508357) - present_state_Q ( 0.009902110838719044)) * f1( 0.12125064906159078)
w2 ( 0.1501899057872994 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.03187858137508357) - present_state_Q (0.009902110838719044)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4352916049318321 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030928244431273934) - present_state_Q ( 0.009425257796119253)) * f1( 0.11736443249177565)
w2 ( 0.14884038775619937 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.030928244431273934) - present_state_Q (0.009425257796119253)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4356631000254583 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03035000338016216) - present_state_Q ( 0.0054133206550213975)) * f1( 0.12473043160196699)
w2 ( 0.1470533574917085 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.03035000338016216) - present_state_Q (0.0054133206550213975)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4363892639034079 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028941870682189216) - present_state_Q ( 0.03389156799579878)) * f1( 0.1243369590275791)
w2 ( 0.14471724155762494 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.028941870682189216) - present_state_Q (0.03389156799579878)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.43710975243246375 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03771163844158693) - present_state_Q ( 0.03240860284334007)) * f1( 0.12856005018880626)
w2 ( 0.1413546641722035 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.03771163844158693) - present_state_Q (0.03240860284334007)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4374739914677825 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.027340516529563236) - present_state_Q ( 0.0076889149991278966)) * f1( 0.11255709192667071)
w2 ( 0.14006024894137625 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.027340516529563236) - present_state_Q (0.0076889149991278966)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4378136621235923 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.026759381924978794) - present_state_Q ( 0.003493749364559137)) * f1( 0.12035102615468389)
w2 ( 0.138366849225582 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.026759381924978794) - present_state_Q (0.003493749364559137)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43814572366921006 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025889796881025412) - present_state_Q ( 0.002837625844860546)) * f1( 0.12007651023034574)
w2 ( 0.13670759941070598 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.025889796881025412) - present_state_Q (0.002837625844860546)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43847008023943884 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024988749920862557) - present_state_Q ( 0.0021377556627131603)) * f1( 0.11993484531907832)
w2 ( 0.1350849355249978 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.024988749920862557) - present_state_Q (0.0021377556627131603)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43911150879422634 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023834992648917794) - present_state_Q ( 0.02846312006443774)) * f1( 0.11992650221833437)
w2 ( 0.1329455299960356 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.023834992648917794) - present_state_Q (0.02846312006443774)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.43974630857107655 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032949329040332286) - present_state_Q ( 0.027939019586320453)) * f1( 0.12196053894045672)
w2 ( 0.1298225537496278 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.032949329040332286) - present_state_Q (0.027939019586320453)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44008530102184407 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02401245043293656) - present_state_Q ( 0.006790234354681832)) * f1( 0.10661981756262423)
w2 ( 0.12791488334547396 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.02401245043293656) - present_state_Q (0.006790234354681832)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.440406347365113 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022973776629828975) - present_state_Q ( 0.006168496865578479)) * f1( 0.10264733612396765)
w2 ( 0.12666381788038977 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.022973776629828975) - present_state_Q (0.006168496865578479)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.440702855488433 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02227409055548886) - present_state_Q ( 0.0013129996451601011)) * f1( 0.11192741472420062)
w2 ( 0.12507435139954254 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.02227409055548886) - present_state_Q (0.0013129996451601011)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44099192966944617 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021316766397027953) - present_state_Q ( 0.0005222249100076681)) * f1( 0.11206134471554462)
w2 ( 0.12352658745785369 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.021316766397027953) - present_state_Q (0.0005222249100076681)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.441554418682906 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020320318406602554) - present_state_Q ( 0.02469780382725121)) * f1( 0.11233763301792074)
w2 ( 0.12152373588140969 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.020320318406602554) - present_state_Q (0.02469780382725121)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4421105223015227 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029639287738167847) - present_state_Q ( 0.024850802639571012)) * f1( 0.11281733429039856)
w2 ( 0.11856619240399388 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.029639287738167847) - present_state_Q (0.024850802639571012)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44241625206669427 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020742212921611394) - present_state_Q ( 0.005863729375335296)) * f1( 0.09800593530410363)
w2 ( 0.11669449087353288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.020742212921611394) - present_state_Q (0.005863729375335296)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4427036615910782 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019619285107412558) - present_state_Q ( 0.005128686096709245)) * f1( 0.09400985837183072)
w2 ( 0.1154715998731138 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.019619285107412558) - present_state_Q (0.005128686096709245)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.44296779764719096 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018825616097128642) - present_state_Q ( -0.00021532649079954863)) * f1( 0.10437012994429049)
w2 ( 0.113953142113674 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.018825616097128642) - present_state_Q (-0.00021532649079954863)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4434694715683891 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017944906156903223) - present_state_Q ( 0.022249727104563077)) * f1( 0.10481947737515673)
w2 ( 0.11108149687887107 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.017944906156903223) - present_state_Q (0.022249727104563077)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4439584570253244 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.026805806906799207) - present_state_Q ( 0.02205845739484346)) * f1( 0.10452115885187432)
w2 ( 0.10827449323115071 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.026805806906799207) - present_state_Q (0.02205845739484346)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44423523082567656 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017975966049465456) - present_state_Q ( 0.005198105326053248)) * f1( 0.08984404513711625)
w2 ( 0.10642613166241376 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.017975966049465456) - present_state_Q (0.005198105326053248)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44450783633863733 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018614236678898527) - present_state_Q ( 0.006211390365506671)) * f1( 0.08584517619456693)
w2 ( 0.10452080261508619 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.018614236678898527) - present_state_Q (0.006211390365506671)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4447622960447577 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017444818952855448) - present_state_Q ( 0.0054288658970591125)) * f1( 0.08184641779059298)
w2 ( 0.10327720655803488 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.017444818952855448) - present_state_Q (0.0054288658970591125)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4451962109084413 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016751119137811667) - present_state_Q ( 0.021134121447125828)) * f1( 0.09258919098315689)
w2 ( 0.10046533494056364 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.016751119137811667) - present_state_Q (0.021134121447125828)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44561893969547184 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025695147107749797) - present_state_Q ( 0.02080664027969071)) * f1( 0.09261707020152754)
w2 ( 0.09772677636095814 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.025695147107749797) - present_state_Q (0.02080664027969071)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44586729156019284 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016896111400619876) - present_state_Q ( 0.006254119440219655)) * f1( 0.07768272282915034)
w2 ( 0.09580857481747812 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.016896111400619876) - present_state_Q (0.006254119440219655)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44610975846616896 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01750391481612458) - present_state_Q ( 0.007249582469001051)) * f1( 0.07368760207230769)
w2 ( 0.09383429231276424 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.01750391481612458) - present_state_Q (0.007249582469001051)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4463345864683747 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.016332161051686833) - present_state_Q ( 0.006487447830127089)) * f1( 0.06969304105994324)
w2 ( 0.09254390234678554 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.016332161051686833) - present_state_Q (0.006487447830127089)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.44669763232235465 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019814166061431666) - present_state_Q ( 0.019814166061431666)) * f1( 0.08025193691540146)
w2 ( 0.08982960633399768 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.019814166061431666) - present_state_Q (0.019814166061431666)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44705781538829736 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024381503794590316) - present_state_Q ( 0.019331231595914204)) * f1( 0.08130800454026688)
w2 ( 0.08717169041553982 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.024381503794590316) - present_state_Q (0.019331231595914204)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4472755615340179 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015628074893080116) - present_state_Q ( 0.007110885039748112)) * f1( 0.06607659828496293)
w2 ( 0.08519447471704286 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.015628074893080116) - present_state_Q (0.007110885039748112)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.447476457784954 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014494311711006846) - present_state_Q ( 0.0063996003550797625)) * f1( 0.062089935956848134)
w2 ( 0.08390024725270333 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.014494311711006846) - present_state_Q (0.0063996003550797625)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4478064264864014 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015491020849863826) - present_state_Q ( 0.01971739416061356)) * f1( 0.07240314230181952)
w2 ( 0.08116581868269514 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.015491020849863826) - present_state_Q (0.01971739416061356)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4479948171199464 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013806875784940748) - present_state_Q ( 0.006522205494928865)) * f1( 0.057882578726912055)
w2 ( 0.07986393726905738 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.013806875784940748) - present_state_Q (0.006522205494928865)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.44829130262454386 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01725036167689989) - present_state_Q ( 0.01725036167689989)) * f1( 0.0690611887251385)
w2 ( 0.07728808669303423 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.01725036167689989) - present_state_Q (0.01725036167689989)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4485841975755642 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02208768704055904) - present_state_Q ( 0.017060101876897455)) * f1( 0.06931301005161526)
w2 ( 0.07475267565719318 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.02208768704055904) - present_state_Q (0.017060101876897455)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44876545341879853 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01338145572278649) - present_state_Q ( 0.007391498266453279)) * f1( 0.054172732848223444)
w2 ( 0.07274514345007214 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.01338145572278649) - present_state_Q (0.007391498266453279)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4489300415421468 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012188569080632504) - present_state_Q ( 0.006613443925504281)) * f1( 0.05017914611812033)
w2 ( 0.07143313927239413 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.012188569080632504) - present_state_Q (0.006613443925504281)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.44919601471724396 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013329798263257992) - present_state_Q ( 0.01745856205873395)) * f1( 0.06109957643924351)
w2 ( 0.06882127329297909 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.013329798263257992) - present_state_Q (0.01745856205873395)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.44934773031599784 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01170396469373378) - present_state_Q ( 0.006905309383717442)) * f1( 0.04577961034380569)
w2 ( 0.06749565607942495 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.01170396469373378) - present_state_Q (0.006905309383717442)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4495833525703106 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015167694674355286) - present_state_Q ( 0.015167694674355286)) * f1( 0.057389836822450406)
w2 ( 0.06503226952153922 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.015167694674355286) - present_state_Q (0.015167694674355286)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4498146667013286 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020178947795126866) - present_state_Q ( 0.015063809164218844)) * f1( 0.05718317514912833)
w2 ( 0.0626051836129863 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.020178947795126866) - present_state_Q (0.015063809164218844)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4499581597587824 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011523879135525536) - present_state_Q ( 0.007988204104174442)) * f1( 0.04190638690263827)
w2 ( 0.06055070359607843 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.011523879135525536) - present_state_Q (0.007988204104174442)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45008536542173355 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01029248317247957) - present_state_Q ( 0.007175631484154368)) * f1( 0.03791310200283806)
w2 ( 0.059208627572421804 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.01029248317247957) - present_state_Q (0.007175631484154368)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.450289645346535 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011201503540808178) - present_state_Q ( 0.015192337557354937)) * f1( 0.049250537519999596)
w2 ( 0.0567199652947548 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.011201503540808178) - present_state_Q (0.015192337557354937)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4504427605260623 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009599034941629832) - present_state_Q ( 0.018532210556798465)) * f1( 0.03404237116249595)
w2 ( 0.05492085231526901 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.009599034941629832) - present_state_Q (0.018532210556798465)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4506272294402982 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014827209121001095) - present_state_Q ( 0.01364034428399392)) * f1( 0.04662645849711528)
w2 ( 0.05254706386748483 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.014827209121001095) - present_state_Q (0.01364034428399392)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45073510279151 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010660426667272009) - present_state_Q ( 0.009052818861967932)) * f1( 0.03047933326130473)
w2 ( 0.05042352625029983 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.010660426667272009) - present_state_Q (0.009052818861967932)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45082775714071355 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009517035723511213) - present_state_Q ( 0.00843883979262136)) * f1( 0.026554113695899797)
w2 ( 0.04902782010450865 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.009517035723511213) - present_state_Q (0.00843883979262136)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4509698375139223 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012239621848804935) - present_state_Q ( 0.012239621848804935)) * f1( 0.036979703376004966)
w2 ( 0.04672254947920263 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.012239621848804935) - present_state_Q (0.012239621848804935)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45111948896009396 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00906265682861411) - present_state_Q ( 0.012778125011379255)) * f1( 0.03810118153070799)
w2 ( 0.044365906874021005 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.00906265682861411) - present_state_Q (0.012778125011379255)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45121476463869736 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00764882960427898) - present_state_Q ( 0.016522010979083186)) * f1( 0.022073642063679278)
w2 ( 0.04263940105629442 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.00764882960427898) - present_state_Q (0.016522010979083186)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4513464089068535 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012789054086651342) - present_state_Q ( 0.011351914706452176)) * f1( 0.035125251622499756)
w2 ( 0.040390689452956645 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.012789054086651342) - present_state_Q (0.011351914706452176)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45141296887472165 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00953665295871159) - present_state_Q ( 0.00953665295871159)) * f1( 0.018494785406227513)
w2 ( 0.03823137914771567 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.00953665295871159) - present_state_Q (0.00953665295871159)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4514650719093525 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0090221634431778) - present_state_Q ( 0.0090221634431778)) * f1( 0.014666390807237353)
w2 ( 0.0368103605667809 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.0090221634431778) - present_state_Q (0.0090221634431778)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4515577001391851 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009765341374232163) - present_state_Q ( 0.009765341374232163)) * f1( 0.025591921026130993)
w2 ( 0.03463870108710181 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.009765341374232163) - present_state_Q (0.009765341374232163)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45165849305962585 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009393326377026261) - present_state_Q ( 0.010466649286193343)) * f1( 0.02729087083908505)
w2 ( 0.032422731042721815 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.009393326377026261) - present_state_Q (0.010466649286193343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45169913491206454 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008042635110934374) - present_state_Q ( 0.01452718131947874)) * f1( 0.00988169187778014)
w2 ( 0.030777593633406035 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.008042635110934374) - present_state_Q (0.01452718131947874)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4517854227900692 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010728362632133536) - present_state_Q ( 0.00879197331467887)) * f1( 0.02456618557306536)
w2 ( 0.02867011436484755 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.010728362632133536) - present_state_Q (0.00879197331467887)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4518110400889656 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008206671532511199) - present_state_Q ( 0.008206671532511199)) * f1( 0.0073630866280799965)
w2 ( 0.026582623056621395 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.008206671532511199) - present_state_Q (0.008206671532511199)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45182313785525025 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008757736382653238) - present_state_Q ( 0.01407426099397752)) * f1( 0.0029794514974919566)
w2 ( 0.024958462865412537 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.008757736382653238) - present_state_Q (0.01407426099397752)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4518852531271825 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010363090384583436) - present_state_Q ( 0.00783176906093292)) * f1( 0.018161841142841858)
w2 ( 0.02290640421859351 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.010363090384583436) - present_state_Q (0.00783176906093292)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4518933340751185 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002630978740118299) - present_state_Q ( 0.001404135791120132)) * f1( 0.0041629944612761125)
w2 ( 0.022129948903922263 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.002630978740118299) - present_state_Q (0.001404135791120132)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4519072000006794 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0025292200885356888) - present_state_Q ( 0.001704079156841197)) * f1( 0.007030866864124833)
w2 ( 0.021735518861969054 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0025292200885356888) - present_state_Q (0.001704079156841197)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4519321843388866 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004715729400213997) - present_state_Q ( 0.004715729400213997)) * f1( 0.011096998220102572)
w2 ( 0.020834938805574438 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.004715729400213997) - present_state_Q (0.004715729400213997)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4521818814304858 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002995041547657829) - present_state_Q ( 5.363790539575017e-05)) * f1( 0.13853221263759544)
w2 ( 0.02047444923156838 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.002995041547657829) - present_state_Q (5.363790539575017e-05)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45221155079553765 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004848303347919308) - present_state_Q ( 0.004848303347919308)) * f1( 0.013108422582792642)
w2 ( 0.019569096513056373 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.004848303347919308) - present_state_Q (0.004848303347919308)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.45223624865497947 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005312313522882235) - present_state_Q ( 0.0016334720953572615)) * f1( 0.012748870921790941)
w2 ( 0.0187941930853467 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.005312313522882235) - present_state_Q (0.0016334720953572615)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.452272488977032 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007934637750476498) - present_state_Q ( 0.006094713296407876)) * f1( 0.015374573874085697)
w2 ( 0.017379897417084716 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.007934637750476498) - present_state_Q (0.006094713296407876)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4522931169725224 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004436294929559504) - present_state_Q ( 0.004436294929559504)) * f1( 0.009265591280121468)
w2 ( 0.01648937700163366 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.004436294929559504) - present_state_Q (0.004436294929559504)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.45232468278803756 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007616222912051197) - present_state_Q ( 0.003022458090796566)) * f1( 0.015374573874085697)
w2 ( 0.015257506156677805 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.007616222912051197) - present_state_Q (0.003022458090796566)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4523537219979129 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005690874121560061) - present_state_Q ( 0.005690874121560061)) * f1( 0.012414093037161964)
w2 ( 0.013853978257133193 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.005690874121560061) - present_state_Q (0.005690874121560061)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45238573511976454 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003357006530797132) - present_state_Q ( 0.0030924429520747523)) * f1( 0.015224705862994672)
w2 ( 0.01259235302221312 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.003357006530797132) - present_state_Q (0.0030924429520747523)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4524088550772102 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003149770134317477) - present_state_Q ( 0.003149770134317477)) * f1( 0.0109546582298391)
w2 ( 0.011326044737979608 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.003149770134317477) - present_state_Q (0.003149770134317477)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.45241518312653844 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0007783099808968234) - present_state_Q ( -0.0020713680565770574)) * f1( 0.008860784623344614)
w2 ( 0.011183212029652627 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0007783099808968234) - present_state_Q (-0.0020713680565770574)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45242880450239653 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0008818584500724499) - present_state_Q ( 0.0008818584500724499)) * f1( 0.01371899324493112)
w2 ( 0.010786058226456563 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0008818584500724499) - present_state_Q (0.0008818584500724499)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4524406415516565 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(7.547540949445677e-05) - present_state_Q ( 0.00062875519103166)) * f1( 0.012132624096600665)
w2 ( 0.010395803021459819 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 7.547540949445677e-05) - present_state_Q (0.00062875519103166)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4524529401607175 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0013100374380845881) - present_state_Q ( 0.0013100374380845881)) * f1( 0.011923951159612556)
w2 ( 0.009983234774695317 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0013100374380845881) - present_state_Q (0.0013100374380845881)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.45246496052355023 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0002926789395953158) - present_state_Q ( -0.0004143067886112914)) * f1( 0.01373734780455016)
w2 ( 0.00963322943148793 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0002926789395953158) - present_state_Q (-0.0004143067886112914)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.452470666807168 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001515571313158342) - present_state_Q ( -0.0037394582891130145)) * f1( 0.010286649582544057)
w2 ( 0.009411338578806362 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001515571313158342) - present_state_Q (-0.0037394582891130145)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4524812814361681 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0012018628727003054) - present_state_Q ( 0.0022342888836410393)) * f1( 0.009238428651040772)
w2 ( 0.008951752672976463 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0012018628727003054) - present_state_Q (0.0022342888836410393)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4524919443904076 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017155802330339307) - present_state_Q ( 0.002312448873509518)) * f1( 0.00917702868540697)
w2 ( 0.008486985498110491 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0017155802330339307) - present_state_Q (0.002312448873509518)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.45249286027782554 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003975886821643353) - present_state_Q ( 0.0006471219701326253)) * f1( 0.008766900346531527)
w2 ( 0.008466091285064552 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.003975886821643353) - present_state_Q (0.0006471219701326253)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4524936032131542 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003578342487503483) - present_state_Q ( 0.0005244864088227103)) * f1( 0.00842024180509499)
w2 ( 0.008448444871913091 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.003578342487503483) - present_state_Q (0.0005244864088227103)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4525174051615568 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0031996129527555634) - present_state_Q ( -0.0031996129527555634)) * f1( 0.007071067811865476)
w2 ( 0.008448444871913091 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0031996129527555634) - present_state_Q (-0.0031996129527555634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45253917029111507 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014527637628422622) - present_state_Q ( -0.005905424867568073)) * f1( 0.007071067811865476)
w2 ( 0.008448444871913091 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0014527637628422622) - present_state_Q (-0.005905424867568073)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45260516712665166 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005455355669664895) - present_state_Q ( -0.00884277894105657)) * f1( 0.023367132434632933)
w2 ( 0.008448444871913091 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005455355669664895) - present_state_Q (-0.00884277894105657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4526582401161646 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005119920117275251) - present_state_Q ( -0.008771556721949857)) * f1( 0.018766222757423595)
w2 ( 0.008448444871913091 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005119920117275251) - present_state_Q (-0.008771556721949857)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45270874812858947 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007283595225872749) - present_state_Q ( -0.009177428551072139)) * f1( 0.017979743019116638)
w2 ( 0.008448444871913091 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.007283595225872749) - present_state_Q (-0.009177428551072139)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45273516815614157 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002711347132494058) - present_state_Q ( -0.002711347132494058)) * f1( 0.010582697686468714)
w2 ( 0.0079491387718078 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002711347132494058) - present_state_Q (-0.002711347132494058)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4527594019113402 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0035836036607740404) - present_state_Q ( 0.00031331497354823105)) * f1( 0.008631117577232183)
w2 ( 0.007387594916525102 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0035836036607740404) - present_state_Q (0.00031331497354823105)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45277589108769345 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004015077677838641) - present_state_Q ( 0.00044406121676684464)) * f1( 0.008625889617409288)
w2 ( 0.007005276637840631 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004015077677838641) - present_state_Q (0.00044406121676684464)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45278736575595313 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0005452393949207303) - present_state_Q ( -0.0043899270880998955)) * f1( 0.008299404236977153)
w2 ( 0.006728758759399015 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0005452393949207303) - present_state_Q (-0.0043899270880998955)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45279937077853377 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003791383017200203) - present_state_Q ( -0.003791383017200203)) * f1( 0.008079783008198747)
w2 ( 0.006431596754715162 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003791383017200203) - present_state_Q (-0.003791383017200203)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45277566012162757 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007268265389733072) - present_state_Q ( -0.011911760204703406)) * f1( 0.020931626156445903)
w2 ( 0.006431596754715162 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( -0.007268265389733072) - present_state_Q (-0.011911760204703406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45277475385092686 ) += alpha ( 0.1 ) * (reward ( 0.0011424541260339233 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0025531337548680114) - present_state_Q ( -0.0012812900056962816)) * f1( 0.0041793850141875885)
w2 ( 0.00647496536984003 ) += alpha ( 0.1) * (reward ( 0.0011424541260339233) + discount_factor ( 0.1) * next_state_max_Q( -0.0025531337548680114) - present_state_Q (-0.0012812900056962816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45276366344272834 ) += alpha ( 0.1 ) * (reward ( 0.0005712270630169617 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019803197719309037) - present_state_Q ( -0.0070144706545372335)) * f1( 0.015012060085442366)
w2 ( 0.00647496536984003 ) += alpha ( 0.1) * (reward ( 0.0005712270630169617) + discount_factor ( 0.1) * next_state_max_Q( -0.0019803197719309037) - present_state_Q (-0.0070144706545372335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45274671021948826 ) += alpha ( 0.1 ) * (reward ( 1.7850845719280052e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004960015995985926) - present_state_Q ( -0.01005546016017946)) * f1( 0.017701446743434247)
w2 ( 0.006666511557966033 ) += alpha ( 0.1) * (reward ( 1.7850845719280052e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.004960015995985926) - present_state_Q (-0.01005546016017946)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4525350064539272 ) += alpha ( 0.1 ) * (reward ( 0.0005709526377939061 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03195702909108343) - present_state_Q ( -0.0330840486324958)) * f1( 0.06950382213362173)
w2 ( 0.006666511557966033 ) += alpha ( 0.1) * (reward ( 0.0005709526377939061) + discount_factor ( 0.1) * next_state_max_Q( -0.03195702909108343) - present_state_Q (-0.0330840486324958)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4523328322581857 ) += alpha ( 0.1 ) * (reward ( 7.136907972423827e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03058764541546456) - present_state_Q ( -0.03058764541546456)) * f1( 0.07325085681508746)
w2 ( 0.006666511557966033 ) += alpha ( 0.1) * (reward ( 7.136907972423827e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.03058764541546456) - present_state_Q (-0.03058764541546456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.45182853055807737 ) += alpha ( 0.1 ) * (reward ( 2.787854676728057e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04960408517461498) - present_state_Q ( -0.051936671695730734)) * f1( 0.10735181412408956)
w2 ( 0.006666511557966033 ) += alpha ( 0.1) * (reward ( 2.787854676728057e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.04960408517461498) - present_state_Q (-0.051936671695730734)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.449643614322185 ) += alpha ( 0.1 ) * (reward ( 0.0011418966954802672 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08479947858901281) - present_state_Q ( -0.1124877813762209)) * f1( 0.2077909502450056)
w2 ( 0.006666511557966033 ) += alpha ( 0.1) * (reward ( 0.0011418966954802672) + discount_factor ( 0.1) * next_state_max_Q( -0.08479947858901281) - present_state_Q (-0.1124877813762209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.44659838895066467 ) += alpha ( 0.1 ) * (reward ( 1.7842135866879175e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11679145620203184) - present_state_Q ( -0.12374065557650232)) * f1( 0.2717026209266617)
w2 ( 0.008908098599809353 ) += alpha ( 0.1) * (reward ( 1.7842135866879175e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.11679145620203184) - present_state_Q (-0.12374065557650232)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.443021582368779 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1264110493341836) - present_state_Q ( -0.13229569293273177)) * f1( 0.2933290198779805)
w2 ( 0.011346866222169804 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.1264110493341836) - present_state_Q (-0.13229569293273177)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4390594822945895 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12076515350597937) - present_state_Q ( -0.13872282015596188)) * f1( 0.31249546613711415)
w2 ( 0.013882647059675469 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( -0.12076515350597937) - present_state_Q (-0.13872282015596188)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4338965671077071 ) += alpha ( 0.1 ) * (reward ( 1.3612467758113754e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15139395632475036) - present_state_Q ( -0.1597344496492691)) * f1( 0.35706028931129985)
w2 ( 0.016774548142733846 ) += alpha ( 0.1) * (reward ( 1.3612467758113754e-10) + discount_factor ( 0.1) * next_state_max_Q( -0.15139395632475036) - present_state_Q (-0.1597344496492691)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.42708811348057757 ) += alpha ( 0.1 ) * (reward ( 5.317370218013185e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17306167865031744) - present_state_Q ( -0.18213294251713372)) * f1( 0.41306721201564756)
w2 ( 0.02007108363578652 ) += alpha ( 0.1) * (reward ( 5.317370218013185e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.17306167865031744) - present_state_Q (-0.18213294251713372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4193171236457784 ) += alpha ( 0.1 ) * (reward ( 4.154195482822801e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19373852406208042) - present_state_Q ( -0.19373852406208042)) * f1( 0.4456745601618142)
w2 ( 0.02007108363578652 ) += alpha ( 0.1) * (reward ( 4.154195482822801e-15) + discount_factor ( 0.1) * next_state_max_Q( -0.19373852406208042) - present_state_Q (-0.19373852406208042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.40762455792104374 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22375025645755484) - present_state_Q ( -0.22756404095527505)) * f1( 0.557434933561228)
w2 ( 0.024266215666725276 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( -0.22375025645755484) - present_state_Q (-0.22756404095527505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3970920971604653 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21710248696542378) - present_state_Q ( -0.21434027492269808)) * f1( 0.5435493754292273)
w2 ( 0.02814165412243548 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( -0.21710248696542378) - present_state_Q (-0.21434027492269808)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.385352073819956 ) += alpha ( 0.1 ) * (reward ( 0.009277909544755727 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20822198467156175) - present_state_Q ( -0.21252940693095754)) * f1( 0.5841240116101262)
w2 ( 0.036181058842777766 ) += alpha ( 0.1) * (reward ( 0.009277909544755727) + discount_factor ( 0.1) * next_state_max_Q( -0.20822198467156175) - present_state_Q (-0.21252940693095754)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.373321385926414 ) += alpha ( 0.1 ) * (reward ( 3.624183415920206e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20722488916309117) - present_state_Q ( -0.21195036830304606)) * f1( 0.6290091323321143)
w2 ( 0.05148218854044946 ) += alpha ( 0.1) * (reward ( 3.624183415920206e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.20722488916309117) - present_state_Q (-0.21195036830304606)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3640411850657463 ) += alpha ( 0.1 ) * (reward ( 0.006851450140959649 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17284493057677353) - present_state_Q ( -0.1693258893274594)) * f1( 0.5840540383220401)
w2 ( 0.0641936162533088 ) += alpha ( 0.1) * (reward ( 0.006851450140959649) + discount_factor ( 0.1) * next_state_max_Q( -0.17284493057677353) - present_state_Q (-0.1693258893274594)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.354682512601739 ) += alpha ( 0.1 ) * (reward ( 0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11890300882771329) - present_state_Q ( -0.1363037743018475)) * f1( 0.6164362186842124)
w2 ( 0.08544827497141076 ) += alpha ( 0.1) * (reward ( 0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.11890300882771329) - present_state_Q (-0.1363037743018475)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3502605188353072 ) += alpha ( 0.1 ) * (reward ( 0.0008564224195159132 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08857571659183222) - present_state_Q ( -0.08636848023999508)) * f1( 0.5642649443316275)
w2 ( 0.09641970131145665 ) += alpha ( 0.1) * (reward ( 0.0008564224195159132) + discount_factor ( 0.1) * next_state_max_Q( -0.08857571659183222) - present_state_Q (-0.08636848023999508)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3470993351037113 ) += alpha ( 0.1 ) * (reward ( 1.3381600304936144e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06548772133180525) - present_state_Q ( -0.061266892521399524)) * f1( 0.5775802995976005)
w2 ( 0.10408211158985001 ) += alpha ( 0.1) * (reward ( 1.3381600304936144e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.06548772133180525) - present_state_Q (-0.061266892521399524)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3437069064185358 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05573955739944908) - present_state_Q ( -0.05573955739944908)) * f1( 0.5720715681536068)
w2 ( 0.11238421996865769 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.05573955739944908) - present_state_Q (-0.05573955739944908)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3444058114384188 ) += alpha ( 0.1 ) * (reward ( 0.0022882536521440806 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019816275488478197) - present_state_Q ( 0.015830209596178813)) * f1( 0.6045719429337169)
w2 ( 0.1100721542896203 ) += alpha ( 0.1) * (reward ( 0.0022882536521440806) + discount_factor ( 0.1) * next_state_max_Q( 0.019816275488478197) - present_state_Q (0.015830209596178813)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34483461870967513 ) += alpha ( 0.1 ) * (reward ( 0.0011441268260720403 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013402157663415293) - present_state_Q ( 0.009538294394946467)) * f1( 0.6078965142664609)
w2 ( 0.10866136392911373 ) += alpha ( 0.1) * (reward ( 0.0011441268260720403) + discount_factor ( 0.1) * next_state_max_Q( 0.013402157663415293) - present_state_Q (0.009538294394946467)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3451557340661509 ) += alpha ( 0.1 ) * (reward ( 0.0005720634130360201 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010155111739597489) - present_state_Q ( 0.006838801402181072)) * f1( 0.611505402027606)
w2 ( 0.10761111856607666 ) += alpha ( 0.1) * (reward ( 0.0005720634130360201) + discount_factor ( 0.1) * next_state_max_Q( 0.010155111739597489) - present_state_Q (0.006838801402181072)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3455542025576134 ) += alpha ( 0.1 ) * (reward ( 0.00028603170651801007 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006516239658858419) - present_state_Q ( 0.007570242989650755)) * f1( 0.6007738344074136)
w2 ( 0.10628460110262729 ) += alpha ( 0.1) * (reward ( 0.00028603170651801007) + discount_factor ( 0.1) * next_state_max_Q( 0.006516239658858419) - present_state_Q (0.007570242989650755)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34577740287007747 ) += alpha ( 0.1 ) * (reward ( 0.00014301585325900504 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005090666808684308) - present_state_Q ( 0.004362081468183787)) * f1( 0.6016182657498744)
w2 ( 0.10554260131581601 ) += alpha ( 0.1) * (reward ( 0.00014301585325900504) + discount_factor ( 0.1) * next_state_max_Q( 0.005090666808684308) - present_state_Q (0.004362081468183787)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34593154239298773 ) += alpha ( 0.1 ) * (reward ( 7.150792662950252e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002920630528934598) - present_state_Q ( 0.002920630528934598)) * f1( 0.6027998954726249)
w2 ( 0.10503118940593369 ) += alpha ( 0.1) * (reward ( 7.150792662950252e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.002920630528934598) - present_state_Q (0.002920630528934598)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34594994246659033 ) += alpha ( 0.1 ) * (reward ( 3.575396331475126e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003157285396190118) - present_state_Q ( 0.0006546021526001033)) * f1( 0.6070234517243331)
w2 ( 0.10497056547600042 ) += alpha ( 0.1) * (reward ( 3.575396331475126e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.003157285396190118) - present_state_Q (0.0006546021526001033)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3451034142922106 ) += alpha ( 0.1 ) * (reward ( 1.787698165737563e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003184495473878557) - present_state_Q ( -0.014082302464210716)) * f1( 0.5871072587939301)
w2 ( 0.10756591869478649 ) += alpha ( 0.1) * (reward ( 1.787698165737563e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.003184495473878557) - present_state_Q (-0.014082302464210716)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34464210250699906 ) += alpha ( 0.1 ) * (reward ( 4.469245414343907e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007500359719506189) - present_state_Q ( -0.007125009582884451)) * f1( 0.5854570959076534)
w2 ( 0.10914182165483637 ) += alpha ( 0.1) * (reward ( 4.469245414343907e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.007500359719506189) - present_state_Q (-0.007125009582884451)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34447932569187195 ) += alpha ( 0.1 ) * (reward ( 2.2346227071719537e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012326127546536753) - present_state_Q ( -0.0015634892556932178)) * f1( 0.5816913276422231)
w2 ( 0.10970148898144717 ) += alpha ( 0.1) * (reward ( 2.2346227071719537e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.012326127546536753) - present_state_Q (-0.0015634892556932178)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3443567194066745 ) += alpha ( 0.1 ) * (reward ( 1.1173113535859768e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013518833441073769) - present_state_Q ( -0.0007809025239081946)) * f1( 0.5745634871478805)
w2 ( 0.11008559155373362 ) += alpha ( 0.1) * (reward ( 1.1173113535859768e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.013518833441073769) - present_state_Q (-0.0007809025239081946)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34536123223273485 ) += alpha ( 0.1 ) * (reward ( 5.586556767929884e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018970248014264474) - present_state_Q ( 0.018970248014264474)) * f1( 0.5883749561745915)
w2 ( 0.10667105864230138 ) += alpha ( 0.1) * (reward ( 5.586556767929884e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.018970248014264474) - present_state_Q (0.018970248014264474)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.344994445578299 ) += alpha ( 0.1 ) * (reward ( 2.793278383964942e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008212941735706941) - present_state_Q ( -0.005597206369357427)) * f1( 0.5714273768856235)
w2 ( 0.10782643901903935 ) += alpha ( 0.1) * (reward ( 2.793278383964942e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.008212941735706941) - present_state_Q (-0.005597206369357427)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3457992383799149 ) += alpha ( 0.1 ) * (reward ( 1.396639191982471e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015280122893349596) - present_state_Q ( 0.015280122893349596)) * f1( 0.5852199696476934)
w2 ( 0.10507604483102026 ) += alpha ( 0.1) * (reward ( 1.396639191982471e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.015280122893349596) - present_state_Q (0.015280122893349596)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34534429815000434 ) += alpha ( 0.1 ) * (reward ( 6.983195959912355e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006250273885082452) - present_state_Q ( -0.007387651176353011)) * f1( 0.5677705169066156)
w2 ( 0.10651833954244802 ) += alpha ( 0.1) * (reward ( 6.983195959912355e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.006250273885082452) - present_state_Q (-0.007387651176353011)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3459681523514993 ) += alpha ( 0.1 ) * (reward ( 3.4915979799561776e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012474105542014141) - present_state_Q ( 0.011974843475087038)) * f1( 0.5815522097806575)
w2 ( 0.10437285994146686 ) += alpha ( 0.1) * (reward ( 3.4915979799561776e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.012474105542014141) - present_state_Q (0.011974843475087038)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3464411261118979 ) += alpha ( 0.1 ) * (reward ( 1.7457989899780888e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009026473641049748) - present_state_Q ( 0.009026473641049748)) * f1( 0.5822069068083338)
w2 ( 0.10274809817767588 ) += alpha ( 0.1) * (reward ( 1.7457989899780888e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.009026473641049748) - present_state_Q (0.009026473641049748)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34597458561311883 ) += alpha ( 0.1 ) * (reward ( 8.728994949890444e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006475497347990855) - present_state_Q ( -0.007626517979979708)) * f1( 0.563858095763893)
w2 ( 0.10423743193755516 ) += alpha ( 0.1) * (reward ( 8.728994949890444e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.006475497347990855) - present_state_Q (-0.007626517979979708)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34669834996175114 ) += alpha ( 0.1 ) * (reward ( 2.182248737472611e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013920451061623568) - present_state_Q ( 0.013920451061623568)) * f1( 0.5776987729113577)
w2 ( 0.10173175118291267 ) += alpha ( 0.1) * (reward ( 2.182248737472611e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.013920451061623568) - present_state_Q (0.013920451061623568)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3470605685947643 ) += alpha ( 0.1 ) * (reward ( 1.0911243687363055e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007076129654014773) - present_state_Q ( 0.007076129654014773)) * f1( 0.5687646162882838)
w2 ( 0.10045804806341488 ) += alpha ( 0.1) * (reward ( 1.0911243687363055e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.007076129654014773) - present_state_Q (0.007076129654014773)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3471030720211153 ) += alpha ( 0.1 ) * (reward ( 5.455621843681528e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003720642857981238) - present_state_Q ( 0.001105901124512948)) * f1( 0.5791949341775987)
w2 ( 0.10031128080478435 ) += alpha ( 0.1) * (reward ( 5.455621843681528e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.003720642857981238) - present_state_Q (0.001105901124512948)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3473393156245059 ) += alpha ( 0.1 ) * (reward ( 2.727810921840764e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004619829567264061) - present_state_Q ( 0.004619829567264061)) * f1( 0.5681874321455929)
w2 ( 0.09947971153723303 ) += alpha ( 0.1) * (reward ( 2.727810921840764e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.004619829567264061) - present_state_Q (0.004619829567264061)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3476732057456154 ) += alpha ( 0.1 ) * (reward ( 1.363905460920382e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0016055892465704769) - present_state_Q ( 0.006217856160755014)) * f1( 0.5512196539371774)
w2 ( 0.0983893980592857 ) += alpha ( 0.1) * (reward ( 1.363905460920382e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.0016055892465704769) - present_state_Q (0.006217856160755014)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3469781638204678 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.000454120029558136) - present_state_Q ( -0.002844538298021476)) * f1( 0.5779915416294363)
w2 ( 0.10079442261444845 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.000454120029558136) - present_state_Q (-0.002844538298021476)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3462334590849495 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00365391581669558) - present_state_Q ( -0.0085771488645153)) * f1( 0.5512196539371774)
w2 ( 0.10349644795116905 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( 0.00365391581669558) - present_state_Q (-0.0085771488645153)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34703624858992527 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013961068839000945) - present_state_Q ( 0.01833990030472607)) * f1( 0.5476053809219974)
w2 ( 0.10056444789074567 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( 0.013961068839000945) - present_state_Q (0.01833990030472607)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3466038072835919 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009450335742507587) - present_state_Q ( -0.006477584019007998)) * f1( 0.5409867300868373)
w2 ( 0.10200328974787416 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( 0.009450335742507587) - present_state_Q (-0.006477584019007998)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3452713414457762 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013591201790966012) - present_state_Q ( -0.019127866211058664)) * f1( 0.641457874425482)
w2 ( 0.10615778185387294 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( 0.013591201790966012) - present_state_Q (-0.019127866211058664)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3465670168760448 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02175246958719748) - present_state_Q ( 0.026460917177104026)) * f1( 0.5366686070416309)
w2 ( 0.10181205388714924 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( 0.02175246958719748) - present_state_Q (0.026460917177104026)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34722034655159423 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015758884664468065) - present_state_Q ( 0.013484596992359515)) * f1( 0.5519227090281854)
w2 ( 0.09944458588895863 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.015758884664468065) - present_state_Q (0.013484596992359515)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3469582610911734 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009163304470753558) - present_state_Q ( -0.004030953223824102)) * f1( 0.5259625662092581)
w2 ( 0.1003415201178669 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.009163304470753558) - present_state_Q (-0.004030953223824102)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34763381381833125 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013858422481953042) - present_state_Q ( 0.013858422481953042)) * f1( 0.542406208571237)
w2 ( 0.09785057249786333 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.013858422481953042) - present_state_Q (0.013858422481953042)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3472720011450565 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005155100592157957) - present_state_Q ( -0.006388461012362456)) * f1( 0.5233882809517882)
w2 ( 0.099094893082784 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.005155100592157957) - present_state_Q (-0.006388461012362456)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34787226587937603 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012366991350733963) - present_state_Q ( 0.012366991350733963)) * f1( 0.5395234724595768)
w2 ( 0.09686972674633888 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.012366991350733963) - present_state_Q (0.012366991350733963)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3476291954194309 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007164152371377186) - present_state_Q ( -0.003950409266155808)) * f1( 0.5205988619075442)
w2 ( 0.09771015660494087 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.007164152371377186) - present_state_Q (-0.003950409266155808)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34833806364587555 ) += alpha ( 0.1 ) * (reward ( 5.575666793723394e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015197096643880303) - present_state_Q ( 0.015197096643880303)) * f1( 0.5182986531809578)
w2 ( 0.09497479072237829 ) += alpha ( 0.1) * (reward ( 5.575666793723394e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.015197096643880303) - present_state_Q (0.015197096643880303)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3487648687667553 ) += alpha ( 0.1 ) * (reward ( 2.787833396861697e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009147603358058864) - present_state_Q ( 0.009147603358058864)) * f1( 0.5184352147078835)
w2 ( 0.09332827787459563 ) += alpha ( 0.1) * (reward ( 2.787833396861697e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.009147603358058864) - present_state_Q (0.009147603358058864)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3490224788784345 ) += alpha ( 0.1 ) * (reward ( 1.3939166984308485e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005514807394165877) - present_state_Q ( 0.005514807394165877)) * f1( 0.5190416924132157)
w2 ( 0.09233564042197974 ) += alpha ( 0.1) * (reward ( 1.3939166984308485e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.005514807394165877) - present_state_Q (0.005514807394165877)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3492603678575999 ) += alpha ( 0.1 ) * (reward ( 6.969583492154242e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0050892360900091915) - present_state_Q ( 0.0050892360900091915)) * f1( 0.519380719151986)
w2 ( 0.09141959186494507 ) += alpha ( 0.1) * (reward ( 6.969583492154242e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.0050892360900091915) - present_state_Q (0.0050892360900091915)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3494560351264398 ) += alpha ( 0.1 ) * (reward ( 3.484791746077121e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00038959326290949337) - present_state_Q ( 0.003937006852683889)) * f1( 0.5019667327415008)
w2 ( 0.09071794958281948 ) += alpha ( 0.1) * (reward ( 3.484791746077121e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.00038959326290949337) - present_state_Q (0.003937006852683889)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34970344397355013 ) += alpha ( 0.1 ) * (reward ( 1.3612467758113754e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017215800202325937) - present_state_Q ( 0.005036237895207241)) * f1( 0.508644716919481)
w2 ( 0.08974513363140763 ) += alpha ( 0.1) * (reward ( 1.3612467758113754e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.0017215800202325937) - present_state_Q (0.005036237895207241)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34991491465588787 ) += alpha ( 0.1 ) * (reward ( 6.806233879056877e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0010302259970396688) - present_state_Q ( 0.004292356460303681)) * f1( 0.5047835594157273)
w2 ( 0.08890726687290015 ) += alpha ( 0.1) * (reward ( 6.806233879056877e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.0010302259970396688) - present_state_Q (0.004292356460303681)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35016674467783143 ) += alpha ( 0.1 ) * (reward ( 3.4031169395284386e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0007331023644039647) - present_state_Q ( 0.00509995845173139)) * f1( 0.5009899496893794)
w2 ( 0.08790193723664819 ) += alpha ( 0.1) * (reward ( 3.4031169395284386e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.0007331023644039647) - present_state_Q (0.00509995845173139)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35038051421987365 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-9.10785550007065e-05) - present_state_Q ( 0.004321803489200221)) * f1( 0.49359020739061465)
w2 ( 0.08703575497111124 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( -9.10785550007065e-05) - present_state_Q (0.004321803489200221)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35033192460241996 ) += alpha ( 0.1 ) * (reward ( 8.507792348821097e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00473405261074944) - present_state_Q ( -0.001465606248930884)) * f1( 0.4897154672465173)
w2 ( 0.08723419517038399 ) += alpha ( 0.1) * (reward ( 8.507792348821097e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.00473405261074944) - present_state_Q (-0.001465606248930884)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.350239473312128 ) += alpha ( 0.1 ) * (reward ( 5.317370218013185e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005696371392546334) - present_state_Q ( -0.00237340553700377)) * f1( 0.5125452379589733)
w2 ( 0.08759494885004016 ) += alpha ( 0.1) * (reward ( 5.317370218013185e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.005696371392546334) - present_state_Q (-0.00237340553700377)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3493137157238053 ) += alpha ( 0.1 ) * (reward ( 1.0385488707057003e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009269878168368911) - present_state_Q ( -0.017857090214423804)) * f1( 0.5468115706462378)
w2 ( 0.09098096932955775 ) += alpha ( 0.1) * (reward ( 1.0385488707057003e-15) + discount_factor ( 0.1) * next_state_max_Q( -0.009269878168368911) - present_state_Q (-0.017857090214423804)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34962589569318636 ) += alpha ( 0.1 ) * (reward ( 2.5963721767642507e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0018812305224111536) - present_state_Q ( 0.0062796717197042)) * f1( 0.5124804650227918)
w2 ( 0.08976265959606519 ) += alpha ( 0.1) * (reward ( 2.5963721767642507e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.0018812305224111536) - present_state_Q (0.0062796717197042)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.34988770332344776 ) += alpha ( 0.1 ) * (reward ( 6.490930441910627e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002154579992861505) - present_state_Q ( 0.005463583751793205)) * f1( 0.49885929302731014)
w2 ( 0.08871303444556379 ) += alpha ( 0.1) * (reward ( 6.490930441910627e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.002154579992861505) - present_state_Q (0.005463583751793205)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3501672973462425 ) += alpha ( 0.1 ) * (reward ( 3.2454652209553133e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0013304162901702221) - present_state_Q ( 0.005749055418767524)) * f1( 0.4978513822473973)
w2 ( 0.0875898316876137 ) += alpha ( 0.1) * (reward ( 3.2454652209553133e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.0013304162901702221) - present_state_Q (0.005749055418767524)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35039736985990794 ) += alpha ( 0.1 ) * (reward ( 1.6227326104776567e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0002452834141251914) - present_state_Q ( 0.00471353951280043)) * f1( 0.49066318090537797)
w2 ( 0.08665202945333611 ) += alpha ( 0.1) * (reward ( 1.6227326104776567e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.0002452834141251914) - present_state_Q (0.00471353951280043)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35058901467841574 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0012973439258880948) - present_state_Q ( 0.004066454875624431)) * f1( 0.4868133750761753)
w2 ( 0.08586468535672899 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0012973439258880948) - present_state_Q (0.004066454875624431)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3505977168700062 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004719246223074736) - present_state_Q ( -0.0002917504191357334)) * f1( 0.48298765512626574)
w2 ( 0.08582865051609465 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.004719246223074736) - present_state_Q (-0.0002917504191357334)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3508376208160545 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0025371245524738184) - present_state_Q ( 0.005268690763932354)) * f1( 0.4783748428839216)
w2 ( 0.08482565485435765 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0025371245524738184) - present_state_Q (0.005268690763932354)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3510407987266494 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001814555789951544) - present_state_Q ( 0.004462620705511011)) * f1( 0.47458554993942)
w2 ( 0.08396942182905448 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.001814555789951544) - present_state_Q (0.004462620705511011)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3512200786870526 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0014040164636130725) - present_state_Q ( 0.003948056707212322)) * f1( 0.470840865409395)
w2 ( 0.08320789081688428 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0014040164636130725) - present_state_Q (0.003948056707212322)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3515162712811022 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002530992478220001) - present_state_Q ( 0.006593436209753029)) * f1( 0.46715591904345566)
w2 ( 0.08193982342449807 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.002530992478220001) - present_state_Q (0.006593436209753029)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35175278851122477 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0012229084954412939) - present_state_Q ( 0.005319911992827497)) * f1( 0.45504899953747213)
w2 ( 0.0809002991958414 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0012229084954412939) - present_state_Q (0.005319911992827497)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3519521394987757 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011170008412846877) - present_state_Q ( 0.004531177442845341)) * f1( 0.45107367086677713)
w2 ( 0.08001640372409802 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0011170008412846877) - present_state_Q (0.004531177442845341)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3521311611325865 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0006634104279859554) - present_state_Q ( 0.004070384455489795)) * f1( 0.44710212992048204)
w2 ( 0.07921559504155978 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0006634104279859554) - present_state_Q (0.004070384455489795)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.352133159768678 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004510075576453387) - present_state_Q ( -0.00040590539553683747)) * f1( 0.4431353172474429)
w2 ( 0.07920657460913809 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.004510075576453387) - present_state_Q (-0.00040590539553683747)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3514760708162696 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010043375897994505) - present_state_Q ( -0.014103641578513265)) * f1( 0.5016212716145235)
w2 ( 0.08182643540688085 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.010043375897994505) - present_state_Q (-0.014103641578513265)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3514650409264609 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0030884507896189184) - present_state_Q ( -0.0005412560701954228)) * f1( 0.47458554993942)
w2 ( 0.08187291760512756 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0030884507896189184) - present_state_Q (-0.0005412560701954228)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35123212705079015 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007688805186905345) - present_state_Q ( -0.005634223352048606)) * f1( 0.4787203772648176)
w2 ( 0.08284598617179918 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.007688805186905345) - present_state_Q (-0.005634223352048606)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3509558876358186 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0048974534988268925) - present_state_Q ( -0.0060892013227636654)) * f1( 0.49333259571898164)
w2 ( 0.08396587736637538 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0048974534988268925) - present_state_Q (-0.0060892013227636654)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.350447744425897 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003896497810123134) - present_state_Q ( -0.01046106173678682)) * f1( 0.5045401897499255)
w2 ( 0.08598015975753029 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.003896497810123134) - present_state_Q (-0.01046106173678682)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3507460430304625 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00317516999691167) - present_state_Q ( 0.006586262028745599)) * f1( 0.475850593991215)
w2 ( 0.0847264107517194 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00317516999691167) - present_state_Q (0.006586262028745599)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35098813579463667 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001921076733033683) - present_state_Q ( 0.005322347835288732)) * f1( 0.4718936278423778)
w2 ( 0.08370036271932232 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.001921076733033683) - present_state_Q (0.005322347835288732)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.351100137143274 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00023517763210781362) - present_state_Q ( 0.002369945739980006)) * f1( 0.46794675785948997)
w2 ( 0.08322167001868416 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00023517763210781362) - present_state_Q (0.002369945739980006)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35079818970260984 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0026560195280657117) - present_state_Q ( -0.006205876575677449)) * f1( 0.5083055242961275)
w2 ( 0.08440972494325834 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0026560195280657117) - present_state_Q (-0.006205876575677449)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35060826618121643 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002876519030718505) - present_state_Q ( -0.004082562618015845)) * f1( 0.5004690114196566)
w2 ( 0.08516870708624713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.002876519030718505) - present_state_Q (-0.004082562618015845)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3508202111659282 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015392513109545036) - present_state_Q ( 0.004601019288159647)) * f1( 0.47659207839146783)
w2 ( 0.08427928825483429 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0015392513109545036) - present_state_Q (0.004601019288159647)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3510077797135324 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0010616440796094584) - present_state_Q ( 0.00407409693790009)) * f1( 0.4727110307167476)
w2 ( 0.08348570174884645 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0010616440796094584) - present_state_Q (0.00407409693790009)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35117937850572567 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0007889357219336146) - present_state_Q ( 0.003738833079542342)) * f1( 0.46885690828676885)
w2 ( 0.08275371384737666 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0007889357219336146) - present_state_Q (0.003738833079542342)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3511727577849318 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0035656936331533162) - present_state_Q ( -0.0004989382851550983)) * f1( 0.4650397508322162)
w2 ( 0.08278218763174461 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0035656936331533162) - present_state_Q (-0.0004989382851550983)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35109085425489994 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0007736060451946358) - present_state_Q ( -0.0017695353961302562)) * f1( 0.484013415386358)
w2 ( 0.08312062259006676 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0007736060451946358) - present_state_Q (-0.0017695353961302562)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35142462320987505 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0032400791088035286) - present_state_Q ( 0.007650084173154076)) * f1( 0.4555903365268866)
w2 ( 0.08165540733761202 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0032400791088035286) - present_state_Q (0.007650084173154076)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3516785693195452 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003044578530857267) - present_state_Q ( 0.005926344723243132)) * f1( 0.4517097471637318)
w2 ( 0.08053102996358054 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.003044578530857267) - present_state_Q (0.005926344723243132)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3518893355442741 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0020831786428303356) - present_state_Q ( 0.004914528148773412)) * f1( 0.44784701912588804)
w2 ( 0.07958978790668246 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0020831786428303356) - present_state_Q (0.004914528148773412)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3520726784328435 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015088872861602765) - present_state_Q ( 0.004280173643508134)) * f1( 0.44400638935865194)
w2 ( 0.07876393092370404 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0015088872861602765) - present_state_Q (0.004280173643508134)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35223818363879084 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011785580022407038) - present_state_Q ( 0.00387768418781545)) * f1( 0.4401935112079224)
w2 ( 0.07801196524618577 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0011785580022407038) - present_state_Q (0.00387768418781545)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35239160031779604 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0010044779231885015) - present_state_Q ( 0.003615824208823837)) * f1( 0.43641607847426533)
w2 ( 0.07730888996288478 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0010044779231885015) - present_state_Q (0.003615824208823837)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35253625368313574 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0009336767316634187) - present_state_Q ( 0.0034365257181659947)) * f1( 0.43268479501313073)
w2 ( 0.07664025835388484 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0009336767316634187) - present_state_Q (0.0034365257181659947)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3526738778728919 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0009353252724837724) - present_state_Q ( 0.003301444032203227)) * f1( 0.42901491996762786)
w2 ( 0.07599867605289387 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0009353252724837724) - present_state_Q (0.003301444032203227)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3528050690926173 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0009932632169107891) - present_state_Q ( 0.00318306729255019)) * f1( 0.4254287923827818)
w2 ( 0.07538192785872205 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0009932632169107891) - present_state_Q (0.00318306729255019)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3529294650397217 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011019132635218531) - present_state_Q ( 0.003058241717620619)) * f1( 0.4219600433998463)
w2 ( 0.07479231778046837 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0011019132635218531) - present_state_Q (0.003058241717620619)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3530456915564964 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0012646395751365058) - present_state_Q ( 0.00290261465149666)) * f1( 0.41866069095806635)
w2 ( 0.07423708764167176 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0012646395751365058) - present_state_Q (0.00290261465149666)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35274107409129724 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029876938246826135) - present_state_Q ( -0.007628125657213769)) * f1( 0.4156128502700425)
w2 ( 0.07570295889662086 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0029876938246826135) - present_state_Q (-0.007628125657213769)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35259551466470307 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0013397086187757479) - present_state_Q ( -0.0035040717447296366)) * f1( 0.4001036885323908)
w2 ( 0.07643056741794231 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0013397086187757479) - present_state_Q (-0.0035040717447296366)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.352481075742479 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003440098395080471) - present_state_Q ( -0.0025441265473186148)) * f1( 0.3962379434226924)
w2 ( 0.0769504319675711 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.003440098395080471) - present_state_Q (-0.0025441265473186148)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3527781780818901 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008046465777564238) - present_state_Q ( 0.008046465777564238)) * f1( 0.41025926112458605)
w2 ( 0.07550206812760954 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.008046465777564238) - present_state_Q (0.008046465777564238)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3530682863126608 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00787266770911385) - present_state_Q ( 0.00787266770911385)) * f1( 0.4094450452429889)
w2 ( 0.07408498793996905 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00787266770911385) - present_state_Q (0.00787266770911385)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35295505511800224 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0018513332529646598) - present_state_Q ( -0.0026966971379455418)) * f1( 0.39291414294779337)
w2 ( 0.07460371742335262 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0018513332529646598) - present_state_Q (-0.0026966971379455418)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35320734429804196 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010051003710743273) - present_state_Q ( 0.007310463034308712)) * f1( 0.40011842857320157)
w2 ( 0.07334264489070574 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.010051003710743273) - present_state_Q (0.007310463034308712)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3534116743440847 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005647236844695869) - present_state_Q ( 0.005647236844695869)) * f1( 0.40202561134860815)
w2 ( 0.07232614225866049 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.005647236844695869) - present_state_Q (0.005647236844695869)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3533635363665522 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002880078280045356) - present_state_Q ( -0.0008701677731432489)) * f1( 0.4156362600351291)
w2 ( 0.07255777737889005 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.002880078280045356) - present_state_Q (-0.0008701677731432489)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35344756636061825 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00559508132378056) - present_state_Q ( 0.0026542191964286643)) * f1( 0.4011531495115381)
w2 ( 0.07213883516607993 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00559508132378056) - present_state_Q (0.0026542191964286643)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35355767082452016 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006314251483947486) - present_state_Q ( 0.003362475126329062)) * f1( 0.4031579970763395)
w2 ( 0.07159262517049306 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.006314251483947486) - present_state_Q (0.003362475126329062)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.353304317168701 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001251388672697612) - present_state_Q ( -0.006455447633250844)) * f1( 0.38500163442744717)
w2 ( 0.07277713074058677 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.001251388672697612) - present_state_Q (-0.006455447633250844)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35363495169633313 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009440074200671195) - present_state_Q ( 0.009440074200671195)) * f1( 0.3891618747476938)
w2 ( 0.07107791738446595 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.009440074200671195) - present_state_Q (0.009440074200671195)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3534168280557159 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0019570042151867972) - present_state_Q ( -0.005557825774796521)) * f1( 0.37911297033273983)
w2 ( 0.07211355209980269 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0019570042151867972) - present_state_Q (-0.005557825774796521)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3531931635778903 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0067330453925116) - present_state_Q ( -0.0051440940484850095)) * f1( 0.38447507842611084)
w2 ( 0.07327703181734993 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0067330453925116) - present_state_Q (-0.0051440940484850095)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3530484917916426 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010172766751035123) - present_state_Q ( -0.002405702078798755)) * f1( 0.4226487999166303)
w2 ( 0.07396162756813038 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.010172766751035123) - present_state_Q (-0.002405702078798755)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3530426460420207 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009035584713285139) - present_state_Q ( 0.0007468140591124772)) * f1( 0.3729478798823849)
w2 ( 0.07398984156232927 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.009035584713285139) - present_state_Q (0.0007468140591124772)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3534511929138832 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014856274311099732) - present_state_Q ( 0.01199981415185783)) * f1( 0.38856725937381276)
w2 ( 0.0718870042181797 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.014856274311099732) - present_state_Q (0.01199981415185783)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35338057791180383 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0065159877532489086) - present_state_Q ( -0.0012495034340981037)) * f1( 0.37144242813122513)
w2 ( 0.07222920261587584 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0065159877532489086) - present_state_Q (-0.0012495034340981037)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35377689019547937 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011704364583071336) - present_state_Q ( 0.011704364583071336)) * f1( 0.376224594454799)
w2 ( 0.070122416990923 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.011704364583071336) - present_state_Q (0.011704364583071336)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35406826637449423 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008617999457817882) - present_state_Q ( 0.008617999457817882)) * f1( 0.37566875189675225)
w2 ( 0.06857117708851579 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.008617999457817882) - present_state_Q (0.008617999457817882)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3538618960902551 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0014499499140206984) - present_state_Q ( -0.005505924387438202)) * f1( 0.36519771457345523)
w2 ( 0.06958834257670704 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0014499499140206984) - present_state_Q (-0.005505924387438202)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35416496839838724 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009061230669533876) - present_state_Q ( 0.009061230669533876)) * f1( 0.37163495922019074)
w2 ( 0.06795732105619094 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.009061230669533876) - present_state_Q (0.009061230669533876)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35415518781839467 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015811565198018684) - present_state_Q ( -0.0001231432634281049)) * f1( 0.34774293210900825)
w2 ( 0.06800794766096444 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0015811565198018684) - present_state_Q (-0.0001231432634281049)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3543449628930087 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005735036100096069) - present_state_Q ( 0.005735036100096069)) * f1( 0.3676719559133845)
w2 ( 0.06697564116294714 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.005735036100096069) - present_state_Q (0.005735036100096069)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35450400848167174 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004803879579095421) - present_state_Q ( 0.004803879579095421)) * f1( 0.36786376058576564)
w2 ( 0.06611094283870997 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.004803879579095421) - present_state_Q (0.004803879579095421)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35441920245347436 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011179966207947034) - present_state_Q ( -0.0023521254896625987)) * f1( 0.34419076463177206)
w2 ( 0.06655444936602355 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0011179966207947034) - present_state_Q (-0.0023521254896625987)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35459272552802173 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0052912196965409775) - present_state_Q ( 0.0052912196965409775)) * f1( 0.36438369075810934)
w2 ( 0.06560202982064617 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0052912196965409775) - present_state_Q (0.0052912196965409775)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35450250989987186 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005690091507571382) - present_state_Q ( -0.0026894786767939355)) * f1( 0.3426893194257551)
w2 ( 0.06607589381775544 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0005690091507571382) - present_state_Q (-0.0026894786767939355)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3549266780953688 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013084735723006002) - present_state_Q ( 0.013084735723006002)) * f1( 0.3601891585536422)
w2 ( 0.06372064138761437 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.013084735723006002) - present_state_Q (0.013084735723006002)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3547659782861928 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0027810325728091223) - present_state_Q ( -0.004990638403792491)) * f1( 0.34100500936316125)
w2 ( 0.06456889771398645 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0027810325728091223) - present_state_Q (-0.004990638403792491)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3551578979242451 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006191476776801333) - present_state_Q ( 0.012409191063804725)) * f1( 0.33241577254381177)
w2 ( 0.06221088903676154 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.006191476776801333) - present_state_Q (0.012409191063804725)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35542637298916585 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0021332141648238295) - present_state_Q ( 0.008372916879038794)) * f1( 0.32902987182723087)
w2 ( 0.06057896994425026 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0021332141648238295) - present_state_Q (0.008372916879038794)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35562182979804413 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00032441686343283316) - present_state_Q ( 0.005949351556751456)) * f1( 0.3267528664651501)
w2 ( 0.05938261129563131 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00032441686343283316) - present_state_Q (0.005949351556751456)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35577628710641457 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001724630459288265) - present_state_Q ( 0.004592650535507642)) * f1( 0.32414192386129115)
w2 ( 0.05842958857934402 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.001724630459288265) - present_state_Q (0.004592650535507642)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35590612717538134 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009871035173775605) - present_state_Q ( 0.003945515163696303)) * f1( 0.32105051627621684)
w2 ( 0.05762074347625721 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0009871035173775605) - present_state_Q (0.003945515163696303)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3556639893262735 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001154597482291242) - present_state_Q ( -0.007744908957603144)) * f1( 0.31737264704552576)
w2 ( 0.05914663331813201 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.001154597482291242) - present_state_Q (-0.007744908957603144)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35586429873724423 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0016053294651698774) - present_state_Q ( 0.0065565413239495884)) * f1( 0.31317878143733224)
w2 ( 0.05799535181019414 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0016053294651698774) - present_state_Q (0.0065565413239495884)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35587433720336004 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005850463603527131) - present_state_Q ( 0.0008910180821969371)) * f1( 0.3280847672887366)
w2 ( 0.057934157465825295 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.005850463603527131) - present_state_Q (0.0008910180821969371)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35573839767602544 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002491719833717751) - present_state_Q ( -0.00414304874886888)) * f1( 0.3095006731714446)
w2 ( 0.058812601612273425 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.002491719833717751) - present_state_Q (-0.00414304874886888)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3556358635720396 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0038163364210473394) - present_state_Q ( -0.0029837722822358276)) * f1( 0.3046708370134499)
w2 ( 0.059418374678654726 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0038163364210473394) - present_state_Q (-0.0029837722822358276)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3558099166818027 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011561101698883261) - present_state_Q ( 0.0065965803297892145)) * f1( 0.3199229196145039)
w2 ( 0.058330280646674545 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.011561101698883261) - present_state_Q (0.0065965803297892145)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.356068012502854 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003974311838955788) - present_state_Q ( 0.008953718860310558)) * f1( 0.30164462768445127)
w2 ( 0.05679014886491985 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.003974311838955788) - present_state_Q (0.008953718860310558)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35602415989205866 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003949882291149204) - present_state_Q ( -0.0009896699584564378)) * f1( 0.3167035098551441)
w2 ( 0.05706708050243412 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.003949882291149204) - present_state_Q (-0.0009896699584564378)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35624344651244455 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0025030967683681987) - present_state_Q ( 0.007465309510486934)) * f1( 0.3039315667939047)
w2 ( 0.0556240805357041 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0025030967683681987) - present_state_Q (0.007465309510486934)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3564228312947349 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011262088542262766) - present_state_Q ( 0.006099871663659398)) * f1( 0.2996112722426527)
w2 ( 0.05442663038005675 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0011262088542262766) - present_state_Q (0.006099871663659398)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35625145466024705 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00010600207271789508) - present_state_Q ( -0.005795657152266445)) * f1( 0.29515852273811777)
w2 ( 0.055587881851964394 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00010600207271789508) - present_state_Q (-0.005795657152266445)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3561678779390765 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0036093815288372605) - present_state_Q ( -0.0025098810875408134)) * f1( 0.291124986184075)
w2 ( 0.056162045700049304 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0036093815288372605) - present_state_Q (-0.0025098810875408134)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3564128867853665 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00388350777392589) - present_state_Q ( 0.00889828561764977)) * f1( 0.28790919189058395)
w2 ( 0.05463025742880301 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00388350777392589) - present_state_Q (0.00889828561764977)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35645582983960916 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007148107618985322) - present_state_Q ( 0.0021252547110545417)) * f1( 0.3044648053430629)
w2 ( 0.05434816863897181 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.007148107618985322) - present_state_Q (0.0021252547110545417)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3563655562895555 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0029887841045960206) - present_state_Q ( -0.0028520272870029284)) * f1( 0.28650032314940754)
w2 ( 0.05497834977846432 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0029887841045960206) - present_state_Q (-0.0028520272870029284)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3562816063255543 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0033190695404820975) - present_state_Q ( -0.002640364729775871)) * f1( 0.2824437767856219)
w2 ( 0.05551335868155265 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0033190695404820975) - present_state_Q (-0.002640364729775871)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35642701377758057 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010996479749380894) - present_state_Q ( 0.005958927717183121)) * f1( 0.2992366353436271)
w2 ( 0.054541502733103646 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.010996479749380894) - present_state_Q (0.005958927717183121)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3558414013944348 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004815236038228143) - present_state_Q ( -0.021339508701988294)) * f1( 0.28076172285560325)
w2 ( 0.05829594005077343 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.004815236038228143) - present_state_Q (-0.021339508701988294)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35591584534064075 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01580814426708249) - present_state_Q ( 0.004148956256927808)) * f1( 0.289874746518823)
w2 ( 0.05778231168472952 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.01580814426708249) - present_state_Q (0.004148956256927808)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3562165814870617 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01777824940684429) - present_state_Q ( 0.012213773544580947)) * f1( 0.2881732728241637)
w2 ( 0.05569512196395022 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.01777824940684429) - present_state_Q (0.012213773544580947)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3564113687832313 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007492026677675331) - present_state_Q ( 0.007492026677675331)) * f1( 0.2888808841568551)
w2 ( 0.05434655716196866 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.007492026677675331) - present_state_Q (0.007492026677675331)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.356505479487652 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009184249330869587) - present_state_Q ( 0.004145024416438853)) * f1( 0.2916714792346039)
w2 ( 0.053701237265298275 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.009184249330869587) - present_state_Q (0.004145024416438853)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35652300868568 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0062528119275977595) - present_state_Q ( 0.0012228631732969647)) * f1( 0.2933354518527842)
w2 ( 0.05358172086919084 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0062528119275977595) - present_state_Q (0.0012228631732969647)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3564746317981305 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004169416363911452) - present_state_Q ( -0.0013160199884591078)) * f1( 0.27915729267051176)
w2 ( 0.05389365396166388 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.004169416363911452) - present_state_Q (-0.0013160199884591078)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3564192588792508 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004287239623957245) - present_state_Q ( -0.0015858919925614695)) * f1( 0.2748559532819054)
w2 ( 0.05425628483355618 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.004287239623957245) - present_state_Q (-0.0015858919925614695)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35653249033750317 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00433323840604502) - present_state_Q ( 0.00433323840604502)) * f1( 0.29034343279154295)
w2 ( 0.053476301920468074 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.00433323840604502) - present_state_Q (0.00433323840604502)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3566488993992822 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004425108024346355) - present_state_Q ( 0.004425108024346355)) * f1( 0.29229433782185615)
w2 ( 0.05267978247608573 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.004425108024346355) - present_state_Q (0.004425108024346355)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3565824126120107 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002069801376723815) - present_state_Q ( 0.0069180589236203655)) * f1( 0.2742748251698485)
w2 ( 0.053116119340085644 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.002069801376723815) - present_state_Q (0.0069180589236203655)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3565677318618567 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004511861197372638) - present_state_Q ( 0.004511861197372638)) * f1( 0.28961189491800665)
w2 ( 0.05321750157204221 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( 0.004511861197372638) - present_state_Q (0.004511861197372638)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3567263496048584 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0036897168628454435) - present_state_Q ( 0.008475737299133854)) * f1( 0.27239995236330183)
w2 ( 0.05216936652309698 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( 0.0036897168628454435) - present_state_Q (0.008475737299133854)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3567683626725686 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002888800073602385) - present_state_Q ( 0.002888800073602385)) * f1( 0.2881508254960241)
w2 ( 0.05187776182171946 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( 0.002888800073602385) - present_state_Q (0.002888800073602385)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35691191136635786 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0012784659833891543) - present_state_Q ( 0.005980366368275258)) * f1( 0.2717916325736908)
w2 ( 0.05092707895347283 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( 0.0012784659833891543) - present_state_Q (0.005980366368275258)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35691279701371453 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0003514527511743193) - present_state_Q ( 0.0003514527511743193)) * f1( 0.2872369536704132)
w2 ( 0.05092091228622918 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( 0.0003514527511743193) - present_state_Q (0.0003514527511743193)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3567804751297759 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0013206549103926962) - present_state_Q ( -0.00459370213455533)) * f1( 0.2717916325736908)
w2 ( 0.05189461322533196 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( 0.0013206549103926962) - present_state_Q (-0.00459370213455533)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3569561840335016 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0023239006401699347) - present_state_Q ( 0.006819278644725035)) * f1( 0.269677481600833)
w2 ( 0.05072181961709725 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.0023239006401699347) - present_state_Q (0.006819278644725035)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3569641278181362 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005952400599311569) - present_state_Q ( 0.0009102500215449477)) * f1( 0.2843914756646558)
w2 ( 0.05066595447827046 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.005952400599311569) - present_state_Q (0.0009102500215449477)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3568410754322424 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017409555626983347) - present_state_Q ( -0.004371008580599467)) * f1( 0.269677481600833)
w2 ( 0.0515785437323923 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( 0.0017409555626983347) - present_state_Q (-0.004371008580599467)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3567735064613122 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00450379452443557) - present_state_Q ( -0.002064479721909901)) * f1( 0.26772921757050205)
w2 ( 0.05208329978063698 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.00450379452443557) - present_state_Q (-0.002064479721909901)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.356991077684201 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004628828437536134) - present_state_Q ( 0.00864790782650239)) * f1( 0.26596113781255803)
w2 ( 0.0506107981797605 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.004628828437536134) - present_state_Q (0.00864790782650239)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35705870303501247 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0026956973038534116) - present_state_Q ( 0.0026956973038534116)) * f1( 0.2789942899937036)
w2 ( 0.05012601871841038 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.0026956973038534116) - present_state_Q (0.0026956973038534116)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3569901588185798 ) += alpha ( 0.1 ) * (reward ( 1.1151333587446788e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002647434792810349) - present_state_Q ( -0.002351659492348321)) * f1( 0.26186721040067346)
w2 ( 0.05059717197730824 ) += alpha ( 0.1) * (reward ( 1.1151333587446788e-06) + discount_factor ( 0.1) * next_state_max_Q( 0.002647434792810349) - present_state_Q (-0.002351659492348321)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3569261939551837 ) += alpha ( 0.1 ) * (reward ( 5.575666793723394e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004503689238291475) - present_state_Q ( -0.001987226305074974)) * f1( 0.26234969158608223)
w2 ( 0.05108480253642494 ) += alpha ( 0.1) * (reward ( 5.575666793723394e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.004503689238291475) - present_state_Q (-0.001987226305074974)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3568946326451718 ) += alpha ( 0.1 ) * (reward ( 2.787833396861697e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006341998408949598) - present_state_Q ( -0.000576226134370153)) * f1( 0.26068543786251375)
w2 ( 0.0513269434881459 ) += alpha ( 0.1) * (reward ( 2.787833396861697e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.006341998408949598) - present_state_Q (-0.000576226134370153)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35688267111728533 ) += alpha ( 0.1 ) * (reward ( 1.3939166984308485e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007625932295508328) - present_state_Q ( 0.00030132175668615147)) * f1( 0.25923810655250795)
w2 ( 0.0514192256610528 ) += alpha ( 0.1) * (reward ( 1.3939166984308485e-07) + discount_factor ( 0.1) * next_state_max_Q( 0.007625932295508328) - present_state_Q (0.00030132175668615147)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3568816723107029 ) += alpha ( 0.1 ) * (reward ( 6.969583492154242e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008544882379714416) - present_state_Q ( 0.0008158480154671771)) * f1( 0.2580234279217375)
w2 ( 0.05142696764472064 ) += alpha ( 0.1) * (reward ( 6.969583492154242e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.008544882379714416) - present_state_Q (0.0008158480154671771)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35695029133990197 ) += alpha ( 0.1 ) * (reward ( 3.484791746077121e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009421554837250115) - present_state_Q ( 0.0036116119498724036)) * f1( 0.2570557933990552)
w2 ( 0.050946471753439256 ) += alpha ( 0.1) * (reward ( 3.484791746077121e-08) + discount_factor ( 0.1) * next_state_max_Q( 0.009421554837250115) - present_state_Q (0.0036116119498724036)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3571321273642966 ) += alpha ( 0.1 ) * (reward ( 8.711979365192803e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013352674630551398) - present_state_Q ( 0.008338470655528868)) * f1( 0.2596472579778767)
w2 ( 0.049545832857340384 ) += alpha ( 0.1) * (reward ( 8.711979365192803e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.013352674630551398) - present_state_Q (0.008338470655528868)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3572873111494435 ) += alpha ( 0.1 ) * (reward ( 4.3559896825964014e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006578809873291319) - present_state_Q ( 0.006578809873291319)) * f1( 0.2620938408543374)
w2 ( 0.048361647951345885 ) += alpha ( 0.1) * (reward ( 4.3559896825964014e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.006578809873291319) - present_state_Q (0.006578809873291319)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3572853427524714 ) += alpha ( 0.1 ) * (reward ( 2.1779948412982007e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005090031484671895) - present_state_Q ( 0.0004286414924755677)) * f1( 0.2449356724862711)
w2 ( 0.04837611344146345 ) += alpha ( 0.1) * (reward ( 2.1779948412982007e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.005090031484671895) - present_state_Q (0.0004286414924755677)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3572558666871844 ) += alpha ( 0.1 ) * (reward ( 1.0889974206491004e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005979283662752036) - present_state_Q ( -0.0005974098358314806)) * f1( 0.24659162052478187)
w2 ( 0.04861518129968427 ) += alpha ( 0.1) * (reward ( 1.0889974206491004e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.005979283662752036) - present_state_Q (-0.0005974098358314806)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3572419053339511 ) += alpha ( 0.1 ) * (reward ( 5.444987103245502e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00725662684579656) - present_state_Q ( 0.00015671880778847513)) * f1( 0.24539045837994772)
w2 ( 0.04872897018394225 ) += alpha ( 0.1) * (reward ( 5.444987103245502e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.00725662684579656) - present_state_Q (0.00015671880778847513)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35723560135500515 ) += alpha ( 0.1 ) * (reward ( 2.722493551622751e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008202635946240905) - present_state_Q ( 0.0005624369867233969)) * f1( 0.24450433338335625)
w2 ( 0.048780535559972256 ) += alpha ( 0.1) * (reward ( 2.722493551622751e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.008202635946240905) - present_state_Q (0.0005624369867233969)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35722908504506945 ) += alpha ( 0.1 ) * (reward ( 1.3612467758113754e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009469765272668837) - present_state_Q ( 0.0006798617189259365)) * f1( 0.24395152988254487)
w2 ( 0.048828616249976066 ) += alpha ( 0.1) * (reward ( 1.3612467758113754e-10) + discount_factor ( 0.1) * next_state_max_Q( 0.009469765272668837) - present_state_Q (0.0006798617189259365)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35742627428470897 ) += alpha ( 0.1 ) * (reward ( 3.4031169395284386e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01619081809251094) - present_state_Q ( 0.009567533520103338)) * f1( 0.24808509588671177)
w2 ( 0.04723892591461185 ) += alpha ( 0.1) * (reward ( 3.4031169395284386e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.01619081809251094) - present_state_Q (0.009567533520103338)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3575915493762502 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005805720300871536) - present_state_Q ( 0.007357281074069022)) * f1( 0.24388695292002266)
w2 ( 0.046019118289757924 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( 0.005805720300871536) - present_state_Q (0.007357281074069022)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.357675712033668 ) += alpha ( 0.1 ) * (reward ( 8.507792348821097e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012283437006080339) - present_state_Q ( 0.004620835189387551)) * f1( 0.24808509588671177)
w2 ( 0.045340619993703576 ) += alpha ( 0.1) * (reward ( 8.507792348821097e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.012283437006080339) - present_state_Q (0.004620835189387551)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3577371664986868 ) += alpha ( 0.1 ) * (reward ( 4.253896174410548e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004515277224173997) - present_state_Q ( 0.003206922697219061)) * f1( 0.22303323362241764)
w2 ( 0.04478954099959402 ) += alpha ( 0.1) * (reward ( 4.253896174410548e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.004515277224173997) - present_state_Q (0.003206922697219061)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3578051196391198 ) += alpha ( 0.1 ) * (reward ( 2.126948087205274e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004639992514102298) - present_state_Q ( 0.00356390588960441)) * f1( 0.21921028085929137)
w2 ( 0.04416955967238057 ) += alpha ( 0.1) * (reward ( 2.126948087205274e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.004639992514102298) - present_state_Q (0.00356390588960441)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35783857468412605 ) += alpha ( 0.1 ) * (reward ( 1.063474043602637e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00406967631146582) - present_state_Q ( 0.0019601128877155954)) * f1( 0.21540190711480825)
w2 ( 0.04388999352638957 ) += alpha ( 0.1) * (reward ( 1.063474043602637e-12) + discount_factor ( 0.1) * next_state_max_Q( 0.00406967631146582) - present_state_Q (0.0019601128877155954)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3577340334876277 ) += alpha ( 0.1 ) * (reward ( 5.317370218013185e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00748080256506993) - present_state_Q ( -0.0038022439865137148)) * f1( 0.22974449929905674)
w2 ( 0.04480005837510006 ) += alpha ( 0.1) * (reward ( 5.317370218013185e-13) + discount_factor ( 0.1) * next_state_max_Q( 0.00748080256506993) - present_state_Q (-0.0038022439865137148)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3578931899643263 ) += alpha ( 0.1 ) * (reward ( 2.6586851090065927e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005347415764642011) - present_state_Q ( 0.007412690344353087)) * f1( 0.23140107912375996)
w2 ( 0.04356202759692791 ) += alpha ( 0.1) * (reward ( 2.6586851090065927e-13) + discount_factor ( 0.1) * next_state_max_Q( 0.005347415764642011) - present_state_Q (0.007412690344353087)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35802416343494614 ) += alpha ( 0.1 ) * (reward ( 1.3293425545032963e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008459004914135593) - present_state_Q ( 0.0064069884301687224)) * f1( 0.23551771175965563)
w2 ( 0.04244981000920347 ) += alpha ( 0.1) * (reward ( 1.3293425545032963e-13) + discount_factor ( 0.1) * next_state_max_Q( 0.008459004914135593) - present_state_Q (0.0064069884301687224)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3580550885630475 ) += alpha ( 0.1 ) * (reward ( 3.323356386258241e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004121557266461254) - present_state_Q ( 0.0017902772894723418)) * f1( 0.22440058218595213)
w2 ( 0.04217418569664487 ) += alpha ( 0.1) * (reward ( 3.323356386258241e-14) + discount_factor ( 0.1) * next_state_max_Q( 0.004121557266461254) - present_state_Q (0.0017902772894723418)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35818430569663856 ) += alpha ( 0.1 ) * (reward ( 8.308390965645602e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004000601065099224) - present_state_Q ( 0.0062233690723934865)) * f1( 0.22189640691959667)
w2 ( 0.04100952390346982 ) += alpha ( 0.1) * (reward ( 8.308390965645602e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.004000601065099224) - present_state_Q (0.0062233690723934865)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3582908192099814 ) += alpha ( 0.1 ) * (reward ( 4.154195482822801e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003021517158913384) - present_state_Q ( 0.005183777868875)) * f1( 0.21819268826601174)
w2 ( 0.040033198672873915 ) += alpha ( 0.1) * (reward ( 4.154195482822801e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.003021517158913384) - present_state_Q (0.005183777868875)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.358382518921651 ) += alpha ( 0.1 ) * (reward ( 2.0770977414114005e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0024206590872596218) - present_state_Q ( 0.00451684721468823)) * f1( 0.21451322326539812)
w2 ( 0.03917824241168188 ) += alpha ( 0.1) * (reward ( 2.0770977414114005e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.0024206590872596218) - present_state_Q (0.00451684721468823)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35846431945443863 ) += alpha ( 0.1 ) * (reward ( 1.0385488707057003e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002061603464395198) - present_state_Q ( 0.004085518973123689)) * f1( 0.21086097125693506)
w2 ( 0.03840237068634525 ) += alpha ( 0.1) * (reward ( 1.0385488707057003e-15) + discount_factor ( 0.1) * next_state_max_Q( 0.002061603464395198) - present_state_Q (0.004085518973123689)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35853927432579535 ) += alpha ( 0.1 ) * (reward ( 5.192744353528501e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0018579451619139264) - present_state_Q ( 0.003802620458166983)) * f1( 0.20723936556313188)
w2 ( 0.03767900549795024 ) += alpha ( 0.1) * (reward ( 5.192744353528501e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.0018579451619139264) - present_state_Q (0.003802620458166983)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3584558030653144 ) += alpha ( 0.1 ) * (reward ( 2.5963721767642507e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017550381162464157) - present_state_Q ( -0.003923208418579099)) * f1( 0.20365240542107066)
w2 ( 0.03849874794399104 ) += alpha ( 0.1) * (reward ( 2.5963721767642507e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.0017550381162464157) - present_state_Q (-0.003923208418579099)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3585665064665612 ) += alpha ( 0.1 ) * (reward ( 1.2981860883821253e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003847675743154269) - present_state_Q ( 0.005917039610727898)) * f1( 0.20010476801962937)
w2 ( 0.03750293897743682 ) += alpha ( 0.1) * (reward ( 1.2981860883821253e-16) + discount_factor ( 0.1) * next_state_max_Q( 0.003847675743154269) - present_state_Q (0.005917039610727898)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3586213497967572 ) += alpha ( 0.1 ) * (reward ( 6.490930441910627e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005190011891329824) - present_state_Q ( 0.00320648694497766)) * f1( 0.20406928697845392)
w2 ( 0.036965441826267896 ) += alpha ( 0.1) * (reward ( 6.490930441910627e-17) + discount_factor ( 0.1) * next_state_max_Q( 0.005190011891329824) - present_state_Q (0.00320648694497766)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35876210743198006 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0011183901752933184) - present_state_Q ( -0.0020178562349610907)) * f1( 0.1947082753253604)
w2 ( 0.035519610774786964 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0011183901752933184) - present_state_Q (-0.0020178562349610907)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35899424945255626 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0009794631646077995) - present_state_Q ( 0.0031063510544472317)) * f1( 0.1911644456223705)
w2 ( 0.03333376687647885 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0009794631646077995) - present_state_Q (0.0031063510544472317)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.35911777800080474 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0002178200907681921) - present_state_Q ( -0.002831302880302547)) * f1( 0.19528193455672907)
w2 ( 0.032068636555756716 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0002178200907681921) - present_state_Q (-0.002831302880302547)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3592889323481913 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00021185522251880584) - present_state_Q ( 0.0005093901934376671)) * f1( 0.1778526860057041)
w2 ( 0.030143961126552276 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.00021185522251880584) - present_state_Q (0.0005093901934376671)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3593376670494625 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003921132109368196) - present_state_Q ( -0.006789486198496196)) * f1( 0.17800683175548992)
w2 ( 0.029596401229096868 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.003921132109368196) - present_state_Q (-0.006789486198496196)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35943377300474577 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001723991069124263) - present_state_Q ( -0.003310967678526483)) * f1( 0.17004469113927062)
w2 ( 0.02846604009121737 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.001723991069124263) - present_state_Q (-0.003310967678526483)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3594896297620743 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0022100662218415904) - present_state_Q ( -0.005856615295670241)) * f1( 0.18268465651689592)
w2 ( 0.027854529979820967 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0022100662218415904) - present_state_Q (-0.005856615295670241)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35965305126336 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002291309037467548) - present_state_Q ( 0.0008567024014283359)) * f1( 0.15988788657696215)
w2 ( 0.025810328823818666 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002291309037467548) - present_state_Q (0.0008567024014283359)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3596488632290055 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003233547704033349) - present_state_Q ( -0.00970772225288357)) * f1( 0.16806253038870403)
w2 ( 0.025860167825347433 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.003233547704033349) - present_state_Q (-0.00970772225288357)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35960971825111016 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005382323093469463) - present_state_Q ( -0.011913099537136318)) * f1( 0.1747781828025596)
w2 ( 0.026308106775938027 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005382323093469463) - present_state_Q (-0.011913099537136318)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35965058700097924 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007096894998380607) - present_state_Q ( -0.007096894998380607)) * f1( 0.14872358445945846)
w2 ( 0.025758513380679256 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.007096894998380607) - present_state_Q (-0.007096894998380607)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35977702850542986 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0014828003025263228) - present_state_Q ( -0.00047583217862533916)) * f1( 0.14856140186943242)
w2 ( 0.024056301327487567 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0014828003025263228) - present_state_Q (-0.00047583217862533916)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35991899098315133 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004157822506436877) - present_state_Q ( 0.0006534377590606336)) * f1( 0.13911899011774992)
w2 ( 0.02201542283057942 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.004157822506436877) - present_state_Q (0.0006534377590606336)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.35996374527068536 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00037626052253268705) - present_state_Q ( -0.006040720888616728)) * f1( 0.14640772604365193)
w2 ( 0.02140405772378614 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.00037626052253268705) - present_state_Q (-0.006040720888616728)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3600950305467548 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.000665761138684641) - present_state_Q ( 0.0020194321973779628)) * f1( 0.11699773720094159)
w2 ( 0.019159821566569573 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.000665761138684641) - present_state_Q (0.0020194321973779628)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36013326311506916 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004676180285553465) - present_state_Q ( -0.006091368656901679)) * f1( 0.10888059021618221)
w2 ( 0.01852776563420136 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.004676180285553465) - present_state_Q (-0.006091368656901679)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3601856770541889 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011590264020452262) - present_state_Q ( 0.0011590264020452262)) * f1( 0.007071067811865476)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.0011590264020452262) - present_state_Q (0.0011590264020452262)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3602304085682306 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0047004809758825865) - present_state_Q ( -0.0011563415982470697)) * f1( 0.007071067811865476)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.0047004809758825865) - present_state_Q (-0.0011563415982470697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3603379774635885 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0035273052925983545) - present_state_Q ( -0.0060052626087586805)) * f1( 0.018452927338517573)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.0035273052925983545) - present_state_Q (-0.0060052626087586805)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36042116117147954 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005625400891152755) - present_state_Q ( -0.005913994770934134)) * f1( 0.014196443220647217)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.005625400891152755) - present_state_Q (-0.005913994770934134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3604921155526418 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007272350684855828) - present_state_Q ( -0.010159828291170606)) * f1( 0.026174608276112413)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.007272350684855828) - present_state_Q (-0.010159828291170606)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3605067446153592 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006262179619201854) - present_state_Q ( -0.001970708910842327)) * f1( 0.004156432556248625)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006262179619201854) - present_state_Q (-0.001970708910842327)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3605814481745528 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0072938550432706944) - present_state_Q ( -0.010329767245360878)) * f1( 0.02772928904694958)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0072938550432706944) - present_state_Q (-0.010329767245360878)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3606051068933135 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029725498911521474) - present_state_Q ( -0.002387450203299093)) * f1( 0.006867454002301241)
w2 ( 0.017045275562990722 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0029725498911521474) - present_state_Q (-0.002387450203299093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36062387517251504 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0033413628615341162) - present_state_Q ( 0.0029727333589843557)) * f1( 0.004790357521017471)
w2 ( 0.01626168982354719 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.0033413628615341162) - present_state_Q (0.0029727333589843557)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3606639924607466 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0021918658231481645) - present_state_Q ( 0.0012989940082739643)) * f1( 0.010663678335952142)
w2 ( 0.014756869930535 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.0021918658231481645) - present_state_Q (0.0012989940082739643)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.36071679676892004 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003536040459111804) - present_state_Q ( 0.0003931250157196229)) * f1( 0.014435211530724695)
w2 ( 0.013293661495768837 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.003536040459111804) - present_state_Q (0.0003931250157196229)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.36076126198271885 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001745276181534118) - present_state_Q ( 0.001003543214532319)) * f1( 0.01574852106315721)
w2 ( 0.012164280174933312 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.001745276181534118) - present_state_Q (0.001003543214532319)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3607908693939633 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(6.551023108399869e-05) - present_state_Q ( 0.0024983662660706613)) * f1( 0.009903027662617575)
w2 ( 0.011566333521583882 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 6.551023108399869e-05) - present_state_Q (0.0024983662660706613)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3608341163545 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0025627734467120336) - present_state_Q ( 0.0003117261679090929)) * f1( 0.01574852106315721)
w2 ( 0.010467894871673997 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.0025627734467120336) - present_state_Q (0.0003117261679090929)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.36086013764410907 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0001708008510213745) - present_state_Q ( -0.0011123422950012514)) * f1( 0.009903027662617575)
w2 ( 0.009416851069897763 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.0001708008510213745) - present_state_Q (-0.0011123422950012514)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.360884855292613 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001192141231894199) - present_state_Q ( -0.0017359939589762505)) * f1( 0.009674109292665078)
w2 ( 0.008905844883050892 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.001192141231894199) - present_state_Q (-0.0017359939589762505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36092151073722867 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0008134112678022517) - present_state_Q ( 0.0009677577088079269)) * f1( 0.012882073081087905)
w2 ( 0.007767660232646997 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0008134112678022517) - present_state_Q (0.0009677577088079269)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.36095653500132896 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-7.290934579434879e-05) - present_state_Q ( -7.290934579434879e-05)) * f1( 0.012810677933838437)
w2 ( 0.006674064272115225 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -7.290934579434879e-05) - present_state_Q (-7.290934579434879e-05)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.36098529063965074 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002117478402149107) - present_state_Q ( -0.0019183950409431016)) * f1( 0.011189456217870273)
w2 ( 0.005646109463163984 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002117478402149107) - present_state_Q (-0.0019183950409431016)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3610001607754047 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00313448419896347) - present_state_Q ( 0.0002876781399101618)) * f1( 0.007879690646524402)
w2 ( 0.005268680032974397 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00313448419896347) - present_state_Q (0.0002876781399101618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36101536604106954 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0034299837293249253) - present_state_Q ( 0.00040639899450251526)) * f1( 0.007994464599653943)
w2 ( 0.004888285186632241 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0034299837293249253) - present_state_Q (0.00040639899450251526)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3610302223210934 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037818322570624405) - present_state_Q ( 0.00047332226586842296)) * f1( 0.007769269020000587)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0037818322570624405) - present_state_Q (0.00047332226586842296)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.361036985073182 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0044377753477775565) - present_state_Q ( -0.0006693297082374647)) * f1( 0.007590393147887741)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0044377753477775565) - present_state_Q (-0.0006693297082374647)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.361039825343083 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005945878015446462) - present_state_Q ( -0.0011765411775151655)) * f1( 0.003320702846688322)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005945878015446462) - present_state_Q (-0.0011765411775151655)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3610396958002405 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005671686473608794) - present_state_Q ( -0.0008943030535863037)) * f1( 0.003959927174797048)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.005671686473608794) - present_state_Q (-0.0008943030535863037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3610981103270073 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005161959701897686) - present_state_Q ( -0.007527719388246246)) * f1( 0.019781976170515032)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005161959701897686) - present_state_Q (-0.007527719388246246)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3611387633598844 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004879987146508078) - present_state_Q ( -0.0077698889343182295)) * f1( 0.020201642806863113)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004879987146508078) - present_state_Q (-0.0077698889343182295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3611857538631982 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006821402023834868) - present_state_Q ( -0.009546862903972367)) * f1( 0.02534438464803276)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006821402023834868) - present_state_Q (-0.009546862903972367)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.361238858366664 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004934395418020824) - present_state_Q ( -0.007219702847524678)) * f1( 0.02568008650639002)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004934395418020824) - present_state_Q (-0.007219702847524678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36127090843973875 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0033224735938306594) - present_state_Q ( -0.004296430969733981)) * f1( 0.022402985898605468)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0033224735938306594) - present_state_Q (-0.004296430969733981)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36128570655347075 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004483541274194617) - present_state_Q ( -0.000689372510336124)) * f1( 0.008207801704178064)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004483541274194617) - present_state_Q (-0.000689372510336124)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3613087369125571 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0034859428589187847) - present_state_Q ( -0.0034859428589187847)) * f1( 0.015218637811802311)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0034859428589187847) - present_state_Q (-0.0034859428589187847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3613250693863509 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003966471382253001) - present_state_Q ( -0.0012490413099611587)) * f1( 0.00937680557376479)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003966471382253001) - present_state_Q (-0.0012490413099611587)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3613327880378788 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005684183272295228) - present_state_Q ( -0.0009150757095582696)) * f1( 0.004306397050792926)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005684183272295228) - present_state_Q (-0.0009150757095582696)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3613522043866391 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006284952310007635) - present_state_Q ( -0.009246145908104363)) * f1( 0.020114952584216354)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006284952310007635) - present_state_Q (-0.009246145908104363)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.361335200454662 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005074120034766759) - present_state_Q ( -0.007574104853097716)) * f1( 0.02406207873890457)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.005074120034766759) - present_state_Q (-0.007574104853097716)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3613192754239671 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005467315293367268) - present_state_Q ( -0.008144388358942043)) * f1( 0.02096045011252674)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.005467315293367268) - present_state_Q (-0.008144388358942043)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3613974661966093 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005279165547258474) - present_state_Q ( -0.007001606128099657)) * f1( 0.019945520121929326)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005279165547258474) - present_state_Q (-0.007001606128099657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36144383832240373 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006410532526939737) - present_state_Q ( -0.006410532526939737)) * f1( 0.011620227690969705)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.006410532526939737) - present_state_Q (-0.006410532526939737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36150738280772127 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006265778862068394) - present_state_Q ( -0.006786648091852923)) * f1( 0.016080782348108863)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.006265778862068394) - present_state_Q (-0.006786648091852923)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3615223983352319 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005210016745436064) - present_state_Q ( -0.0016918838868586045)) * f1( 0.005723551095416148)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.005210016745436064) - present_state_Q (-0.0016918838868586045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36155775603260054 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004992772718150783) - present_state_Q ( -0.006549105142391278)) * f1( 0.01655657021952419)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004992772718150783) - present_state_Q (-0.006549105142391278)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3615780410104575 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0034749440377088133) - present_state_Q ( -0.0034749440377088133)) * f1( 0.013395706325641831)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0034749440377088133) - present_state_Q (-0.0034749440377088133)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36159433320513185 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003987221096381032) - present_state_Q ( -0.001233989895089273)) * f1( 0.009344492439535598)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003987221096381032) - present_state_Q (-0.001233989895089273)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36168986445860707 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004853362440447346) - present_state_Q ( -0.006950381330465578)) * f1( 0.01975991265135332)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.004853362440447346) - present_state_Q (-0.006950381330465578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3617382470925896 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004300333775460516) - present_state_Q ( -0.0009149788616026843)) * f1( 0.008905966644773526)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.004300333775460516) - present_state_Q (-0.0009149788616026843)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36182950562475913 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006499916842966314) - present_state_Q ( -0.009163424898765324)) * f1( 0.019711287182841736)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.006499916842966314) - present_state_Q (-0.009163424898765324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3619283997954462 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005370370581904287) - present_state_Q ( -0.007970516870213808)) * f1( 0.02087363322354931)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005370370581904287) - present_state_Q (-0.007970516870213808)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36195550017438244 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004927964782747268) - present_state_Q ( -0.001780957160952614)) * f1( 0.006105380072290572)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004927964782747268) - present_state_Q (-0.001780957160952614)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3619773181652595 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029951506861785417) - present_state_Q ( -0.0025525993989446052)) * f1( 0.005024549754751632)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0029951506861785417) - present_state_Q (-0.0025525993989446052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3620302630621503 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004852506224891163) - present_state_Q ( -0.005126515696156158)) * f1( 0.012902501882268365)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004852506224891163) - present_state_Q (-0.005126515696156158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36209992031136956 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005958039601057421) - present_state_Q ( -0.009318530996717292)) * f1( 0.025040384719380852)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005958039601057421) - present_state_Q (-0.009318530996717292)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36214827433957164 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0031987693499468026) - present_state_Q ( -0.0031987693499468026)) * f1( 0.014364660183257763)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0031987693499468026) - present_state_Q (-0.0031987693499468026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3621931605782349 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003274266237975255) - present_state_Q ( -0.0038955769878882373)) * f1( 0.013613218543656366)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.003274266237975255) - present_state_Q (-0.0038955769878882373)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3622327195638492 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00423917867947238) - present_state_Q ( -0.0010339757416420506)) * f1( 0.011009821810561449)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.00423917867947238) - present_state_Q (-0.0010339757416420506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3622782797411388 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005056904599225126) - present_state_Q ( -0.0058725191663075126)) * f1( 0.014614865001027169)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005056904599225126) - present_state_Q (-0.0058725191663075126)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3623379520638422 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0052820452915475075) - present_state_Q ( -0.007785128815519115)) * f1( 0.02037727097396956)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0052820452915475075) - present_state_Q (-0.007785128815519115)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3623723253102322 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005807758067321302) - present_state_Q ( -0.005807758067321302)) * f1( 0.010977060521415456)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005807758067321302) - present_state_Q (-0.005807758067321302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36250738517160747 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006723220016553614) - present_state_Q ( -0.008800001305279553)) * f1( 0.024196242030295734)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006723220016553614) - present_state_Q (-0.008800001305279553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36259131290019614 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004780799646807579) - present_state_Q ( -0.007825550121531872)) * f1( 0.02189701256259132)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004780799646807579) - present_state_Q (-0.007825550121531872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3626711076688409 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006812701123743528) - present_state_Q ( -0.0075737391110568455)) * f1( 0.020574468013206518)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.006812701123743528) - present_state_Q (-0.0075737391110568455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36276060364226337 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00656374952963882) - present_state_Q ( -0.009238456384604916)) * f1( 0.024126948277687)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00656374952963882) - present_state_Q (-0.009238456384604916)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3627811582696528 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005324535817378455) - present_state_Q ( -0.0007938921230379403)) * f1( 0.004526013027281886)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005324535817378455) - present_state_Q (-0.0007938921230379403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3627973526118007 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005966815666943928) - present_state_Q ( -0.0012197928794544365)) * f1( 0.003594529027938529)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005966815666943928) - present_state_Q (-0.0012197928794544365)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36284159246707587 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007258815602149408) - present_state_Q ( -0.008170392927857114)) * f1( 0.02216313904676373)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007258815602149408) - present_state_Q (-0.008170392927857114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36288489218344644 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004296463253190763) - present_state_Q ( -0.00767189191500189)) * f1( 0.02147454873656955)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004296463253190763) - present_state_Q (-0.00767189191500189)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3629220809604054 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006169063175642371) - present_state_Q ( -0.006418587837282132)) * f1( 0.017213969372318807)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006169063175642371) - present_state_Q (-0.006418587837282132)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3629629756208853 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0047715197158890044) - present_state_Q ( -0.0068608177779675055)) * f1( 0.019453405543332285)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0047715197158890044) - present_state_Q (-0.0068608177779675055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36299979974812246 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004527225823399121) - present_state_Q ( -0.0067063718799302)) * f1( 0.017409397135143063)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004527225823399121) - present_state_Q (-0.0067063718799302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3630097609606367 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0060784720809192476) - present_state_Q ( -0.0016782151528849406)) * f1( 0.003782478061075662)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0060784720809192476) - present_state_Q (-0.0016782151528849406)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36302077642229896 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0054862121200712636) - present_state_Q ( -0.0008792442166211308)) * f1( 0.0040685150942567725)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0054862121200712636) - present_state_Q (-0.0008792442166211308)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3630520410306583 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006256334111752667) - present_state_Q ( -0.006256334111752667)) * f1( 0.014358149947125656)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006256334111752667) - present_state_Q (-0.006256334111752667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36307479746737803 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0043612132027130435) - present_state_Q ( -0.0019276963760667203)) * f1( 0.008781541764641559)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0043612132027130435) - present_state_Q (-0.0019276963760667203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3631478925485472 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006101356633496959) - present_state_Q ( -0.010211722808876222)) * f1( 0.027133450522499096)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006101356633496959) - present_state_Q (-0.010211722808876222)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.363216869321632 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006742209645245838) - present_state_Q ( -0.009367987994413955)) * f1( 0.02476998029843031)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006742209645245838) - present_state_Q (-0.009367987994413955)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36328341690050103 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006393900390949876) - present_state_Q ( -0.009070929555286454)) * f1( 0.023674703063568014)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006393900390949876) - present_state_Q (-0.009070929555286454)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36332285715459806 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004971476446773204) - present_state_Q ( -0.004971476446773204)) * f1( 0.01229957274459538)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004971476446773204) - present_state_Q (-0.004971476446773204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36339754824343323 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005536572188352882) - present_state_Q ( -0.010301479062742534)) * f1( 0.027877227880440125)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005536572188352882) - present_state_Q (-0.010301479062742534)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3634444328573459 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004122885214442776) - present_state_Q ( -0.004921281494128077)) * f1( 0.014636943531290176)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004122885214442776) - present_state_Q (-0.004921281494128077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3634885476985503 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003475071930357701) - present_state_Q ( -0.0041348239472361525)) * f1( 0.01346879327784425)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.003475071930357701) - present_state_Q (-0.0041348239472361525)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3635545886329261 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006419530860230631) - present_state_Q ( -0.008911105678331779)) * f1( 0.023359513017823658)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006419530860230631) - present_state_Q (-0.008911105678331779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3636025153091459 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005703005778113244) - present_state_Q ( -0.005825579189752237)) * f1( 0.015319177293057222)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005703005778113244) - present_state_Q (-0.005825579189752237)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3636123312489308 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006023689968227462) - present_state_Q ( -0.0013768391762445464)) * f1( 0.0027444722595028957)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006023689968227462) - present_state_Q (-0.0013768391762445464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36366515459038773 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005748056722056185) - present_state_Q ( -0.005929026005242797)) * f1( 0.01693790358175887)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005748056722056185) - present_state_Q (-0.005929026005242797)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.363718739868595 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006159615810760317) - present_state_Q ( -0.00748988789686554)) * f1( 0.018062394320943073)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006159615810760317) - present_state_Q (-0.00748988789686554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36380664558135317 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005805414057021173) - present_state_Q ( -0.006797060431682184)) * f1( 0.01808963650925713)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005805414057021173) - present_state_Q (-0.006797060431682184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36389976508140254 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005670605524500409) - present_state_Q ( -0.006729245342996106)) * f1( 0.019141151319596154)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005670605524500409) - present_state_Q (-0.006729245342996106)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3639246870941093 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005612951721374392) - present_state_Q ( -0.001937225674953919)) * f1( 0.004663977550279141)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005612951721374392) - present_state_Q (-0.001937225674953919)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36397915380063506 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0062553154575190825) - present_state_Q ( -0.0062553154575190825)) * f1( 0.011074689132091394)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.0062553154575190825) - present_state_Q (-0.0062553154575190825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3640481541891232 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006031743605844199) - present_state_Q ( -0.006501784313971997)) * f1( 0.017346695482676013)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.006031743605844199) - present_state_Q (-0.006501784313971997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36408780588531653 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004314204628347624) - present_state_Q ( -0.0019891394183430737)) * f1( 0.00898761668574246)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004314204628347624) - present_state_Q (-0.0019891394183430737)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36414228748072985 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006539742103532144) - present_state_Q ( -0.0092577675435904)) * f1( 0.019501663430796134)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006539742103532144) - present_state_Q (-0.0092577675435904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3641604129746745 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002027021049414147) - present_state_Q ( -0.003235455914134917)) * f1( 0.005409313750590266)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.002027021049414147) - present_state_Q (-0.003235455914134917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3642074381688411 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037234267842442955) - present_state_Q ( -0.006434007334471793)) * f1( 0.015428706722478687)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0037234267842442955) - present_state_Q (-0.006434007334471793)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3642308986068841 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005007166608457949) - present_state_Q ( -0.0026642376940492433)) * f1( 0.006824424121245462)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005007166608457949) - present_state_Q (-0.0026642376940492433)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36426079527441707 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004822481205933338) - present_state_Q ( -0.003448177274682749)) * f1( 0.00890450650110762)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004822481205933338) - present_state_Q (-0.003448177274682749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3643025848768794 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032748370114960355) - present_state_Q ( -0.0035992435954410227)) * f1( 0.012561150134964599)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0032748370114960355) - present_state_Q (-0.0035992435954410227)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3643371966135186 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004991503902749496) - present_state_Q ( -0.003735301548696642)) * f1( 0.010392498424448272)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004991503902749496) - present_state_Q (-0.003735301548696642)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3644327458582397 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004743873723415913) - present_state_Q ( -0.009740458787360656)) * f1( 0.02624273367244688)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004743873723415913) - present_state_Q (-0.009740458787360656)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36451651283182285 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005592806538382789) - present_state_Q ( -0.008632854477032551)) * f1( 0.022277094509698912)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005592806538382789) - present_state_Q (-0.008632854477032551)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36459493707354323 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0071202354287978) - present_state_Q ( -0.0071202354287978)) * f1( 0.019971717366645932)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0071202354287978) - present_state_Q (-0.0071202354287978)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36466638023570624 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0050240414112157645) - present_state_Q ( -0.007005487469460901)) * f1( 0.018237961130968598)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0050240414112157645) - present_state_Q (-0.007005487469460901)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36475366991910585 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005943749663263002) - present_state_Q ( -0.008992607049822687)) * f1( 0.023416103094244214)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005943749663263002) - present_state_Q (-0.008992607049822687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36484871844544914 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00715457971110645) - present_state_Q ( -0.009834769023265558)) * f1( 0.02600040838262015)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00715457971110645) - present_state_Q (-0.009834769023265558)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36493950720394214 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0062480430275891315) - present_state_Q ( -0.00935980987268256)) * f1( 0.02457678742375917)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0062480430275891315) - present_state_Q (-0.00935980987268256)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3650087543545127 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005398869536854999) - present_state_Q ( -0.00781128632328872)) * f1( 0.0180310164894928)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005398869536854999) - present_state_Q (-0.00781128632328872)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3651129865031162 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005789524202375634) - present_state_Q ( -0.009116828653456694)) * f1( 0.028066182212428334)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005789524202375634) - present_state_Q (-0.009116828653456694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36521089065336504 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0062403009417130745) - present_state_Q ( -0.010660424661374058)) * f1( 0.02747071049968705)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0062403009417130745) - present_state_Q (-0.010660424661374058)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36527408315971954 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005247156537877497) - present_state_Q ( -0.007723348564160781)) * f1( 0.02153649499701639)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005247156537877497) - present_state_Q (-0.007723348564160781)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.365326988055513 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006064006507518778) - present_state_Q ( -0.006492252287362951)) * f1( 0.017258253114894057)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006064006507518778) - present_state_Q (-0.006492252287362951)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36537374057697797 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005354170031480539) - present_state_Q ( -0.0068710826836613816)) * f1( 0.015478392311906465)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005354170031480539) - present_state_Q (-0.0068710826836613816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3654263123976852 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005065120732946676) - present_state_Q ( -0.007734302939861532)) * f1( 0.01793470534880147)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005065120732946676) - present_state_Q (-0.007734302939861532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36543469809838525 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006228847257530898) - present_state_Q ( -0.001547873080927708)) * f1( 0.002354495443434165)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006228847257530898) - present_state_Q (-0.001547873080927708)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36550374479499487 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0068043306120833145) - present_state_Q ( -0.00947086062738428)) * f1( 0.024881457256392087)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0068043306120833145) - present_state_Q (-0.00947086062738428)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36552915113206974 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004898125909055802) - present_state_Q ( -0.0016044468179600747)) * f1( 0.007171652782884754)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004898125909055802) - present_state_Q (-0.0016044468179600747)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3655395546199855 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006401248249734971) - present_state_Q ( -0.0017838989040212063)) * f1( 0.0029390944591189978)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006401248249734971) - present_state_Q (-0.0017838989040212063)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3655626350738761 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004707687853682304) - present_state_Q ( -0.0008166417212900689)) * f1( 0.006376728997947033)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004707687853682304) - present_state_Q (-0.0008166417212900689)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3656293351837745 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0065062355291028535) - present_state_Q ( -0.00918289263937324)) * f1( 0.023814305827523564)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0065062355291028535) - present_state_Q (-0.00918289263937324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3656988754189429 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00680589661675864) - present_state_Q ( -0.00962636723258913)) * f1( 0.025200382715752607)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.00680589661675864) - present_state_Q (-0.00962636723258913)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36577020546356614 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0071795164804477) - present_state_Q ( -0.009933406740406882)) * f1( 0.026104092106473426)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0071795164804477) - present_state_Q (-0.009933406740406882)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36584526315920224 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0068085026695337905) - present_state_Q ( -0.010832893333464402)) * f1( 0.028443177140373966)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0068085026695337905) - present_state_Q (-0.010832893333464402)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.365898725450959 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006225278344414699) - present_state_Q ( -0.00656039103082192)) * f1( 0.017469723393733042)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006225278344414699) - present_state_Q (-0.00656039103082192)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3659533441072722 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006226589979824532) - present_state_Q ( -0.006226589979824532)) * f1( 0.017654938081555407)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006226589979824532) - present_state_Q (-0.006226589979824532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3660144478930627 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007350630442763398) - present_state_Q ( -0.007350630442763398)) * f1( 0.020418892450237487)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.007350630442763398) - present_state_Q (-0.007350630442763398)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36607351174622593 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007745193326698641) - present_state_Q ( -0.008694333983828139)) * f1( 0.02063663415289098)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.007745193326698641) - present_state_Q (-0.008694333983828139)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36613890819620126 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00685152893482536) - present_state_Q ( -0.0080728311323899)) * f1( 0.022432142084336353)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.00685152893482536) - present_state_Q (-0.0080728311323899)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36615706736638376 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00551786982798416) - present_state_Q ( -0.0006998260599960326)) * f1( 0.004989790458306814)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.00551786982798416) - present_state_Q (-0.0006998260599960326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3662051743460945 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005492109729446674) - present_state_Q ( -0.007115071069324354)) * f1( 0.0160491252250508)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005492109729446674) - present_state_Q (-0.007115071069324354)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3662333059585593 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004154482004072972) - present_state_Q ( -0.0012424991587194824)) * f1( 0.00787699413975954)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004154482004072972) - present_state_Q (-0.0012424991587194824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36623820235945054 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005801551240989125) - present_state_Q ( -0.0012555589605049012)) * f1( 0.0013652200857002428)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005801551240989125) - present_state_Q (-0.0012555589605049012)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36630758042745887 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007418412295463541) - present_state_Q ( -0.009089349803301953)) * f1( 0.024608101940368696)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.007418412295463541) - present_state_Q (-0.009089349803301953)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36637651986936126 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006807218399576553) - present_state_Q ( -0.009627203096100882)) * f1( 0.024983300950490266)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006807218399576553) - present_state_Q (-0.009627203096100882)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36644754833578025 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006730802929060102) - present_state_Q ( -0.009766743881879177)) * f1( 0.02587838318649738)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006730802929060102) - present_state_Q (-0.009766743881879177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3664542664976224 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006249084524884603) - present_state_Q ( -0.002130618359375284)) * f1( 0.004007341540717682)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006249084524884603) - present_state_Q (-0.002130618359375284)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3664767342252223 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006093049608248795) - present_state_Q ( -0.006306171880391045)) * f1( 0.017869142926160417)
w2 ( 0.004505848177807291 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006093049608248795) - present_state_Q (-0.006306171880391045)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36656609217442226 ) += alpha ( 0.1 ) * (reward ( -0.12789241464770973 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00169021220361609) - present_state_Q ( -0.00169021220361609)) * f1( 0.007071067811865476)
w2 ( 0.001978423704518186 ) += alpha ( 0.1) * (reward ( -0.12789241464770973) + discount_factor ( 0.1) * next_state_max_Q( -0.00169021220361609) - present_state_Q (-0.00169021220361609)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3666493084899886 ) += alpha ( 0.1 ) * (reward ( -0.11875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004046226994427015) - present_state_Q ( -0.0014762227876431659)) * f1( 0.007071067811865476)
w2 ( -0.00037528913717527115 ) += alpha ( 0.1) * (reward ( -0.11875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.004046226994427015) - present_state_Q (-0.0014762227876431659)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3667727163288088 ) += alpha ( 0.1 ) * (reward ( -0.11875724217287331 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005695703945787857) - present_state_Q ( -0.005695703945787857)) * f1( 0.01086039204555054)
w2 ( -0.002647911309608556 ) += alpha ( 0.1) * (reward ( -0.11875724217287331) + discount_factor ( 0.1) * next_state_max_Q( -0.005695703945787857) - present_state_Q (-0.005695703945787857)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36701005932505026 ) += alpha ( 0.1 ) * (reward ( -0.10048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007007800720023698) - present_state_Q ( -0.009903727974232694)) * f1( 0.02600051794504448)
w2 ( -0.002647911309608556 ) += alpha ( 0.1) * (reward ( -0.10048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.007007800720023698) - present_state_Q (-0.009903727974232694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3670658460170969 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006413097042294962) - present_state_Q ( -0.006413097042294962)) * f1( 0.011375927421032309)
w2 ( -0.002647911309608556 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.006413097042294962) - present_state_Q (-0.006413097042294962)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36709968980421687 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0026104403895382184) - present_state_Q ( -0.004501891268089221)) * f1( 0.008167919572729226)
w2 ( -0.003476611612509489 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0026104403895382184) - present_state_Q (-0.004501891268089221)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36717886470646244 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004426303649393356) - present_state_Q ( -0.007884351304153825)) * f1( 0.020707906408782874)
w2 ( -0.003476611612509489 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004426303649393356) - present_state_Q (-0.007884351304153825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36726850734508687 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0074576101985641425) - present_state_Q ( -0.010205346730978747)) * f1( 0.024752030546488012)
w2 ( -0.004200937145770683 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0074576101985641425) - present_state_Q (-0.010205346730978747)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3672896890178231 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00367882722746875) - present_state_Q ( -0.0035590340789418014)) * f1( 0.004985716562189249)
w2 ( -0.005050631366130425 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00367882722746875) - present_state_Q (-0.0035590340789418014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3673430758269391 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00575127195747185) - present_state_Q ( -0.006314011535003618)) * f1( 0.013367763847647962)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00575127195747185) - present_state_Q (-0.006314011535003618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3673587664273857 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0034500701970720253) - present_state_Q ( -0.0036406219501543633)) * f1( 0.0047196766622616924)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0034500701970720253) - present_state_Q (-0.0036406219501543633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36739397209222957 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003992181133222255) - present_state_Q ( -0.0014169743289690524)) * f1( 0.009910686194258132)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.003992181133222255) - present_state_Q (-0.0014169743289690524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3674253543978016 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0050186546449116545) - present_state_Q ( -0.006547118576441855)) * f1( 0.01469190882323773)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0050186546449116545) - present_state_Q (-0.006547118576441855)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3674424381107376 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00544045845033896) - present_state_Q ( -0.00544045845033896)) * f1( 0.012773889157003851)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00544045845033896) - present_state_Q (-0.00544045845033896)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36747618877054894 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005652591964735312) - present_state_Q ( -0.0009569098841511521)) * f1( 0.004643112613580583)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.005652591964735312) - present_state_Q (-0.0009569098841511521)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3675674335255449 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005691721354870334) - present_state_Q ( -0.005737911518148778)) * f1( 0.01343560704413765)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.005691721354870334) - present_state_Q (-0.005737911518148778)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.367705484454203 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006464279576979932) - present_state_Q ( -0.009287595575127245)) * f1( 0.024961726705359722)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006464279576979932) - present_state_Q (-0.009287595575127245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36775511138359024 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00508626452704524) - present_state_Q ( -0.0023103309827662796)) * f1( 0.007985731183969955)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.00508626452704524) - present_state_Q (-0.0023103309827662796)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36790779073670554 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00676654955174995) - present_state_Q ( -0.010896916673704455)) * f1( 0.02841817885103818)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.00676654955174995) - present_state_Q (-0.010896916673704455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679587106002133 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005428159936032665) - present_state_Q ( -0.002768942086685201)) * f1( 0.008250129049140159)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.005428159936032665) - present_state_Q (-0.002768942086685201)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36809137821082333 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006316813375716117) - present_state_Q ( -0.0091322724278319)) * f1( 0.023927520266180315)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006316813375716117) - present_state_Q (-0.0091322724278319)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36810940543055476 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006298867862061074) - present_state_Q ( -0.0016140433249982411)) * f1( 0.0033491111313418)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.006298867862061074) - present_state_Q (-0.0016140433249982411)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681789118472061 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005963964817347153) - present_state_Q ( -0.00797891288842973)) * f1( 0.01815104292801768)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005963964817347153) - present_state_Q (-0.00797891288842973)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36835428311652973 ) += alpha ( 0.1 ) * (reward ( -0.09135172474836409 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005419442547685673) - present_state_Q ( -0.008152485039058972)) * f1( 0.020942057542306686)
w2 ( -0.005849370926828938 ) += alpha ( 0.1) * (reward ( -0.09135172474836409) + discount_factor ( 0.1) * next_state_max_Q( -0.005419442547685673) - present_state_Q (-0.008152485039058972)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36854933049456595 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005321137131849507) - present_state_Q ( -0.011429294138126178)) * f1( 0.027348443064013836)
w2 ( -0.007275758363800667 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.005321137131849507) - present_state_Q (-0.011429294138126178)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36874993202135653 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0073118058809227525) - present_state_Q ( -0.012901098405513976)) * f1( 0.028638281960037506)
w2 ( -0.008676691052922786 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.0073118058809227525) - present_state_Q (-0.012901098405513976)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36889799648672367 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006548187170363152) - present_state_Q ( -0.008767133593551564)) * f1( 0.019980566640725422)
w2 ( -0.010158775800863034 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.006548187170363152) - present_state_Q (-0.008767133593551564)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36891562493768143 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008230628355582744) - present_state_Q ( -0.0034645109428237814)) * f1( 0.002215322385366121)
w2 ( -0.011750277884188278 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.008230628355582744) - present_state_Q (-0.0034645109428237814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3690965036109351 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00562141545844305) - present_state_Q ( -0.009951143198973505)) * f1( 0.024836572384051785)
w2 ( -0.013206828896596247 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.00562141545844305) - present_state_Q (-0.009951143198973505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36914496368750305 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007466009094604153) - present_state_Q ( -0.0059789468631232506)) * f1( 0.006294807582559506)
w2 ( -0.016286197149390842 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.007466009094604153) - present_state_Q (-0.0059789468631232506)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3692835017855818 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009264566603358056) - present_state_Q ( -0.012049365463290959)) * f1( 0.02235983165050198)
w2 ( -0.017525366569305565 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.009264566603358056) - present_state_Q (-0.012049365463290959)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3694401879651896 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00832849584743774) - present_state_Q ( -0.010500505310025388)) * f1( 0.024708559842103465)
w2 ( -0.01879364105077376 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.00832849584743774) - present_state_Q (-0.010500505310025388)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3695340062596235 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006691459207496603) - present_state_Q ( -0.00709324603459609)) * f1( 0.016309954378301367)
w2 ( -0.01879364105077376 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006691459207496603) - present_state_Q (-0.00709324603459609)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36964312767542157 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003750203908230763) - present_state_Q ( -0.005149064521740416)) * f1( 0.018441343008244477)
w2 ( -0.01879364105077376 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.003750203908230763) - present_state_Q (-0.005149064521740416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3697049937742235 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002512910385466602) - present_state_Q ( -0.0032364735718257566)) * f1( 0.010148467650352001)
w2 ( -0.01879364105077376 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.002512910385466602) - present_state_Q (-0.0032364735718257566)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.369737283162529 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04324042750010592) - present_state_Q ( -0.048258068170615585)) * f1( 0.01613486648362967)
w2 ( -0.01879364105077376 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.04324042750010592) - present_state_Q (-0.048258068170615585)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3698972313241741 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00617850632027523) - present_state_Q ( -0.009819022805425947)) * f1( 0.029216925554169257)
w2 ( -0.01879364105077376 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.00617850632027523) - present_state_Q (-0.009819022805425947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3700272341592346 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010785393619430563) - present_state_Q ( -0.013449691811394996)) * f1( 0.025206533541663877)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.010785393619430563) - present_state_Q (-0.013449691811394996)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3701263351924216 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006626652587066318) - present_state_Q ( -0.007876774691749381)) * f1( 0.020820889652197045)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.006626652587066318) - present_state_Q (-0.007876774691749381)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3701511423646063 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005388118613022366) - present_state_Q ( -0.0008040834953561681)) * f1( 0.005462860487022465)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005388118613022366) - present_state_Q (-0.0008040834953561681)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3701631486134523 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006191771443584249) - present_state_Q ( -0.0019593364573004353)) * f1( 0.00270803168034133)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.006191771443584249) - present_state_Q (-0.0019593364573004353)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37025631197567016 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007066155083401218) - present_state_Q ( -0.009812659674562636)) * f1( 0.02547547862778497)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.007066155083401218) - present_state_Q (-0.009812659674562636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3703156181263132 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005889641865093236) - present_state_Q ( -0.0065840503275188715)) * f1( 0.014945814137942201)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005889641865093236) - present_state_Q (-0.0065840503275188715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37041615880910833 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005085296742478997) - present_state_Q ( -0.007529137499258172)) * f1( 0.026009577214725403)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005085296742478997) - present_state_Q (-0.007529137499258172)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3705373452286694 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007443149608522652) - present_state_Q ( -0.008017910745414455)) * f1( 0.02138359540340497)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.007443149608522652) - present_state_Q (-0.008017910745414455)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3706399058895818 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006400681695928963) - present_state_Q ( -0.006726174785280542)) * f1( 0.01772562779136489)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006400681695928963) - present_state_Q (-0.006726174785280542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3706617155752821 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006103381143601208) - present_state_Q ( -0.0022026521057316568)) * f1( 0.003497726370356296)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006103381143601208) - present_state_Q (-0.0022026521057316568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3707856913579553 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005899156771427416) - present_state_Q ( -0.008783702155050798)) * f1( 0.022236842955350163)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.005899156771427416) - present_state_Q (-0.008783702155050798)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.370910180232697 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008368654716117986) - present_state_Q ( -0.008368654716117986)) * f1( 0.022066854357444394)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.008368654716117986) - present_state_Q (-0.008368654716117986)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3709511844005005 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004698115977171349) - present_state_Q ( -0.0007478010588245728)) * f1( 0.006440288291387005)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.004698115977171349) - present_state_Q (-0.0007478010588245728)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37102690965085267 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006008962599891795) - present_state_Q ( -0.006008962599891795)) * f1( 0.012936053157705976)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006008962599891795) - present_state_Q (-0.006008962599891795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3711332468914335 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005630025906071669) - present_state_Q ( -0.007157691401643439)) * f1( 0.018541312128831852)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.005630025906071669) - present_state_Q (-0.007157691401643439)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37129221935815443 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004110210876155899) - present_state_Q ( -0.006040522594175379)) * f1( 0.027260193197294776)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.004110210876155899) - present_state_Q (-0.006040522594175379)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.371316695394819 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005776866445755362) - present_state_Q ( -0.0008530603165034901)) * f1( 0.0038441520647206602)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.005776866445755362) - present_state_Q (-0.0008530603165034901)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37144127036642904 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006290624848699811) - present_state_Q ( -0.008899332276869938)) * f1( 0.022375011024965214)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006290624848699811) - present_state_Q (-0.008899332276869938)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37151996999583614 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006029785768885188) - present_state_Q ( -0.006029785768885188)) * f1( 0.013448468239549614)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006029785768885188) - present_state_Q (-0.006029785768885188)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3715471659301116 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006201960086756192) - present_state_Q ( -0.0022803134411333316)) * f1( 0.005116738449180035)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.006201960086756192) - present_state_Q (-0.0022803134411333316)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3716672597177078 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005920476239492636) - present_state_Q ( -0.00680453489247879)) * f1( 0.02471139438833655)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005920476239492636) - present_state_Q (-0.00680453489247879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37174473071260095 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005985562823725382) - present_state_Q ( -0.006791663400652165)) * f1( 0.015934655899397396)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.005985562823725382) - present_state_Q (-0.006791663400652165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37182469880157526 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00818166242111614) - present_state_Q ( -0.009245109419866816)) * f1( 0.021468566256282514)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00818166242111614) - present_state_Q (-0.009245109419866816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3718403896177866 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006049175534657196) - present_state_Q ( -0.0011355793328967519)) * f1( 0.0034756332909472694)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.006049175534657196) - present_state_Q (-0.0011355793328967519)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.371884710105242 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004383949857312215) - present_state_Q ( -0.0026671611821440884)) * f1( 0.010201024085486456)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.004383949857312215) - present_state_Q (-0.0026671611821440884)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3719751738644865 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00674975020577791) - present_state_Q ( -0.009565406144901442)) * f1( 0.024592279109097)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00674975020577791) - present_state_Q (-0.009565406144901442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3720680112481939 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00421512441661253) - present_state_Q ( -0.007842761272543698)) * f1( 0.02426828429526426)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00421512441661253) - present_state_Q (-0.007842761272543698)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37214807707278 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0049608816994441045) - present_state_Q ( -0.008197443879869603)) * f1( 0.021084098680738513)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0049608816994441045) - present_state_Q (-0.008197443879869603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3722372278349454 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00681831939470101) - present_state_Q ( -0.009478107847994003)) * f1( 0.02417347120971049)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00681831939470101) - present_state_Q (-0.009478107847994003)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3722686021524077 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004765887437110367) - present_state_Q ( -0.002473707473849352)) * f1( 0.009082534434156464)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004765887437110367) - present_state_Q (-0.002473707473849352)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3723032403187239 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003878461926361451) - present_state_Q ( -0.0013869587620097576)) * f1( 0.009745815722990929)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.003878461926361451) - present_state_Q (-0.0013869587620097576)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3723232156118292 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006549993046418809) - present_state_Q ( -0.0023842454974432313)) * f1( 0.00573813981220356)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006549993046418809) - present_state_Q (-0.0023842454974432313)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3723796708621466 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006429207711364376) - present_state_Q ( -0.006631034108700229)) * f1( 0.01847806524705145)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006429207711364376) - present_state_Q (-0.006631034108700229)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37239666914951886 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005711710215140263) - present_state_Q ( -0.0010801205912681722)) * f1( 0.004717587109810767)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.005711710215140263) - present_state_Q (-0.0010801205912681722)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3724605206099075 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006110936554521905) - present_state_Q ( -0.008929664102479596)) * f1( 0.02262461559504771)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006110936554521905) - present_state_Q (-0.008929664102479596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3725045199784898 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007439475422701637) - present_state_Q ( -0.00847615265080971)) * f1( 0.02236500284023652)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007439475422701637) - present_state_Q (-0.00847615265080971)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3725482257739293 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007882626591673617) - present_state_Q ( -0.007882626591673617)) * f1( 0.021518125721763936)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007882626591673617) - present_state_Q (-0.007882626591673617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3725907159281235 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007069213683746634) - present_state_Q ( -0.007996496246796481)) * f1( 0.021122626542025184)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007069213683746634) - present_state_Q (-0.007996496246796481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37263946824491034 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007535847515172697) - present_state_Q ( -0.010795613291621532)) * f1( 0.028077488983607065)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007535847515172697) - present_state_Q (-0.010795613291621532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3726808325575536 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007155842892098179) - present_state_Q ( -0.007155842892098179)) * f1( 0.019729931786280554)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007155842892098179) - present_state_Q (-0.007155842892098179)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37272657971919804 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004565471275542693) - present_state_Q ( -0.007029791050976867)) * f1( 0.021959754724840158)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004565471275542693) - present_state_Q (-0.007029791050976867)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37276524198088096 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005124791184055579) - present_state_Q ( -0.007421393204269944)) * f1( 0.018862765231939443)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.005124791184055579) - present_state_Q (-0.007421393204269944)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3727869042535478 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00674834684009258) - present_state_Q ( -0.00674834684009258)) * f1( 0.01776057197262759)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00674834684009258) - present_state_Q (-0.00674834684009258)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.372792340797075 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006158808708686933) - present_state_Q ( -0.0017538657727845972)) * f1( 0.003173260141660087)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006158808708686933) - present_state_Q (-0.0017538657727845972)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37279863865372836 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006376039276186339) - present_state_Q ( -0.0014780263703401265)) * f1( 0.0036132442073806897)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006376039276186339) - present_state_Q (-0.0014780263703401265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37281875885301796 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0055466085649589405) - present_state_Q ( -0.0071309561042492385)) * f1( 0.01720550177435136)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0055466085649589405) - present_state_Q (-0.0071309561042492385)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3728241238378155 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006109757952289908) - present_state_Q ( -0.006975646692552947)) * f1( 0.019364669708393897)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.006109757952289908) - present_state_Q (-0.006975646692552947)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37282902262917017 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005912861350484889) - present_state_Q ( -0.0070468271497058765)) * f1( 0.018281586208712685)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005912861350484889) - present_state_Q (-0.0070468271497058765)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3728492226574855 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00633852412212201) - present_state_Q ( -0.007633341473060897)) * f1( 0.017922355244858006)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00633852412212201) - present_state_Q (-0.007633341473060897)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3728712097240251 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006185744417736499) - present_state_Q ( -0.007221979040040666)) * f1( 0.018845614940451207)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006185744417736499) - present_state_Q (-0.007221979040040666)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3728957228763433 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001704762085575203) - present_state_Q ( -0.005847659034203401)) * f1( 0.01946544646754015)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.001704762085575203) - present_state_Q (-0.005847659034203401)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3729125866886425 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003294087093304929) - present_state_Q ( -0.0022814998363813936)) * f1( 0.010334324053602795)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003294087093304929) - present_state_Q (-0.0022814998363813936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37292874909475265 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005272260571178413) - present_state_Q ( -0.004167290866514625)) * f1( 0.01104722941404694)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005272260571178413) - present_state_Q (-0.004167290866514625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3729518644227996 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005815205202261399) - present_state_Q ( -0.009764206539074416)) * f1( 0.025435954653313417)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005815205202261399) - present_state_Q (-0.009764206539074416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37297380468267377 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005220133542470418) - present_state_Q ( -0.008485475118628056)) * f1( 0.021286997708007533)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005220133542470418) - present_state_Q (-0.008485475118628056)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37299538677075667 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004837632640281585) - present_state_Q ( -0.00938114267745748)) * f1( 0.023025890791381603)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004837632640281585) - present_state_Q (-0.00938114267745748)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37301856693417257 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0070405334543565175) - present_state_Q ( -0.009156696232032413)) * f1( 0.023610579407467167)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0070405334543565175) - present_state_Q (-0.009156696232032413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37305853256864085 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007950593413516312) - present_state_Q ( -0.008869080220044456)) * f1( 0.02067384404181024)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007950593413516312) - present_state_Q (-0.008869080220044456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3731112477613479 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0042699596359434345) - present_state_Q ( -0.0074011757351108475)) * f1( 0.025801146064146765)
w2 ( -0.01982514214826182 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0042699596359434345) - present_state_Q (-0.0074011757351108475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37313088667139976 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005044146183262235) - present_state_Q ( -0.006442981597779129)) * f1( 0.009148439645443271)
w2 ( -0.020254481157162947 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.005044146183262235) - present_state_Q (-0.006442981597779129)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3731587144012555 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004979224199198718) - present_state_Q ( -0.007319502282728097)) * f1( 0.013519148011556617)
w2 ( -0.020254481157162947 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.004979224199198718) - present_state_Q (-0.007319502282728097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3731879354786219 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009583028305481377) - present_state_Q ( -0.010402976473162372)) * f1( 0.016269323268573162)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.009583028305481377) - present_state_Q (-0.010402976473162372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3732187311958248 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00489179614949689) - present_state_Q ( -0.0074303119242284795)) * f1( 0.027182574977123376)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00489179614949689) - present_state_Q (-0.0074303119242284795)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37323820179172373 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00549243334339759) - present_state_Q ( -0.005881848393109831)) * f1( 0.01504945690908624)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00549243334339759) - present_state_Q (-0.005881848393109831)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3732601354526369 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0060991729261214815) - present_state_Q ( -0.008677063182461038)) * f1( 0.021496847003181805)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0060991729261214815) - present_state_Q (-0.008677063182461038)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37328134558899373 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00624642926880835) - present_state_Q ( -0.006652865886392693)) * f1( 0.01732553912943317)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00624642926880835) - present_state_Q (-0.006652865886392693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37329333768108564 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004791692422309513) - present_state_Q ( -0.0005677825529396028)) * f1( 0.0065956820450575725)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004791692422309513) - present_state_Q (-0.0005677825529396028)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3732999346176699 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002342947151064817) - present_state_Q ( -0.005396370356062214)) * f1( 0.005032652617129087)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.002342947151064817) - present_state_Q (-0.005396370356062214)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37306801463920286 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04092348692706726) - present_state_Q ( -0.043643479760372625)) * f1( 0.10898092635354614)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04092348692706726) - present_state_Q (-0.043643479760372625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3727982168465405 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0067610735596900385) - present_state_Q ( -0.04248264015987415)) * f1( 0.11463104999556384)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0067610735596900385) - present_state_Q (-0.04248264015987415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3724083090754867 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007023177374378354) - present_state_Q ( -0.0480751046808334)) * f1( 0.13397768171409602)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.007023177374378354) - present_state_Q (-0.0480751046808334)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3719794453442192 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0074100071686765325) - present_state_Q ( -0.051425305659978285)) * f1( 0.13230834225574048)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0074100071686765325) - present_state_Q (-0.051425305659978285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3715617700545639 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004431032293347215) - present_state_Q ( -0.049099057550732184)) * f1( 0.13745825681675972)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004431032293347215) - present_state_Q (-0.049099057550732184)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3715716198204651 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005224345895457887) - present_state_Q ( -0.005224345895457887)) * f1( 0.007259324222540668)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005224345895457887) - present_state_Q (-0.005224345895457887)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37119701346498796 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04441957458799345) - present_state_Q ( -0.05111874713581578)) * f1( 0.1318737205844498)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04441957458799345) - present_state_Q (-0.05111874713581578)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3708841749901604 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004645498875256619) - present_state_Q ( -0.04455890228037155)) * f1( 0.12114249715733565)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004645498875256619) - present_state_Q (-0.04455890228037155)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3708501851442633 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04479611993326423) - present_state_Q ( -0.04990942812991146)) * f1( 0.012514914470600373)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04479611993326423) - present_state_Q (-0.04990942812991146)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3708608031817318 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005067870907720905) - present_state_Q ( -0.005067870907720905)) * f1( 0.007745156624920977)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005067870907720905) - present_state_Q (-0.005067870907720905)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37046089494919665 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.045567666530680166) - present_state_Q ( -0.052313439654504766)) * f1( 0.1356249689120635)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.045567666530680166) - present_state_Q (-0.052313439654504766)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3705709285300814 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020384562921174727) - present_state_Q ( -0.009447409260218469)) * f1( 0.12189680573870851)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0020384562921174727) - present_state_Q (-0.009447409260218469)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37058811932419083 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005003428965419378) - present_state_Q ( -0.009479212355531631)) * f1( 0.018501683749482824)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005003428965419378) - present_state_Q (-0.009479212355531631)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3703492029974004 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04122724093515681) - present_state_Q ( -0.04397205567203524)) * f1( 0.1107171207340629)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04122724093515681) - present_state_Q (-0.04397205567203524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3703577865008595 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032929257297873464) - present_state_Q ( -0.0032929257297873464)) * f1( 0.00560767300989759)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0032929257297873464) - present_state_Q (-0.0032929257297873464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3701004961134165 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00453993868081445) - present_state_Q ( -0.041714373778791274)) * f1( 0.1119138739366906)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00453993868081445) - present_state_Q (-0.041714373778791274)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3697987159915407 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00462626694179983) - present_state_Q ( -0.04342876904371865)) * f1( 0.12219897863166218)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00462626694179983) - present_state_Q (-0.04342876904371865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36953198426806216 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005108286658691203) - present_state_Q ( -0.0421889599229391)) * f1( 0.1139499993616331)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005108286658691203) - present_state_Q (-0.0421889599229391)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.369557584568306 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005138234527844171) - present_state_Q ( -0.008286636423906172)) * f1( 0.024386970476641463)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005138234527844171) - present_state_Q (-0.008286636423906172)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3691675291974067 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007253254060183017) - present_state_Q ( -0.04946379632636144)) * f1( 0.12802079500222724)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.007253254060183017) - present_state_Q (-0.04946379632636144)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3688874144308103 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04395893107928777) - present_state_Q ( -0.04615003992560479)) * f1( 0.1192799905955817)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.04395893107928777) - present_state_Q (-0.04615003992560479)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3689021248004043 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007119533583914378) - present_state_Q ( -0.007119533583914378)) * f1( 0.012400456332126141)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.007119533583914378) - present_state_Q (-0.007119533583914378)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36886018321071584 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0439579684237464) - present_state_Q ( -0.04896560012999702)) * f1( 0.0159477009562306)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0439579684237464) - present_state_Q (-0.04896560012999702)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36845739264868876 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005777508695401994) - present_state_Q ( -0.04835633764194288)) * f1( 0.13650103738705563)
w2 ( -0.02061369803280085 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005777508695401994) - present_state_Q (-0.04835633764194288)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684641668067901 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00764514436188745) - present_state_Q ( -0.00764514436188745)) * f1( 0.0059476098279036575)
w2 ( -0.02084149233328033 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00764514436188745) - present_state_Q (-0.00764514436188745)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3684792346563517 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006773733576389392) - present_state_Q ( -0.006773733576389392)) * f1( 0.007071067811865476)
w2 ( -0.021267675477395505 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006773733576389392) - present_state_Q (-0.006773733576389392)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3684926254089784 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007046932739277086) - present_state_Q ( -0.009172826723289298)) * f1( 0.007071067811865476)
w2 ( -0.021646423156898457 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007046932739277086) - present_state_Q (-0.009172826723289298)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36852029308537615 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010841903101254564) - present_state_Q ( -0.011806134532888972)) * f1( 0.016583783379717224)
w2 ( -0.022313766084968285 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.010841903101254564) - present_state_Q (-0.011806134532888972)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3685423915425346 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011098147117007663) - present_state_Q ( -0.014084672108451848)) * f1( 0.015313545683952268)
w2 ( -0.02289099248607861 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.011098147117007663) - present_state_Q (-0.014084672108451848)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.36856691298383526 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009716915468375102) - present_state_Q ( -0.012340451184977409)) * f1( 0.015290772378885443)
w2 ( -0.023532462797533384 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.009716915468375102) - present_state_Q (-0.012340451184977409)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3685793719967084 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00700666704407328) - present_state_Q ( -0.012212971710174079)) * f1( 0.018435837871009133)
w2 ( -0.023667623596411506 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00700666704407328) - present_state_Q (-0.012212971710174079)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36858588155249344 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00043548692776010877) - present_state_Q ( -0.00043548692776010877)) * f1( 0.007445249213797554)
w2 ( -0.023667623596411506 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.00043548692776010877) - present_state_Q (-0.00043548692776010877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36859114400711435 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001707696125482962) - present_state_Q ( -0.006868804887058265)) * f1( 0.021592771306395708)
w2 ( -0.023667623596411506 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001707696125482962) - present_state_Q (-0.006868804887058265)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36859026046642845 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006054717436407864) - present_state_Q ( -0.01062421341543662)) * f1( 0.009999677319203667)
w2 ( -0.023649952212472316 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.006054717436407864) - present_state_Q (-0.01062421341543662)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3685938049010626 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005487903376560324) - present_state_Q ( -0.005487903376560324)) * f1( 0.008447055358112271)
w2 ( -0.023733873401190958 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005487903376560324) - present_state_Q (-0.005487903376560324)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3685888725541665 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01078756952727992) - present_state_Q ( -0.01528618204703362)) * f1( 0.009724174377969828)
w2 ( -0.02353098329641219 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.01078756952727992) - present_state_Q (-0.01528618204703362)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3685663217178695 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00628553672897182) - present_state_Q ( -0.010991733388254259)) * f1( 0.02176053770791835)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00628553672897182) - present_state_Q (-0.010991733388254259)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3685586948610287 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009779572103381711) - present_state_Q ( -0.006188403687247122)) * f1( 0.011449061689597528)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( -0.0009779572103381711) - present_state_Q (-0.006188403687247122)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685450457018798 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001512476060206863) - present_state_Q ( -0.006591187385222481)) * f1( 0.021187207185828547)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.001512476060206863) - present_state_Q (-0.006591187385222481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685312728295372 ) += alpha ( 0.1 ) * (reward ( 4.3559896825964014e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015366677725804929) - present_state_Q ( -0.006639950291541572)) * f1( 0.021233828374692235)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( 4.3559896825964014e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.0015366677725804929) - present_state_Q (-0.006639950291541572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36852991395556095 ) += alpha ( 0.1 ) * (reward ( 3.4031169395284386e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00160207885334653) - present_state_Q ( -0.00160207885334653)) * f1( 0.009424379672575133)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( 3.4031169395284386e-11) + discount_factor ( 0.1) * next_state_max_Q( -0.00160207885334653) - present_state_Q (-0.00160207885334653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36853852498461015 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0011158003617239255) - present_state_Q ( -0.004096760218000285)) * f1( 0.006027952207161271)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0011158003617239255) - present_state_Q (-0.004096760218000285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685631475279224 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017330459760993705) - present_state_Q ( -0.00693469688097959)) * f1( 0.021394251958615433)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0017330459760993705) - present_state_Q (-0.00693469688097959)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.368588848629178 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001764339077345023) - present_state_Q ( -0.0056684785275848215)) * f1( 0.020113082798363692)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.001764339077345023) - present_state_Q (-0.0056684785275848215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36861009169433717 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0021510817346663746) - present_state_Q ( -0.006718789741349892)) * f1( 0.018053601492511215)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0021510817346663746) - present_state_Q (-0.006718789741349892)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3686222972104357 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002268224410191197) - present_state_Q ( -0.004808432586673026)) * f1( 0.008916467645300765)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.002268224410191197) - present_state_Q (-0.004808432586673026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3686440758735874 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017500209097114636) - present_state_Q ( -0.0017500209097114636)) * f1( 0.013044766529778914)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0017500209097114636) - present_state_Q (-0.0017500209097114636)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36865697334185216 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0006988597587146618) - present_state_Q ( -0.005847344757404412)) * f1( 0.0103238499824516)
w2 ( -0.023323719702105047 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0006988597587146618) - present_state_Q (-0.005847344757404412)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36867033125375065 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005702189099255655) - present_state_Q ( -0.006515250405315334)) * f1( 0.01083778676140094)
w2 ( -0.023570225971190708 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005702189099255655) - present_state_Q (-0.006515250405315334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36868283589112283 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005975073399524716) - present_state_Q ( -0.01227974849240043)) * f1( 0.01898063199525561)
w2 ( -0.023701988047135206 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005975073399524716) - present_state_Q (-0.01227974849240043)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3686805982952521 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006134910109381859) - present_state_Q ( -0.01083902505343604)) * f1( 0.02052159519436599)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.006134910109381859) - present_state_Q (-0.01083902505343604)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36866761280242194 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0012639259835297001) - present_state_Q ( -0.006185109005104662)) * f1( 0.021432745747434874)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0012639259835297001) - present_state_Q (-0.006185109005104662)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36865556058047694 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001335470611259238) - present_state_Q ( -0.0060566966190855245)) * f1( 0.02034765765591987)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.001335470611259238) - present_state_Q (-0.0060566966190855245)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3686403016444873 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002334405054183959) - present_state_Q ( -0.006428576410831803)) * f1( 0.022552093016010875)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( -0.002334405054183959) - present_state_Q (-0.006428576410831803)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36862243326391286 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001401379106360056) - present_state_Q ( -0.006423148509182529)) * f1( 0.011589160985690827)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001401379106360056) - present_state_Q (-0.006423148509182529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36860029871854844 ) += alpha ( 0.1 ) * (reward ( 0.001141896559354551 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015607514196645882) - present_state_Q ( -0.008831492331772293)) * f1( 0.022546437783271554)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 0.001141896559354551) + discount_factor ( 0.1) * next_state_max_Q( -0.0015607514196645882) - present_state_Q (-0.008831492331772293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3686000301478775 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00037147086356856853) - present_state_Q ( -0.00037147086356856853)) * f1( 0.005629694252155355)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( -0.00037147086356856853) - present_state_Q (-0.00037147086356856853)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36858347006246195 ) += alpha ( 0.1 ) * (reward ( 3.568426747982972e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001808018918729124) - present_state_Q ( -0.009079669418862502)) * f1( 0.018534880983953235)
w2 ( -0.023680180815781975 ) += alpha ( 0.1) * (reward ( 3.568426747982972e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.001808018918729124) - present_state_Q (-0.009079669418862502)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685776288000187 ) += alpha ( 0.1 ) * (reward ( 1.7015584697642193e-11 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006307054893752173) - present_state_Q ( -0.006307054893752173)) * f1( 0.010290526550830476)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.7015584697642193e-11) + discount_factor ( 0.1) * next_state_max_Q( -0.006307054893752173) - present_state_Q (-0.006307054893752173)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3685710081905108 ) += alpha ( 0.1 ) * (reward ( 1.4810364920827766e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013590858157932195) - present_state_Q ( -0.00620163022678486)) * f1( 0.010914526328437753)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.4810364920827766e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.0013590858157932195) - present_state_Q (-0.00620163022678486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36857013055702764 ) += alpha ( 0.1 ) * (reward ( 6.969583492154242e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001225362368790291) - present_state_Q ( -0.001225362368790291)) * f1( 0.00795753743063123)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 6.969583492154242e-08) + discount_factor ( 0.1) * next_state_max_Q( -0.001225362368790291) - present_state_Q (-0.001225362368790291)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685628821037817 ) += alpha ( 0.1 ) * (reward ( 4.3559896825964014e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001398409327934777) - present_state_Q ( -0.006411678643661036)) * f1( 0.011557136115218051)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 4.3559896825964014e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.001398409327934777) - present_state_Q (-0.006411678643661036)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36855747574975084 ) += alpha ( 0.1 ) * (reward ( 0.0011463570938530037 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0003291338170498365) - present_state_Q ( -0.005180404240689551)) * f1( 0.00858990250695039)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0011463570938530037) + discount_factor ( 0.1) * next_state_max_Q( -0.0003291338170498365) - present_state_Q (-0.005180404240689551)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685503729431321 ) += alpha ( 0.1 ) * (reward ( 0.0002865892734632509 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00011325035499531018) - present_state_Q ( -0.005306187910733674)) * f1( 0.012725732353389547)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0002865892734632509) + discount_factor ( 0.1) * next_state_max_Q( -0.00011325035499531018) - present_state_Q (-0.005306187910733674)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36853463034003986 ) += alpha ( 0.1 ) * (reward ( 1.119489349465824e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0021752362536855987) - present_state_Q ( -0.007996832116646828)) * f1( 0.02023359528736562)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.119489349465824e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.0021752362536855987) - present_state_Q (-0.007996832116646828)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36852008745227954 ) += alpha ( 0.1 ) * (reward ( 5.59744674732912e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001728137777638078) - present_state_Q ( -0.006874641465454349)) * f1( 0.021698070884548396)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 5.59744674732912e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.001728137777638078) - present_state_Q (-0.006874641465454349)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685073941891501 ) += alpha ( 0.1 ) * (reward ( 2.787833480270153e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001459502386122847) - present_state_Q ( -0.006233615278619456)) * f1( 0.020849836164321442)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 2.787833480270153e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.001459502386122847) - present_state_Q (-0.006233615278619456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36850075460172893 ) += alpha ( 0.1 ) * (reward ( 1.742395924966004e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010759190899287977) - present_state_Q ( -0.006153365355470824)) * f1( 0.010982165217788907)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.742395924966004e-08) + discount_factor ( 0.1) * next_state_max_Q( -0.0010759190899287977) - present_state_Q (-0.006153365355470824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36849237614183844 ) += alpha ( 0.1 ) * (reward ( 2.177994906207505e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013737941442457692) - present_state_Q ( -0.006155539772586783)) * f1( 0.013921956810176127)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 2.177994906207505e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.0013737941442457692) - present_state_Q (-0.006155539772586783)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684769808311783 ) += alpha ( 0.1 ) * (reward ( 6.646712772516482e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029176917840281905) - present_state_Q ( -0.007797018058344185)) * f1( 0.020512725035852823)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 6.646712772516482e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.0029176917840281905) - present_state_Q (-0.007797018058344185)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684617235492392 ) += alpha ( 0.1 ) * (reward ( 0.00014273706991931888 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016916283180604809) - present_state_Q ( -0.006718754065365816)) * f1( 0.02279816716584)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.00014273706991931888) + discount_factor ( 0.1) * next_state_max_Q( -0.0016916283180604809) - present_state_Q (-0.006718754065365816)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684477555730894 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016403050774896012) - present_state_Q ( -0.0065649119883191345)) * f1( 0.021581329701786715)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.0016403050774896012) - present_state_Q (-0.0065649119883191345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684440708643664 ) += alpha ( 0.1 ) * (reward ( 1.1151333587446788e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010465182739078363) - present_state_Q ( -0.00428568040852736)) * f1( 0.008810574132596652)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.1151333587446788e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.0010465182739078363) - present_state_Q (-0.00428568040852736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.368433685008475 ) += alpha ( 0.1 ) * (reward ( 5.575666793723394e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002288334938319961) - present_state_Q ( -0.006211994257996399)) * f1( 0.017356859593233968)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 5.575666793723394e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.002288334938319961) - present_state_Q (-0.006211994257996399)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684322380457828 ) += alpha ( 0.1 ) * (reward ( 1.3939166984308485e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013797844098993328) - present_state_Q ( -0.0013797844098993328)) * f1( 0.011650775776190406)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.3939166984308485e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.0013797844098993328) - present_state_Q (-0.0013797844098993328)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36842999925790765 ) += alpha ( 0.1 ) * (reward ( 4.3559896825964014e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022718783884406405) - present_state_Q ( -0.0022718783884406405)) * f1( 0.010949251494749616)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 4.3559896825964014e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.0022718783884406405) - present_state_Q (-0.0022718783884406405)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36842898196446733 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013518810064511542) - present_state_Q ( -0.0013518810064511542)) * f1( 0.008361135628271834)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0013518810064511542) - present_state_Q (-0.0013518810064511542)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684187186413428 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018243816085973877) - present_state_Q ( -0.006766426688127605)) * f1( 0.015588306513675626)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0018243816085973877) - present_state_Q (-0.006766426688127605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36841658725278353 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017505577335476415) - present_state_Q ( -0.0035582924114505254)) * f1( 0.0062998506674383095)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0017505577335476415) - present_state_Q (-0.0035582924114505254)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36840387965157895 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015368092903940188) - present_state_Q ( -0.0062136147290792546)) * f1( 0.020969868028119742)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0015368092903940188) - present_state_Q (-0.0062136147290792546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36839018191846845 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016131320602664335) - present_state_Q ( -0.0065337417864366)) * f1( 0.021495310520390914)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0016131320602664335) - present_state_Q (-0.0065337417864366)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36838196119006383 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015955473553773962) - present_state_Q ( -0.006732319224613832)) * f1( 0.012507261470073373)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0015955473553773962) - present_state_Q (-0.006732319224613832)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36836536482274373 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002297044159776789) - present_state_Q ( -0.007487870633824464)) * f1( 0.022000475278706248)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( -0.002297044159776789) - present_state_Q (-0.007487870633824464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683518854339695 ) += alpha ( 0.1 ) * (reward ( 1.7423958730385606e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023413092274603797) - present_state_Q ( -0.007351238205310231)) * f1( 0.018939374157480125)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.7423958730385606e-08) + discount_factor ( 0.1) * next_state_max_Q( -0.0023413092274603797) - present_state_Q (-0.007351238205310231)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36834550105173136 ) += alpha ( 0.1 ) * (reward ( 1.063474043602637e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.000880634872690919) - present_state_Q ( -0.0060226253387958215)) * f1( 0.010757967305322943)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.063474043602637e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.000880634872690919) - present_state_Q (-0.0060226253387958215)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36834508192301874 ) += alpha ( 0.1 ) * (reward ( 3.323356386258241e-14 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00060764313581643) - present_state_Q ( -0.00060764313581643)) * f1( 0.007664014321997834)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 3.323356386258241e-14) + discount_factor ( 0.1) * next_state_max_Q( -0.00060764313581643) - present_state_Q (-0.00060764313581643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683275582461901 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002680298296771247) - present_state_Q ( -0.007877076912524436)) * f1( 0.023030054404714787)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.002680298296771247) - present_state_Q (-0.007877076912524436)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683271182539966 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0006600420356399276) - present_state_Q ( -0.0006600420356399276)) * f1( 0.00740680424263906)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0006600420356399276) - present_state_Q (-0.0006600420356399276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683209125005148 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009546558299920447) - present_state_Q ( -0.005986134096643465)) * f1( 0.010534888302482687)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0009546558299920447) - present_state_Q (-0.005986134096643465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683204437971245 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0006599888312366333) - present_state_Q ( -0.0006599888312366333)) * f1( 0.00789076299743401)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0006599888312366333) - present_state_Q (-0.0006599888312366333)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683068579449182 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0011654760950196245) - present_state_Q ( -0.008230852242564729)) * f1( 0.016743088681819555)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0011654760950196245) - present_state_Q (-0.008230852242564729)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36829398725947576 ) += alpha ( 0.1 ) * (reward ( 0.0002141056048789783 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002022908519106296) - present_state_Q ( -0.0072225565799546145)) * f1( 0.017791021292827513)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0002141056048789783) + discount_factor ( 0.1) * next_state_max_Q( -0.002022908519106296) - present_state_Q (-0.0072225565799546145)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36829137164890374 ) += alpha ( 0.1 ) * (reward ( 5.352640121974458e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020899060976972464) - present_state_Q ( -0.0020899060976972464)) * f1( 0.013521267227949567)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 5.352640121974458e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.0020899060976972464) - present_state_Q (-0.0020899060976972464)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36828957817381125 ) += alpha ( 0.1 ) * (reward ( 3.8945582651463757e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002243414640994197) - present_state_Q ( -0.002243414640994197)) * f1( 0.00888266514063049)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 3.8945582651463757e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.002243414640994197) - present_state_Q (-0.002243414640994197)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36827489364876975 ) += alpha ( 0.1 ) * (reward ( 1.9472791325731879e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015934246658811317) - present_state_Q ( -0.008500934499829554)) * f1( 0.01760398372752154)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 1.9472791325731879e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.0015934246658811317) - present_state_Q (-0.008500934499829554)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36825749870851066 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002071332682419019) - present_state_Q ( -0.00917860741994761)) * f1( 0.019389166111323696)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.002071332682419019) - present_state_Q (-0.00917860741994761)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.368242190286169 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029001302837261458) - present_state_Q ( -0.007768880137888352)) * f1( 0.020468905407011625)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0029001302837261458) - present_state_Q (-0.007768880137888352)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3682404648812139 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016248001049906633) - present_state_Q ( -0.0016248001049906633)) * f1( 0.011799092151975435)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0016248001049906633) - present_state_Q (-0.0016248001049906633)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3682224402103438 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002203171273377021) - present_state_Q ( -0.009326070518891526)) * f1( 0.019794815535918636)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.002203171273377021) - present_state_Q (-0.009326070518891526)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36820658200044365 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0030053234872748774) - present_state_Q ( -0.007944292284590232)) * f1( 0.020746609042121084)
w2 ( -0.023566653827354126 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0030053234872748774) - present_state_Q (-0.007944292284590232)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3682038538480258 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0050175506138378494) - present_state_Q ( -0.0050175506138378494)) * f1( 0.006041355030693211)
w2 ( -0.023476337916305046 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0050175506138378494) - present_state_Q (-0.0050175506138378494)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3681870840702979 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006115102323604231) - present_state_Q ( -0.010933109638256286)) * f1( 0.01624726659935406)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.006115102323604231) - present_state_Q (-0.010933109638256286)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36816410115208975 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0025310078256963477) - present_state_Q ( -0.007720546164507763)) * f1( 0.01909668286303219)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( -0.0025310078256963477) - present_state_Q (-0.007720546164507763)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36815889857481515 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014332844530411265) - present_state_Q ( -0.0056879511854689025)) * f1( 0.009383104168545748)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0014332844530411265) - present_state_Q (-0.0056879511854689025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681536894538967 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015603371181088775) - present_state_Q ( -0.0045653824200141985)) * f1( 0.011813810299806939)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0015603371181088775) - present_state_Q (-0.0045653824200141985)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.368144721457209 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017081505723513621) - present_state_Q ( -0.00581416042779627)) * f1( 0.015891277422946732)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0017081505723513621) - present_state_Q (-0.00581416042779627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681276115156798 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001808028019444841) - present_state_Q ( -0.009169822117436824)) * f1( 0.019034269399896674)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.001808028019444841) - present_state_Q (-0.009169822117436824)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36812649193864455 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001358872365941852) - present_state_Q ( -0.001358872365941852)) * f1( 0.009154461557704989)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.001358872365941852) - present_state_Q (-0.001358872365941852)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681195143690612 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001611165180914878) - present_state_Q ( -0.006352261689211295)) * f1( 0.011270240626678788)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.001611165180914878) - present_state_Q (-0.006352261689211295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681111559565187 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005759230461092218) - present_state_Q ( -0.005238991286511617)) * f1( 0.00857362315613025)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( -0.0005759230461092218) - present_state_Q (-0.005238991286511617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36810921869289326 ) += alpha ( 0.1 ) * (reward ( 0.00028547413983863776 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014444575328650324) - present_state_Q ( -0.0014444575328650324)) * f1( 0.012218737497134759)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.00028547413983863776) + discount_factor ( 0.1) * next_state_max_Q( -0.0014444575328650324) - present_state_Q (-0.0014444575328650324)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680957225535884 ) += alpha ( 0.1 ) * (reward ( 4.460533434978715e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002324273231012163) - present_state_Q ( -0.007524492781881099)) * f1( 0.018496664466396723)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 4.460533434978715e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.002324273231012163) - present_state_Q (-0.007524492781881099)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680955906516179 ) += alpha ( 0.1 ) * (reward ( 3.345408584026385e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00024916093668917344) - present_state_Q ( -0.00024916093668917344)) * f1( 0.0057955896430709085)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 3.345408584026385e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.00024916093668917344) - present_state_Q (-0.00024916093668917344)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680916703919263 ) += alpha ( 0.1 ) * (reward ( 0.0005709515466778459 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002398257888887678) - present_state_Q ( -0.002398257888887678)) * f1( 0.01436316838914916)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0005709515466778459) + discount_factor ( 0.1) * next_state_max_Q( -0.002398257888887678) - present_state_Q (-0.002398257888887678)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36807752749063943 ) += alpha ( 0.1 ) * (reward ( 5.575698697944701e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002431601342547911) - present_state_Q ( -0.006457632550881643)) * f1( 0.022755966290767617)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 5.575698697944701e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.002431601342547911) - present_state_Q (-0.006457632550881643)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36807231336550444 ) += alpha ( 0.1 ) * (reward ( 1.3939246744861754e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00026134830945448234) - present_state_Q ( -0.005436286648949772)) * f1( 0.009637420486550584)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 1.3939246744861754e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.00026134830945448234) - present_state_Q (-0.005436286648949772)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36806673266849793 ) += alpha ( 0.1 ) * (reward ( 1.7424058431077192e-08 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023831699016226243) - present_state_Q ( -0.0034437468707336017)) * f1( 0.01741004133326981)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 1.7424058431077192e-08) + discount_factor ( 0.1) * next_state_max_Q( -0.0023831699016226243) - present_state_Q (-0.0034437468707336017)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36804838454903077 ) += alpha ( 0.1 ) * (reward ( 2.722509129855811e-10 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0024059218901419494) - present_state_Q ( -0.009391927431732129)) * f1( 0.020049663174465195)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 2.722509129855811e-10) + discount_factor ( 0.1) * next_state_max_Q( -0.0024059218901419494) - present_state_Q (-0.009391927431732129)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680435660579228 ) += alpha ( 0.1 ) * (reward ( 2.126948087205274e-12 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010904358514432061) - present_state_Q ( -0.005486904120491344)) * f1( 0.008959866241193711)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 2.126948087205274e-12) + discount_factor ( 0.1) * next_state_max_Q( -0.0010904358514432061) - present_state_Q (-0.005486904120491344)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680359128993811 ) += alpha ( 0.1 ) * (reward ( 3.2454652209553133e-17 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006061635492303212) - present_state_Q ( -0.006736599358242529)) * f1( 0.01248387354528235)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 3.2454652209553133e-17) + discount_factor ( 0.1) * next_state_max_Q( -0.006061635492303212) - present_state_Q (-0.006736599358242529)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680233808393184 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0021448724666769006) - present_state_Q ( -0.007330788631179936)) * f1( 0.01761035597780666)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0021448724666769006) - present_state_Q (-0.007330788631179936)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36801485625299907 ) += alpha ( 0.1 ) * (reward ( 0.004567586237418204 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038860445202916196) - present_state_Q ( -0.0038860445202916196)) * f1( 0.010569818369158865)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.004567586237418204) + discount_factor ( 0.1) * next_state_max_Q( -0.0038860445202916196) - present_state_Q (-0.0038860445202916196)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680093463505676 ) += alpha ( 0.1 ) * (reward ( 0.002283793118709102 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020305220176377644) - present_state_Q ( -0.0020305220176377644)) * f1( 0.013401970438680973)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.002283793118709102) + discount_factor ( 0.1) * next_state_max_Q( -0.0020305220176377644) - present_state_Q (-0.0020305220176377644)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36798999041370173 ) += alpha ( 0.1 ) * (reward ( 8.92106686995743e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0026604295615433476) - present_state_Q ( -0.009616955946343826)) * f1( 0.020679786359285652)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 8.92106686995743e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.0026604295615433476) - present_state_Q (-0.009616955946343826)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679723766929685 ) += alpha ( 0.1 ) * (reward ( 1.3293425545032963e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023178879293430927) - present_state_Q ( -0.009215571279722348)) * f1( 0.019606130000232477)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 1.3293425545032963e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.0023178879293430927) - present_state_Q (-0.009215571279722348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679657575327795 ) += alpha ( 0.1 ) * (reward ( 1.2981860883821253e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0008995325544523706) - present_state_Q ( -0.006085520199661234)) * f1( 0.011040090537927398)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 1.2981860883821253e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.0008995325544523706) - present_state_Q (-0.006085520199661234)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679636749620032 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015150672440299086) - present_state_Q ( -0.0037067779236927647)) * f1( 0.005857698778899026)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0015150672440299086) - present_state_Q (-0.0037067779236927647)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36795960091543445 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023661871775118694) - present_state_Q ( -0.0028380410852416635)) * f1( 0.015660842390311364)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0023661871775118694) - present_state_Q (-0.0028380410852416635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36794912163468185 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016841845230625393) - present_state_Q ( -0.0063803119370771134)) * f1( 0.016869704508446045)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0016841845230625393) - present_state_Q (-0.0063803119370771134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36793354805200745 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0028790444737857683) - present_state_Q ( -0.008023705503967513)) * f1( 0.020131829348371807)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0028790444737857683) - present_state_Q (-0.008023705503967513)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679327589201741 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009806756747673788) - present_state_Q ( -0.0009806756747673788)) * f1( 0.008940908506124967)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0009806756747673788) - present_state_Q (-0.0009806756747673788)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36793606351933783 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001591824109556253) - present_state_Q ( -0.006416505264082615)) * f1( 0.011482876446455957)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001591824109556253) - present_state_Q (-0.006416505264082615)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.367941837473954 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014785824513241488) - present_state_Q ( -0.006545916604066429)) * f1( 0.02109504526169064)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0014785824513241488) - present_state_Q (-0.006545916604066429)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679475549564826 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022138154023590814) - present_state_Q ( -0.00688872593404758)) * f1( 0.023168074683219284)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0022138154023590814) - present_state_Q (-0.00688872593404758)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36794827758913007 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002017598079995511) - present_state_Q ( -0.008954013046296531)) * f1( 0.01887167262600366)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002017598079995511) - present_state_Q (-0.008954013046296531)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36795165294813875 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014177350256663426) - present_state_Q ( -0.006044857723809994)) * f1( 0.010443276123281114)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0014177350256663426) - present_state_Q (-0.006044857723809994)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679518273323898 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002017450426196948) - present_state_Q ( -0.009247591069337561)) * f1( 0.01952212974892628)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002017450426196948) - present_state_Q (-0.009247591069337561)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36796101963606087 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014137409920975052) - present_state_Q ( -0.0014137409920975052)) * f1( 0.011690869849571652)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0014137409920975052) - present_state_Q (-0.0014137409920975052)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36796563926080267 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020131932651367185) - present_state_Q ( -0.007216803057273668)) * f1( 0.021793882496580996)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0020131932651367185) - present_state_Q (-0.007216803057273668)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36797565936103477 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023597702921698788) - present_state_Q ( -0.0023597702921698788)) * f1( 0.014291197108727154)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0023597702921698788) - present_state_Q (-0.0023597702921698788)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36798351339483576 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001306282043691174) - present_state_Q ( -0.001306282043691174)) * f1( 0.009867473349401284)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001306282043691174) - present_state_Q (-0.001306282043691174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3679911056049825 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001303195251309252) - present_state_Q ( -0.001303195251309252)) * f1( 0.009535201194332672)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001303195251309252) - present_state_Q (-0.001303195251309252)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3680124974418857 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002420128093179988) - present_state_Q ( -0.008099030633525338)) * f1( 0.020542749349413338)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.002420128093179988) - present_state_Q (-0.008099030633525338)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36803753515282667 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019055684658424441) - present_state_Q ( -0.007084657714831002)) * f1( 0.022008767359228475)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0019055684658424441) - present_state_Q (-0.007084657714831002)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36805596115918565 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018899925789665388) - present_state_Q ( -0.0018899925789665388)) * f1( 0.011120535535731063)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0018899925789665388) - present_state_Q (-0.0018899925789665388)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36807642952631964 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014990445615355413) - present_state_Q ( -0.0014990445615355413)) * f1( 0.012096282340610127)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0014990445615355413) - present_state_Q (-0.0014990445615355413)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36809444907751354 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0020623245314997734) - present_state_Q ( -0.0090956628773627)) * f1( 0.019208736147223782)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0020623245314997734) - present_state_Q (-0.0090956628773627)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36809900967823717 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002474209184883757) - present_state_Q ( -0.0069743963676083005)) * f1( 0.0189378222583321)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002474209184883757) - present_state_Q (-0.0069743963676083005)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681078762355129 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001484562538492915) - present_state_Q ( -0.004299074099631376)) * f1( 0.01778806319835917)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.001484562538492915) - present_state_Q (-0.004299074099631376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681113266350414 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010643972245497058) - present_state_Q ( -0.006269941989525094)) * f1( 0.011610977286560435)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0010643972245497058) - present_state_Q (-0.006269941989525094)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36811671669551543 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010671382765950824) - present_state_Q ( -0.006583975674121301)) * f1( 0.0202793141969099)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0010671382765950824) - present_state_Q (-0.006583975674121301)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36812658759074074 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002426031120200588) - present_state_Q ( -0.002426031120200588)) * f1( 0.014199162919002901)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002426031120200588) - present_state_Q (-0.002426031120200588)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681289310810442 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003251835118425139) - present_state_Q ( -0.00835370294410154)) * f1( 0.02117637790108999)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.003251835118425139) - present_state_Q (-0.00835370294410154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36813230147759085 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005695865019273533) - present_state_Q ( -0.005133585939020243)) * f1( 0.008304445046589918)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0005695865019273533) - present_state_Q (-0.005133585939020243)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681397439477135 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015779595585725154) - present_state_Q ( -0.0015779595585725154)) * f1( 0.009646742143797635)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0015779595585725154) - present_state_Q (-0.0015779595585725154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36813944180404123 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002455200318087804) - present_state_Q ( -0.009528881428721929)) * f1( 0.020389086308651563)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002455200318087804) - present_state_Q (-0.009528881428721929)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681443254452542 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018967098589056767) - present_state_Q ( -0.0070908695061394)) * f1( 0.02186077954481294)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0018967098589056767) - present_state_Q (-0.0070908695061394)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.368154203522321 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002081420184837357) - present_state_Q ( -0.002081420184837357)) * f1( 0.013602617508795505)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002081420184837357) - present_state_Q (-0.002081420184837357)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681641726364179 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0024270125372919285) - present_state_Q ( -0.0024270125372919285)) * f1( 0.014342271874786632)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0024270125372919285) - present_state_Q (-0.0024270125372919285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36816910935545366 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00208095148809684) - present_state_Q ( -0.00655823724907528)) * f1( 0.01772590733960369)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.00208095148809684) - present_state_Q (-0.00655823724907528)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681732510013852 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002488857737297074) - present_state_Q ( -0.0032951570128218572)) * f1( 0.006801959452446591)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002488857737297074) - present_state_Q (-0.0032951570128218572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36818324643792427 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019741133568432693) - present_state_Q ( -0.0019741133568432693)) * f1( 0.01358357909026457)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0019741133568432693) - present_state_Q (-0.0019741133568432693)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36819369274814495 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029532139906830283) - present_state_Q ( -0.0029532139906830283)) * f1( 0.01612761901447369)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0029532139906830283) - present_state_Q (-0.0029532139906830283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3681976708935673 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002280034148214847) - present_state_Q ( -0.0072448818058292635)) * f1( 0.01877994869899897)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002280034148214847) - present_state_Q (-0.0072448818058292635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.368203895348632 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0030144013080601636) - present_state_Q ( -0.004465829100677555)) * f1( 0.012522080389310325)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0030144013080601636) - present_state_Q (-0.004465829100677555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3682081196303304 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013964201786653596) - present_state_Q ( -0.0042994250068822204)) * f1( 0.008490353791291103)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0013964201786653596) - present_state_Q (-0.0042994250068822204)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3682136659815316 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002586558246581622) - present_state_Q ( -0.006384394987771359)) * f1( 0.018429885718428584)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.002586558246581622) - present_state_Q (-0.006384394987771359)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36822935869074974 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00036800714671866006) - present_state_Q ( -0.00036800714671866006)) * f1( 0.005796161986360445)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.00036800714671866006) - present_state_Q (-0.00036800714671866006)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36826276793137674 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001681972482829635) - present_state_Q ( -0.001681972482829635)) * f1( 0.01290343476382211)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.001681972482829635) - present_state_Q (-0.001681972482829635)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3682754154633554 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010001522134847854) - present_state_Q ( -0.0010001522134847854)) * f1( 0.004771684756951911)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0010001522134847854) - present_state_Q (-0.0010001522134847854)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683259254785124 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018236130674890987) - present_state_Q ( -0.0067590854423018355)) * f1( 0.024250091907030164)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0018236130674890987) - present_state_Q (-0.0067590854423018355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36835301327776765 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014839072774177572) - present_state_Q ( -0.0014839072774177572)) * f1( 0.01039040979910231)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0014839072774177572) - present_state_Q (-0.0014839072774177572)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3683949935654988 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0014476770970494283) - present_state_Q ( -0.00879253613377025)) * f1( 0.02238023748487608)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0014476770970494283) - present_state_Q (-0.00879253613377025)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684038753289087 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019341802295239753) - present_state_Q ( -0.0019341802295239753)) * f1( 0.0034606850267899654)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0019341802295239753) - present_state_Q (-0.0019341802295239753)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684129938971097 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00168271270297625) - present_state_Q ( -0.00168271270297625)) * f1( 0.0035218961945929646)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.00168271270297625) - present_state_Q (-0.00168271270297625)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3684533046062215 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010797366541820833) - present_state_Q ( -0.008403482572713988)) * f1( 0.021094029903489388)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0010797366541820833) - present_state_Q (-0.008403482572713988)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36849532350149894 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002998945520020821) - present_state_Q ( -0.007938958917135683)) * f1( 0.021257680956322873)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002998945520020821) - present_state_Q (-0.007938958917135683)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685410269288312 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0031146957722504227) - present_state_Q ( -0.009533078943883532)) * f1( 0.025133996051256432)
w2 ( -0.02326990592818713 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0031146957722504227) - present_state_Q (-0.009533078943883532)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3685617408087099 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006910647507948783) - present_state_Q ( -0.01050644308127233)) * f1( 0.011775847688298285)
w2 ( -0.023621708710067763 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006910647507948783) - present_state_Q (-0.01050644308127233)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36858772805762025 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006586638686286543) - present_state_Q ( -0.011699499948311175)) * f1( 0.015880082454877056)
w2 ( -0.023949002336964297 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006586638686286543) - present_state_Q (-0.011699499948311175)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.36861726454226984 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008165660298523317) - present_state_Q ( -0.01313274319895718)) * f1( 0.019574404281197392)
w2 ( -0.024250789142072383 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.008165660298523317) - present_state_Q (-0.01313274319895718)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3686550555493042 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006869101221410108) - present_state_Q ( -0.010591571575479953)) * f1( 0.014187928621186383)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006869101221410108) - present_state_Q (-0.010591571575479953)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3686950774503684 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00261983318392302) - present_state_Q ( -0.007329197527538634)) * f1( 0.01967809243494021)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.00261983318392302) - present_state_Q (-0.007329197527538634)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36873574221083744 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002941814387879587) - present_state_Q ( -0.007487204659088611)) * f1( 0.020118625666976628)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002941814387879587) - present_state_Q (-0.007487204659088611)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36874938954004544 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0022503808198736886) - present_state_Q ( -0.0022503808198736886)) * f1( 0.0053771612593231845)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0022503808198736886) - present_state_Q (-0.0022503808198736886)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36878364319754203 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002352409975571701) - present_state_Q ( -0.007567181443341018)) * f1( 0.017064052676250017)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002352409975571701) - present_state_Q (-0.007567181443341018)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3688217677979585 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003444281294855039) - present_state_Q ( -0.008493045251575158)) * f1( 0.019797890531573322)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.003444281294855039) - present_state_Q (-0.008493045251575158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3688520679879862 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018770164294809235) - present_state_Q ( -0.0034979692996962787)) * f1( 0.0125751715876607)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0018770164294809235) - present_state_Q (-0.0034979692996962787)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3688732443032499 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0008934591690576021) - present_state_Q ( -0.005166805654679458)) * f1( 0.009484172587157261)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0008934591690576021) - present_state_Q (-0.005166805654679458)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36889301411415 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0007490863224806825) - present_state_Q ( -0.0007490863224806825)) * f1( 0.007395742636402287)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0007490863224806825) - present_state_Q (-0.0007490863224806825)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3689315976506247 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0025797723842526247) - present_state_Q ( -0.007582830646102785)) * f1( 0.019214273214473467)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0025797723842526247) - present_state_Q (-0.007582830646102785)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3689549763900514 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015993388562767184) - present_state_Q ( -0.003621337522107389)) * f1( 0.009763877516481695)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0015993388562767184) - present_state_Q (-0.003621337522107389)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36897671249453734 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013868364606846302) - present_state_Q ( -0.005400074195510871)) * f1( 0.009815742390102262)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0013868364606846302) - present_state_Q (-0.005400074195510871)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36901397806400404 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032161734346994107) - present_state_Q ( -0.008421268375393481)) * f1( 0.019302717997169948)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0032161734346994107) - present_state_Q (-0.008421268375393481)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3690372431533425 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0019641018178868227) - present_state_Q ( -0.005774766591288103)) * f1( 0.010658779363222426)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0019641018178868227) - present_state_Q (-0.005774766591288103)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36907608472706904 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0012034714051309698) - present_state_Q ( -0.006269379436765817)) * f1( 0.018272811093713783)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0012034714051309698) - present_state_Q (-0.006269379436765817)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3691041164777089 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0027658555691409993) - present_state_Q ( -0.0027658555691409993)) * f1( 0.011250390227727232)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0027658555691409993) - present_state_Q (-0.0027658555691409993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36913576466119963 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0008640515890457686) - present_state_Q ( -0.005609894023015095)) * f1( 0.014463093950988807)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0008640515890457686) - present_state_Q (-0.005609894023015095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36913410355025866 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002610184023704158) - present_state_Q ( -0.002610184023704158)) * f1( 0.007071067811865476)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.002610184023704158) - present_state_Q (-0.002610184023704158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36912714666873625 ) += alpha ( 0.1 ) * (reward ( 0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004817259379879288) - present_state_Q ( -0.0011850696639516506)) * f1( 0.007071067811865476)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.004817259379879288) - present_state_Q (-0.0011850696639516506)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3690992974774738 ) += alpha ( 0.1 ) * (reward ( 1.784213373991486e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0076556418357963734) - present_state_Q ( -0.010776621624961097)) * f1( 0.0277689402051484)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 1.784213373991486e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.0076556418357963734) - present_state_Q (-0.010776621624961097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3690880218968947 ) += alpha ( 0.1 ) * (reward ( 0.0011419052713339162 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006085425493403687) - present_state_Q ( -0.006085425493403687)) * f1( 0.01703571743364299)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 0.0011419052713339162) + discount_factor ( 0.1) * next_state_max_Q( -0.006085425493403687) - present_state_Q (-0.006085425493403687)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3690701960183664 ) += alpha ( 0.1 ) * (reward ( 0.0003571214602649315 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005407524857467712) - present_state_Q ( -0.0081541691538991)) * f1( 0.022364711442480114)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 0.0003571214602649315) + discount_factor ( 0.1) * next_state_max_Q( -0.005407524857467712) - present_state_Q (-0.0081541691538991)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36905159226280065 ) += alpha ( 0.1 ) * (reward ( 4.4640182533116436e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0059973155190367194) - present_state_Q ( -0.008746618105670217)) * f1( 0.022710974601720352)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 4.4640182533116436e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.0059973155190367194) - present_state_Q (-0.008746618105670217)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3690513772806001 ) += alpha ( 0.1 ) * (reward ( 1.3950057041598886e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005609181403701937) - present_state_Q ( -0.0011255081243423553)) * f1( 0.0037983728269586447)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 1.3950057041598886e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.005609181403701937) - present_state_Q (-0.0011255081243423553)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3690431020848955 ) += alpha ( 0.1 ) * (reward ( 2.7246205159372825e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005859144472952814) - present_state_Q ( -0.0059474879250756355)) * f1( 0.015434259240580355)
w2 ( -0.024783509710992517 ) += alpha ( 0.1) * (reward ( 2.7246205159372825e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.005859144472952814) - present_state_Q (-0.0059474879250756355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0001544719158084 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38418861169915813) - present_state_Q ( 0.19292893218813453)) * f1( 0.007071067811865476)
w2 ( 0.9956308744331586 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.38418861169915813) - present_state_Q (0.19292893218813453)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.000302292256841 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38245048899368334) - present_state_Q ( 0.1833483726584028)) * f1( 0.007071067811865476)
w2 ( 0.9914498838115008 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.38245048899368334) - present_state_Q (0.1833483726584028)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0055052113963656 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7309258647599909) - present_state_Q ( 0.7167825465366399)) * f1( 0.07352534224974848)
w2 ( 0.9348389904207411 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.7309258647599909) - present_state_Q (0.7167825465366399)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0106131132341487 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6880200684505737) - present_state_Q ( 0.6738296790410572)) * f1( 0.07635427920518041)
w2 ( 0.8813210800591528 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.6880200684505737) - present_state_Q (0.6738296790410572)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0134239153297577 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6336080195586818) - present_state_Q ( 0.47163231951901213)) * f1( 0.059523434794438)
w2 ( 0.8529880165659328 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.6336080195586818) - present_state_Q (0.47163231951901213)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0160764003458562 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6136097586975023) - present_state_Q ( 0.4551290917995753)) * f1( 0.05795066663510154)
w2 ( 0.8163708707056384 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.6136097586975023) - present_state_Q (0.4551290917995753)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0185372240353858 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.586885617400031) - present_state_Q ( 0.4348577686955942)) * f1( 0.05591314481812548)
w2 ( 0.7811616375632827 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.586885617400031) - present_state_Q (0.4348577686955942)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0208256253619552 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5611727868051363) - present_state_Q ( 0.4152040924746381)) * f1( 0.0540950992554101)
w2 ( 0.7557796562962039 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.5611727868051363) - present_state_Q (0.4152040924746381)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0237344447713987 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5341522041827312) - present_state_Q ( 0.5433259542705681)) * f1( 0.052519327522852724)
w2 ( 0.7225482398256349 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.5341522041827312) - present_state_Q (0.5433259542705681)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.026045465540613 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3770244438554522) - present_state_Q ( 0.3770244438554522)) * f1( 0.05863555114602117)
w2 ( 0.6989002577664993 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.3770244438554522) - present_state_Q (0.3770244438554522)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0281143275058022 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3555780202132049) - present_state_Q ( 0.3555780202132049)) * f1( 0.05519448947773395)
w2 ( 0.6764103825840452 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 0.3555780202132049) - present_state_Q (0.3555780202132049)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0259480394845644 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14994573136239894) - present_state_Q ( -0.17050676857850355)) * f1( 0.15784456515724415)
w2 ( 0.6764103825840452 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.14994573136239894) - present_state_Q (-0.17050676857850355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.023819593094677 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15792897638221615) - present_state_Q ( -0.16939648690566877)) * f1( 0.15727446723212812)
w2 ( 0.6764103825840452 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.15792897638221615) - present_state_Q (-0.16939648690566877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0173647428386987 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.258918457091945) - present_state_Q ( -0.26962894615454375)) * f1( 0.26482838452513485)
w2 ( 0.6764103825840452 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.258918457091945) - present_state_Q (-0.26962894615454375)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0115102331427366 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26135291955481965) - present_state_Q ( -0.26135291955481965)) * f1( 0.24889757437459048)
w2 ( 0.6764103825840452 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.26135291955481965) - present_state_Q (-0.26135291955481965)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0032104261625934 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2881598676242078) - present_state_Q ( -0.30838563182364287)) * f1( 0.2968779739418976)
w2 ( 0.6764103825840452 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.2881598676242078) - present_state_Q (-0.30838563182364287)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9965582515929772 ) += alpha ( 0.1 ) * (reward ( 0.0005709482796772755 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2188496225557097) - present_state_Q ( -0.2188496225557097)) * f1( 0.33675824918059133)
w2 ( 0.6803610947556415 ) += alpha ( 0.1) * (reward ( 0.0005709482796772755) + discount_factor ( 0.1) * next_state_max_Q( -0.2188496225557097) - present_state_Q (-0.2188496225557097)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9892413979962563 ) += alpha ( 0.1 ) * (reward ( 7.136853495965944e-05 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22765240661656155) - present_state_Q ( -0.22765240661656155)) * f1( 0.3569918966747243)
w2 ( 0.6844602654454388 ) += alpha ( 0.1) * (reward ( 7.136853495965944e-05) + discount_factor ( 0.1) * next_state_max_Q( -0.22765240661656155) - present_state_Q (-0.22765240661656155)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9913330151801921 ) += alpha ( 0.1 ) * (reward ( 8.711979365192803e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06822935268440394) - present_state_Q ( 0.06822935268440394)) * f1( 0.34061871196842874)
w2 ( 0.6807758809231997 ) += alpha ( 0.1) * (reward ( 8.711979365192803e-09) + discount_factor ( 0.1) * next_state_max_Q( 0.06822935268440394) - present_state_Q (0.06822935268440394)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9946562271167815 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06519667269235718) - present_state_Q ( 0.06519667269235718)) * f1( 0.3490120114055336)
w2 ( 0.6750628192038517 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06519667269235718) - present_state_Q (0.06519667269235718)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9937424098968349 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06333212567513147) - present_state_Q ( -0.05772023663922515)) * f1( 0.33214306487282436)
w2 ( 0.6767135847622953 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06333212567513147) - present_state_Q (-0.05772023663922515)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9927710260847392 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06196337130368346) - present_state_Q ( -0.05982434128456915)) * f1( 0.3295061704593195)
w2 ( 0.6778927843029189 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06196337130368346) - present_state_Q (-0.05982434128456915)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9960650016893557 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06587488813614811) - present_state_Q ( 0.06506706274953344)) * f1( 0.3466603302983126)
w2 ( 0.672191568472803 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06587488813614811) - present_state_Q (0.06506706274953344)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9948939339813009 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05559281863574439) - present_state_Q ( -0.06645344180681673)) * f1( 0.33013830433673336)
w2 ( 0.6736104498236448 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.05559281863574439) - present_state_Q (-0.06645344180681673)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9985489318945315 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06809774833064292) - present_state_Q ( 0.08214964839271771)) * f1( 0.32668747816685056)
w2 ( 0.6668976160161049 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06809774833064292) - present_state_Q (0.08214964839271771)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0021039927550133 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06575226705783199) - present_state_Q ( 0.07987074552560625)) * f1( 0.32366929830685004)
w2 ( 0.6603074434929548 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06575226705783199) - present_state_Q (0.07987074552560625)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0055622186113264 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06345632069166957) - present_state_Q ( 0.0776274418213988)) * f1( 0.3207332298442476)
w2 ( 0.6538380935138601 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06345632069166957) - present_state_Q (0.0776274418213988)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.008926323483567 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.061198824964322385) - present_state_Q ( 0.07540586310287473)) * f1( 0.31788818982607575)
w2 ( 0.6474884932835129 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.061198824964322385) - present_state_Q (0.07540586310287473)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0121541084796852 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07309779173365738) - present_state_Q ( 0.07319160237881567)) * f1( 0.3151440926679541)
w2 ( 0.6413431424972251 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.07309779173365738) - present_state_Q (0.07319160237881567)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0113286440794869 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05720267542832003) - present_state_Q ( -0.05723427637835021)) * f1( 0.31251191117963495)
w2 ( 0.6429279737385353 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.05720267542832003) - present_state_Q (-0.05723427637835021)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0146093395428306 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.061152248409172716) - present_state_Q ( 0.07540215276761869)) * f1( 0.31000371460087583)
w2 ( 0.6365783166689725 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.061152248409172716) - present_state_Q (0.07540215276761869)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0178084043616593 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05927945066235668) - present_state_Q ( 0.07359859835476246)) * f1( 0.3069785611592568)
w2 ( 0.6303256360777001 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.05927945066235668) - present_state_Q (0.07359859835476246)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0171199671818758 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.060403309570021324) - present_state_Q ( -0.053116741103834963)) * f1( 0.30439757113568)
w2 ( 0.6316826190073895 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.060403309570021324) - present_state_Q (-0.053116741103834963)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0166732684241986 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06702822132701464) - present_state_Q ( -0.044965488132931886)) * f1( 0.2952868639357299)
w2 ( 0.6325902762293668 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.06702822132701464) - present_state_Q (-0.044965488132931886)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0163092020636157 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0704217738882717) - present_state_Q ( -0.04193974979225856)) * f1( 0.29262874128855404)
w2 ( 0.6333367504662712 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.0704217738882717) - present_state_Q (-0.04193974979225856)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.016997508668414 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07400412063219708) - present_state_Q ( -0.005416881457030065)) * f1( 0.29013830642090715)
w2 ( 0.6323878146111074 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.07400412063219708) - present_state_Q (-0.005416881457030065)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0177159532378355 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07625164651688193) - present_state_Q ( 0.009774243993783016)) * f1( 0.24309063496795438)
w2 ( 0.6306145388051111 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.07625164651688193) - present_state_Q (0.009774243993783016)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0154674686700365 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1228745908075855) - present_state_Q ( -0.10915107550894473)) * f1( 0.23911649711813368)
w2 ( 0.6343758594917188 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.1228745908075855) - present_state_Q (-0.10915107550894473)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.019066249851423 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13553423445722768) - present_state_Q ( 0.13553423445722768)) * f1( 0.24090431962975495)
w2 ( 0.6254126797855579 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.13553423445722768) - present_state_Q (0.13553423445722768)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0225183974221874 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12975014629323192) - present_state_Q ( 0.12859961796044694)) * f1( 0.24135808265606096)
w2 ( 0.61683087254022 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.12975014629323192) - present_state_Q (0.12859961796044694)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0259369298862782 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12784367314895878) - present_state_Q ( 0.12662107365369013)) * f1( 0.24203332211899714)
w2 ( 0.6083563391144218 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.12784367314895878) - present_state_Q (0.12662107365369013)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0264586909765936 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06080230400160047) - present_state_Q ( 0.010816201896285488)) * f1( 0.22679036496115337)
w2 ( 0.6074360864565899 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.06080230400160047) - present_state_Q (0.010816201896285488)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0295596903378654 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1248363325091672) - present_state_Q ( 0.12483633250916712)) * f1( 0.23740063478659024)
w2 ( 0.5995987038041145 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.1248363325091672) - present_state_Q (0.12483633250916712)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0297512779498217 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10982459562981833) - present_state_Q ( 0.010452250801714302)) * f1( 0.22264778601473878)
w2 ( 0.5992545052555718 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10982459562981833) - present_state_Q (0.010452250801714302)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.032541745957205 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12378383322034595) - present_state_Q ( 0.12238815105469458)) * f1( 0.23420784823289162)
w2 ( 0.592105808843122 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.12378383322034595) - present_state_Q (0.12238815105469458)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0327458898426363 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10884968366539341) - present_state_Q ( 0.011079356815913044)) * f1( 0.2188140332537067)
w2 ( 0.5917326264061537 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.10884968366539341) - present_state_Q (0.011079356815913044)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0354773046130044 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12208796784731524) - present_state_Q ( 0.12208796784731515)) * f1( 0.2295029900752512)
w2 ( 0.5845917657939085 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.12208796784731524) - present_state_Q (0.12208796784731515)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0354906286275518 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1076533705064448) - present_state_Q ( 0.011386075554168623)) * f1( 0.21464778601473883)
w2 ( 0.5845669362537675 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1076533705064448) - present_state_Q (0.011386075554168623)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0379379633106687 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12189028280156344) - present_state_Q ( 0.12027163333942481)) * f1( 0.226431874192419)
w2 ( 0.5780819799502115 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12189028280156344) - present_state_Q (0.12027163333942481)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0379769352031638 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10745109471187397) - present_state_Q ( 0.012593457988045037)) * f1( 0.21084710020743827)
w2 ( 0.5780080460095371 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.10745109471187397) - present_state_Q (0.012593457988045037)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.040254160816561 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11773604390889297) - present_state_Q ( 0.11451922466842707)) * f1( 0.22163724421983766)
w2 ( 0.5718433087928849 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11773604390889297) - present_state_Q (0.11451922466842707)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0403749992438198 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11085974585027258) - present_state_Q ( 0.01690881000574168)) * f1( 0.20752506043548075)
w2 ( 0.5714939386676421 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11085974585027258) - present_state_Q (0.01690881000574168)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0404100339408475 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12121969667622462) - present_state_Q ( 0.01384246464499142)) * f1( 0.2036314984264372)
w2 ( 0.5713907089689999 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12121969667622462) - present_state_Q (0.01384246464499142)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0405470193478277 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1257005092546473) - present_state_Q ( 0.0193216985643547)) * f1( 0.2028918188666277)
w2 ( 0.5711206430634443 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1257005092546473) - present_state_Q (0.0193216985643547)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0429848350243773 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1297882040326787) - present_state_Q ( 0.1297882040326787)) * f1( 0.20870032875816907)
w2 ( 0.5641120800456797 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1297882040326787) - present_state_Q (0.1297882040326787)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0432733392057771 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11692953859745761) - present_state_Q ( 0.02649332640981139)) * f1( 0.19493035085700194)
w2 ( 0.5632240576926758 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11692953859745761) - present_state_Q (0.02649332640981139)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0435524417759723 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11734554883362017) - present_state_Q ( 0.026351554500833846)) * f1( 0.19094381713019423)
w2 ( 0.5626393777079769 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11734554883362017) - present_state_Q (0.026351554500833846)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0458546693333692 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12745129345270997) - present_state_Q ( 0.12745129345270997)) * f1( 0.20070652482463922)
w2 ( 0.5557570078615306 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12745129345270997) - present_state_Q (0.12745129345270997)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0462127718274872 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12048525126427831) - present_state_Q ( 0.031230121437891167)) * f1( 0.18669066343756904)
w2 ( 0.5546061120828428 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12048525126427831) - present_state_Q (0.031230121437891167)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0466239724093567 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12376512500206233) - present_state_Q ( 0.03488397116668626)) * f1( 0.182695251366532)
w2 ( 0.553255664562854 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12376512500206233) - present_state_Q (0.03488397116668626)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0470085392449466 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11960195730616288) - present_state_Q ( 0.033480415013824344)) * f1( 0.17870024023877903)
w2 ( 0.5523948557915257 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11960195730616288) - present_state_Q (0.033480415013824344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.047447042758737 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12734317156962086) - present_state_Q ( 0.03772104867972831)) * f1( 0.17549454733239692)
w2 ( 0.551395386530615 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12734317156962086) - present_state_Q (0.03772104867972831)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0498007582599398 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14167807344908653) - present_state_Q ( 0.14167807344908653)) * f1( 0.1845902744238444)
w2 ( 0.5437447705643643 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.14167807344908653) - present_state_Q (0.14167807344908653)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0502399348544509 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1260641035257741) - present_state_Q ( 0.038284687564691416)) * f1( 0.1710303969706974)
w2 ( 0.5427176394758798 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1260641035257741) - present_state_Q (0.038284687564691416)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.052517354358082 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14001102962635895) - present_state_Q ( 0.14001102962635895)) * f1( 0.18073334093025004)
w2 ( 0.5351570438760564 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.14001102962635895) - present_state_Q (0.14001102962635895)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0529551418222478 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1247119784729202) - present_state_Q ( 0.03873032371819282)) * f1( 0.16671821686607857)
w2 ( 0.5341066788412204 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1247119784729202) - present_state_Q (0.03873032371819282)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0551576339341773 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13820819115708124) - present_state_Q ( 0.13820819115708124)) * f1( 0.17706717939156147)
w2 ( 0.526643436518738 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.13820819115708124) - present_state_Q (0.13820819115708124)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0555917924501095 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12327272577555648) - present_state_Q ( 0.0390309004936957)) * f1( 0.16258409430196774)
w2 ( 0.5255752914020925 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12327272577555648) - present_state_Q (0.0390309004936957)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.057719220902119 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13710450445421968) - present_state_Q ( 0.13624682486197578)) * f1( 0.1736160762172925)
w2 ( 0.5182231089370992 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.13710450445421968) - present_state_Q (0.13624682486197578)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0581531113210034 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12204258233205978) - present_state_Q ( 0.03955162413457863)) * f1( 0.158658943771496)
w2 ( 0.5171292143010443 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12204258233205978) - present_state_Q (0.03955162413457863)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0602148734771302 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1355319495213509) - present_state_Q ( 0.13535110961283767)) * f1( 0.1692772952533873)
w2 ( 0.5098213394214022 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1355319495213509) - present_state_Q (0.13535110961283767)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.060645835481191 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12073188617537622) - present_state_Q ( 0.039951931901364146)) * f1( 0.1545844443823659)
w2 ( 0.5087061896900491 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12073188617537622) - present_state_Q (0.039951931901364146)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0626373305358912 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.134583408900316) - present_state_Q ( 0.13362946110411106)) * f1( 0.16572160192503013)
w2 ( 0.5014959224772043 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.134583408900316) - present_state_Q (0.13362946110411106)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0630685236071225 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11962882750461523) - present_state_Q ( 0.04058149576884382)) * f1( 0.15066875216987183)
w2 ( 0.500351177956469 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11962882750461523) - present_state_Q (0.04058149576884382)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0649979528279203 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1330897530376028) - present_state_Q ( 0.13288752804186427)) * f1( 0.16135244796143292)
w2 ( 0.49317646479218274 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1330897530376028) - present_state_Q (0.13288752804186427)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0654262754849644 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11842278824749486) - present_state_Q ( 0.0410623887935182)) * f1( 0.1465848887981462)
w2 ( 0.492007660393432 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11842278824749486) - present_state_Q (0.0410623887935182)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0672885802071126 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13227939160772553) - present_state_Q ( 0.13120667820713927)) * f1( 0.15785087526798816)
w2 ( 0.48492893605065 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.13227939160772553) - present_state_Q (0.13120667820713927)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0677173198130212 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11743321445858351) - present_state_Q ( 0.04179206234408178)) * f1( 0.14268138800252417)
w2 ( 0.48372698641472106 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11743321445858351) - present_state_Q (0.04179206234408178)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0695207230526251 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13084009552424963) - present_state_Q ( 0.13061115893689715)) * f1( 0.15344567183403596)
w2 ( 0.47667535745165274 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.13084009552424963) - present_state_Q (0.13061115893689715)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0699462854283783 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11631618620973541) - present_state_Q ( 0.04233919357761928)) * f1( 0.13858547161541815)
w2 ( 0.4754470544533869 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11631618620973541) - present_state_Q (0.04233919357761928)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0716412119823697 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1255393812152276) - present_state_Q ( 0.1255393812152276)) * f1( 0.15001282533234622)
w2 ( 0.4686679278677646 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1255393812152276) - present_state_Q (0.1255393812152276)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0733255330268936 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12545337896186537) - present_state_Q ( 0.12520803988953902)) * f1( 0.149501211556528)
w2 ( 0.46190816574816346 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12545337896186537) - present_state_Q (0.12520803988953902)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0738607670925637 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11551854696820124) - present_state_Q ( 0.05439076316253946)) * f1( 0.1249411072409658)
w2 ( 0.4593378312402203 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11551854696820124) - present_state_Q (0.05439076316253946)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0743430827409393 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11380543696211423) - present_state_Q ( 0.051088433621206464)) * f1( 0.12146594777174587)
w2 ( 0.45774951564322053 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11380543696211423) - present_state_Q (0.051088433621206464)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0758626680326702 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12359943743778345) - present_state_Q ( 0.11920638632656211)) * f1( 0.14222142122828127)
w2 ( 0.4513387290882535 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12359943743778345) - present_state_Q (0.11920638632656211)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0763292140960106 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11792453252772298) - present_state_Q ( 0.047819796445155494)) * f1( 0.1294977708594957)
w2 ( 0.44989763536055816 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.11792453252772298) - present_state_Q (0.047819796445155494)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0767714703189464 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12197629447712355) - present_state_Q ( 0.047185807012761594)) * f1( 0.12640161726444973)
w2 ( 0.4477983447066552 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.12197629447712355) - present_state_Q (0.047185807012761594)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0782497739321841 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1314515712077606) - present_state_Q ( 0.13298416088153117)) * f1( 0.12335746850741716)
w2 ( 0.443004784556225 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1314515712077606) - present_state_Q (0.13298416088153117)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0808339142068455 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2204510014779008) - present_state_Q ( 0.2064299232121508)) * f1( 0.140149293836371)
w2 ( 0.4282539987110761 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.2204510014779008) - present_state_Q (0.2064299232121508)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0820874865734669 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19047690915187954) - present_state_Q ( 0.12088420393129357)) * f1( 0.12309655245395132)
w2 ( 0.42214380793010975 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.19047690915187954) - present_state_Q (0.12088420393129357)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0843743369021972 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19788890571419) - present_state_Q ( 0.18309568408360058)) * f1( 0.14003399855864804)
w2 ( 0.4090792644491352 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.19788890571419) - present_state_Q (0.18309568408360058)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0866094973989182 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18988760184822584) - present_state_Q ( 0.175414137228511)) * f1( 0.14288989030831897)
w2 ( 0.39656523428564017 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.18988760184822584) - present_state_Q (0.175414137228511)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0887144031047755 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16813223681451317) - present_state_Q ( 0.18296370780460733)) * f1( 0.12668670313937233)
w2 ( 0.3832731955557877 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.16813223681451317) - present_state_Q (0.18296370780460733)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0897350627927525 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16002700209876414) - present_state_Q ( 0.09891890702621606)) * f1( 0.12309531841438309)
w2 ( 0.37663989901048056 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.16002700209876414) - present_state_Q (0.09891890702621606)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0916134580258963 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17277438584761615) - present_state_Q ( 0.1733334334106744)) * f1( 0.12036674625920707)
w2 ( 0.3672765393209258 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.17277438584761615) - present_state_Q (0.1733334334106744)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0934011697938242 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16840149891907621) - present_state_Q ( 0.16840149891907621)) * f1( 0.11795301238757495)
w2 ( 0.35515163139875233 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.16840149891907621) - present_state_Q (0.16840149891907621)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0942721093388839 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14633078282318643) - present_state_Q ( 0.09055418740320327)) * f1( 0.11471638851758856)
w2 ( 0.34907794266908154 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.14633078282318643) - present_state_Q (0.09055418740320327)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0943589008679624 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14769253710073818) - present_state_Q ( 0.022513699954387914)) * f1( 0.11206938022496735)
w2 ( 0.3486132758944227 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.14769253710073818) - present_state_Q (0.022513699954387914)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0959283098341943 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16880171667306057) - present_state_Q ( 0.16880171667306057)) * f1( 0.10330391032900607)
w2 ( 0.33645955229396235 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.16880171667306057) - present_state_Q (0.16880171667306057)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0973931151422038 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16179072529318278) - present_state_Q ( 0.16179072529318278)) * f1( 0.10059670913734373)
w2 ( 0.32481062007285316 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.16179072529318278) - present_state_Q (0.16179072529318278)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0987607607448637 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1550966013557779) - present_state_Q ( 0.1550966013557779)) * f1( 0.09797804799680046)
w2 ( 0.31364366477523714 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.1550966013557779) - present_state_Q (0.1550966013557779)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.099954824703934 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13899059841929678) - present_state_Q ( 0.13899059841929678)) * f1( 0.09545521404964394)
w2 ( 0.3036363416890478 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.13899059841929678) - present_state_Q (0.13899059841929678)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1016464221944682 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13944312382980967) - present_state_Q ( 0.14157438266161196)) * f1( 0.09760759280659623)
w2 ( 0.28977186707682273 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.13944312382980967) - present_state_Q (0.14157438266161196)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1031522927347 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13547919695822971) - present_state_Q ( 0.13675214422319643)) * f1( 0.08916803442371196)
w2 ( 0.27626146012469827 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.13547919695822971) - present_state_Q (0.13675214422319643)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1045608608390685 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11328878820349776) - present_state_Q ( 0.1288822079196143)) * f1( 0.08629388479190318)
w2 ( 0.26320312480682256 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.11328878820349776) - present_state_Q (0.1288822079196143)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.105427248695477 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10568861107965424) - present_state_Q ( 0.0686365444152747)) * f1( 0.08351245860330202)
w2 ( 0.25490364115230324 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.10568861107965424) - present_state_Q (0.0686365444152747)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.10625086890819 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10208406844330518) - present_state_Q ( 0.06642371140449793)) * f1( 0.08083332809836678)
w2 ( 0.24675234779755528 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.10208406844330518) - present_state_Q (0.06642371140449793)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1070389042592448 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0986137211877542) - present_state_Q ( 0.06487103006147578)) * f1( 0.07826699892641974)
w2 ( 0.2386975061722047 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.0986137211877542) - present_state_Q (0.06487103006147578)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.10814811418714 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09522033345779508) - present_state_Q ( 0.11083577200175221)) * f1( 0.07546179594485714)
w2 ( 0.22693833808979233 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.09522033345779508) - present_state_Q (0.11083577200175221)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1088446469111812 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0888484087602324) - present_state_Q ( 0.05908957429874448)) * f1( 0.07264584854235077)
w2 ( 0.22118550234197812 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.0888484087602324) - present_state_Q (0.05908957429874448)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1098217055193167 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08731263124528467) - present_state_Q ( 0.10294703909665585)) * f1( 0.06984396063163398)
w2 ( 0.20999417127427336 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.08731263124528467) - present_state_Q (0.10294703909665585)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.110658527683633 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0869366867860494) - present_state_Q ( 0.0878594227251775)) * f1( 0.0670307056499344)
w2 ( 0.20250367428902807 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.0869366867860494) - present_state_Q (0.0878594227251775)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1115349366011338 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07222053996603617) - present_state_Q ( 0.08755471458447135)) * f1( 0.0695515586485175)
w2 ( 0.1924229924520641 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.07222053996603617) - present_state_Q (0.08755471458447135)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.112351692824765 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06722530995434169) - present_state_Q ( 0.08289474044323204)) * f1( 0.0670307056499344)
w2 ( 0.1826751467063057 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.06722530995434169) - present_state_Q (0.08289474044323204)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.113096993098638 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07817214096971155) - present_state_Q ( 0.07817214096971155)) * f1( 0.06423297460186018)
w2 ( 0.1733926835665519 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.07817214096971155) - present_state_Q (0.07817214096971155)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1137953780622454 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05815813073429152) - present_state_Q ( 0.07384043533744207)) * f1( 0.06142321783672787)
w2 ( 0.1642966447954963 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.05815813073429152) - present_state_Q (0.07384043533744207)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1144399114501544 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05396624622225048) - present_state_Q ( 0.06965160753305398)) * f1( 0.05863080432411954)
w2 ( 0.15550217717269543 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.05396624622225048) - present_state_Q (0.06965160753305398)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1150338633284227 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0500320392573073) - present_state_Q ( 0.0657213436296454)) * f1( 0.0558256919250277)
w2 ( 0.1469906570064477 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.0500320392573073) - present_state_Q (0.0657213436296454)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1155968085637866 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04968138554627434) - present_state_Q ( 0.06542776892206503)) * f1( 0.05304024326097281)
w2 ( 0.13849981758711813 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.04968138554627434) - present_state_Q (0.06542776892206503)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1160842456299094 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.058608761055307344) - present_state_Q ( 0.058608761055307344)) * f1( 0.04952433527229943)
w2 ( 0.13062591780120145 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.058608761055307344) - present_state_Q (0.058608761055307344)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.116530062472334 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.055131699360865004) - present_state_Q ( 0.055131699360865004)) * f1( 0.046783114305945075)
w2 ( 0.1230023664572846 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.055131699360865004) - present_state_Q (0.055131699360865004)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1169375502285257 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05160529857914251) - present_state_Q ( 0.05160529857914251)) * f1( 0.04423414726388567)
w2 ( 0.11563271596965177 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.05160529857914251) - present_state_Q (0.05160529857914251)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.117321740607107 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03368781823629198) - present_state_Q ( 0.04935773206824467)) * f1( 0.041912525385177195)
w2 ( 0.10829953096014797 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.03368781823629198) - present_state_Q (0.04935773206824467)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1176707911255717 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030948606152533045) - present_state_Q ( 0.046608987633284804)) * f1( 0.039135616098102226)
w2 ( 0.10116433180877088 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.030948606152533045) - present_state_Q (0.046608987633284804)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1179431527999955 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032406483243211326) - present_state_Q ( 0.032406483243211326)) * f1( 0.03639170198895763)
w2 ( 0.0951769960253251 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.032406483243211326) - present_state_Q (0.032406483243211326)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.118202500311866 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005523517603892289) - present_state_Q ( -0.007742728393409712)) * f1( 0.06937990498475183)
w2 ( 0.09218653344769445 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( 0.005523517603892289) - present_state_Q (-0.007742728393409712)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1183974503778897 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024036888713220433) - present_state_Q ( -0.024036888713220433)) * f1( 0.08108505693241683)
w2 ( 0.09026312044511176 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.024036888713220433) - present_state_Q (-0.024036888713220433)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.11820632181029 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05745848456500378) - present_state_Q ( -0.05745848456500378)) * f1( 0.07863069666315618)
w2 ( 0.09123540519247152 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.05745848456500378) - present_state_Q (-0.05745848456500378)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.1177485949797765 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06046851387773047) - present_state_Q ( -0.07621665207644059)) * f1( 0.10703484206340662)
w2 ( 0.09209069085775469 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.06046851387773047) - present_state_Q (-0.07621665207644059)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1168621707321544 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08827679321593239) - present_state_Q ( -0.12178689292094833)) * f1( 0.1036102807072893)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.08827679321593239) - present_state_Q (-0.12178689292094833)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1161360025350573 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10313539779254184) - present_state_Q ( -0.10288861751798632)) * f1( 0.08702888561539901)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.10313539779254184) - present_state_Q (-0.10288861751798632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.114080053756167 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1569819880933944) - present_state_Q ( -0.16854104770989206)) * f1( 0.14306464553777293)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.1569819880933944) - present_state_Q (-0.16854104770989206)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1099599941533307 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22389074398282732) - present_state_Q ( -0.23490054873343486)) * f1( 0.20258307212539084)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.22389074398282732) - present_state_Q (-0.23490054873343486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1013493745464216 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.310710761899004) - present_state_Q ( -0.3257893412384694)) * f1( 0.3015101324559686)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.310710761899004) - present_state_Q (-0.3257893412384694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0954495800955355 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27849480653134756) - present_state_Q ( -0.28268253262566917)) * f1( 0.2594142552972278)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.27849480653134756) - present_state_Q (-0.28268253262566917)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0901157950730636 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2682652026599874) - present_state_Q ( -0.2694365661621865)) * f1( 0.24784724839904115)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.2682652026599874) - present_state_Q (-0.2694365661621865)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0857542349386855 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.241610260700511) - present_state_Q ( -0.241610260700511)) * f1( 0.22950299007525116)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.241610260700511) - present_state_Q (-0.241610260700511)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.085254796590496 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09950141079472981) - present_state_Q ( -0.08414652768646033)) * f1( 0.08930336090116259)
w2 ( 0.0938017647812516 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.09950141079472981) - present_state_Q (-0.08414652768646033)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0834248106024513 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13757571735151142) - present_state_Q ( -0.15257167467785746)) * f1( 0.14111667804801784)
w2 ( 0.096395343390609 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.13757571735151142) - present_state_Q (-0.15257167467785746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.082051599227453 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10087622497702776) - present_state_Q ( -0.1159765988516543)) * f1( 0.12968407310011462)
w2 ( 0.09851312291768803 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.10087622497702776) - present_state_Q (-0.1159765988516543)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.077701289117762 ) += alpha ( 0.1 ) * (reward ( 1.1151333587446788e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1799325203651037) - present_state_Q ( -0.21337016021878)) * f1( 0.2226611793826614)
w2 ( 0.1024206833840006 ) += alpha ( 0.1) * (reward ( 1.1151333587446788e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.1799325203651037) - present_state_Q (-0.21337016021878)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0740096497538154 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21668823781493804) - present_state_Q ( -0.21668823781493804)) * f1( 0.22024661669668696)
w2 ( 0.10577296131617929 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.21668823781493804) - present_state_Q (-0.21668823781493804)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0696449002826933 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22477597807249922) - present_state_Q ( -0.23477088219757902)) * f1( 0.23607562267378498)
w2 ( 0.10947071665549568 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.22477597807249922) - present_state_Q (-0.23477088219757902)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.069711141847089 ) += alpha ( 0.1 ) * (reward ( -0.10048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00756353162451501) - present_state_Q ( -0.00756353162451501)) * f1( 0.007071067811865476)
w2 ( 0.10947071665549568 ) += alpha ( 0.1) * (reward ( -0.10048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.00756353162451501) - present_state_Q (-0.00756353162451501)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0697707971553863 ) += alpha ( 0.1 ) * (reward ( -0.10048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007535329491047137) - present_state_Q ( -0.016875084110824293)) * f1( 0.007071067811865476)
w2 ( 0.10947071665549568 ) += alpha ( 0.1) * (reward ( -0.10048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.007535329491047137) - present_state_Q (-0.016875084110824293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0704729031898932 ) += alpha ( 0.1 ) * (reward ( -0.09135172474836409 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004689513051469443) - present_state_Q ( -0.02658365638256858)) * f1( 0.10762386725334672)
w2 ( 0.10816597626207683 ) += alpha ( 0.1) * (reward ( -0.09135172474836409) + discount_factor ( 0.1) * next_state_max_Q( -0.004689513051469443) - present_state_Q (-0.02658365638256858)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0706284706214793 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010840466756845381) - present_state_Q ( 0.010935021002749077)) * f1( 0.01689710121800497)
w2 ( 0.10816597626207683 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.010840466756845381) - present_state_Q (0.010935021002749077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0707074839768287 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015899820545310476) - present_state_Q ( 0.005944131049435419)) * f1( 0.009127031916317712)
w2 ( 0.10643456223670819 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.015899820545310476) - present_state_Q (0.005944131049435419)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0708378936431735 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030477943606356174) - present_state_Q ( 0.009823357758724536)) * f1( 0.014654069673556083)
w2 ( 0.10465471992327587 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.030477943606356174) - present_state_Q (0.009823357758724536)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0709221397991313 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02548610196177247) - present_state_Q ( 0.009413875363404873)) * f1( 0.009457166274574885)
w2 ( 0.10287308357446076 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.02548610196177247) - present_state_Q (0.009413875363404873)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0710730698093192 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04012443150796231) - present_state_Q ( 0.020491351432651413)) * f1( 0.015292497683122377)
w2 ( 0.09892526515224545 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.04012443150796231) - present_state_Q (0.020491351432651413)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.071260363535349 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.034551576923690694) - present_state_Q ( 0.018333262982789835)) * f1( 0.019289807568090442)
w2 ( 0.09504147884968751 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.034551576923690694) - present_state_Q (0.018333262982789835)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0714383358735358 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023889336602922945) - present_state_Q ( 0.02290073110104954)) * f1( 0.017324559255715723)
w2 ( 0.09093234486111612 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.023889336602922945) - present_state_Q (0.02290073110104954)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0715696988344368 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015402879250940804) - present_state_Q ( 0.018935396853972768)) * f1( 0.01318750830127759)
w2 ( 0.08694787841301986 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.015402879250940804) - present_state_Q (0.018935396853972768)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0717351689736045 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02161721688213345) - present_state_Q ( 0.02161721688213345)) * f1( 0.01627489003019197)
w2 ( 0.08288099651432194 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.02161721688213345) - present_state_Q (0.02161721688213345)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.07187454466998 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022831321719802986) - present_state_Q ( 0.009010396874270594)) * f1( 0.01567008265603974)
w2 ( 0.07932324383528923 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.022831321719802986) - present_state_Q (0.009010396874270594)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0719594050234025 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005542229658426479) - present_state_Q ( 0.004745110173446436)) * f1( 0.010981993502989047)
w2 ( 0.07777779849516332 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.005542229658426479) - present_state_Q (0.004745110173446436)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0720433589820657 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008718376244203517) - present_state_Q ( 0.008718376244203517)) * f1( 0.010373917963538364)
w2 ( 0.07615924012679383 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.008718376244203517) - present_state_Q (0.008718376244203517)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0721259884620606 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00805483211053353) - present_state_Q ( 0.00805483211053353)) * f1( 0.010286160891867012)
w2 ( 0.0745526255528304 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.00805483211053353) - present_state_Q (0.00805483211053353)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0721891119482776 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009575798750615764) - present_state_Q ( 0.009575798750615764)) * f1( 0.007726290855923046)
w2 ( 0.0729186335793455 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.009575798750615764) - present_state_Q (0.009575798750615764)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0722637022660169 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006216769639376668) - present_state_Q ( 0.006216769639376668)) * f1( 0.00948063828913799)
w2 ( 0.07134510412986289 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.006216769639376668) - present_state_Q (0.006216769639376668)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0723313273908601 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005117869150890943) - present_state_Q ( 0.005117869150890943)) * f1( 0.008704766904772183)
w2 ( 0.06979135488917303 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.005117869150890943) - present_state_Q (0.005117869150890943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0724249272290314 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007341858786761031) - present_state_Q ( 0.007341858786761031)) * f1( 0.011745633181977256)
w2 ( 0.06819757383503751 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.007341858786761031) - present_state_Q (0.007341858786761031)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.072518168314652 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004404829874637839) - present_state_Q ( 0.002824789704803307)) * f1( 0.014057042519338918)
w2 ( 0.06687096355421362 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.004404829874637839) - present_state_Q (0.002824789704803307)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0725860338660724 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005518417935912715) - present_state_Q ( 0.003748470047735819)) * f1( 0.010107638564271555)
w2 ( 0.06552810684265363 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.005518417935912715) - present_state_Q (0.003748470047735819)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0734705699120615 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011093488284145588) - present_state_Q ( -0.0008624620535095003)) * f1( 0.14046316823816918)
w2 ( 0.06426865063490356 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.0011093488284145588) - present_state_Q (-0.0008624620535095003)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.073526552338901 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011245889006592565) - present_state_Q ( -0.0019235927311162387)) * f1( 0.009192814742890748)
w2 ( 0.06305069012106197 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.011245889006592565) - present_state_Q (-0.0019235927311162387)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0735961607292364 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005019005634066143) - present_state_Q ( 0.005019005634066143)) * f1( 0.01016725424182203)
w2 ( 0.06168142387317169 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.005019005634066143) - present_state_Q (0.005019005634066143)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0736457563906507 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006567530070366605) - present_state_Q ( 0.006567530070366605)) * f1( 0.007099599538907958)
w2 ( 0.06028428418542799 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.006567530070366605) - present_state_Q (0.006567530070366605)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0737142649322218 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0020368582816377902) - present_state_Q ( 0.0020368582816377902)) * f1( 0.010414896249076124)
w2 ( 0.058968696589881416 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.0020368582816377902) - present_state_Q (0.0020368582816377902)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0737750633587264 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011661720140259835) - present_state_Q ( 0.0011661720140259835)) * f1( 0.009354213952761398)
w2 ( 0.05766878134715185 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.0011661720140259835) - present_state_Q (0.0011661720140259835)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0738643391861757 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0033893093976641014) - present_state_Q ( 0.0033893093976641014)) * f1( 0.01332542940922328)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.0033893093976641014) - present_state_Q (0.0033893093976641014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0739134133178825 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00360619180821029) - present_state_Q ( -0.015679868962396298)) * f1( 0.012426415170400178)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.00360619180821029) - present_state_Q (-0.015679868962396298)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0739794650664296 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01856474600333502) - present_state_Q ( -0.01958108835243863)) * f1( 0.01781022449551246)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.01856474600333502) - present_state_Q (-0.01958108835243863)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.074044680241794 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015622149859412737) - present_state_Q ( -0.02031974684884317)) * f1( 0.01808844355275575)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.015622149859412737) - present_state_Q (-0.02031974684884317)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0741056763522157 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013530134865972359) - present_state_Q ( -0.022968533018907757)) * f1( 0.018374804499852532)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.013530134865972359) - present_state_Q (-0.022968533018907757)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0742728542314068 ) += alpha ( 0.1 ) * (reward ( -0.10048689722320049 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015381971489208428) - present_state_Q ( -0.021907928662422697)) * f1( 0.02086667416528923)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.10048689722320049) + discount_factor ( 0.1) * next_state_max_Q( -0.015381971489208428) - present_state_Q (-0.021907928662422697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0743260711940383 ) += alpha ( 0.1 ) * (reward ( -0.09135172474836409 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01526554236142377) - present_state_Q ( -0.003404804983189744)) * f1( 0.005947792150176304)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.09135172474836409) + discount_factor ( 0.1) * next_state_max_Q( -0.01526554236142377) - present_state_Q (-0.003404804983189744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0744707930703756 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01745096940087963) - present_state_Q ( -0.02957178120417283)) * f1( 0.026608241871840806)
w2 ( 0.0563288496315168 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.01745096940087963) - present_state_Q (-0.02957178120417283)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0744957588472341 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015690031363213095) - present_state_Q ( 0.0017965924776981394)) * f1( 0.00297721142007379)
w2 ( 0.054651724742764925 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.0015690031363213095) - present_state_Q (0.0017965924776981394)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0745638387253567 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004774921145547436) - present_state_Q ( -0.004488590373855085)) * f1( 0.008812875612510958)
w2 ( 0.053106715347062565 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.004774921145547436) - present_state_Q (-0.004488590373855085)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0746809828787895 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0007298424261740176) - present_state_Q ( -0.0005095943449927691)) * f1( 0.014349926647407316)
w2 ( 0.05147403587334422 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.0007298424261740176) - present_state_Q (-0.0005095943449927691)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.074759854686202 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0029777514394925724) - present_state_Q ( -0.0006538355611603128)) * f1( 0.009705514566337498)
w2 ( 0.04984873704197586 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.0029777514394925724) - present_state_Q (-0.0006538355611603128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0759475313670332 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0019208762736707928) - present_state_Q ( 0.0019208762736707928)) * f1( 0.14148214395430167)
w2 ( 0.04816983022357923 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.0019208762736707928) - present_state_Q (0.0019208762736707928)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0760509568296264 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0004999596541631242) - present_state_Q ( 0.0004999596541631242)) * f1( 0.012511167476830651)
w2 ( 0.046516499904333736 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.0004999596541631242) - present_state_Q (0.0004999596541631242)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0772320661818116 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0026900136186011857) - present_state_Q ( 0.0026900136186011857)) * f1( 0.1395490712357022)
w2 ( 0.04482374861372836 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( 0.0026900136186011857) - present_state_Q (0.0026900136186011857)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0773448873274707 ) += alpha ( 0.1 ) * (reward ( -0.08221655227352767 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00048172047695619474) - present_state_Q ( -0.0018660036452475062)) * f1( 0.01403270409164073)
w2 ( 0.04321577420020885 ) += alpha ( 0.1) * (reward ( -0.08221655227352767) + discount_factor ( 0.1) * next_state_max_Q( -0.00048172047695619474) - present_state_Q (-0.0018660036452475062)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0783431670032442 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010963952073763505) - present_state_Q ( -0.0010963952073763505)) * f1( 0.13846797706053254)
w2 ( 0.041773881717967795 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.0010963952073763505) - present_state_Q (-0.0010963952073763505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0784392238356921 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0004143561868498978) - present_state_Q ( 0.0004143561868498978)) * f1( 0.013077087654253148)
w2 ( 0.04030479571063067 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.0004143561868498978) - present_state_Q (0.0004143561868498978)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0785207009319595 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004986970507203332) - present_state_Q ( -0.008876156572606484)) * f1( 0.012789442645652117)
w2 ( 0.03903066518712338 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( 0.004986970507203332) - present_state_Q (-0.008876156572606484)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0785599404448747 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00013559439702907732) - present_state_Q ( -0.0021397340761757297)) * f1( 0.005530181024123695)
w2 ( 0.03761156108387901 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.00013559439702907732) - present_state_Q (-0.0021397340761757297)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0784834928445783 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002988931429919105) - present_state_Q ( -0.1465463902637591)) * f1( 0.00922176746816829)
w2 ( 0.03926954260553693 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( 0.002988931429919105) - present_state_Q (-0.1465463902637591)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0785402580136982 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003748096281532231) - present_state_Q ( -0.005381281901475641)) * f1( 0.009631052645769057)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.003748096281532231) - present_state_Q (-0.005381281901475641)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0785953183416115 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013259073702536907) - present_state_Q ( -0.0024769930154316055)) * f1( 0.01026097279819766)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.013259073702536907) - present_state_Q (-0.0024769930154316055)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0786632383207424 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01854634506975138) - present_state_Q ( -0.01854634506975138)) * f1( 0.01781772903969891)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.01854634506975138) - present_state_Q (-0.01854634506975138)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.07871193406464 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020986009230590474) - present_state_Q ( -0.02902876372864555)) * f1( 0.025977021406728227)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.020986009230590474) - present_state_Q (-0.02902876372864555)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0787459535011654 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014424742475744587) - present_state_Q ( -0.02085739936806851)) * f1( 0.01986447727421687)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.014424742475744587) - present_state_Q (-0.02085739936806851)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0788605307149668 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01930582901186734) - present_state_Q ( -0.027566024862921527)) * f1( 0.024149003903145453)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.01930582901186734) - present_state_Q (-0.027566024862921527)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0789499812122116 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019419100276270647) - present_state_Q ( -0.020907240013391473)) * f1( 0.016529384083655136)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.019419100276270647) - present_state_Q (-0.020907240013391473)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0790600431632296 ) += alpha ( 0.1 ) * (reward ( -0.07308137979869127 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020030119844265157) - present_state_Q ( -0.023461179994120486)) * f1( 0.021320244751087944)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.07308137979869127) + discount_factor ( 0.1) * next_state_max_Q( -0.020030119844265157) - present_state_Q (-0.023461179994120486)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.079082090882453 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0070742013498236505) - present_state_Q ( -0.015058375001959456)) * f1( 0.004445530193142391)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.0070742013498236505) - present_state_Q (-0.015058375001959456)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0781381980729168 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12773730169100223) - present_state_Q ( -0.1482612263700064)) * f1( 0.13193679123650276)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.12773730169100223) - present_state_Q (-0.1482612263700064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0774412432567044 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01757249245738384) - present_state_Q ( -0.12404872540473114)) * f1( 0.11945352727429001)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.01757249245738384) - present_state_Q (-0.12404872540473114)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0762621227657614 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010135386834143357) - present_state_Q ( -0.14290230816800112)) * f1( 0.13541010177605362)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.010135386834143357) - present_state_Q (-0.14290230816800112)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0763570963038132 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014982199718797592) - present_state_Q ( -0.015733575065186963)) * f1( 0.02340651804814754)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.014982199718797592) - present_state_Q (-0.015733575065186963)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0764099888554202 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015321565002066275) - present_state_Q ( -0.018347268894908127)) * f1( 0.013920586260433075)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.015321565002066275) - present_state_Q (-0.018347268894908127)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0764809498003767 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02073816259724901) - present_state_Q ( -0.030182094595610987)) * f1( 0.02657438939744365)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.02073816259724901) - present_state_Q (-0.030182094595610987)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0765939186109623 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012250313889323575) - present_state_Q ( -0.021284486414913536)) * f1( 0.025740982112081798)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.012250313889323575) - present_state_Q (-0.021284486414913536)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0767014352022288 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01367813712290459) - present_state_Q ( -0.01864967917239768)) * f1( 0.023040417366323483)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.01367813712290459) - present_state_Q (-0.01864967917239768)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0767259407715541 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01885753430005882) - present_state_Q ( -0.005380951075243997)) * f1( 0.004053789912805181)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.01885753430005882) - present_state_Q (-0.005380951075243997)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0768147368844383 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016399733459425393) - present_state_Q ( -0.024904705998543095)) * f1( 0.021827161773695522)
w2 ( 0.03809074790452628 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.016399733459425393) - present_state_Q (-0.024904705998543095)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0768375178817704 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(5.174453735527773e-05) - present_state_Q ( -0.0019076237977627601)) * f1( 0.004306570274832859)
w2 ( 0.03703278317257588 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( 5.174453735527773e-05) - present_state_Q (-0.0019076237977627601)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0769460887816762 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011486922539974947) - present_state_Q ( -0.02089415806982659)) * f1( 0.024563139938752712)
w2 ( 0.036148768342415365 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.011486922539974947) - present_state_Q (-0.02089415806982659)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.076976330704484 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0026302536391995795) - present_state_Q ( -0.0044747918369906665)) * f1( 0.00506272803041648)
w2 ( 0.03495407952539968 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.0026302536391995795) - present_state_Q (-0.0044747918369906665)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0770505522768175 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00549957162973532) - present_state_Q ( 0.003795792676645187)) * f1( 0.010868274305833467)
w2 ( 0.03358824038213021 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.00549957162973532) - present_state_Q (0.003795792676645187)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0771118292828668 ) += alpha ( 0.1 ) * (reward ( -0.06394620732385486 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006679968632418799) - present_state_Q ( 0.004061944948970756)) * f1( 0.008922603672463341)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.06394620732385486) + discount_factor ( 0.1) * next_state_max_Q( -0.006679968632418799) - present_state_Q (0.004061944948970756)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0771524324481956 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012781203919306632) - present_state_Q ( -0.0027432787901238887)) * f1( 0.007611303446540771)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.012781203919306632) - present_state_Q (-0.0027432787901238887)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0772183256753478 ) += alpha ( 0.1 ) * (reward ( -0.05481103484901845 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013597802129812774) - present_state_Q ( -0.022709632594502736)) * f1( 0.019692438310030435)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.05481103484901845) + discount_factor ( 0.1) * next_state_max_Q( -0.013597802129812774) - present_state_Q (-0.022709632594502736)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0772644542574081 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01777017332530862) - present_state_Q ( -0.022538720080913417)) * f1( 0.01851500622661586)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.01777017332530862) - present_state_Q (-0.022538720080913417)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0773124981939155 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019237482010467506) - present_state_Q ( -0.028305171792596305)) * f1( 0.02490040630292615)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.019237482010467506) - present_state_Q (-0.028305171792596305)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0773605939368707 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014722259469538581) - present_state_Q ( -0.02440758825366801)) * f1( 0.02114981764366289)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.014722259469538581) - present_state_Q (-0.02440758825366801)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.077390149041631 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0172796430381626) - present_state_Q ( -0.0187962612872438)) * f1( 0.015177952133641938)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0172796430381626) - present_state_Q (-0.0187962612872438)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0774037391778035 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018731185301292516) - present_state_Q ( -0.005950629465044497)) * f1( 0.004186323276386257)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.018731185301292516) - present_state_Q (-0.005950629465044497)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0774388064419427 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018251374378709943) - present_state_Q ( -0.018251374378709943)) * f1( 0.017433864202819845)
w2 ( 0.03221471739940886 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.018251374378709943) - present_state_Q (-0.018251374378709943)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0774460292979413 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003981645456836565) - present_state_Q ( -0.0021778975907616885)) * f1( 0.0020778645503989656)
w2 ( 0.0315194982623235 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.003981645456836565) - present_state_Q (-0.0021778975907616885)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0774795181591523 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007580217918454418) - present_state_Q ( 0.004555903602902974)) * f1( 0.008001234983462599)
w2 ( 0.030682405956441623 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.007580217918454418) - present_state_Q (0.004555903602902974)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0775452697526489 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013186740217475993) - present_state_Q ( -0.02187905193916158)) * f1( 0.026179703445395268)
w2 ( 0.030682405956441623 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.013186740217475993) - present_state_Q (-0.02187905193916158)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.077583505978015 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0098414137212899) - present_state_Q ( -0.015977894912578224)) * f1( 0.012462059102043038)
w2 ( 0.030682405956441623 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.0098414137212899) - present_state_Q (-0.015977894912578224)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.077641341451873 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01418491397847675) - present_state_Q ( -0.021145859431978605)) * f1( 0.022288566380851128)
w2 ( 0.030682405956441623 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.01418491397847675) - present_state_Q (-0.021145859431978605)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0776813457252228 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019445161190312105) - present_state_Q ( -0.02723438034121208)) * f1( 0.0196234067380111)
w2 ( 0.030682405956441623 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.019445161190312105) - present_state_Q (-0.02723438034121208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.077763139653564 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007177580314692693) - present_state_Q ( -0.007177580314692693)) * f1( 0.020857263546111025)
w2 ( 0.02989808515462245 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.007177580314692693) - present_state_Q (-0.007177580314692693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0778164556359016 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00546053367827068) - present_state_Q ( -0.01130485735864078)) * f1( 0.015269322447553212)
w2 ( 0.02989808515462245 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00546053367827068) - present_state_Q (-0.01130485735864078)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0778382079503144 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010664568955008388) - present_state_Q ( -0.0038500891624640545)) * f1( 0.006632359963969778)
w2 ( 0.029242140226093817 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0010664568955008388) - present_state_Q (-0.0038500891624640545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.077872998395217 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001427588897355306) - present_state_Q ( -0.01114119146385264)) * f1( 0.013774717373816475)
w2 ( 0.028737005435178667 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.001427588897355306) - present_state_Q (-0.01114119146385264)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0778993628733406 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001325716417884348) - present_state_Q ( 0.001325716417884348)) * f1( 0.0069869596743451666)
w2 ( 0.027982328741669837 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.001325716417884348) - present_state_Q (0.001325716417884348)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0779136361214539 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001934773633064408) - present_state_Q ( -0.001934773633064408)) * f1( 0.004101579546220025)
w2 ( 0.027286340869078083 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.001934773633064408) - present_state_Q (-0.001934773633064408)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0779398180515218 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0010354167992997236) - present_state_Q ( 0.0010354167992997236)) * f1( 0.006986959674345164)
w2 ( 0.026536889568703776 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.0010354167992997236) - present_state_Q (0.0010354167992997236)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0779539844807864 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002224144126356191) - present_state_Q ( -0.002224144126356191)) * f1( 0.004101579546220025)
w2 ( 0.025846110364991275 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.002224144126356191) - present_state_Q (-0.002224144126356191)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0779799851757879 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0007472051801679109) - present_state_Q ( 0.0007472051801679109)) * f1( 0.006986959674345164)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( 0.0007472051801679109) - present_state_Q (0.0007472051801679109)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0780055760447813 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010229200290717081) - present_state_Q ( -0.010229200290717081)) * f1( 0.014061506405131749)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.010229200290717081) - present_state_Q (-0.010229200290717081)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0780155720111346 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018746117355443744) - present_state_Q ( -0.018746117355443744)) * f1( 0.009489230256022788)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.018746117355443744) - present_state_Q (-0.018746117355443744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0779963588203532 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02094114094121914) - present_state_Q ( -0.0356861532080862)) * f1( 0.03105653183788281)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.02094114094121914) - present_state_Q (-0.0356861532080862)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0779826973643931 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019464265924763847) - present_state_Q ( -0.03347882823882604)) * f1( 0.033103560036252984)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.019464265924763847) - present_state_Q (-0.03347882823882604)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0768151985166474 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020940502330211396) - present_state_Q ( -0.1261940294298085)) * f1( 0.1207410255300151)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.020940502330211396) - present_state_Q (-0.1261940294298085)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.076784060808594 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024869862809237704) - present_state_Q ( -0.03929865459055042)) * f1( 0.033103560036252984)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.024869862809237704) - present_state_Q (-0.03929865459055042)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0758552690486227 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023163098977435966) - present_state_Q ( -0.11967491412275881)) * f1( 0.1032529058208922)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.023163098977435966) - present_state_Q (-0.11967491412275881)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757430898237976 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020019544362297793) - present_state_Q ( -0.1450840518347335)) * f1( 0.009697660913753023)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.020019544362297793) - present_state_Q (-0.1450840518347335)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757451578895214 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01882465987756382) - present_state_Q ( -0.007944150978167839)) * f1( 0.006728726680208355)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.01882465987756382) - present_state_Q (-0.007944150978167839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757008586183765 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020754519638482623) - present_state_Q ( -0.028621466808743983)) * f1( 0.025443496760989663)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.020754519638482623) - present_state_Q (-0.028621466808743983)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.075688609446756 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01944356383868775) - present_state_Q ( -0.01944356383868775)) * f1( 0.014645050684142203)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.01944356383868775) - present_state_Q (-0.01944356383868775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.07569840816707 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009208711095424978) - present_state_Q ( -0.013298867912577133)) * f1( 0.01662956782268325)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009208711095424978) - present_state_Q (-0.013298867912577133)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757087417010986 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012018098926214266) - present_state_Q ( -0.005237521638522148)) * f1( 0.007259431192105036)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012018098926214266) - present_state_Q (-0.005237521638522148)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757135746861874 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005631858239304988) - present_state_Q ( -0.015639611384634645)) * f1( 0.015131831771980662)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.005631858239304988) - present_state_Q (-0.015639611384634645)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757125592179506 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011073035901580154) - present_state_Q ( -0.020002427611839263)) * f1( 0.01625323706269265)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.011073035901580154) - present_state_Q (-0.020002427611839263)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757187422514383 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014400932803725416) - present_state_Q ( -0.014400932803725416)) * f1( 0.011645215497700642)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014400932803725416) - present_state_Q (-0.014400932803725416)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.07571771967177 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015010380672927131) - present_state_Q ( -0.02029722693015105)) * f1( 0.019446448702944503)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015010380672927131) - present_state_Q (-0.02029722693015105)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.075714312951488 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02211720756281657) - present_state_Q ( -0.02211720756281657)) * f1( 0.020834402029111253)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.02211720756281657) - present_state_Q (-0.02211720756281657)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757228165720323 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015241982680707625) - present_state_Q ( -0.004991605624814178)) * f1( 0.005744549344313462)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015241982680707625) - present_state_Q (-0.004991605624814178)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.075731229389934 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018475357468418732) - present_state_Q ( -0.006141186608125623)) * f1( 0.006019175813931838)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.018475357468418732) - present_state_Q (-0.006141186608125623)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757326795872446 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019131349613904104) - present_state_Q ( -0.0052498819952084596)) * f1( 0.0009710970650075758)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.019131349613904104) - present_state_Q (-0.0052498819952084596)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757331538487376 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016259694045448903) - present_state_Q ( -0.01962574788848026)) * f1( 0.017528465387400268)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.016259694045448903) - present_state_Q (-0.01962574788848026)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757217598062563 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017685806537907276) - present_state_Q ( -0.02527178871177462)) * f1( 0.021774012133535626)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.017685806537907276) - present_state_Q (-0.02527178871177462)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.07571743531471 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015513804271430062) - present_state_Q ( -0.022303528151550744)) * f1( 0.017424799384247826)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015513804271430062) - present_state_Q (-0.022303528151550744)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.07572522944018 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01515969890312696) - present_state_Q ( -0.011753516947325451)) * f1( 0.009702877595234962)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01515969890312696) - present_state_Q (-0.011753516947325451)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757299353860597 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01656994961695177) - present_state_Q ( -0.01656994961695177)) * f1( 0.0140166780355075)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01656994961695177) - present_state_Q (-0.01656994961695177)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0757133812643707 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019210358406409247) - present_state_Q ( -0.027071948799696603)) * f1( 0.024059237066626587)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.019210358406409247) - present_state_Q (-0.027071948799696603)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.075697249454135 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01875106229291961) - present_state_Q ( -0.026926218851755602)) * f1( 0.023790536727033473)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01875106229291961) - present_state_Q (-0.026926218851755602)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.075698929947071 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006747362227836412) - present_state_Q ( -0.01583668551771769)) * f1( 0.005406303195123835)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006747362227836412) - present_state_Q (-0.01583668551771769)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0756779054306447 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01412160057579403) - present_state_Q ( -0.028397404089538714)) * f1( 0.024124796199806165)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01412160057579403) - present_state_Q (-0.028397404089538714)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0756369264463228 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022520208237043735) - present_state_Q ( -0.03604528098292482)) * f1( 0.026399026064789326)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.022520208237043735) - present_state_Q (-0.03604528098292482)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0740534819282068 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013536322563729318) - present_state_Q ( -0.14122356981792358)) * f1( 0.13021791307887967)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013536322563729318) - present_state_Q (-0.14122356981792358)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0727796369466218 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019185668224126207) - present_state_Q ( -0.13051287368624584)) * f1( 0.11546403514552613)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.019185668224126207) - present_state_Q (-0.13051287368624584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.072730323594516 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025927417773995345) - present_state_Q ( -0.03834945350373904)) * f1( 0.02820102811273805)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.025927417773995345) - present_state_Q (-0.03834945350373904)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.072706612401722 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019464558667581074) - present_state_Q ( -0.02868031731699694)) * f1( 0.028015769559163295)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.019464558667581074) - present_state_Q (-0.02868031731699694)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0710849734087338 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020971285735683107) - present_state_Q ( -0.1399589869939871)) * f1( 0.1355981662850122)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.020971285735683107) - present_state_Q (-0.1399589869939871)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0695446922373413 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12707877952308963) - present_state_Q ( -0.1474528770907586)) * f1( 0.13224174668045405)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12707877952308963) - present_state_Q (-0.1474528770907586)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0694814945217066 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012659746243789828) - present_state_Q ( -0.025417799399701697)) * f1( 0.10745206565035678)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012659746243789828) - present_state_Q (-0.025417799399701697)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0682897094317931 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12565751301799202) - present_state_Q ( -0.13211651019411264)) * f1( 0.11767182256861325)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12565751301799202) - present_state_Q (-0.13211651019411264)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.067037679881687 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01507856032420942) - present_state_Q ( -0.12583026743874715)) * f1( 0.11805800602838006)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01507856032420942) - present_state_Q (-0.12583026743874715)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.067039353109766 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018397431341267582) - present_state_Q ( -0.018397431341267582)) * f1( 0.00976978070099526)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.018397431341267582) - present_state_Q (-0.018397431341267582)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0654246965794802 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01466487163382995) - present_state_Q ( -0.13977888571253302)) * f1( 0.13450757312708253)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01466487163382995) - present_state_Q (-0.13977888571253302)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0638784976493894 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0186273543460292) - present_state_Q ( -0.13636487219803392)) * f1( 0.13302719556869202)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0186273543460292) - present_state_Q (-0.13636487219803392)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.062579998908461 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01644752645910443) - present_state_Q ( -0.13087795667517546)) * f1( 0.11702102412456093)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01644752645910443) - present_state_Q (-0.13087795667517546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0609683593459693 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012603870145852493) - present_state_Q ( -0.13945228341406685)) * f1( 0.13439115346720723)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012603870145852493) - present_state_Q (-0.13945228341406685)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.060948046108236 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018157014037412274) - present_state_Q ( -0.030204929905764403)) * f1( 0.020074583948344837)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.018157014037412274) - present_state_Q (-0.030204929905764403)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0597077790632439 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01565788963014354) - present_state_Q ( -0.12485426235529475)) * f1( 0.11810028072151557)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01565788963014354) - present_state_Q (-0.12485426235529475)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.058062728188318 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013290605931166537) - present_state_Q ( -0.1395496537369653)) * f1( 0.13714443277040678)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013290605931166537) - present_state_Q (-0.1395496537369653)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0580591913303408 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012961096660550233) - present_state_Q ( -0.021165703506342084)) * f1( 0.02211574444609668)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.012961096660550233) - present_state_Q (-0.021165703506342084)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.058062575918683 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01575455222638894) - present_state_Q ( -0.01575455222638894)) * f1( 0.008272752927531001)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.01575455222638894) - present_state_Q (-0.01575455222638894)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0565708183649285 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12516778076120777) - present_state_Q ( -0.1446153168973336)) * f1( 0.13105343263533342)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.12516778076120777) - present_state_Q (-0.1446153168973336)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0565542345520478 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015230431402104158) - present_state_Q ( -0.02822095162803767)) * f1( 0.01967806330471113)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.015230431402104158) - present_state_Q (-0.02822095162803767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.056471167446759 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014682412892771944) - present_state_Q ( -0.02761138831907027)) * f1( 0.10551148681661708)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.014682412892771944) - present_state_Q (-0.02761138831907027)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0564666680084545 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013500097822474506) - present_state_Q ( -0.021624644284188015)) * f1( 0.02244904335022367)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.013500097822474506) - present_state_Q (-0.021624644284188015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0564587216428536 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010212971893901033) - present_state_Q ( -0.02268761429065355)) * f1( 0.023399383876426674)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.010212971893901033) - present_state_Q (-0.02268761429065355)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055021810522288 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018906351355793858) - present_state_Q ( -0.13140566936437134)) * f1( 0.12916671617108033)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.018906351355793858) - present_state_Q (-0.13140566936437134)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055049360636162 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007460130765200188) - present_state_Q ( -0.007460130765200188)) * f1( 0.007071067811865476)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.007460130765200188) - present_state_Q (-0.007460130765200188)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0550729991719985 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005387718295077834) - present_state_Q ( -0.012784696229167137)) * f1( 0.007071067811865476)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.005387718295077834) - present_state_Q (-0.012784696229167137)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551185051975127 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00992321602863633) - present_state_Q ( -0.023421298538137198)) * f1( 0.01957510636591879)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.00992321602863633) - present_state_Q (-0.023421298538137198)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551655859036477 ) += alpha ( 0.1 ) * (reward ( -0.045675862374182044 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013905141775881303) - present_state_Q ( -0.027475634656960093)) * f1( 0.024032120063540444)
w2 ( 0.02510184687376134 ) += alpha ( 0.1) * (reward ( -0.045675862374182044) + discount_factor ( 0.1) * next_state_max_Q( -0.013905141775881303) - present_state_Q (-0.027475634656960093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551966710753458 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008220880823124287) - present_state_Q ( -0.013241250197876557)) * f1( 0.012886900024148568)
w2 ( 0.024619416318085712 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.008220880823124287) - present_state_Q (-0.013241250197876557)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0552274128716865 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001537167596039276) - present_state_Q ( -0.003386715667577868)) * f1( 0.012881494595201999)
w2 ( 0.023664812918192616 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( 0.001537167596039276) - present_state_Q (-0.003386715667577868)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0552612452169363 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0012078030808636842) - present_state_Q ( -0.0012078030808636842)) * f1( 0.012854969745638973)
w2 ( 0.022085703239088702 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0012078030808636842) - present_state_Q (-0.0012078030808636842)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0552819898318766 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00451338484957029) - present_state_Q ( -0.00451338484957029)) * f1( 0.014600351207567308)
w2 ( 0.02151737129568632 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00451338484957029) - present_state_Q (-0.00451338484957029)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.055286631333898 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004214500796455481) - present_state_Q ( 0.00200231473715398)) * f1( 0.0022429097331627245)
w2 ( 0.020689606905027427 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004214500796455481) - present_state_Q (0.00200231473715398)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0552965607634037 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0006956700949687089) - present_state_Q ( -0.0024747656484891533)) * f1( 0.006258643514017304)
w2 ( 0.020055001052600205 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0006956700949687089) - present_state_Q (-0.0024747656484891533)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.055311205942093 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004240694622001496) - present_state_Q ( -0.003470473956884648)) * f1( 0.010187382357825566)
w2 ( 0.019479968991376682 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.004240694622001496) - present_state_Q (-0.003470473956884648)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.055322306164224 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0047236234889383825) - present_state_Q ( -0.00599583738818355)) * f1( 0.009405258053541974)
w2 ( 0.019007883182872866 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0047236234889383825) - present_state_Q (-0.00599583738818355)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.055332543408434 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005345542313287583) - present_state_Q ( -0.006189716547174373)) * f1( 0.008866428587070534)
w2 ( 0.01854604021602608 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.005345542313287583) - present_state_Q (-0.006189716547174373)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.055343106495426 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0035294306450597817) - present_state_Q ( -0.005076809385073128)) * f1( 0.00822632366212002)
w2 ( 0.01803241651602233 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.0035294306450597817) - present_state_Q (-0.005076809385073128)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0553636656110543 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003352332551059168) - present_state_Q ( 0.003352332551059168)) * f1( 0.009657860000059158)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( 0.003352332551059168) - present_state_Q (0.003352332551059168)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0553239989869923 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006593227294432296) - present_state_Q ( -0.0180200282731099)) * f1( 0.022848509216533235)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.006593227294432296) - present_state_Q (-0.0180200282731099)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552856249580644 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00488970138656723) - present_state_Q ( -0.018300809971091847)) * f1( 0.021544112954479305)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00488970138656723) - present_state_Q (-0.018300809971091847)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552397636086612 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0065366911617161585) - present_state_Q ( -0.021425657206535165)) * f1( 0.022078459319238696)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0065366911617161585) - present_state_Q (-0.021425657206535165)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551952283497557 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005704211354836356) - present_state_Q ( -0.020505029123828584)) * f1( 0.022340674535256025)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.005704211354836356) - present_state_Q (-0.020505029123828584)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551746596045122 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00441979736983819) - present_state_Q ( -0.01836886732932008)) * f1( 0.011473684507485933)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00441979736983819) - present_state_Q (-0.01836886732932008)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551692519617857 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004425040753701433) - present_state_Q ( -0.0104982640600208)) * f1( 0.005377656919815466)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.004425040753701433) - present_state_Q (-0.0104982640600208)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0551914168312435 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0026128300549935015) - present_state_Q ( -0.0026128300549935015)) * f1( 0.008846849072610635)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0026128300549935015) - present_state_Q (-0.0026128300549935015)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055193811390748 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005745074295219194) - present_state_Q ( -0.026761450375987046)) * f1( 0.019650497755340134)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.005745074295219194) - present_state_Q (-0.026761450375987046)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552043668456765 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002106339034176078) - present_state_Q ( -0.014682028338319613)) * f1( 0.008160935949862726)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.002106339034176078) - present_state_Q (-0.014682028338319613)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552260744581767 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032949556852712295) - present_state_Q ( -0.0032949556852712295)) * f1( 0.008881981014502115)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0032949556852712295) - present_state_Q (-0.0032949556852712295)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552404943109663 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0038868342876528725) - present_state_Q ( -0.01880571839125898)) * f1( 0.01604258878010723)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0038868342876528725) - present_state_Q (-0.01880571839125898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055245500793929 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0042220610787783225) - present_state_Q ( -0.025053177819754174)) * f1( 0.018044334032185833)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0042220610787783225) - present_state_Q (-0.025053177819754174)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552668262198888 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006436452428368475) - present_state_Q ( -0.008857834315056087)) * f1( 0.011112011408696115)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006436452428368475) - present_state_Q (-0.008857834315056087)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055277032352724 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003089021362278492) - present_state_Q ( -0.0203666309056154)) * f1( 0.013890074026807912)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.003089021362278492) - present_state_Q (-0.0203666309056154)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552911794759994 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002694145625954093) - present_state_Q ( -0.002694145625954093)) * f1( 0.008928100467985254)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.002694145625954093) - present_state_Q (-0.002694145625954093)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552851024444565 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003111330789029908) - present_state_Q ( -0.016017913738281344)) * f1( 0.009247403941815306)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.003111330789029908) - present_state_Q (-0.016017913738281344)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552824904602514 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003177289871800137) - present_state_Q ( -0.012643874102093628)) * f1( 0.008185542465392636)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.003177289871800137) - present_state_Q (-0.012643874102093628)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552968494267856 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011254738724805279) - present_state_Q ( -0.011254738724805279)) * f1( 0.008311389564472619)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.011254738724805279) - present_state_Q (-0.011254738724805279)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0553269752953536 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00631037438034097) - present_state_Q ( -0.00631037438034097)) * f1( 0.013866159582335658)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.00631037438034097) - present_state_Q (-0.00631037438034097)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0553385081415607 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010155719495491827) - present_state_Q ( -0.014353049779088632)) * f1( 0.008197905706262238)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.010155719495491827) - present_state_Q (-0.014353049779088632)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0553686894468024 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005793681579636266) - present_state_Q ( -0.005793681579636266)) * f1( 0.013600571306415867)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.005793681579636266) - present_state_Q (-0.005793681579636266)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0553997007013924 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006746849152624714) - present_state_Q ( -0.006746849152624714)) * f1( 0.014536512060711921)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006746849152624714) - present_state_Q (-0.006746849152624714)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554124100464446 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007322049397719609) - present_state_Q ( -0.02162595185109753)) * f1( 0.019517495320933328)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.007322049397719609) - present_state_Q (-0.02162595185109753)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554150710255983 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006355570240406733) - present_state_Q ( -0.026695457550608166)) * f1( 0.01977516154651667)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006355570240406733) - present_state_Q (-0.026695457550608166)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554261564137688 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005191976342646757) - present_state_Q ( -0.015159238511890312)) * f1( 0.00901383077507805)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.0005191976342646757) - present_state_Q (-0.015159238511890312)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554270995187633 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006091404102853089) - present_state_Q ( -0.018016767942071268)) * f1( 0.01093179498811674)
w2 ( 0.017180918746197288 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.006091404102853089) - present_state_Q (-0.018016767942071268)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554354580139556 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0020277353480416312) - present_state_Q ( 0.0020277353480416312)) * f1( 0.007626270785098666)
w2 ( 0.01696171606043581 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0020277353480416312) - present_state_Q (0.0020277353480416312)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0554432007631447 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0013016129073815108) - present_state_Q ( 0.0013016129073815108)) * f1( 0.007512400879654273)
w2 ( 0.016755583578606216 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.0013016129073815108) - present_state_Q (0.0013016129073815108)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0554432083606022 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00039613710936229653) - present_state_Q ( -0.009085618949489824)) * f1( 0.007643460160372562)
w2 ( 0.016755384782318007 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.00039613710936229653) - present_state_Q (-0.009085618949489824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0554436751712357 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006129803716270486) - present_state_Q ( -0.009365888488482486)) * f1( 0.012211722692865521)
w2 ( 0.016747739495158388 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.006129803716270486) - present_state_Q (-0.009365888488482486)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0554434953801484 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0006603613907088055) - present_state_Q ( -0.009401358039590187)) * f1( 0.008982843034691124)
w2 ( 0.016751742483672047 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0006603613907088055) - present_state_Q (-0.009401358039590187)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.055443558148822 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005188501045420454) - present_state_Q ( -0.009612308942225666)) * f1( 0.015047518724095113)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005188501045420454) - present_state_Q (-0.009612308942225666)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.055435925791143 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00832596029532704) - present_state_Q ( -0.010266562779753113)) * f1( 0.008090295292482097)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.00832596029532704) - present_state_Q (-0.010266562779753113)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554182528599518 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006168313240562661) - present_state_Q ( -0.01517628866568839)) * f1( 0.012138454597933447)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.006168313240562661) - present_state_Q (-0.01517628866568839)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0553971587061435 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004132422638150563) - present_state_Q ( -0.015083184463685695)) * f1( 0.014379166271332306)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.004132422638150563) - present_state_Q (-0.015083184463685695)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055392764660524 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00592980837919941) - present_state_Q ( -0.020848593729918877)) * f1( 0.022133262347163583)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.00592980837919941) - present_state_Q (-0.020848593729918877)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.05539129352662 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004642203943490647) - present_state_Q ( -0.01942379667518568)) * f1( 0.02134455932685962)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004642203943490647) - present_state_Q (-0.01942379667518568)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554040718381479 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0024277446482995537) - present_state_Q ( -0.0024277446482995537)) * f1( 0.007944055835467277)
w2 ( 0.01675090821092899 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0024277446482995537) - present_state_Q (-0.0024277446482995537)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554126692068595 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00020416186859245492) - present_state_Q ( -0.00020416186859245492)) * f1( 0.00960446743322153)
w2 ( 0.016571879675066927 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.00020416186859245492) - present_state_Q (-0.00020416186859245492)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0554112172638537 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002073311088603676) - present_state_Q ( -0.010655021088303318)) * f1( 0.008406438467470027)
w2 ( 0.016606423269513473 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( 0.002073311088603676) - present_state_Q (-0.010655021088303318)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0554120441052404 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0008478833168701254) - present_state_Q ( -0.008104389079112395)) * f1( 0.007411817333473854)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0008478833168701254) - present_state_Q (-0.008104389079112395)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.055432287316524 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006292486251245178) - present_state_Q ( -0.02707508906624285)) * f1( 0.020053009574368952)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006292486251245178) - present_state_Q (-0.02707508906624285)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554690262218471 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0062088949080162095) - present_state_Q ( -0.015616140919268037)) * f1( 0.017051825319137768)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0062088949080162095) - present_state_Q (-0.015616140919268037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554928781782555 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006975116465283831) - present_state_Q ( -0.0105248193960849)) * f1( 0.008928841834642409)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006975116465283831) - present_state_Q (-0.0105248193960849)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555196233601742 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0036498950800213873) - present_state_Q ( -0.02458387440569725)) * f1( 0.021705571476859836)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0036498950800213873) - present_state_Q (-0.02458387440569725)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555410068182138 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018719719055535788) - present_state_Q ( -0.015553685609871856)) * f1( 0.010098826187000663)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.0018719719055535788) - present_state_Q (-0.015553685609871856)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555624489672613 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004384194473684128) - present_state_Q ( -0.02518410559258212)) * f1( 0.018179009938854195)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.004384194473684128) - present_state_Q (-0.02518410559258212)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555987636746036 ) += alpha ( 0.1 ) * (reward ( -0.036540689899345634 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006881839231641176) - present_state_Q ( -0.02170528497336239)) * f1( 0.02339324217818227)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.036540689899345634) + discount_factor ( 0.1) * next_state_max_Q( -0.006881839231641176) - present_state_Q (-0.02170528497336239)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0556142511671143 ) += alpha ( 0.1 ) * (reward ( -0.027405517424509224 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006350939552892397) - present_state_Q ( -0.01958772667798915)) * f1( 0.018322138603519986)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.027405517424509224) + discount_factor ( 0.1) * next_state_max_Q( -0.006350939552892397) - present_state_Q (-0.01958772667798915)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0556007646718053 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009641857296094666) - present_state_Q ( -0.024558580032857203)) * f1( 0.025331274023556307)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.009641857296094666) - present_state_Q (-0.024558580032857203)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555659643001494 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009462074112021357) - present_state_Q ( -0.027071481721546505)) * f1( 0.020482732824529823)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.009462074112021357) - present_state_Q (-0.027071481721546505)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555518741332623 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0050912550113392935) - present_state_Q ( -0.026199478689420852)) * f1( 0.01898942215960275)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0050912550113392935) - present_state_Q (-0.026199478689420852)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555687250811479 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003894803397662898) - present_state_Q ( -0.003894803397662898)) * f1( 0.011412748324361049)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.003894803397662898) - present_state_Q (-0.003894803397662898)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555783885297427 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010416416398786998) - present_state_Q ( -0.012071635425050175)) * f1( 0.01334665733094001)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.010416416398786998) - present_state_Q (-0.012071635425050175)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555830721328139 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0036516303116072464) - present_state_Q ( -0.013889283623786407)) * f1( 0.009868060838943694)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.0036516303116072464) - present_state_Q (-0.013889283623786407)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.05559808398301 ) += alpha ( 0.1 ) * (reward ( -0.018270344949672817 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004334423558712379) - present_state_Q ( -0.004334423558712379)) * f1( 0.01044712240604747)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.018270344949672817) + discount_factor ( 0.1) * next_state_max_Q( -0.004334423558712379) - present_state_Q (-0.004334423558712379)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055591779635373 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017967941046968055) - present_state_Q ( -0.015966928287380514)) * f1( 0.009477262820893066)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0017967941046968055) - present_state_Q (-0.015966928287380514)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555662941749673 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006482039160826338) - present_state_Q ( -0.02136991175312276)) * f1( 0.02199575594358759)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.006482039160826338) - present_state_Q (-0.02136991175312276)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555429420351867 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0054659124141228175) - present_state_Q ( -0.020190921120417277)) * f1( 0.02222075365566026)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0054659124141228175) - present_state_Q (-0.020190921120417277)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555474157595464 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006421252308657309) - present_state_Q ( -0.006421252308657309)) * f1( 0.013330345183970325)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.006421252308657309) - present_state_Q (-0.006421252308657309)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555252836334734 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005030959764138109) - present_state_Q ( -0.01988125534056376)) * f1( 0.02160710182683828)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005030959764138109) - present_state_Q (-0.01988125534056376)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055524439785789 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0039003902905424283) - present_state_Q ( -0.011080180684484993)) * f1( 0.005426780768184025)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.0039003902905424283) - present_state_Q (-0.011080180684484993)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555289483046766 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005551599318567242) - present_state_Q ( -0.005551599318567242)) * f1( 0.010893475832939776)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.005551599318567242) - present_state_Q (-0.005551599318567242)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.05552031218741 ) += alpha ( 0.1 ) * (reward ( -0.009135172474836408 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003059103621136899) - present_state_Q ( -0.0174522006705191)) * f1( 0.010780165073138295)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( -0.009135172474836408) + discount_factor ( 0.1) * next_state_max_Q( -0.003059103621136899) - present_state_Q (-0.0174522006705191)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555116252733485 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009154101987567222) - present_state_Q ( -0.009506300156495546)) * f1( 0.010111774337922366)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.009154101987567222) - present_state_Q (-0.009506300156495546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0555002426733235 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029224765196601407) - present_state_Q ( -0.013417036460443293)) * f1( 0.008672596710712936)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0029224765196601407) - present_state_Q (-0.013417036460443293)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554554536380225 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005859262929941737) - present_state_Q ( -0.020761985174640788)) * f1( 0.022199100212648325)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.005859262929941737) - present_state_Q (-0.020761985174640788)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055444515230352 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0018531818215208576) - present_state_Q ( -0.013523598244724834)) * f1( 0.008200763231318119)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0018531818215208576) - present_state_Q (-0.013523598244724834)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554035751201767 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004833641625007235) - present_state_Q ( -0.01973089581068996)) * f1( 0.02127031711059979)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.004833641625007235) - present_state_Q (-0.01973089581068996)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055346160325163 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008919540360348425) - present_state_Q ( -0.023762391148162136)) * f1( 0.02510437152212667)
w2 ( 0.016584111834965252 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.008919540360348425) - present_state_Q (-0.023762391148162136)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0553461776901838 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(2.401637302811054e-05) - present_state_Q ( 2.401637302811054e-05)) * f1( 0.008033880717388248)
w2 ( 0.016583679540250747 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 2.401637302811054e-05) - present_state_Q (2.401637302811054e-05)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0553475742222729 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0024305566253780076) - present_state_Q ( 0.0024305566253780076)) * f1( 0.006384143882822672)
w2 ( 0.016539929520993944 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0024305566253780076) - present_state_Q (0.0024305566253780076)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0553417136623886 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0006458262046871196) - present_state_Q ( -0.008954761844255966)) * f1( 0.006497767002135901)
w2 ( 0.016720316410288438 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0006458262046871196) - present_state_Q (-0.008954761844255966)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.055334027069196 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0011968293495027938) - present_state_Q ( -0.00973570678620353)) * f1( 0.0077993802479559985)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( 0.0011968293495027938) - present_state_Q (-0.00973570678620353)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0553331299604984 ) += alpha ( 0.1 ) * (reward ( 2.2302667174893575e-06 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0013192663811621465) - present_state_Q ( -0.0013192663811621465)) * f1( 0.007541453552217017)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 2.2302667174893575e-06) + discount_factor ( 0.1) * next_state_max_Q( -0.0013192663811621465) - present_state_Q (-0.0013192663811621465)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552956876270807 ) += alpha ( 0.1 ) * (reward ( 2.787833396861697e-07 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0069172301955546436) - present_state_Q ( -0.01750165064315924)) * f1( 0.02227357149058923)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 2.787833396861697e-07) + discount_factor ( 0.1) * next_state_max_Q( -0.0069172301955546436) - present_state_Q (-0.01750165064315924)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552882068715417 ) += alpha ( 0.1 ) * (reward ( 1.0889974206491004e-09 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006028510664358682) - present_state_Q ( -0.006028510664358682)) * f1( 0.013787731984397304)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 1.0889974206491004e-09) + discount_factor ( 0.1) * next_state_max_Q( -0.006028510664358682) - present_state_Q (-0.006028510664358682)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552847875302418 ) += alpha ( 0.1 ) * (reward ( 5.317370218013185e-13 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00418908690758428) - present_state_Q ( -0.00418908690758428)) * f1( 0.009069442086508554)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 5.317370218013185e-13) + discount_factor ( 0.1) * next_state_max_Q( -0.00418908690758428) - present_state_Q (-0.00418908690758428)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0552487857484978 ) += alpha ( 0.1 ) * (reward ( 4.154195482822801e-15 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005825422745086519) - present_state_Q ( -0.02066773221027415)) * f1( 0.017924541345725492)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 4.154195482822801e-15) + discount_factor ( 0.1) * next_state_max_Q( -0.005825422745086519) - present_state_Q (-0.02066773221027415)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055207758593018 ) += alpha ( 0.1 ) * (reward ( 5.192744353528501e-16 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0077011760184856745) - present_state_Q ( -0.021438915833644684)) * f1( 0.019849802112216947)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 5.192744353528501e-16) + discount_factor ( 0.1) * next_state_max_Q( -0.0077011760184856745) - present_state_Q (-0.021438915833644684)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.055171471622245 ) += alpha ( 0.1 ) * (reward ( 0.0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0062268714333298825) - present_state_Q ( -0.021107425477610284)) * f1( 0.017714149031813717)
w2 ( 0.016917424204711515 ) += alpha ( 0.1) * (reward ( 0.0) + discount_factor ( 0.1) * next_state_max_Q( -0.0062268714333298825) - present_state_Q (-0.021107425477610284)) * f2(0.0)
============================================================================
