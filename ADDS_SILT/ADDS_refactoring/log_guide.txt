GUIDE learning . . .
w1 ( 0.9278724583989048 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5148153000446647) - present_state_Q ( 0.4951597366792507)) * f1( 0.29515973667925066)
w2 ( 0.9511264358665044 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5148153000446647) - present_state_Q (0.4951597366792507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.8655643787039169 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4883147424090228) - present_state_Q ( 0.4883147424090228)) * f1( 0.32126123858670375)
w2 ( 0.912336770503142 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.4883147424090228) - present_state_Q (0.4883147424090228)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.767920878341903 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6519159061255677) - present_state_Q ( 0.6062990676004105)) * f1( 0.38425568869584864)
w2 ( 0.8361035461935064 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6519159061255677) - present_state_Q (0.6062990676004105)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.6544783040279821 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6692047016516978) - present_state_Q ( 0.6692047016516978)) * f1( 0.43593460292044284)
w2 ( 0.7320121769340452 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6692047016516978) - present_state_Q (0.6692047016516978)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.5261219308588576 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6713289453415218) - present_state_Q ( 0.6216583605524848)) * f1( 0.5024666024143812)
w2 ( 0.6298311582933118 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6713289453415218) - present_state_Q (0.6216583605524848)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.3798151233003034 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.593098318247266) - present_state_Q ( 0.5878496065444965)) * f1( 0.5786217366296676)
w2 ( 0.5160468684309222 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.593098318247266) - present_state_Q (0.5878496065444965)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.842036245797368 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7326292609584131) - present_state_Q ( 0.7419218545298557)) * f1( 0.5919218545298557)
w2 ( 0.9599701160734898 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7326292609584131) - present_state_Q (0.7419218545298557)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.6628512047548264 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7979028772229314) - present_state_Q ( 0.795562959584158)) * f1( 0.6597939617666778)
w2 ( 0.8920757992769431 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7979028772229314) - present_state_Q (0.795562959584158)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.5018174695703436 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6733971133168004) - present_state_Q ( 0.6382139244625565)) * f1( 0.626377340291464)
w2 ( 0.8278039439486712 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6733971133168004) - present_state_Q (0.6382139244625565)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.33370160610019733 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5525036519106419) - present_state_Q ( 0.5456990858794025)) * f1( 0.6750424615194665)
w2 ( 0.7655427259314627 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5525036519106419) - present_state_Q (0.5456990858794025)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.17179551426232834 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42099079016081054) - present_state_Q ( 0.41871836072024726)) * f1( 0.6812453853432237)
w2 ( 0.7061272438888585 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.42099079016081054) - present_state_Q (0.41871836072024726)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.014981186543256808 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2954258362617513) - present_state_Q ( 0.2954258362617513)) * f1( 0.692067111298307)
w2 ( 0.6494801625729691 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2954258362617513) - present_state_Q (0.2954258362617513)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.9097907484152647 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5226993667483077) - present_state_Q ( 0.4726993667483077)) * f1( 0.37269936674830767)
w2 ( 0.9757957056992652 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5226993667483077) - present_state_Q (0.4726993667483077)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.9260301216037706 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6447148059193711) - present_state_Q ( 0.6447148059193711)) * f1( 0.38687587758249614)
w2 ( 0.9883884059394422 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6447148059193711) - present_state_Q (0.6447148059193711)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.9419463379446569 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6533223785722353) - present_state_Q ( 0.6614194116088219)) * f1( 0.394050777954201)
w2 ( 1.0005057907268942 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6533223785722353) - present_state_Q (0.6614194116088219)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.9574292586065213 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6647683121975556) - present_state_Q ( 0.6521256125116114)) * f1( 0.373666589183367)
w2 ( 1.0129363272881384 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6647683121975556) - present_state_Q (0.6521256125116114)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.9527745218465862 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6769984843496799) - present_state_Q ( 0.6847199241444508)) * f1( 0.39777249601949366)
w2 ( 1.009425725016854 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6769984843496799) - present_state_Q (0.6847199241444508)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.8486487053798544 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.674474819192652) - present_state_Q ( 0.6822399507128045)) * f1( 0.3982182819838674)
w2 ( 0.9309819509530477 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.674474819192652) - present_state_Q (0.6822399507128045)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.7482324011943726 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6543185004226829) - present_state_Q ( 0.6137093655707995)) * f1( 0.394055606477596)
w2 ( 0.8545336254871918 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6543185004226829) - present_state_Q (0.6137093655707995)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( 0.6494142288756762 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.591105368026141) - present_state_Q ( 0.591105368026141)) * f1( 0.3902779385649252)
w2 ( 0.7659138063943683 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.591105368026141) - present_state_Q (0.591105368026141)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.5559479191806754 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5234167414782294) - present_state_Q ( 0.5145861768563792)) * f1( 0.3795980033346995)
w2 ( 0.6797352487995689 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5234167414782294) - present_state_Q (0.5145861768563792)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.46424311015524955 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45314293887191504) - present_state_Q ( 0.4499292174139056)) * f1( 0.38137004028456895)
w2 ( 0.5955737264761339 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.45314293887191504) - present_state_Q (0.4499292174139056)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.374245701558799 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4206698344591885) - present_state_Q ( 0.4143496371799237)) * f1( 0.37937051242520964)
w2 ( 0.5006824203267737 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.4206698344591885) - present_state_Q (0.4143496371799237)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.28524434816852406 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.34711495856916846) - present_state_Q ( 0.3444795432327761)) * f1( 0.3853259356124088)
w2 ( 0.4082916984317393 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.34711495856916846) - present_state_Q (0.3444795432327761)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.2001781576310866 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27525177525670863) - present_state_Q ( 0.27145147037860134)) * f1( 0.3790952974185449)
w2 ( 0.31853464671762205 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.27525177525670863) - present_state_Q (0.27145147037860134)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.12969273146210442 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2046259893201111) - present_state_Q ( 0.19237791437568663)) * f1( 0.32453118990315477)
w2 ( 0.231658034099875 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2046259893201111) - present_state_Q (0.19237791437568663)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04938597602346774 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1422662149076956) - present_state_Q ( 0.14162077264262377)) * f1( 0.37748884190151344)
w2 ( 0.14656226805380082 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1422662149076956) - present_state_Q (0.14162077264262377)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0319553127052922 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07802915808474757) - present_state_Q ( 0.07802915808474757)) * f1( 0.3929101422235034)
w2 ( 0.0637532183627499 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07802915808474757) - present_state_Q (0.07802915808474757)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08460272564764855 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.017153900237172438) - present_state_Q ( 0.017153900237172438)) * f1( 0.2612206359834836)
w2 ( -0.01686432204578832 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.017153900237172438) - present_state_Q (0.017153900237172438)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12476547998322049 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02224220875584143) - present_state_Q ( -0.02392145410863221)) * f1( 0.20301621678065007)
w2 ( -0.0959964327164664 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.02224220875584143) - present_state_Q (-0.02392145410863221)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.14966869549410156 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04469201220136139) - present_state_Q ( -0.0494918338371847)) * f1( 0.12738365121954323)
w2 ( -0.16442064057486971 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.04469201220136139) - present_state_Q (-0.0494918338371847)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.16143672723566516 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03360484461413192) - present_state_Q ( -0.03360484461413192)) * f1( 0.05974361237252752)
w2 ( -0.19396697517257894 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03360484461413192) - present_state_Q (-0.03360484461413192)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.16215842384957854 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04136490733095302) - present_state_Q ( -0.05106325608958197)) * f1( 0.015928917418422023)
w2 ( -0.20529380603866676 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04136490733095302) - present_state_Q (-0.05106325608958197)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.9660814976087172 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6920790686379912) - present_state_Q ( 0.6420790686379912)) * f1( 0.5920790686379912)
w2 ( 0.997135644191129 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.6920790686379912) - present_state_Q (0.6420790686379912)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.9541682091252476 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8177647113810685) - present_state_Q ( 0.7679079291715121)) * f1( 0.6400470188833718)
w2 ( 0.9943436723206279 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.8177647113810685) - present_state_Q (0.7679079291715121)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.7658430715216343 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8489733553032682) - present_state_Q ( 0.8489733553032682)) * f1( 0.6813312523115173)
w2 ( 0.9390621519251691 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.8489733553032682) - present_state_Q (0.8489733553032682)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.5875022839931938 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7165299077257787) - present_state_Q ( 0.7062298588315095)) * f1( 0.6769238342999501)
w2 ( 0.8863706145639905 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.7165299077257787) - present_state_Q (0.7062298588315095)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.4826995611515626 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5814686827974325) - present_state_Q ( 0.5814686827974325)) * f1( 0.687988065573745)
w2 ( 0.8559041782736366 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.5814686827974325) - present_state_Q (0.5814686827974325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.42478759101482355 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4989541495426608) - present_state_Q ( 0.47375890055432524)) * f1( 0.626845535508145)
w2 ( 0.8374269085616355 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.4989541495426608) - present_state_Q (0.47375890055432524)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.29450851356225344 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45992015323574775) - present_state_Q ( 0.4570657130994302)) * f1( 0.681706192723972)
w2 ( 0.7992054346061184 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.45992015323574775) - present_state_Q (0.4570657130994302)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1375322913782477 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36098601362355365) - present_state_Q ( 0.3588738352696769)) * f1( 0.6758132250271317)
w2 ( 0.752749929927972 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.36098601362355365) - present_state_Q (0.3588738352696769)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18867641431136437 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2825092670649145) - present_state_Q ( 0.2825092670649145)) * f1( 0.6858155538433761)
w2 ( 0.7713934714190114 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2825092670649145) - present_state_Q (0.2825092670649145)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( 0.8747193996852102 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6072234095781323) - present_state_Q ( 0.5944514405140335)) * f1( 0.4944514405140335)
w2 ( 0.9746627090044377 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6072234095781323) - present_state_Q (0.5944514405140335)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7289361890156757 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6173257746973532) - present_state_Q ( 0.5998779982801912)) * f1( 0.5743690234383196)
w2 ( 0.9492812547963332 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6173257746973532) - present_state_Q (0.5998779982801912)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.5589694725800772 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.585820971685759) - present_state_Q ( 0.5852726591621421)) * f1( 0.672685128096945)
w2 ( 0.9240143491763975 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.585820971685759) - present_state_Q (0.5852726591621421)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.39452942911846944 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47153689558381756) - present_state_Q ( 0.47153689558381756)) * f1( 0.6782757901181506)
w2 ( 0.8997705171161431 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.47153689558381756) - present_state_Q (0.47153689558381756)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.23657392263412694 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3583034946993171) - present_state_Q ( 0.3583034946993171)) * f1( 0.6801176875125572)
w2 ( 0.8765457856638492 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.3583034946993171) - present_state_Q (0.3583034946993171)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.08565634643018102 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24823300428384015) - present_state_Q ( 0.24823300428384015)) * f1( 0.6787663827420133)
w2 ( 0.8543116886252946 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.24823300428384015) - present_state_Q (0.24823300428384015)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.05955462110263213 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14384154603422317) - present_state_Q ( 0.14384154603422317)) * f1( 0.6819153466848408)
w2 ( 0.8330171147109866 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.14384154603422317) - present_state_Q (0.14384154603422317)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.16310377291849532 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05318831532600135) - present_state_Q ( 0.05318831532600135)) * f1( 0.5056433167999183)
w2 ( 0.8125384198730526 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05318831532600135) - present_state_Q (0.05318831532600135)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2537186725481468 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04808021417373956) - present_state_Q ( 0.007453293180086917)) * f1( 0.4524760371061267)
w2 ( 0.7925119671554255 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.04808021417373956) - present_state_Q (0.007453293180086917)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3337719396121515 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018199715748382042) - present_state_Q ( 0.018143931420927628)) * f1( 0.3970258185600064)
w2 ( 0.7622671077577341 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.018199715748382042) - present_state_Q (0.018143931420927628)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3694756696877138 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004726458323005389) - present_state_Q ( -0.005421669942956786)) * f1( 0.358813075316583)
w2 ( 0.7473413431193939 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.004726458323005389) - present_state_Q (-0.005421669942956786)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.35565381524785206 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0083969790316972) - present_state_Q ( 0.0083969790316972)) * f1( 0.2806794356008996)
w2 ( 0.754727983902466 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0083969790316972) - present_state_Q (0.0083969790316972)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.37720222422354704 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03570784216154595) - present_state_Q ( 0.039208681738449686)) * f1( 0.20806894984480884)
w2 ( 0.7391934154396316 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.03570784216154595) - present_state_Q (0.039208681738449686)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3620014184966761 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05507662163540338) - present_state_Q ( 0.05081982336294351)) * f1( 0.15922278580575777)
w2 ( 0.7535137330216405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05507662163540338) - present_state_Q (0.05081982336294351)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3404968048444057 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017031907790828055) - present_state_Q ( -0.0023035268988471636)) * f1( 0.21451545831918956)
w2 ( 0.7635384714814081 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0017031907790828055) - present_state_Q (-0.0023035268988471636)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3271834424098817 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029774090539711667) - present_state_Q ( 0.029774090539711667)) * f1( 0.13679939413738212)
w2 ( 0.7732705046665507 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.029774090539711667) - present_state_Q (0.029774090539711667)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.318326420607304 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012065224463127314) - present_state_Q ( 0.009444118730231897)) * f1( 0.08930588384265116)
w2 ( 0.7782293166851311 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012065224463127314) - present_state_Q (0.009444118730231897)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3530369857861861 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0023065383673796236) - present_state_Q ( -0.036604927466876935)) * f1( 0.2372294236748025)
w2 ( 0.7709134945916492 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.0023065383673796236) - present_state_Q (-0.036604927466876935)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.32838288120090675 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06689213268316829) - present_state_Q ( 0.062044263497140595)) * f1( 0.2609880526143856)
w2 ( 0.7898063935870727 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06689213268316829) - present_state_Q (0.062044263497140595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3133016940341806 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1365280563851911) - present_state_Q ( 0.1407187483881533)) * f1( 0.17276433473371394)
w2 ( 0.8116297450183318 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1365280563851911) - present_state_Q (0.1407187483881533)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.30256143909708777 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19794248272529782) - present_state_Q ( 0.16360603037433671)) * f1( 0.1254426855284049)
w2 ( 0.8330344504657867 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.19794248272529782) - present_state_Q (0.16360603037433671)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.2956813486578522 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2242560413283509) - present_state_Q ( 0.22384350898545025)) * f1( 0.0861538279037645)
w2 ( 0.8569919133202083 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2242560413283509) - present_state_Q (0.22384350898545025)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.28168518013964816 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08378940124383233) - present_state_Q ( 0.08378940124383233)) * f1( 0.15137710226691461)
w2 ( 0.8708607564034165 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08378940124383233) - present_state_Q (0.08378940124383233)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.26860818282096544 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0905258041128699) - present_state_Q ( 0.0905258041128699)) * f1( 0.14236925537850803)
w2 ( 0.8846386580478928 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0905258041128699) - present_state_Q (0.0905258041128699)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.25171101862993484 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08361513534656417) - present_state_Q ( 0.08361513534656417)) * f1( 0.18272214511548715)
w2 ( 0.8985098537207141 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08361513534656417) - present_state_Q (0.08361513534656417)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23093816716642682 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03582155299709126) - present_state_Q ( 0.03582155299709126)) * f1( 0.21464865808840158)
w2 ( 0.9081874597437403 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03582155299709126) - present_state_Q (0.03582155299709126)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.21409901954188024 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050094673884263294) - present_state_Q ( 0.050094673884263294)) * f1( 0.17634188661748026)
w2 ( 0.9177366076787818 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.050094673884263294) - present_state_Q (0.050094673884263294)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.19748318962889166 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05436904390459628) - present_state_Q ( 0.05436904390459628)) * f1( 0.17470709087467415)
w2 ( 0.9272472862836405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05436904390459628) - present_state_Q (0.05436904390459628)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.18030671542846488 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012069212984812443) - present_state_Q ( 0.012069212984812443)) * f1( 0.17365098970607531)
w2 ( 0.9321929748252088 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012069212984812443) - present_state_Q (0.012069212984812443)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.1650879050672553 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018699397259612595) - present_state_Q ( 0.018699397259612595)) * f1( 0.1547931890130903)
w2 ( 0.9371088275375405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.018699397259612595) - present_state_Q (0.018699397259612595)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.13155234770233779 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03646893604316679) - present_state_Q ( 0.03646893604316679)) * f1( 0.3467361626962764)
w2 ( 0.946780607113152 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03646893604316679) - present_state_Q (0.03646893604316679)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.10020816307174728 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05077260250038842) - present_state_Q ( 0.006064280588476335)) * f1( 0.3137515254427328)
w2 ( 0.9517756720114599 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05077260250038842) - present_state_Q (0.006064280588476335)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.07561759080902164 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06792415479724122) - present_state_Q ( 0.06890391034492273)) * f1( 0.2621907841720618)
w2 ( 0.9611545570628078 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06792415479724122) - present_state_Q (0.06890391034492273)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.05502131410056246 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08020430218239014) - present_state_Q ( 0.07934489626556003)) * f1( 0.22178119219741038)
w2 ( 0.9704413124023347 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08020430218239014) - present_state_Q (0.07934489626556003)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0401280420243792 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08748440193392619) - present_state_Q ( 0.04006269267089531)) * f1( 0.15374719938095685)
w2 ( 0.9752847411399471 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08748440193392619) - present_state_Q (0.04006269267089531)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.029734614229937166 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09300670306791439) - present_state_Q ( 0.09297693961430939)) * f1( 0.11342528242270379)
w2 ( 0.9844479784468719 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09300670306791439) - present_state_Q (0.09297693961430939)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.024341688936104264 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09668839053669422) - present_state_Q ( 0.09668839053669422)) * f1( 0.059069449982122275)
w2 ( 0.9935777829320417 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09668839053669422) - present_state_Q (0.09668839053669422)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.02211672369520397 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0491122489518458) - present_state_Q ( 0.0491122489518458)) * f1( 0.02327858992215736)
w2 ( 0.9983567778117584 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0491122489518458) - present_state_Q (0.0491122489518458)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11346439512721133 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09007064114807727) - present_state_Q ( 0.09012787418087526)) * f1( 0.4389349767210652)
w2 ( 0.9775455697110976 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09007064114807727) - present_state_Q (0.09012787418087526)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.21568692521089142 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28926165578352475) - present_state_Q ( 0.2908635912696099)) * f1( 0.45192465946506216)
w2 ( 0.8983777598119036 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.28926165578352475) - present_state_Q (0.2908635912696099)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.3088292923274112 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2740705245195208) - present_state_Q ( 0.2697595207353185)) * f1( 0.41537790527563645)
w2 ( 0.808683661080569 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2740705245195208) - present_state_Q (0.2697595207353185)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.38531113626504726 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21059956495229676) - present_state_Q ( 0.17329360504284913)) * f1( 0.35536032061039435)
w2 ( 0.7333554833814023 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.21059956495229676) - present_state_Q (0.17329360504284913)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.4577959189847589 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17089305978900823) - present_state_Q ( 0.16320377633204977)) * f1( 0.337748911910482)
w2 ( 0.6475109045672763 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.17089305978900823) - present_state_Q (0.16320377633204977)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.435482101068207 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13644208822716225) - present_state_Q ( 0.11318478454402843)) * f1( 0.24780481290899217)
w2 ( 0.6790269844170305 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.13644208822716225) - present_state_Q (0.11318478454402843)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.41636213446286774 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17350714366859665) - present_state_Q ( 0.17299794463407164)) * f1( 0.2264452405525971)
w2 ( 0.7128010952063419 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.17350714366859665) - present_state_Q (0.17299794463407164)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4038890643852816 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22034110200595444) - present_state_Q ( 0.22034110200595444)) * f1( 0.15558411948327527)
w2 ( 0.7448688155341275 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.22034110200595444) - present_state_Q (0.22034110200595444)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3939350605235648 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24214267308834547) - present_state_Q ( 0.24626891782232968)) * f1( 0.1279524823727923)
w2 ( 0.7759866295135878 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.24214267308834547) - present_state_Q (0.24626891782232968)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.38053653709667923 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013355265268259336) - present_state_Q ( -0.013355265268259336)) * f1( 0.1323938942490215)
w2 ( 0.7810467282072949 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.013355265268259336) - present_state_Q (-0.013355265268259336)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.37209515156889295 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006733913717047499) - present_state_Q ( 0.006733913717047499)) * f1( 0.08492856675443604)
w2 ( 0.7860164255955682 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.006733913717047499) - present_state_Q (0.006733913717047499)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3624199909036276 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010585022834878138) - present_state_Q ( 0.00322196236671847)) * f1( 0.09696137872514038)
w2 ( 0.791005608295152 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.010585022834878138) - present_state_Q (0.00322196236671847)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.40204751738678507 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017670843849593623) - present_state_Q ( -0.017670843849593623)) * f1( 0.2670145331603456)
w2 ( 0.7761646458897983 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.017670843849593623) - present_state_Q (-0.017670843849593623)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.39092294915997194 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05256220429585326) - present_state_Q ( 0.055735692469236)) * f1( 0.2474763116445351)
w2 ( 0.7851550564490053 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.05256220429585326) - present_state_Q (0.055735692469236)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37336576046006614 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07912318136144493) - present_state_Q ( 0.08283696871960174)) * f1( 0.18979198517157903)
w2 ( 0.8036565634373362 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07912318136144493) - present_state_Q (0.08283696871960174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3609008798008049 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1091232401391489) - present_state_Q ( 0.1091232401391489)) * f1( 0.1382239027079671)
w2 ( 0.8216923451148315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1091232401391489) - present_state_Q (0.1091232401391489)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.34894357141615806 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11614686447632255) - present_state_Q ( 0.11614686447632255)) * f1( 0.13353141331559668)
w2 ( 0.8396017015542576 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.11614686447632255) - present_state_Q (0.11614686447632255)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.34086019176386056 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13578694018858545) - present_state_Q ( 0.13578694018858545)) * f1( 0.09208766905163314)
w2 ( 0.857157536630863 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.13578694018858545) - present_state_Q (0.13578694018858545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.33231358597510235 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1795427518994555) - present_state_Q ( 0.1795427518994555)) * f1( 0.1019380763663122)
w2 ( 0.8781178247131253 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1795427518994555) - present_state_Q (0.1795427518994555)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.31379308701457265 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024857259028484846) - present_state_Q ( 0.024857259028484846)) * f1( 0.18944312270020877)
w2 ( 0.8878941093818689 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.024857259028484846) - present_state_Q (0.024857259028484846)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.2802390159858719 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023620291181862166) - present_state_Q ( -0.014731416703166383)) * f1( 0.3299015559146009)
w2 ( 0.8980650438400823 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023620291181862166) - present_state_Q (-0.014731416703166383)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.25227812458060744 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052468852365730775) - present_state_Q ( 0.052468852365730775)) * f1( 0.2934670032327965)
w2 ( 0.912356714333145 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.052468852365730775) - present_state_Q (0.052468852365730775)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23281214316616466 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08207765271816655) - present_state_Q ( 0.08373315909963341)) * f1( 0.2105626404930937)
w2 ( 0.9262238334257278 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08207765271816655) - present_state_Q (0.08373315909963341)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.21855539462725013 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09959263393965173) - present_state_Q ( 0.10236286804610373)) * f1( 0.15708247203262898)
w2 ( 0.9398377793559457 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09959263393965173) - present_state_Q (0.10236286804610373)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2071141008953357 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1131353506930687) - present_state_Q ( 0.1131353506930687)) * f1( 0.1273833403096973)
w2 ( 0.9533104521215893 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1131353506930687) - present_state_Q (0.1131353506930687)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.18537861070792205 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002544912331481193) - present_state_Q ( 0.002544912331481193)) * f1( 0.21785387899494008)
w2 ( 0.9582990000160976 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.002544912331481193) - present_state_Q (0.002544912331481193)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.1643503717079205 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012345399595624161) - present_state_Q ( 0.008642174323583482)) * f1( 0.21185171000714106)
w2 ( 0.9632619618442775 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.012345399595624161) - present_state_Q (0.008642174323583482)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.14723333011143838 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02125395087970922) - present_state_Q ( 0.019532797440082572)) * f1( 0.17420283480108206)
w2 ( 0.9681749248325169 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.02125395087970922) - present_state_Q (0.019532797440082572)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.1344040847901155 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029013368408632334) - present_state_Q ( 0.029013368408632334)) * f1( 0.13173224988060436)
w2 ( 0.973044364674678 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.029013368408632334) - present_state_Q (0.029013368408632334)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.12321950383877106 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03315729175021384) - present_state_Q ( 0.03315729175021384)) * f1( 0.11528612770747879)
w2 ( 0.9778951568618021 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03315729175021384) - present_state_Q (0.03315729175021384)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.112909647481441 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03576844701507835) - present_state_Q ( 0.03576844701507835)) * f1( 0.10652786627989619)
w2 ( 0.9827341988502342 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03576844701507835) - present_state_Q (0.03576844701507835)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11045665371918022 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002762796867830775) - present_state_Q ( -0.002762796867830775)) * f1( 0.024469094797987898)
w2 ( 0.9827341988502342 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.002762796867830775) - present_state_Q (-0.002762796867830775)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09946393693481229 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08389083579814295) - present_state_Q ( 0.08512209180654566)) * f1( 0.11906324911773153)
w2 ( 0.9919668687679668 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08389083579814295) - present_state_Q (0.08512209180654566)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.08862136254312158 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3320560699787723) - present_state_Q ( 0.3318126419880967)) * f1( 0.15458630086972008)
w2 ( 1.0165156225433092 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3320560699787723) - present_state_Q (0.3318126419880967)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.08036751660142324 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3451694612552108) - present_state_Q ( 0.3451694612552108)) * f1( 0.11973418519473045)
w2 ( 1.04064278451377 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3451694612552108) - present_state_Q (0.3451694612552108)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.05242078393733893 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028970749039497708) - present_state_Q ( 0.028970749039497708)) * f1( 0.2869491451448233)
w2 ( 1.0455124161430922 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.028970749039497708) - present_state_Q (0.028970749039497708)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.03515709317179636 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04286274319098415) - present_state_Q ( 0.04286274319098415)) * f1( 0.17956384680210305)
w2 ( 1.0503195337987328 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04286274319098415) - present_state_Q (0.04286274319098415)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.017947378875427484 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.046203031930703416) - present_state_Q ( 0.046203031930703416)) * f1( 0.17956389990448865)
w2 ( 1.0551116201550446 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.046203031930703416) - present_state_Q (0.046203031930703416)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.0031008538639416706 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10278653218891594) - present_state_Q ( 0.10257566205616943)) * f1( 0.16356148603705883)
w2 ( 1.0641886500666717 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10278653218891594) - present_state_Q (0.10257566205616943)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.007460901938847908 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10605680097622032) - present_state_Q ( 0.10605680097622032)) * f1( 0.1167626874188195)
w2 ( 1.0732341388578857 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10605680097622032) - present_state_Q (0.10605680097622032)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.02520594969524165 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10879105400146631) - present_state_Q ( 0.10879105400146631)) * f1( 0.1967108169638228)
w2 ( 1.0822550193718725 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10879105400146631) - present_state_Q (0.10879105400146631)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.006545307154754796 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12136218080334354) - present_state_Q ( 0.12136218080334354)) * f1( 0.5211737317969897)
w2 ( 1.0761627597446424 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12136218080334354) - present_state_Q (0.12136218080334354)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.09725837242484703 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1047812284899677) - present_state_Q ( 0.1047812284899677)) * f1( 0.43314200807780917)
w2 ( 1.0552197286882325 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1047812284899677) - present_state_Q (0.1047812284899677)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0996298320978842 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06762729584235366) - present_state_Q ( 0.06762729584235366)) * f1( 0.3896289448576919)
w2 ( 1.0546110830256514 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.06762729584235366) - present_state_Q (0.06762729584235366)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.0675570522878623 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07271364813197223) - present_state_Q ( 0.07132037134429964)) * f1( 0.3426758455712639)
w2 ( 1.0639705929603405 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07271364813197223) - present_state_Q (0.07132037134429964)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.04342756188277719 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08868197932619344) - present_state_Q ( 0.08868197932619344)) * f1( 0.26222399246130823)
w2 ( 1.0731724551464048 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.08868197932619344) - present_state_Q (0.08868197932619344)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.024609829417663126 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09835155223782054) - present_state_Q ( 0.09835155223782054)) * f1( 0.20645168386428875)
w2 ( 1.0822872911762644 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09835155223782054) - present_state_Q (0.09835155223782054)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.011740446916466507 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10456775371149733) - present_state_Q ( 0.10473193523464785)) * f1( 0.14208931820018397)
w2 ( 1.0913445395776296 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10456775371149733) - present_state_Q (0.10473193523464785)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.0014893705014063108 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052936284873477235) - present_state_Q ( 0.052936284873477235)) * f1( 0.1389165265179789)
w2 ( 1.096106326295699 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.052936284873477235) - present_state_Q (0.052936284873477235)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.0237469883295433 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05515412933986352) - present_state_Q ( 0.05515412933986352)) * f1( 0.23420164744045593)
w2 ( 1.1008581327136695 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05515412933986352) - present_state_Q (0.05515412933986352)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.05356617154826859 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06254645276904702) - present_state_Q ( 0.06254645276904702)) * f1( 0.31597885294904876)
w2 ( 1.1055766736762087 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06254645276904702) - present_state_Q (0.06254645276904702)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.0889166395757322 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07559713047031064) - present_state_Q ( 0.07559713047031064)) * f1( 0.37931209566080965)
w2 ( 1.1102364865890924 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07559713047031064) - present_state_Q (0.07559713047031064)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.10643823935570756 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09293019297141084) - present_state_Q ( 0.09293019297141084)) * f1( 0.4208252675820728)
w2 ( 1.112318300720721 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.09293019297141084) - present_state_Q (0.09293019297141084)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.1251568109675934 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10471899704932566) - present_state_Q ( 0.10471899704932566)) * f1( 0.4613293334286682)
w2 ( 1.1143470652339988 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.10471899704932566) - present_state_Q (0.10471899704932566)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.0933251591921474 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12113309386011042) - present_state_Q ( 0.12113309386011042)) * f1( 0.522670241377023)
w2 ( 1.1113019663116284 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.12113309386011042) - present_state_Q (0.12113309386011042)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.003955323460094293 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10779048600731966) - present_state_Q ( 0.10779048600731966)) * f1( 0.5596067356736167)
w2 ( 1.1033169091245953 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.10779048600731966) - present_state_Q (0.10779048600731966)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11486268034857006 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057456439427759236) - present_state_Q ( 0.057456439427759236)) * f1( 0.5791167257594809)
w2 ( 1.0930583551471704 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.057456439427759236) - present_state_Q (0.057456439427759236)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.18834393769838736 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012691293160100169) - present_state_Q ( 0.012691293160100169)) * f1( 0.36531991478797793)
w2 ( 1.08300124432795 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.012691293160100169) - present_state_Q (0.012691293160100169)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.24919432345387876 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0032375520259167664) - present_state_Q ( -0.0032375520259167664)) * f1( 0.3046958396623011)
w2 ( 1.0730158133120666 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.0032375520259167664) - present_state_Q (-0.0032375520259167664)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.15033058663019058 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18125107897917525) - present_state_Q ( 0.17904387441336495)) * f1( 0.603310647709962)
w2 ( 0.966211229687348 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.18125107897917525) - present_state_Q (0.17904387441336495)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.1353060026315654 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.256490051733661) - present_state_Q ( 0.2547864521007204)) * f1( 0.5546940257406334)
w2 ( 0.9756914190448905 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.256490051733661) - present_state_Q (0.2547864521007204)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.09654014723227458 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2732650254031948) - present_state_Q ( 0.22715575503534466)) * f1( 0.48446978998129137)
w2 ( 0.9996965414700398 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2732650254031948) - present_state_Q (0.22715575503534466)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.06456827815312619 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3072316584902006) - present_state_Q ( 0.3072316584902006)) * f1( 0.44191077233048676)
w2 ( 1.0250187442275984 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3072316584902006) - present_state_Q (0.3072316584902006)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.03839837663222413 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3842032368479849) - present_state_Q ( 0.38417990536671454)) * f1( 0.4000043529591069)
w2 ( 1.0511883609603219 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3842032368479849) - present_state_Q (0.38417990536671454)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.014956212115648026 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.40705475057002044) - present_state_Q ( 0.4062869031554299)) * f1( 0.36950627795008945)
w2 ( 1.0765651038363848 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.40705475057002044) - present_state_Q (0.4062869031554299)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0031984079670310493 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42607986735683706) - present_state_Q ( 0.42622094650314135)) * f1( 0.29453279997304843)
w2 ( 1.1012205854456865 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.42607986735683706) - present_state_Q (0.42622094650314135)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0272738678691454 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44176648523467227) - present_state_Q ( 0.44176648523467227)) * f1( 0.3996522862542178)
w2 ( 1.1253169919772383 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.44176648523467227) - present_state_Q (0.44176648523467227)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0565356808093786 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.46205435255565935) - present_state_Q ( 0.40633348676222014)) * f1( 0.4573073254599393)
w2 ( 1.1477125101745054 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.46205435255565935) - present_state_Q (0.40633348676222014)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.08790712567135239 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4866418425549949) - present_state_Q ( 0.4303855721054042)) * f1( 0.5073998072305623)
w2 ( 1.1693522615997587 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4866418425549949) - present_state_Q (0.4303855721054042)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( 0.11784198798751205 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5814067666438669) - present_state_Q ( 0.5814067666438669)) * f1( 0.6279155245086551)
w2 ( 1.190805287550682 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5814067666438669) - present_state_Q (0.5814067666438669)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.05432830761426208 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6153089463859922) - present_state_Q ( 0.6153089463859922)) * f1( 0.67417877400884)
w2 ( 1.0758852752220494 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6153089463859922) - present_state_Q (0.6153089463859922)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.026423484219906687 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4583433509605738) - present_state_Q ( 0.4583433509605738)) * f1( 0.4749830065123205)
w2 ( 1.102322369508146 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4583433509605738) - present_state_Q (0.4583433509605738)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.0043281793519649135 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.48538581940990216) - present_state_Q ( 0.4856725285148704)) * f1( 0.392549963414021)
w2 ( 1.1276513419123215 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.48538581940990216) - present_state_Q (0.4856725285148704)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.01718325943637685 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5058537334613544) - present_state_Q ( 0.4495115528959419)) * f1( 0.3578834754810995)
w2 ( 1.1516942947303292 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5058537334613544) - present_state_Q (0.4495115528959419)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04115032574878218 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5260841484480011) - present_state_Q ( 0.5260841484480011)) * f1( 0.4551939548089752)
w2 ( 1.1753878867181853 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5260841484480011) - present_state_Q (0.5260841484480011)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.06417182440395922 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6086603783187166) - present_state_Q ( 0.6086424798092502)) * f1( 0.5090734051060966)
w2 ( 1.1979990646193164 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6086603783187166) - present_state_Q (0.6086424798092502)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( 0.09196687185828359 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6342847980199973) - present_state_Q ( 0.5756679080462898)) * f1( 0.5698502311139089)
w2 ( 1.2199482903483234 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6342847980199973) - present_state_Q (0.5756679080462898)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( 0.08601877317409606 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6662922102567991) - present_state_Q ( 0.6653729222702521)) * f1( 0.602377529828971)
w2 ( 1.2150111052860948 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6662922102567991) - present_state_Q (0.6653729222702521)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.084143131336924 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6634562319781447) - present_state_Q ( 0.6044250533013369)) * f1( 0.6704356943788774)
w2 ( 1.1007975309314362 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6634562319781447) - present_state_Q (0.6044250533013369)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.22666789344336236 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5014764015396871) - present_state_Q ( 0.5014764015396871)) * f1( 0.5814183896976355)
w2 ( 0.9782310928621503 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5014764015396871) - present_state_Q (0.5014764015396871)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.34693563292849 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4165218392867717) - present_state_Q ( 0.3721407834392545)) * f1( 0.5160623377878137)
w2 ( 0.8617066628866215 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.4165218392867717) - present_state_Q (0.3721407834392545)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.3841909933046 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3076173522260123) - present_state_Q ( 0.30754449818241525)) * f1( 0.47961105926390557)
w2 ( 0.8189836109238317 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.3076173522260123) - present_state_Q (0.30754449818241525)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.3522372561979783 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28545862306868475) - present_state_Q ( 0.2852746530922976)) * f1( 0.42990683226366055)
w2 ( 0.8598635274306331 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.28545862306868475) - present_state_Q (0.2852746530922976)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.32544452309230704 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3370601947631088) - present_state_Q ( 0.3011103753904808)) * f1( 0.3657233471419913)
w2 ( 0.8964933096349246 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3370601947631088) - present_state_Q (0.3011103753904808)) * f2(0.5)
============================================================================
GUIDE learning . . .
w1 ( -0.3041826996074795 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38698475546291555) - present_state_Q ( 0.3869090567709938)) * f1( 0.32620694464138694)
w2 ( 0.932341727667566 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.38698475546291555) - present_state_Q (0.3869090567709938)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.2868715199059174 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43097246234000514) - present_state_Q ( 0.4272795104752696)) * f1( 0.2811088199698165)
w2 ( 0.9662117031342962 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.43097246234000514) - present_state_Q (0.4272795104752696)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.27205004621597684 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4617355398458266) - present_state_Q ( 0.45900368362902005)) * f1( 0.25242224504750926)
w2 ( 0.9985060460038521 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4617355398458266) - present_state_Q (0.45900368362902005)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.2609171817177372 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4973287365207921) - present_state_Q ( 0.49461846758355604)) * f1( 0.20055081216655363)
w2 ( 1.0290373383376208 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4973287365207921) - present_state_Q (0.49461846758355604)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.25354977351856106 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5265954282499442) - present_state_Q ( 0.5292447247856976)) * f1( 0.14075658436217608)
w2 ( 1.0578251533297822 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5265954282499442) - present_state_Q (0.5292447247856976)) * f2(0.55)
============================================================================
GUIDE learning . . .
w1 ( -0.24684248405280682 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.299288501872338) - present_state_Q ( 0.2942316272230893)) * f1( 0.09116915568513895)
w2 ( 1.0798960700187066 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.299288501872338) - present_state_Q (0.2942316272230893)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.2216978430517103 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0583430652987688) - present_state_Q ( -0.0583430652987688)) * f1( 0.4550994097745013)
w2 ( 1.0826586138125511 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.0583430652987688) - present_state_Q (-0.0583430652987688)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.18004248817255555 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035367254362729784) - present_state_Q ( -0.035367254362729784)) * f1( 0.4037034543113787)
w2 ( 1.0878177664571833 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.035367254362729784) - present_state_Q (-0.035367254362729784)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.14462291853531697 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0458383504874446) - present_state_Q ( -0.00855253783541457)) * f1( 0.3496031786560724)
w2 ( 1.0928834483216041 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0458383504874446) - present_state_Q (-0.00855253783541457)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.11553132503542224 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06490009853476925) - present_state_Q ( 0.06461864457481115)) * f1( 0.3088701342065705)
w2 ( 1.1023021619743907 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06490009853476925) - present_state_Q (0.06461864457481115)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.09254809054278834 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13511651911063297) - present_state_Q ( 0.13511651911063297)) * f1( 0.2616502942059863)
w2 ( 1.115478088966397 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.13511651911063297) - present_state_Q (0.13511651911063297)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07583695133887913 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2041485653327174) - present_state_Q ( 0.2041485653327174)) * f1( 0.204726562692313)
w2 ( 1.1318034147904081 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2041485653327174) - present_state_Q (0.2041485653327174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06390162166367075 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21431933521026997) - present_state_Q ( 0.21513480867550042)) * f1( 0.14802644468681445)
w2 ( 1.1479293572873186 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.21431933521026997) - present_state_Q (0.21513480867550042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05680525181752139 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28007114868680655) - present_state_Q ( 0.2239461308148095)) * f1( 0.08825661220833352)
w2 ( 1.164010576968396 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.28007114868680655) - present_state_Q (0.2239461308148095)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0386081550243214 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04740296945113372) - present_state_Q ( 0.04740296945113372)) * f1( 0.19008030158851635)
w2 ( 1.1687972636058659 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.04740296945113372) - present_state_Q (0.04740296945113372)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.02429737709952009 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05263995984300844) - present_state_Q ( 0.05263995984300844)) * f1( 0.15022482513425417)
w2 ( 1.1735603837865725 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05263995984300844) - present_state_Q (0.05263995984300844)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.005990740103305737 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05400275723125524) - present_state_Q ( 0.05400275723125524)) * f1( 0.19241838075459286)
w2 ( 1.1783173713790318 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.05400275723125524) - present_state_Q (0.05400275723125524)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.007547667704028522 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.058060100353465136) - present_state_Q ( 0.058060100353465136)) * f1( 0.1428484963008549)
w2 ( 1.1830561009274412 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.058060100353465136) - present_state_Q (0.058060100353465136)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.029549098466033555 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.060909711318907156) - present_state_Q ( 0.060909711318907156)) * f1( 0.23277472477986189)
w2 ( 1.1877820072265062 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.060909711318907156) - present_state_Q (0.060909711318907156)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.06089406362644912 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06952861369028022) - present_state_Q ( 0.06926677127620282)) * f1( 0.3342799417800109)
w2 ( 1.1924704376769704 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.06952861369028022) - present_state_Q (0.06926677127620282)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( 0.09424583124360103 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14375931669259576) - present_state_Q ( 0.1425418978400166)) * f1( 0.38254720879231213)
w2 ( 1.2011887780152628 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14375931669259576) - present_state_Q (0.1425418978400166)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.12277944278389796 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16251602861091416) - present_state_Q ( 0.15120595325496303)) * f1( 0.3298509339164796)
w2 ( 1.209839234511324 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.16251602861091416) - present_state_Q (0.15120595325496303)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.1656606846635463 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.184088393824553) - present_state_Q ( 0.184088393824553)) * f1( 0.5139660919009849)
w2 ( 1.2181824389669031 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.184088393824553) - present_state_Q (0.184088393824553)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.21326467649796452 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21847106887847567) - present_state_Q ( 0.2201909688866086)) * f1( 0.5938205868804142)
w2 ( 1.2261990003469156 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.21847106887847567) - present_state_Q (0.2201909688866086)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.2629762249926974 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2592146657083734) - present_state_Q ( 0.26126707797649523)) * f1( 0.6501178733325159)
w2 ( 1.233845544232859 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.2592146657083734) - present_state_Q (0.26126707797649523)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.3124598115290836 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3021098099853737) - present_state_Q ( 0.3021098099853737)) * f1( 0.6796251469768828)
w2 ( 1.2411265559429907 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3021098099853737) - present_state_Q (0.3021098099853737)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.35985648647416335 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3365587491633768) - present_state_Q ( 0.3365587491633768)) * f1( 0.6799149385946018)
w2 ( 1.2480975272005204 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3365587491633768) - present_state_Q (0.3365587491633768)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.405311411801603 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3700685739095104) - present_state_Q ( 0.3700685739095104)) * f1( 0.6815462007993213)
w2 ( 1.2547669100353347 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.3700685739095104) - present_state_Q (0.3700685739095104)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.44874170906174526 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4008492176753828) - present_state_Q ( 0.4008492176753828)) * f1( 0.6794097542129954)
w2 ( 1.2611592670762561 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4008492176753828) - present_state_Q (0.4008492176753828)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.4902938093619145 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4305329054183778) - present_state_Q ( 0.4305329054183778)) * f1( 0.6783790598543749)
w2 ( 1.2672844709274906 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4305329054183778) - present_state_Q (0.4305329054183778)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.530168141783263 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4607156451897517) - present_state_Q ( 0.4607156451897517)) * f1( 0.6811980729099257)
w2 ( 1.2731380301207829 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.4607156451897517) - present_state_Q (0.4607156451897517)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.5683455444595696 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.48855841464394906) - present_state_Q ( 0.48855841464394906)) * f1( 0.6813774407809484)
w2 ( 1.2787410043889873 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.48855841464394906) - present_state_Q (0.48855841464394906)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.6048850868692236 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5148821216029891) - present_state_Q ( 0.5148821216029891)) * f1( 0.6809378993761442)
w2 ( 1.2841070652945603 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5148821216029891) - present_state_Q (0.5148821216029891)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.6397548340167662 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.540267417970296) - present_state_Q ( 0.5340403990599025)) * f1( 0.6705896728747485)
w2 ( 1.2893069287219316 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.540267417970296) - present_state_Q (0.5340403990599025)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.6731897395801931 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5648231733319724) - present_state_Q ( 0.5582597430038013)) * f1( 0.6710837141097031)
w2 ( 1.2942891544652255 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5648231733319724) - present_state_Q (0.5582597430038013)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7052244104297342 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5824297049209678) - present_state_Q ( 0.5893896023217917)) * f1( 0.6832556407679425)
w2 ( 1.2989776881469286 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.5824297049209678) - present_state_Q (0.5893896023217917)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7017327457589309 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6042546945439483) - present_state_Q ( 0.6115496760053107)) * f1( 0.6829767944321714)
w2 ( 1.2984664460814195 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.6042546945439483) - present_state_Q (0.6115496760053107)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7325864885356955 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6081332480428017) - present_state_Q ( 0.6081332480428017)) * f1( 0.6815794279592697)
w2 ( 1.3029932468490342 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6081332480428017) - present_state_Q (0.6081332480428017)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7620657843309995 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6239623011166842) - present_state_Q ( 0.6315588887790706)) * f1( 0.6842326086249448)
w2 ( 1.3073016202623602 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6239623011166842) - present_state_Q (0.6315588887790706)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7903418650188768 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6502784885713029) - present_state_Q ( 0.6502784885713029)) * f1( 0.681763093459401)
w2 ( 1.3114491138652185 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6502784885713029) - present_state_Q (0.6502784885713029)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8174176060461656 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6692502829791713) - present_state_Q ( 0.6692502829791713)) * f1( 0.6808514079913977)
w2 ( 1.315425861318406 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6692502829791713) - present_state_Q (0.6692502829791713)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8433349291479248 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6883840327971562) - present_state_Q ( 0.6883840327971562)) * f1( 0.6812202753482981)
w2 ( 1.3192304050232315 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.6883840327971562) - present_state_Q (0.6883840327971562)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8681411831270929 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7067115138463846) - present_state_Q ( 0.7067115138463846)) * f1( 0.6815660699893066)
w2 ( 1.322870001398614 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.7067115138463846) - present_state_Q (0.7067115138463846)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8918811347018852 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7243064388014477) - present_state_Q ( 0.7243064388014477)) * f1( 0.681939124842689)
w2 ( 1.3263512434494011 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.7243064388014477) - present_state_Q (0.7243064388014477)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.8466525804119723 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.739075699624707) - present_state_Q ( 0.739075699624707)) * f1( 0.679956724819022)
w2 ( 1.3196995621527787 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.739075699624707) - present_state_Q (0.739075699624707)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.7372126444302207 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7092376546546788) - present_state_Q ( 0.7005514039648569)) * f1( 0.6715640640614516)
w2 ( 1.3034032857677849 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( 0.7092376546546788) - present_state_Q (0.7005514039648569)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.561657394085316 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6261836518616716) - present_state_Q ( 0.6337119359764827)) * f1( 0.6828038167857955)
w2 ( 1.2776923500598818 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.6261836518616716) - present_state_Q (0.6337119359764827)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.3950511049157659 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5085191326171681) - present_state_Q ( 0.5085191326171681)) * f1( 0.6779041843315318)
w2 ( 1.2531156778663273 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5085191326171681) - present_state_Q (0.5085191326171681)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.23431702823189673 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39080742154915615) - present_state_Q ( 0.3948543398649905)) * f1( 0.6822984892950257)
w2 ( 1.2295579418892266 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.39080742154915615) - present_state_Q (0.3948543398649905)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( 0.08071172131097373 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2826129570675525) - present_state_Q ( 0.2826129570675525)) * f1( 0.6813724298373307)
w2 ( 1.2070144252756188 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2826129570675525) - present_state_Q (0.2826129570675525)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.06629069903602977 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17567922957091148) - present_state_Q ( 0.17567922957091148)) * f1( 0.6811623658913928)
w2 ( 1.1854333122094804 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.17567922957091148) - present_state_Q (0.17567922957091148)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.18782727540997435 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07965357680766046) - present_state_Q ( 0.07965357680766046)) * f1( 0.5866547642249264)
w2 ( 1.1647164300182116 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07965357680766046) - present_state_Q (0.07965357680766046)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.29015064102858934 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01946535342374843) - present_state_Q ( 0.021296880374812158)) * f1( 0.5067142800174743)
w2 ( 1.1445229265678871 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.01946535342374843) - present_state_Q (0.021296880374812158)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.37887549657559866 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017994750164149337) - present_state_Q ( -0.01512922017557708)) * f1( 0.44660081526270956)
w2 ( 1.1246562240194788 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.017994750164149337) - present_state_Q (-0.01512922017557708)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4563643452307167 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040550057941609666) - present_state_Q ( -0.03676849170948328)) * f1( 0.3938869508856027)
w2 ( 1.104983358878632 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.040550057941609666) - present_state_Q (-0.03676849170948328)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5234619153438603 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05054546715875077) - present_state_Q ( -0.045789551265547734)) * f1( 0.34246296580071134)
w2 ( 1.0853907089241286 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.05054546715875077) - present_state_Q (-0.045789551265547734)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.58047510859739 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04888620924162097) - present_state_Q ( -0.04362969668607651)) * f1( 0.29069692200726815)
w2 ( 1.0657781196817477 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.04888620924162097) - present_state_Q (-0.04362969668607651)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6310272409699117 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03546094090550986) - present_state_Q ( -0.04310401125799751)) * f1( 0.25786088155933257)
w2 ( 1.0461736988534223 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03546094090550986) - present_state_Q (-0.04310401125799751)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.6665940183144874 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008006428478300368) - present_state_Q ( -0.008006428478300368)) * f1( 0.1784769199353989)
w2 ( 1.026245756709727 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.008006428478300368) - present_state_Q (-0.008006428478300368)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.25442637235346977 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020379686713496027) - present_state_Q ( 0.017690808187515138)) * f1( 0.14707352931051249)
w2 ( 1.0773178058211257 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.020379686713496027) - present_state_Q (0.017690808187515138)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3092927553212446 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039142648802489646) - present_state_Q ( 0.039142648802489646)) * f1( 0.26958342071683256)
w2 ( 1.0569655219819032 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.039142648802489646) - present_state_Q (0.039142648802489646)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.3371099934464778 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15954752914970313) - present_state_Q ( 0.15902988586395284)) * f1( 0.16929985468958414)
w2 ( 1.0241040193229236 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( 0.15954752914970313) - present_state_Q (0.15902988586395284)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3054652155088096 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08904526049992646) - present_state_Q ( 0.09253554690992172)) * f1( 0.2813489090396091)
w2 ( 1.0486416047058076 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.08904526049992646) - present_state_Q (0.09253554690992172)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4158444555903989 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1528935083298602) - present_state_Q ( 0.10046142809456976)) * f1( 0.5293531468469893)
w2 ( 0.9965123027742679 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1528935083298602) - present_state_Q (0.10046142809456976)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.5158785415048754 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.154895187509271) - present_state_Q ( 0.1542836333195946)) * f1( 0.46771255462637407)
w2 ( 0.9216545087643646 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.154895187509271) - present_state_Q (0.1542836333195946)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.5982693757861013 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12103812348220302) - present_state_Q ( 0.12103812348220302)) * f1( 0.3906752042785248)
w2 ( 0.8478418078746752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.12103812348220302) - present_state_Q (0.12103812348220302)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6691978193768098 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07662075204266916) - present_state_Q ( 0.04619109200934979)) * f1( 0.3479393376596241)
w2 ( 0.7866859373705227 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07662075204266916) - present_state_Q (0.04619109200934979)) * f2(0.30000000000000004)
============================================================================
GUIDE learning . . .
w1 ( -0.686190357382889 ) += alpha ( 0.1 ) * (reward ( -0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06839729781383574) - present_state_Q ( 0.0753031025352853)) * f1( 0.2989205430027881)
w2 ( 0.7667897193241362 ) += alpha ( 0.1) * (reward ( -0.5) + discount_factor ( 0.1) * next_state_max_Q( 0.06839729781383574) - present_state_Q (0.0753031025352853)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6625533736517799 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09160766456021308) - present_state_Q ( 0.09160766456021308)) * f1( 0.25760889132488785)
w2 ( 0.7989040778904895 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.09160766456021308) - present_state_Q (0.09160766456021308)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6450898731890207 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14635832642908786) - present_state_Q ( 0.14635832642908786)) * f1( 0.2011280994587771)
w2 ( 0.8292937906079731 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.14635832642908786) - present_state_Q (0.14635832642908786)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6324018828959925 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19821270597507204) - present_state_Q ( 0.1914462063495418)) * f1( 0.15316721664656646)
w2 ( 0.8582869178566519 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.19821270597507204) - present_state_Q (0.1914462063495418)) * f2(0.35000000000000003)
============================================================================
GUIDE learning . . .
w1 ( -0.6077119734485709 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10029766013986369) - present_state_Q ( -0.10029766013986369)) * f1( 0.2264572733036115)
w2 ( 0.8637382573272813 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10029766013986369) - present_state_Q (-0.10029766013986369)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5484498487762718 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2442227734200389) - present_state_Q ( -0.2505395090723667)) * f1( 0.48333163533363904)
w2 ( 0.8698688434859331 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.2442227734200389) - present_state_Q (-0.2505395090723667)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.49865219677935607 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18977864886652504) - present_state_Q ( -0.18977864886652504)) * f1( 0.4253298484105882)
w2 ( 0.8757228474058324 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.18977864886652504) - present_state_Q (-0.18977864886652504)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4581409116779815 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1412409105457028) - present_state_Q ( -0.13624016489886484)) * f1( 0.361025798004886)
w2 ( 0.8813334277750539 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.1412409105457028) - present_state_Q (-0.13624016489886484)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4240519051371493 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10406134167857364) - present_state_Q ( -0.09935241875275011)) * f1( 0.3130458042182213)
w2 ( 0.8867781591979783 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.10406134167857364) - present_state_Q (-0.09935241875275011)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.39425637660226526 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07354648781671562) - present_state_Q ( -0.07410367193096291)) * f1( 0.279311514595258)
w2 ( 0.8921119043137248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.07354648781671562) - present_state_Q (-0.07410367193096291)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3703510442174115 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042776048089180146) - present_state_Q ( -0.04587848471672902)) * f1( 0.22950568539237007)
w2 ( 0.8973199087132638 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.042776048089180146) - present_state_Q (-0.04587848471672902)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3325833053656997 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08506072487058083) - present_state_Q ( -0.08506072487058083)) * f1( 0.3508204508530334)
w2 ( 0.9027026819751814 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.08506072487058083) - present_state_Q (-0.08506072487058083)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.2993213903437489 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052428672453885924) - present_state_Q ( -0.05976850455541538)) * f1( 0.31542063886467553)
w2 ( 0.9079753101617316 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.052428672453885924) - present_state_Q (-0.05976850455541538)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.27163386679325524 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0306481671897441) - present_state_Q ( -0.034917613622527655)) * f1( 0.26832823086374386)
w2 ( 0.9131345741462493 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0306481671897441) - present_state_Q (-0.034917613622527655)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.24829768774192634 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011328964617565537) - present_state_Q ( -0.016757072587502288)) * f1( 0.22977179551148852)
w2 ( 0.9182126950268781 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.011328964617565537) - present_state_Q (-0.016757072587502288)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.23426607843794275 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010730718104932778) - present_state_Q ( 0.010730718104932778)) * f1( 0.14168443116142165)
w2 ( 0.9231644067954059 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.010730718104932778) - present_state_Q (0.010730718104932778)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.22362706989785025 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023287402124422155) - present_state_Q ( 0.020766466044129166)) * f1( 0.10838852327639668)
w2 ( 0.9280722181662475 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.023287402124422155) - present_state_Q (0.020766466044129166)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.21535093107832817 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03182713554540041) - present_state_Q ( 0.027435896719092596)) * f1( 0.08481850698076926)
w2 ( 0.9329509522504248 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.03182713554540041) - present_state_Q (0.027435896719092596)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.2737576926783171 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08813708928347272) - present_state_Q ( -0.09028887510154646)) * f1( 0.6358756938193163)
w2 ( 0.9283583280812908 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.08813708928347272) - present_state_Q (-0.09028887510154646)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.3532343602553513 ) += alpha ( 0.1 ) * (reward ( -1.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05738547397775347) - present_state_Q ( -0.05738547397775347)) * f1( 0.5487382119427864)
w2 ( 0.9138747973470907 ) += alpha ( 0.1) * (reward ( -1.5) + discount_factor ( 0.1) * next_state_max_Q( -0.05738547397775347) - present_state_Q (-0.05738547397775347)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.4476120740746383 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08165859136516454) - present_state_Q ( -0.08165859136516454)) * f1( 0.48989025579159257)
w2 ( 0.8946097246693772 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08165859136516454) - present_state_Q (-0.08165859136516454)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5429036799038789 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08403579480996123) - present_state_Q ( -0.08790547093065537)) * f1( 0.4961817218407469)
w2 ( 0.865802253041122 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.08403579480996123) - present_state_Q (-0.08790547093065537)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6221503813262014 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09898147217932657) - present_state_Q ( -0.09478078039128007)) * f1( 0.41379553438875727)
w2 ( 0.8370754925387223 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09898147217932657) - present_state_Q (-0.09478078039128007)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6927799852713785 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061537384914196164) - present_state_Q ( -0.05816201508821048)) * f1( 0.36257650942060904)
w2 ( 0.7981156580706581 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.061537384914196164) - present_state_Q (-0.05816201508821048)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7197314700538839 ) += alpha ( 0.1 ) * (reward ( -1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03276449597792544) - present_state_Q ( -0.03276449597792544)) * f1( 0.2777037900664729)
w2 ( 0.7787054189982607 ) += alpha ( 0.1) * (reward ( -1) + discount_factor ( 0.1) * next_state_max_Q( -0.03276449597792544) - present_state_Q (-0.03276449597792544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6978367750892431 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0016134332275249685) - present_state_Q ( -0.0016134332275249685)) * f1( 0.21862947998563478)
w2 ( 0.7987344607963562 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.0016134332275249685) - present_state_Q (-0.0016134332275249685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6815425018219838 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07745566178820591) - present_state_Q ( 0.07745566178820591)) * f1( 0.1751526399497246)
w2 ( 0.8219917084061216 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.07745566178820591) - present_state_Q (0.07745566178820591)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.6721082985508091 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1602018712396986) - present_state_Q ( 0.13270603861029773)) * f1( 0.10680462083675836)
w2 ( 0.8440745621189634 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.1602018712396986) - present_state_Q (0.13270603861029773)) * f2(0.25)
============================================================================
GUIDE learning . . .
w1 ( -0.6667150431983851 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12785300280745707) - present_state_Q ( 0.12785300280745707)) * f1( 0.060945400770466805)
w2 ( 0.8617732080684292 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.12785300280745707) - present_state_Q (0.12785300280745707)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6537226189118733 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2427315398810318) - present_state_Q ( -0.2868300747344595)) * f1( 0.49484219458313855)
w2 ( 0.8630859926721609 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.2427315398810318) - present_state_Q (-0.2868300747344595)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.6186179373238029 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09302984524868044) - present_state_Q ( -0.08406156043535468)) * f1( 0.3266285319782755)
w2 ( 0.8792073713108182 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.09302984524868044) - present_state_Q (-0.08406156043535468)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5938981489005355 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020102732204440116) - present_state_Q ( 0.020102732204440116)) * f1( 0.25175270980900327)
w2 ( 0.8988455221311382 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.020102732204440116) - present_state_Q (0.020102732204440116)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.575651009241716 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0646992402558521) - present_state_Q ( 0.0646992402558521)) * f1( 0.19375353229068096)
w2 ( 0.9176809358065329 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.0646992402558521) - present_state_Q (0.0646992402558521)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5633024036672694 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10066815268334615) - present_state_Q ( 0.0626239999225881)) * f1( 0.13033615722696928)
w2 ( 0.9318925780367191 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.10066815268334615) - present_state_Q (0.0626239999225881)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5475791096093646 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011485758602710922) - present_state_Q ( 0.004336323691783231)) * f1( 0.15773576241363282)
w2 ( 0.941860700558404 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( 0.011485758602710922) - present_state_Q (0.004336323691783231)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.5326210604020005 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022420081672877262) - present_state_Q ( -0.03241511025253829)) * f1( 0.14519937646485181)
w2 ( 0.9470115660688302 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.022420081672877262) - present_state_Q (-0.03241511025253829)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.5154729207382858 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10604045117771325) - present_state_Q ( -0.10604045117771325)) * f1( 0.2879927980417851)
w2 ( 0.9499887480991299 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.10604045117771325) - present_state_Q (-0.10604045117771325)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4905131613611678 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06764006757191757) - present_state_Q ( -0.07315162634283084)) * f1( 0.2340589755422747)
w2 ( 0.9553206861970581 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.06764006757191757) - present_state_Q (-0.07315162634283084)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.480759497807247 ) += alpha ( 0.1 ) * (reward ( 0.5 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04129903098059154) - present_state_Q ( -0.04129903098059154)) * f1( 0.18157528137122766)
w2 ( 0.9580065318364708 ) += alpha ( 0.1) * (reward ( 0.5) + discount_factor ( 0.1) * next_state_max_Q( -0.04129903098059154) - present_state_Q (-0.04129903098059154)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4628464515084261 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03554851109701422) - present_state_Q ( -0.03554851109701422)) * f1( 0.1735770963848857)
w2 ( 0.9631665001364074 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.03554851109701422) - present_state_Q (-0.03554851109701422)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.4476048915257026 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02107362332653811) - present_state_Q ( -0.02107362332653811)) * f1( 0.14957865207290696)
w2 ( 0.9682613314413768 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.02107362332653811) - present_state_Q (-0.02107362332653811)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.434573444641677 ) += alpha ( 0.1 ) * (reward ( 1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009425674863260826) - present_state_Q ( -0.009425674863260826)) * f1( 0.1292182961588757)
w2 ( 0.9733037469782615 ) += alpha ( 0.1) * (reward ( 1) + discount_factor ( 0.1) * next_state_max_Q( -0.009425674863260826) - present_state_Q (-0.009425674863260826)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.7311661221515354 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14350794603001946) - present_state_Q ( -0.14350794603001946)) * f1( 0.3445029014719833)
w2 ( 0.8430647795826993 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.14350794603001946) - present_state_Q (-0.14350794603001946)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.7405577822808141 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03310818026878459) - present_state_Q ( 0.019300780372514814)) * f1( 0.434819286381918)
w2 ( 0.8344251810888739 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03310818026878459) - present_state_Q (0.019300780372514814)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.749920128248383 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06433390274199852) - present_state_Q ( 0.056411865710881026)) * f1( 0.37452608474445326)
w2 ( 0.8244260420714066 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06433390274199852) - present_state_Q (0.056411865710881026)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7653435901211482 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09545014564350174) - present_state_Q ( 0.0880532918810433)) * f1( 0.3223238260214555)
w2 ( 0.8052857109787388 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09545014564350174) - present_state_Q (0.0880532918810433)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7598065096796386 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25224862673354154) - present_state_Q ( -0.2667791628879594)) * f1( 0.5293963100061295)
w2 ( 0.8441887510705752 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.25224862673354154) - present_state_Q (-0.2667791628879594)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.850015609305907 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2095507809110536) - present_state_Q ( -0.2095507809110536)) * f1( 0.4980064349339554)
w2 ( 0.8079606651269742 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2095507809110536) - present_state_Q (-0.2095507809110536)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9293879592304964 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21116841260731636) - present_state_Q ( -0.21116841260731636)) * f1( 0.43853376520590537)
w2 ( 0.7717616965539059 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21116841260731636) - present_state_Q (-0.21116841260731636)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9306298265810475 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18436072405284815) - present_state_Q ( -0.18436072405284815)) * f1( 0.36444744092022985)
w2 ( 0.7710801895868572 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18436072405284815) - present_state_Q (-0.18436072405284815)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9130806505805972 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15069467784435372) - present_state_Q ( -0.15069467784435372)) * f1( 0.32763909672002206)
w2 ( 0.7817926937880556 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.15069467784435372) - present_state_Q (-0.15069467784435372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8988806772253093 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.031978756061100844) - present_state_Q ( 0.016507180346092443)) * f1( 0.3672178702344934)
w2 ( 0.7991937750747564 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.031978756061100844) - present_state_Q (0.016507180346092443)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8891998111764904 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08821550784302962) - present_state_Q ( 0.08821550784302962)) * f1( 0.30195519585362884)
w2 ( 0.8136210470071137 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08821550784302962) - present_state_Q (0.08821550784302962)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8829300518829314 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14385405816145347) - present_state_Q ( 0.15291202225108333)) * f1( 0.23978575593714105)
w2 ( 0.8253873492675414 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14385405816145347) - present_state_Q (0.15291202225108333)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8782213286822897 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1908038775174527) - present_state_Q ( 0.1899659433751122)) * f1( 0.20551839118886533)
w2 ( 0.8356974992644899 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1908038775174527) - present_state_Q (0.1899659433751122)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8759473531987024 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24714747707982773) - present_state_Q ( 0.2569938281085472)) * f1( 0.13558090958588948)
w2 ( 0.8432449406464645 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24714747707982773) - present_state_Q (0.2569938281085472)) * f2(0.45)
============================================================================
GUIDE learning . . .
w1 ( -0.8726801269539092 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.036150906135923325) - present_state_Q ( 0.011363250996392765)) * f1( 0.08329409615979849)
w2 ( 0.8471674590426365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.036150906135923325) - present_state_Q (0.011363250996392765)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.8659947634546886 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11288328062147046) - present_state_Q ( -0.11567001722320348)) * f1( 0.13254572168034787)
w2 ( 0.8471674590426365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11288328062147046) - present_state_Q (-0.11567001722320348)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8586713027837775 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12397173490810326) - present_state_Q ( -0.12397173490810326)) * f1( 0.14315529393451099)
w2 ( 0.8471674590426365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12397173490810326) - present_state_Q (-0.12397173490810326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7584562799570501 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30856456342218297) - present_state_Q ( -0.3129584537891965)) * f1( 0.5340319193707977)
w2 ( 0.8531837180556641 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.30856456342218297) - present_state_Q (-0.3129584537891965)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.8428191783701647 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2809018509793072) - present_state_Q ( -0.2809018509793072)) * f1( 0.482849483170753)
w2 ( 0.8357118347144779 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2809018509793072) - present_state_Q (-0.2809018509793072)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.9194936379750113 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.287570601663348) - present_state_Q ( -0.287570601663348)) * f1( 0.4403575460308177)
w2 ( 0.818299970129448 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.287570601663348) - present_state_Q (-0.287570601663348)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -0.945739174246136 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22183488025774267) - present_state_Q ( -0.22183488025774267)) * f1( 0.3747496029836852)
w2 ( 0.8077947410129275 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.22183488025774267) - present_state_Q (-0.22183488025774267)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9532978594795556 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18448466600746044) - present_state_Q ( -0.18485236340389677)) * f1( 0.3235792519642328)
w2 ( 0.8042907994649747 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18448466600746044) - present_state_Q (-0.18485236340389677)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9504076267492969 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12475287596354054) - present_state_Q ( -0.12475287596354054)) * f1( 0.2574184903942392)
w2 ( 0.8059749632904826 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.12475287596354054) - present_state_Q (-0.12475287596354054)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9451672766288657 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06907592572818107) - present_state_Q ( -0.06907592572818107)) * f1( 0.1998849387094252)
w2 ( 0.809907488287813 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06907592572818107) - present_state_Q (-0.06907592572818107)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9398312943758044 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.017152163043488197) - present_state_Q ( -0.003910552470275261)) * f1( 0.13267141046260125)
w2 ( 0.815940418330302 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.017152163043488197) - present_state_Q (-0.003910552470275261)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9354014954751787 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.056438406142702596) - present_state_Q ( 0.015641385226187493)) * f1( 0.11358387208659228)
w2 ( 0.8217904551611231 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.056438406142702596) - present_state_Q (0.015641385226187493)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.927836992257827 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09001939891997049) - present_state_Q ( -0.10233879657701059)) * f1( 0.1533334295795689)
w2 ( 0.8242571394445483 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09001939891997049) - present_state_Q (-0.10233879657701059)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -0.9842545847440238 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.254347575337544) - present_state_Q ( -0.254347575337544)) * f1( 0.31854779964155727)
w2 ( 0.8154017035335672 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.254347575337544) - present_state_Q (-0.254347575337544)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.0283691878242323 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19750767548529613) - present_state_Q ( -0.19750767548529613)) * f1( 0.24208956133432047)
w2 ( 0.8062904880732511 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.19750767548529613) - present_state_Q (-0.19750767548529613)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.064556800263519 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1602139621779257) - present_state_Q ( -0.1602139621779257)) * f1( 0.1949965916480399)
w2 ( 0.7970114509030518 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1602139621779257) - present_state_Q (-0.1602139621779257)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.0964560744366922 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14148814982074068) - present_state_Q ( -0.14148814982074068)) * f1( 0.17034198862945116)
w2 ( 0.7876481475772451 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.14148814982074068) - present_state_Q (-0.14148814982074068)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1268990458572872 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13861779536698457) - present_state_Q ( -0.13861779536698457)) * f1( 0.16234138958762664)
w2 ( 0.7782719276563965 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13861779536698457) - present_state_Q (-0.13861779536698457)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1560056146666353 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13575812399780327) - present_state_Q ( -0.13575812399780327)) * f1( 0.15500210158376856)
w2 ( 0.7688828392143866 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13575812399780327) - present_state_Q (-0.13575812399780327)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1704755970145675 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16933511261121442) - present_state_Q ( -0.19243948477477332)) * f1( 0.1997253506437977)
w2 ( 0.7652603690819548 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.16933511261121442) - present_state_Q (-0.19243948477477332)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1692688493291163 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1075969860186631) - present_state_Q ( -0.1075969860186631)) * f1( 0.12461601492999387)
w2 ( 0.7657445555190389 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1075969860186631) - present_state_Q (-0.1075969860186631)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1668245543024263 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07027955808744794) - present_state_Q ( -0.07027955808744794)) * f1( 0.09285014812948411)
w2 ( 0.7670608135304324 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07027955808744794) - present_state_Q (-0.07027955808744794)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.161313574861623 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07042810644336175) - present_state_Q ( -0.09376286581722049)) * f1( 0.11322688231627609)
w2 ( 0.7694944138062968 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07042810644336175) - present_state_Q (-0.09376286581722049)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.1574631401751871 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04677505190603056) - present_state_Q ( -0.05977937145737288)) * f1( 0.0846059964117747)
w2 ( 0.7717699231376306 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04677505190603056) - present_state_Q (-0.05977937145737288)) * f2(0.05)
============================================================================
GUIDE learning . . .
w1 ( -1.158949574913475 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12991058287754118) - present_state_Q ( -0.12991058287754118)) * f1( 0.17891504964897684)
w2 ( 0.7709391183835285 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12991058287754118) - present_state_Q (-0.12991058287754118)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1522307852021596 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028158302704870436) - present_state_Q ( -0.028664444149824803)) * f1( 0.15777413598014556)
w2 ( 0.7794560906611152 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.028158302704870436) - present_state_Q (-0.028664444149824803)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1469820133172441 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003497171209501465) - present_state_Q ( 0.003497171209501465)) * f1( 0.13226000283960818)
w2 ( 0.7873931415793441 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.003497171209501465) - present_state_Q (0.003497171209501465)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.138539779719253 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11394100934980814) - present_state_Q ( -0.11394100934980814)) * f1( 0.16798896693286597)
w2 ( 0.7924186106634924 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11394100934980814) - present_state_Q (-0.11394100934980814)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1321247258011091 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05321729319767156) - present_state_Q ( -0.07595042112903198)) * f1( 0.13630817733364517)
w2 ( 0.797124897581585 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05321729319767156) - present_state_Q (-0.07595042112903198)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1676723730969965 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13446984131244374) - present_state_Q ( -0.13446984131244374)) * f1( 0.18918616137373334)
w2 ( 0.778335126153397 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.13446984131244374) - present_state_Q (-0.13446984131244374)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.200431687791308 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12480834937445064) - present_state_Q ( -0.12480834937445064)) * f1( 0.17354342421609836)
w2 ( 0.759458401297767 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.12480834937445064) - present_state_Q (-0.12480834937445064)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.212935345827113 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1118279250786545) - present_state_Q ( -0.1118279250786545)) * f1( 0.1564218664986418)
w2 ( 0.7514648526234748 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.1118279250786545) - present_state_Q (-0.1118279250786545)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2170492424456174 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07487441754112825) - present_state_Q ( -0.07487441754112825)) * f1( 0.12368417106451371)
w2 ( 0.748138722381345 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07487441754112825) - present_state_Q (-0.07487441754112825)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2166441019865728 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04552616858639341) - present_state_Q ( -0.04552616858639341)) * f1( 0.09887853065230857)
w2 ( 0.7485484578986226 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.04552616858639341) - present_state_Q (-0.04552616858639341)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2130394908062103 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028234389016196293) - present_state_Q ( -0.028234389016196293)) * f1( 0.08473244939726529)
w2 ( 0.7528025673997684 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.028234389016196293) - present_state_Q (-0.028234389016196293)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2050485918092895 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12918840002871887) - present_state_Q ( -0.11701135259935797)) * f1( 0.15852048576879715)
w2 ( 0.7578434925257332 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12918840002871887) - present_state_Q (-0.11701135259935797)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.2002749296052104 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06455723330119661) - present_state_Q ( -0.05302960778797916)) * f1( 0.1068952388443922)
w2 ( 0.7623092313703118 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06455723330119661) - present_state_Q (-0.05302960778797916)) * f2(0.1)
============================================================================
GUIDE learning . . .
w1 ( -1.1721893840374213 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3307314025197562) - present_state_Q ( -0.3307314025197562)) * f1( 0.4025688089250923)
w2 ( 0.7762623966156674 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3307314025197562) - present_state_Q (-0.3307314025197562)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1469660284747751 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2669112195750304) - present_state_Q ( -0.29029361251626884)) * f1( 0.38009736131954136)
w2 ( 0.7895344464268428 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2669112195750304) - present_state_Q (-0.29029361251626884)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1257761496818277 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21404186917066204) - present_state_Q ( -0.2369383275522814)) * f1( 0.3442518845677683)
w2 ( 0.8018451292395471 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.21404186917066204) - present_state_Q (-0.2369383275522814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.070875571783147 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5455674233872508) - present_state_Q ( -0.5613267396279464)) * f1( 0.6054520778455972)
w2 ( 0.8154466791988855 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5455674233872508) - present_state_Q (-0.5613267396279464)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.0276313062452873 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4620401518008852) - present_state_Q ( -0.4522245405252603)) * f1( 0.5365156863634556)
w2 ( 0.8275369870790631 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4620401518008852) - present_state_Q (-0.4522245405252603)) * f2(0.15000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2951325009969519 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45069437837031423) - present_state_Q ( 0.43921438736456364)) * f1( 0.396206467826691)
w2 ( 0.47501183742700787 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.45069437837031423) - present_state_Q (0.43921438736456364)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.3094888207327362 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11095809629485415) - present_state_Q ( 0.01747330503897976)) * f1( 0.35327546180474156)
w2 ( 0.4343740878860584 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11095809629485415) - present_state_Q (0.01747330503897976)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3008311409219329 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12585522246578096) - present_state_Q ( 0.12585522246578096)) * f1( 0.30194506187249703)
w2 ( 0.4687817238597541 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12585522246578096) - present_state_Q (0.12585522246578096)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.29666125230084 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2509902485279568) - present_state_Q ( 0.2509902485279568)) * f1( 0.23949904818771953)
w2 ( 0.48967477701873474 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2509902485279568) - present_state_Q (0.2509902485279568)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.2947154037608535 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36147186942724446) - present_state_Q ( 0.3359060478753312)) * f1( 0.194116762647545)
w2 ( 0.501703713706822 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.36147186942724446) - present_state_Q (0.3359060478753312)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.294798768772784 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.43662201097325437) - present_state_Q ( 0.4507985228837996)) * f1( 0.11681789922715977)
w2 ( 0.500847355092445 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.43662201097325437) - present_state_Q (0.4507985228837996)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.271847602527267 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3133921288661841) - present_state_Q ( -0.32700567093219685)) * f1( 0.3299162404638092)
w2 ( 0.5147606842533566 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3133921288661841) - present_state_Q (-0.32700567093219685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.255481649069601 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25032253622803374) - present_state_Q ( -0.23711579777904612)) * f1( 0.2673810399563393)
w2 ( 0.5270023551364815 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25032253622803374) - present_state_Q (-0.23711579777904612)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2434011068002457 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18276664641308799) - present_state_Q ( -0.16966632991545183)) * f1( 0.2190926495393953)
w2 ( 0.5380301484419644 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18276664641308799) - present_state_Q (-0.16966632991545183)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1712708333435298 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8004719663162319) - present_state_Q ( -0.8004719663162319)) * f1( 0.6437761410524696)
w2 ( 0.5380301484419644 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.8004719663162319) - present_state_Q (-0.8004719663162319)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2474823332559637 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34830413394097637) - present_state_Q ( -0.4587702284286325)) * f1( 0.48355704077445355)
w2 ( 0.5065089447426551 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.34830413394097637) - present_state_Q (-0.4587702284286325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.3288181759264843 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2833340036796669) - present_state_Q ( -0.27469027349301434)) * f1( 0.4638106888684)
w2 ( 0.4012903571301579 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.2833340036796669) - present_state_Q (-0.27469027349301434)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4090691479446702 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008534264691706372) - present_state_Q ( -0.07172380673432516)) * f1( 0.4163641386864383)
w2 ( 0.16999962511457736 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.008534264691706372) - present_state_Q (-0.07172380673432516)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4734785588868193 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28116869104740994) - present_state_Q ( -0.28166837794382693)) * f1( 0.3688022364709677)
w2 ( -0.07450316364795062 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.28116869104740994) - present_state_Q (-0.28166837794382693)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.51888216736081 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5282132495096243) - present_state_Q ( -0.5282132495096243)) * f1( 0.2978051159859391)
w2 ( -0.2574561327009113 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5282132495096243) - present_state_Q (-0.5282132495096243)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.514142309673797 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6125207238135721) - present_state_Q ( -0.6640119503537545)) * f1( 0.23376704180391855)
w2 ( -0.2331249473442236 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.6125207238135721) - present_state_Q (-0.6640119503537545)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4961984104874637 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5756793477240953) - present_state_Q ( -0.5756793477240953)) * f1( 0.19544359141168266)
w2 ( -0.12295157779002126 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5756793477240953) - present_state_Q (-0.5756793477240953)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.4900325805130146 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20144159108232743) - present_state_Q ( -0.22603190664033168)) * f1( 0.10176521970552826)
w2 ( -0.08659831293809532 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.20144159108232743) - present_state_Q (-0.22603190664033168)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4745622271596581 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34270410761113135) - present_state_Q ( -0.34270410761113135)) * f1( 0.21837404717115863)
w2 ( -0.07242963900109495 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.34270410761113135) - present_state_Q (-0.34270410761113135)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4633072483603289 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27190483754519856) - present_state_Q ( -0.27190483754519856)) * f1( 0.17457310719319513)
w2 ( -0.05953535192528137 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.27190483754519856) - present_state_Q (-0.27190483754519856)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4544349794893991 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19675693181670542) - present_state_Q ( -0.2260229222553841)) * f1( 0.14632323601912708)
w2 ( -0.0474084073438071 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19675693181670542) - present_state_Q (-0.2260229222553841)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4493839673896527 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14740017547267045) - present_state_Q ( -0.14740017547267045)) * f1( 0.09482616682687828)
w2 ( -0.03675520418529903 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14740017547267045) - present_state_Q (-0.14740017547267045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4464834053057871 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09408921161797279) - present_state_Q ( -0.09408921161797279)) * f1( 0.05984485321521034)
w2 ( -0.02706159837617552 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09408921161797279) - present_state_Q (-0.09408921161797279)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.4600468909665616 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34224333122734596) - present_state_Q ( -0.34224333122734596)) * f1( 0.2291202862481604)
w2 ( -0.05074083845199107 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.34224333122734596) - present_state_Q (-0.34224333122734596)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.4742043697434966 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5882723876196226) - present_state_Q ( -0.5882723876196226)) * f1( 0.38206162281482753)
w2 ( -0.07297412952053145 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.5882723876196226) - present_state_Q (-0.5882723876196226)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4546527333302903 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4921129132476801) - present_state_Q ( -0.4921129132476801)) * f1( 0.30411552477854065)
w2 ( -0.03440003220515672 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4921129132476801) - present_state_Q (-0.4921129132476801)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4378966989829454 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3580981892714937) - present_state_Q ( -0.3580981892714937)) * f1( 0.23198538195148544)
w2 ( 0.008937270015503943 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3580981892714937) - present_state_Q (-0.3580981892714937)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.4263230639440132 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2579070703755122) - present_state_Q ( -0.2579070703755122)) * f1( 0.18309342567587128)
w2 ( 0.04686425181578161 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2579070703755122) - present_state_Q (-0.2579070703755122)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.420835452001253 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12468896260010193) - present_state_Q ( -0.12468896260010193)) * f1( 0.10713387278968448)
w2 ( 0.07759745579618713 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12468896260010193) - present_state_Q (-0.12468896260010193)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.415358521902915 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06992489176780323) - present_state_Q ( -0.08514882378944189)) * f1( 0.11454266527232282)
w2 ( 0.1254130892574533 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06992489176780323) - present_state_Q (-0.08514882378944189)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3855443754737211 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47561565318610305) - present_state_Q ( -0.48114060070775855)) * f1( 0.35766430252501996)
w2 ( 0.14208466996523625 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.47561565318610305) - present_state_Q (-0.48114060070775855)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.3612235659939915 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38115510180073797) - present_state_Q ( -0.3908404364637235)) * f1( 0.32310354859385654)
w2 ( 0.17219366701658223 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.38115510180073797) - present_state_Q (-0.3908404364637235)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.344327799627539 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29155278695912334) - present_state_Q ( -0.282916400600698)) * f1( 0.2584394482991809)
w2 ( 0.19834411189277365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.29155278695912334) - present_state_Q (-0.282916400600698)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3343482627438898 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1717795107321235) - present_state_Q ( -0.19322486340968725)) * f1( 0.17324173899607506)
w2 ( 0.20986505013950316 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1717795107321235) - present_state_Q (-0.19322486340968725)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.3251738085151472 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10258547181875957) - present_state_Q ( -0.1449898428774241)) * f1( 0.17157129763293777)
w2 ( 0.23125430196732508 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10258547181875957) - present_state_Q (-0.1449898428774241)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3213012476742985 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028151427773426566) - present_state_Q ( -0.028151427773426566)) * f1( 0.09104703683779265)
w2 ( 0.24826775336716844 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.028151427773426566) - present_state_Q (-0.028151427773426566)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3073075792532696 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19434532529501303) - present_state_Q ( -0.2124740189345229)) * f1( 0.2359652053838403)
w2 ( 0.2719893328233693 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19434532529501303) - present_state_Q (-0.2124740189345229)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2995857582525314 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11278492658541535) - present_state_Q ( -0.09842657669281109)) * f1( 0.15851075378949736)
w2 ( 0.2914752561847401 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11278492658541535) - present_state_Q (-0.09842657669281109)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.295144917213181 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034784719897657745) - present_state_Q ( -0.02147824462242867)) * f1( 0.10624027404084226)
w2 ( 0.3081952470900466 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.034784719897657745) - present_state_Q (-0.02147824462242867)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2929855851697496 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.045463750756622026) - present_state_Q ( 0.04540722554657324)) * f1( 0.06012522016223752)
w2 ( 0.32256081307121015 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.045463750756622026) - present_state_Q (0.04540722554657324)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2663895743981466 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3921892280265895) - present_state_Q ( -0.3921892280265895)) * f1( 0.3532146033792585)
w2 ( 0.33762021917568874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3921892280265895) - present_state_Q (-0.3921892280265895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2467919756700916 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3040898287408445) - present_state_Q ( -0.34655308427371617)) * f1( 0.2736544040473612)
w2 ( 0.33762021917568874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3040898287408445) - present_state_Q (-0.34655308427371617)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2323352079157568 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22084965220182384) - present_state_Q ( -0.275747511290077)) * f1( 0.22116561276541405)
w2 ( 0.33762021917568874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22084965220182384) - present_state_Q (-0.275747511290077)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2226639275264377 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14069925957712037) - present_state_Q ( -0.15344157905224487)) * f1( 0.1793064269105001)
w2 ( 0.3484076522375794 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14069925957712037) - present_state_Q (-0.15344157905224487)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2172227778954936 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08578815946705325) - present_state_Q ( -0.07343231822556855)) * f1( 0.11705084729424954)
w2 ( 0.3577047222831567 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08578815946705325) - present_state_Q (-0.07343231822556855)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2145887632262482 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.020966351767772937) - present_state_Q ( -0.007537899607669166)) * f1( 0.06496661539724319)
w2 ( 0.36581354757177453 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.020966351767772937) - present_state_Q (-0.007537899607669166)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1909915810641907 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26626106742668276) - present_state_Q ( -0.33942377694103765)) * f1( 0.4601655493662679)
w2 ( 0.3965814077836767 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.26626106742668276) - present_state_Q (-0.33942377694103765)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1774896551123555 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15561795091630903) - present_state_Q ( -0.15561795091630903)) * f1( 0.3970498907479376)
w2 ( 0.4237859002496509 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15561795091630903) - present_state_Q (-0.15561795091630903)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1624177773843007 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055439877920510416) - present_state_Q ( -0.055439877920510416)) * f1( 0.33500812207355757)
w2 ( 0.45977757145992765 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.055439877920510416) - present_state_Q (-0.055439877920510416)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.149698189727363 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0009917926883782902) - present_state_Q ( -0.0009917926883782902)) * f1( 0.31728166673967584)
w2 ( 0.4918489805334909 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0009917926883782902) - present_state_Q (-0.0009917926883782902)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1419578653776492 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10348193848535037) - present_state_Q ( 0.10348193848535037)) * f1( 0.25223771641339343)
w2 ( 0.5163982809625457 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10348193848535037) - present_state_Q (0.10348193848535037)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1371713654183462 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18184338033538613) - present_state_Q ( 0.18184338033538613)) * f1( 0.2025251994373426)
w2 ( 0.5353055575783979 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18184338033538613) - present_state_Q (0.18184338033538613)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.13520460259685 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26696196889253543) - present_state_Q ( 0.27791744487167924)) * f1( 0.13219379748955984)
w2 ( 0.5472078577398038 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.26696196889253543) - present_state_Q (0.27791744487167924)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1646219779583027 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41416133433048335) - present_state_Q ( -0.4125129946783888)) * f1( 0.5561958930839014)
w2 ( 0.5260517321896174 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.41416133433048335) - present_state_Q (-0.4125129946783888)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2501197483982558 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3830729025760983) - present_state_Q ( -0.39580378048768894)) * f1( 0.520533258719973)
w2 ( 0.46035159179882057 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3830729025760983) - present_state_Q (-0.39580378048768894)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3363532501485689 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3826986810404458) - present_state_Q ( -0.3699684668987724)) * f1( 0.5168940197977008)
w2 ( 0.3602535077265042 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3826986810404458) - present_state_Q (-0.3699684668987724)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.31732500831634 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41664543116003055) - present_state_Q ( -0.43323857642002944)) * f1( 0.48594238161484327)
w2 ( 0.3837479497247458 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.41664543116003055) - present_state_Q (-0.43323857642002944)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2921943122423765 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23465508920647654) - present_state_Q ( -0.23465508920647654)) * f1( 0.4111767753339437)
w2 ( 0.4326431161476121 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23465508920647654) - present_state_Q (-0.23465508920647654)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.273694346352597 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12244094315865062) - present_state_Q ( -0.12244094315865062)) * f1( 0.3626044718178991)
w2 ( 0.47345886405503496 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12244094315865062) - present_state_Q (-0.12244094315865062)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2584731018412127 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05368108704419755) - present_state_Q ( -0.05368108704419755)) * f1( 0.3395227273533888)
w2 ( 0.5093239023222171 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05368108704419755) - present_state_Q (-0.05368108704419755)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2477839033105922 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05060622902765871) - present_state_Q ( 0.03948729092198455)) * f1( 0.2923954674894735)
w2 ( 0.5385697688806796 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05060622902765871) - present_state_Q (0.03948729092198455)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2414017472776886 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13190579776971179) - present_state_Q ( 0.13968734991598275)) * f1( 0.23334847036897924)
w2 ( 0.5604500272695587 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.13190579776971179) - present_state_Q (0.13968734991598275)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.235405748252444 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20542033387568734) - present_state_Q ( 0.10234465510419757)) * f1( 0.1884364685087003)
w2 ( 0.579541869966561 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20542033387568734) - present_state_Q (0.10234465510419757)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2336343351696413 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29596403674554994) - present_state_Q ( 0.2977063538219248)) * f1( 0.1343098349558742)
w2 ( 0.5900930739547714 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.29596403674554994) - present_state_Q (0.2977063538219248)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2324843666125627 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.34627493160243694) - present_state_Q ( 0.3327815945616691)) * f1( 0.11291260354146465)
w2 ( 0.5982407458426574 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.34627493160243694) - present_state_Q (0.3327815945616691)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.22663988152762 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04415781022797503) - present_state_Q ( -0.04415781022797503)) * f1( 0.13290712956199283)
w2 ( 0.6070355864267609 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04415781022797503) - present_state_Q (-0.04415781022797503)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2220024215045224 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08634616555304134) - present_state_Q ( 0.07321891294279662)) * f1( 0.1382600746819832)
w2 ( 0.6204522145712612 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08634616555304134) - present_state_Q (0.07321891294279662)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.211244177349362 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1301357530373526) - present_state_Q ( -0.1301357530373526)) * f1( 0.20804066463191045)
w2 ( 0.6307946581259336 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1301357530373526) - present_state_Q (-0.1301357530373526)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1608917081679015 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5531394093052702) - present_state_Q ( -0.5531394093052702)) * f1( 0.5608269196529853)
w2 ( 0.6487511674934284 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5531394093052702) - present_state_Q (-0.5531394093052702)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.112369824145767 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5195203101592771) - present_state_Q ( -0.5195203101592771)) * f1( 0.5592860549263721)
w2 ( 0.6661025330762954 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5195203101592771) - present_state_Q (-0.5195203101592771)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0680090072365822 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45492274901733454) - present_state_Q ( -0.4672605914262218)) * f1( 0.5398214559646242)
w2 ( 0.6825378994067852 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.45492274901733454) - present_state_Q (-0.4672605914262218)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0339153186601733 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36396124840745536) - present_state_Q ( -0.36396124840745536)) * f1( 0.4685998197559677)
w2 ( 0.6970892018781194 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.36396124840745536) - present_state_Q (-0.36396124840745536)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.004535345684003 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3087151001085645) - present_state_Q ( -0.3087151001085645)) * f1( 0.43343292472435113)
w2 ( 0.7106460736800736 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3087151001085645) - present_state_Q (-0.3087151001085645)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9814667670177208 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23617389030974253) - present_state_Q ( -0.23617389030974253)) * f1( 0.37659511601173684)
w2 ( 0.7228972037056489 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23617389030974253) - present_state_Q (-0.23617389030974253)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9622767159480495 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16662677512007087) - present_state_Q ( -0.18614750075825867)) * f1( 0.3369721243892276)
w2 ( 0.734286900170574 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.16662677512007087) - present_state_Q (-0.18614750075825867)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9504445497310678 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08989883120328676) - present_state_Q ( -0.08989883120328676)) * f1( 0.24603755584395054)
w2 ( 0.7439050791322331 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08989883120328676) - present_state_Q (-0.08989883120328676)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9420420685627595 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0338959068350316) - present_state_Q ( -0.035874824544074924)) * f1( 0.19428365434129816)
w2 ( 0.7525547838094446 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0338959068350316) - present_state_Q (-0.035874824544074924)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9341062878822012 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0017818171560819995) - present_state_Q ( -0.02522459631846219)) * f1( 0.18654745785234908)
w2 ( 0.7610628393701261 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0017818171560819995) - present_state_Q (-0.02522459631846219)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9296453907692088 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05736232481056147) - present_state_Q ( 0.03868690038331138)) * f1( 0.12153399346887858)
w2 ( 0.768403826012081 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05736232481056147) - present_state_Q (0.03868690038331138)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9264152585241823 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07634612532877535) - present_state_Q ( 0.06582772197493116)) * f1( 0.0945016713897689)
w2 ( 0.7752399638232399 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07634612532877535) - present_state_Q (0.06582772197493116)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9478807866436841 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0869488198608542) - present_state_Q ( -0.0869488198608542)) * f1( 0.26121850908523797)
w2 ( 0.7588050425807352 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.0869488198608542) - present_state_Q (-0.0869488198608542)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0395873024676676 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37894688122562714) - present_state_Q ( 0.389091933527587)) * f1( 0.39004177979200494)
w2 ( 0.5236853180402328 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.37894688122562714) - present_state_Q (0.389091933527587)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1188618020334768 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13571517596295135) - present_state_Q ( 0.1352541681937297)) * f1( 0.37363975966663343)
w2 ( 0.3115170529804893 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.13571517596295135) - present_state_Q (0.1352541681937297)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.184241173351892 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054123093936036204) - present_state_Q ( -0.06557984958599855)) * f1( 0.3370361754071259)
w2 ( 0.11753380699972876 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.054123093936036204) - present_state_Q (-0.06557984958599855)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.233491860012371 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21569005326183877) - present_state_Q ( -0.20322303398981634)) * f1( 0.2708543227573068)
w2 ( -0.06430079013390802 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21569005326183877) - present_state_Q (-0.20322303398981634)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2726032505062417 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3251899537384019) - present_state_Q ( -0.35124785435825956)) * f1( 0.23262988068804416)
w2 ( -0.2324279042354661 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.3251899537384019) - present_state_Q (-0.35124785435825956)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.3002473531356014 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45341825528670454) - present_state_Q ( -0.45341825528670454)) * f1( 0.17365219754336514)
w2 ( -0.3916202612596627 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.45341825528670454) - present_state_Q (-0.45341825528670454)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2990979009389163 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3576204343794067) - present_state_Q ( -0.3576204343794067)) * f1( 0.0943268812105924)
w2 ( -0.38430875780317475 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3576204343794067) - present_state_Q (-0.3576204343794067)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2985461616410199 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2494373750017284) - present_state_Q ( -0.2633736397770243)) * f1( 0.14357030989087805)
w2 ( -0.3835401597576377 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2494373750017284) - present_state_Q (-0.2633736397770243)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2764548585470483 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42472657188928076) - present_state_Q ( -0.437522744213174)) * f1( 0.2778605204189829)
w2 ( -0.3676391580171528 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.42472657188928076) - present_state_Q (-0.437522744213174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2559809817369243 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4659971130188325) - present_state_Q ( -0.4659971130188325)) * f1( 0.2498650443267639)
w2 ( -0.3348632619484748 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4659971130188325) - present_state_Q (-0.4659971130188325)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2405122935721853 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3662452127750052) - present_state_Q ( -0.3913637328767441)) * f1( 0.2049540811847043)
w2 ( -0.30467369348450507 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3662452127750052) - present_state_Q (-0.3913637328767441)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2317738046847393 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2687842525409838) - present_state_Q ( -0.28626721947214434)) * f1( 0.13252407326407203)
w2 ( -0.2782981417157832 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2687842525409838) - present_state_Q (-0.28626721947214434)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2206185356301071 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26950545052418384) - present_state_Q ( -0.26950545052418384)) * f1( 0.1736080288180499)
w2 ( -0.2654470436063479 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.26950545052418384) - present_state_Q (-0.26950545052418384)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2127630225646877 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25787576469136814) - present_state_Q ( -0.25787576469136814)) * f1( 0.12427875115833796)
w2 ( -0.24016351607745864 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25787576469136814) - present_state_Q (-0.25787576469136814)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.3661929041502776 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.083883396674927) - present_state_Q ( -1.1097166169278885)) * f1( 0.6773671564834729)
w2 ( -0.443460463122014 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -1.083883396674927) - present_state_Q (-1.1097166169278885)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.3494557293879974 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1795792900892823) - present_state_Q ( -1.2682713827136851)) * f1( 0.668648628931534)
w2 ( -0.4234353868256334 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.1795792900892823) - present_state_Q (-1.2682713827136851)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3395368291601133 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1773475486329958) - present_state_Q ( -1.1773475486329958)) * f1( 0.6214351615319822)
w2 ( -0.4106663633240577 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.1773475486329958) - present_state_Q (-1.1773475486329958)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3339985973678312 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.1060205138941879) - present_state_Q ( -1.1060205138941879)) * f1( 0.5804151153667232)
w2 ( -0.4030328863236762 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.1060205138941879) - present_state_Q (-1.1060205138941879)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3322909004621784 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.051132935017936) - present_state_Q ( -1.0369936382075169)) * f1( 0.5356582312444096)
w2 ( -0.40048245874721833 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -1.051132935017936) - present_state_Q (-1.0369936382075169)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2947121317971504 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.0625057318423332) - present_state_Q ( -1.0625057318423332)) * f1( 0.496905948141998)
w2 ( -0.3248569428814083 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -1.0625057318423332) - present_state_Q (-1.0625057318423332)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.243190528295988 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8831431456270771) - present_state_Q ( -0.8831431456270771)) * f1( 0.43120489028764153)
w2 ( -0.20537405977497136 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.8831431456270771) - present_state_Q (-0.8831431456270771)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.197983648790342 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7348762776570767) - present_state_Q ( -0.7348762776570767)) * f1( 0.42592201744641794)
w2 ( -0.09923519478583444 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.7348762776570767) - present_state_Q (-0.7348762776570767)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1652892717130485 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5483461556807386) - present_state_Q ( -0.554457304208984)) * f1( 0.36342321608946876)
w2 ( 0.008719527851074799 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5483461556807386) - present_state_Q (-0.554457304208984)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1445923336176111 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.333849693231852) - present_state_Q ( -0.333849693231852)) * f1( 0.29547438134994564)
w2 ( 0.09277529472011484 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.333849693231852) - present_state_Q (-0.333849693231852)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1322581784389434 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1644561522655514) - present_state_Q ( -0.15218530170738506)) * f1( 0.2302266471929376)
w2 ( 0.15706405709781446 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1644561522655514) - present_state_Q (-0.15218530170738506)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1243985389669717 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03799624942969557) - present_state_Q ( -0.0447522193873138)) * f1( 0.1782422775372438)
w2 ( 0.2011593165422489 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03799624942969557) - present_state_Q (-0.0447522193873138)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1197882606032883 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08233130716195328) - present_state_Q ( 0.08233130716195328)) * f1( 0.14146218371546432)
w2 ( 0.24026753536875795 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08233130716195328) - present_state_Q (0.08233130716195328)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.3076197346392102 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23398889304148074) - present_state_Q ( -0.252269647416741)) * f1( 0.13519859412879856)
w2 ( -0.3969627445953859 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.23398889304148074) - present_state_Q (-0.252269647416741)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2902511927075841 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.383999182532775) - present_state_Q ( -0.383999182532775)) * f1( 0.23294741242012748)
w2 ( -0.38205075930979593 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.383999182532775) - present_state_Q (-0.383999182532775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.275129748472145 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4069569217410206) - present_state_Q ( -0.40731898259347543)) * f1( 0.1972473889642319)
w2 ( -0.351385827693021 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4069569217410206) - present_state_Q (-0.40731898259347543)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2658194194526653 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.242664369986698) - present_state_Q ( -0.31294153552530224)) * f1( 0.13519189294630402)
w2 ( -0.3238388237519557 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.242664369986698) - present_state_Q (-0.31294153552530224)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2381899936630665 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43622671770026555) - present_state_Q ( -0.43945997155669275)) * f1( 0.3471743005386292)
w2 ( -0.3238388237519557 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.43622671770026555) - present_state_Q (-0.43945997155669275)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.212758956486324 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45045693858252017) - present_state_Q ( -0.45401123755447326)) * f1( 0.3143648994065463)
w2 ( -0.3076595128780313 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.45045693858252017) - present_state_Q (-0.45401123755447326)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1939900773543644 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3580537494566862) - present_state_Q ( -0.3710976117195389)) * f1( 0.2552574091399205)
w2 ( -0.29295366814255386 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3580537494566862) - present_state_Q (-0.3710976117195389)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1812517280526287 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28900169452531127) - present_state_Q ( -0.28900169452531127)) * f1( 0.19297560781019524)
w2 ( -0.2797516376410982 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.28900169452531127) - present_state_Q (-0.28900169452531127)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1744121445578208 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13903781264376672) - present_state_Q ( -0.19498814017198637)) * f1( 0.11770379618659244)
w2 ( -0.268129950462946 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13903781264376672) - present_state_Q (-0.19498814017198637)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1692653233842445 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16400631446715708) - present_state_Q ( -0.16400631446715708)) * f1( 0.09398772388898217)
w2 ( -0.2571778368025372 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.16400631446715708) - present_state_Q (-0.16400631446715708)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1440511596790528 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.391749291078759) - present_state_Q ( -0.391749291078759)) * f1( 0.33503883442374366)
w2 ( -0.2571778368025372 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.391749291078759) - present_state_Q (-0.391749291078759)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1314396043265167 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5462260988161742) - present_state_Q ( -0.5462260988161742)) * f1( 0.4324898648714916)
w2 ( -0.25134576702384603 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5462260988161742) - present_state_Q (-0.5462260988161742)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1217155135537196 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49494020426130536) - present_state_Q ( -0.49621213866335184)) * f1( 0.3941376840207284)
w2 ( -0.2464114046591016 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.49494020426130536) - present_state_Q (-0.49621213866335184)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1142946740421145 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4519274109755965) - present_state_Q ( -0.4519274109755965)) * f1( 0.3589547663187359)
w2 ( -0.24227671126154088 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4519274109755965) - present_state_Q (-0.4519274109755965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1109020305517232 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3579443243763266) - present_state_Q ( -0.3579443243763266)) * f1( 0.2777442891307595)
w2 ( -0.239833713422767 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3579443243763266) - present_state_Q (-0.3579443243763266)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.109464631281318 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2942597229900072) - present_state_Q ( -0.2942597229900072)) * f1( 0.22170540113526824)
w2 ( -0.23853703840894686 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2942597229900072) - present_state_Q (-0.2942597229900072)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1095087892060722 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21904515387530965) - present_state_Q ( -0.21904515387530965)) * f1( 0.1544328150376842)
w2 ( -0.23859422563919128 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21904515387530965) - present_state_Q (-0.21904515387530965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1100081578364929 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17205598562257812) - present_state_Q ( -0.17297025873130703)) * f1( 0.11288906840755589)
w2 ( -0.23947893243581028 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17205598562257812) - present_state_Q (-0.17297025873130703)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1105321884845136 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16618694262947747) - present_state_Q ( -0.1683137510144025)) * f1( 0.10848385543575285)
w2 ( -0.24044503130078118 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16618694262947747) - present_state_Q (-0.1683137510144025)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1761799714360124 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5929496557814943) - present_state_Q ( -0.5935699039331894)) * f1( 0.44788606451078394)
w2 ( -0.2990740337665796 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.5929496557814943) - present_state_Q (-0.5935699039331894)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2325912905699539 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7349435362359648) - present_state_Q ( -0.7349435362359648)) * f1( 0.42143576770612245)
w2 ( -0.4061580991575901 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7349435362359648) - present_state_Q (-0.7349435362359648)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2397794821637125 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7869039198048642) - present_state_Q ( -0.7869039198048642)) * f1( 0.3748018049561037)
w2 ( -0.4215010169316399 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.7869039198048642) - present_state_Q (-0.7869039198048642)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2245429100307137 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7414406499924906) - present_state_Q ( -0.7414406499924906)) * f1( 0.3260578532415161)
w2 ( -0.38411729013218054 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7414406499924906) - present_state_Q (-0.7414406499924906)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2627924676306965 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6352240984799086) - present_state_Q ( -0.6352240984799086)) * f1( 0.2677981013878387)
w2 ( -0.4983811550416271 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6352240984799086) - present_state_Q (-0.6352240984799086)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2945143737711815 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.684218059454913) - present_state_Q ( -0.6891308392300344)) * f1( 0.22998705063678576)
w2 ( -0.6087244323788636 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.684218059454913) - present_state_Q (-0.6891308392300344)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.3165910028036953 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6948829546776958) - present_state_Q ( -0.6948829546776958)) * f1( 0.16060339922602807)
w2 ( -0.7186928596420695 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6948829546776958) - present_state_Q (-0.6948829546776958)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0864106103138933 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6744305915842125) - present_state_Q ( -0.6868283542612167)) * f1( 0.5751650916779957)
w2 ( -0.23205732539872528 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6744305915842125) - present_state_Q (-0.6868283542612167)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0739117908862563 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.670073260004082) - present_state_Q ( -0.6809896330974257)) * f1( 0.5841052747398462)
w2 ( -0.22777767925678494 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.670073260004082) - present_state_Q (-0.6809896330974257)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0449060893684088 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7169669426591028) - present_state_Q ( -0.7280825264824647)) * f1( 0.635552190061947)
w2 ( -0.21864996261245384 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7169669426591028) - present_state_Q (-0.7280825264824647)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0124357840369158 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7923798440319645) - present_state_Q ( -0.7923798440319645)) * f1( 0.6327744408726214)
w2 ( -0.18786145103472776 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7923798440319645) - present_state_Q (-0.7923798440319645)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9998366880908287 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6899705402624232) - present_state_Q ( -0.6899705402624232)) * f1( 0.5701632426897096)
w2 ( -0.17460304186055692 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.6899705402624232) - present_state_Q (-0.6899705402624232)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0168230078931526 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6451130094185369) - present_state_Q ( -0.6569346944763571)) * f1( 0.5522630604947971)
w2 ( -0.1930576382484867 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.6451130094185369) - present_state_Q (-0.6569346944763571)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9707430155978635 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6097050434306122) - present_state_Q ( -0.6097050434306122)) * f1( 0.4856995333974738)
w2 ( -0.13613356590323364 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.6097050434306122) - present_state_Q (-0.6097050434306122)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9333808098874692 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.50577996244094) - present_state_Q ( -0.50577996244094)) * f1( 0.4368816628959254)
w2 ( -0.08482144793142286 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.50577996244094) - present_state_Q (-0.50577996244094)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8997864659762133 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42453561303279214) - present_state_Q ( -0.4427175307022754)) * f1( 0.419790783989507)
w2 ( -0.03680560976748309 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.42453561303279214) - present_state_Q (-0.4427175307022754)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8826527069392975 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3314606911619756) - present_state_Q ( -0.3314606911619756)) * f1( 0.3438341617706266)
w2 ( -0.006906732444736399 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3314606911619756) - present_state_Q (-0.3314606911619756)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8706912414601465 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2516817576613925) - present_state_Q ( -0.2516817576613925)) * f1( 0.28044746959754635)
w2 ( 0.0186840824689788 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2516817576613925) - present_state_Q (-0.2516817576613925)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8625297741902586 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18343089384736633) - present_state_Q ( -0.18343089384736633)) * f1( 0.2235480662494556)
w2 ( 0.040589350736736585 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18343089384736633) - present_state_Q (-0.18343089384736633)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8544177364337545 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.114688529905738) - present_state_Q ( -0.114688529905738)) * f1( 0.16120271381740123)
w2 ( 0.07078253135164644 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.114688529905738) - present_state_Q (-0.114688529905738)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8444333245365006 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1462980408209317) - present_state_Q ( -0.1462980408209317)) * f1( 0.18779402656244076)
w2 ( 0.08141589608642322 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1462980408209317) - present_state_Q (-0.1462980408209317)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8450813049344436 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18585471621322674) - present_state_Q ( -0.19448639147903557)) * f1( 0.2688817971960444)
w2 ( 0.08045193288073173 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18585471621322674) - present_state_Q (-0.19448639147903557)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8427583976999052 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1423619189187697) - present_state_Q ( -0.13304562525025818)) * f1( 0.19551538702582963)
w2 ( 0.08520431021506697 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1423619189187697) - present_state_Q (-0.13304562525025818)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8386367912536983 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09801828852928886) - present_state_Q ( -0.08992011945829516)) * f1( 0.14713806932420187)
w2 ( 0.09640904183928162 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09801828852928886) - present_state_Q (-0.08992011945829516)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8681446604386123 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11095253951539456) - present_state_Q ( -0.11095253951539456)) * f1( 0.15529291016264674)
w2 ( 0.058406187550558715 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.11095253951539456) - present_state_Q (-0.11095253951539456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8909163889256144 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09140467545278992) - present_state_Q ( -0.09140467545278992)) * f1( 0.11874278292608471)
w2 ( 0.020051471708708933 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.09140467545278992) - present_state_Q (-0.09140467545278992)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.123156831003176 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7439383459519994) - present_state_Q ( -0.764665605929189)) * f1( 0.6019524695822885)
w2 ( -0.24883416044742163 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.7439383459519994) - present_state_Q (-0.764665605929189)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.195082743568912 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7060379655208698) - present_state_Q ( -0.770819734693606)) * f1( 0.553368168424019)
w2 ( -0.3268212041589305 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.7060379655208698) - present_state_Q (-0.770819734693606)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2384930757261907 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6444254846637334) - present_state_Q ( -0.7097897254955196)) * f1( 0.3204535613935136)
w2 ( -0.4622864864560159 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6444254846637334) - present_state_Q (-0.7097897254955196)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2096212686013506 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7853527113650861) - present_state_Q ( -0.7853527113650861)) * f1( 0.2608542843242303)
w2 ( -0.35160474243315815 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.7853527113650861) - present_state_Q (-0.7853527113650861)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.191243022305542 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5456256951169789) - present_state_Q ( -0.5340521858256285)) * f1( 0.20896490367714074)
w2 ( -0.28124557312804366 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.5456256951169789) - present_state_Q (-0.5340521858256285)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1621439762067776 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6981943598099276) - present_state_Q ( -0.7100510585693929)) * f1( 0.3795630827987778)
w2 ( -0.40361723659017323 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.6981943598099276) - present_state_Q (-0.7100510585693929)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.1685519483410969 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6759247392690262) - present_state_Q ( -0.7566481865870608)) * f1( 0.3037755710348178)
w2 ( -0.42471166532415744 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.6759247392690262) - present_state_Q (-0.7566481865870608)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.1487856737884194 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5507276132798724) - present_state_Q ( -0.6356699463447039)) * f1( 0.2532199056323043)
w2 ( -0.3622638905228201 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5507276132798724) - present_state_Q (-0.6356699463447039)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.140278745259342 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6746967333536301) - present_state_Q ( -0.6746967333536301)) * f1( 0.2088989009889454)
w2 ( -0.31339664332062805 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6746967333536301) - present_state_Q (-0.6746967333536301)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.129604752310396 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40914971327335997) - present_state_Q ( -0.40914971327335997)) * f1( 0.13894181512681278)
w2 ( -0.2519378639649461 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.40914971327335997) - present_state_Q (-0.40914971327335997)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1073619231434362 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4106245417111391) - present_state_Q ( -0.42236235544904766)) * f1( 0.2846900291498619)
w2 ( -0.22068586791382874 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4106245417111391) - present_state_Q (-0.42236235544904766)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0805441825338864 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42406111357537096) - present_state_Q ( -0.42406111357537096)) * f1( 0.34308922137590403)
w2 ( -0.20505276786947207 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.42406111357537096) - present_state_Q (-0.42406111357537096)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.059671817717483 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35462038097202186) - present_state_Q ( -0.35462038097202186)) * f1( 0.29023322920744377)
w2 ( -0.19066960101197566 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.35462038097202186) - present_state_Q (-0.35462038097202186)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0374229478423613 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3626906399317961) - present_state_Q ( -0.3626906399317961)) * f1( 0.3062804108808812)
w2 ( -0.17614116949320333 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3626906399317961) - present_state_Q (-0.3626906399317961)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0116361396282156 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39101256871920753) - present_state_Q ( -0.39101256871920753)) * f1( 0.3429501299932967)
w2 ( -0.1611029432562576 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.39101256871920753) - present_state_Q (-0.39101256871920753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9913868532440259 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32719145530298355) - present_state_Q ( -0.32719145530298355)) * f1( 0.2915780240513513)
w2 ( -0.1472134970608039 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.32719145530298355) - present_state_Q (-0.32719145530298355)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9741354080561315 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2886583651818107) - present_state_Q ( -0.2886583651818107)) * f1( 0.26146772566273385)
w2 ( -0.13401764648753128 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2886583651818107) - present_state_Q (-0.2886583651818107)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9527467155399836 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3269257758763754) - present_state_Q ( -0.3269257758763754)) * f1( 0.30809089177628535)
w2 ( -0.12013298252175653 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3269257758763754) - present_state_Q (-0.3269257758763754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9380421793836901 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2486261182972601) - present_state_Q ( -0.2486261182972601)) * f1( 0.23573896202372488)
w2 ( -0.10765771239240585 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2486261182972601) - present_state_Q (-0.2486261182972601)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9278770977272289 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18883208119499395) - present_state_Q ( -0.18883208119499395)) * f1( 0.17835076331688213)
w2 ( -0.09625873493089596 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18883208119499395) - present_state_Q (-0.18883208119499395)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.916494974449958 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2010464126846419) - present_state_Q ( -0.2010464126846419)) * f1( 0.1959253721680988)
w2 ( -0.0846398995025724 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2010464126846419) - present_state_Q (-0.2010464126846419)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8967409820261392 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2905992946376696) - present_state_Q ( -0.2905992946376696)) * f1( 0.29860645433588023)
w2 ( -0.07140911219909435 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2905992946376696) - present_state_Q (-0.2905992946376696)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8797392672757329 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2560969356402334) - present_state_Q ( -0.2560969356402334)) * f1( 0.26965993307682445)
w2 ( -0.058799367357570145 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2560969356402334) - present_state_Q (-0.2560969356402334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8669668218802826 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2042284285627629) - present_state_Q ( -0.2042284285627629)) * f1( 0.21877908858980633)
w2 ( -0.047123255643440415 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2042284285627629) - present_state_Q (-0.2042284285627629)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8577871941284958 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15655736862604086) - present_state_Q ( -0.15655736862604086)) * f1( 0.169709744114833)
w2 ( -0.036305223008171676 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.15655736862604086) - present_state_Q (-0.15655736862604086)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8509063927405954 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11974173530173401) - present_state_Q ( -0.12280375302695337)) * f1( 0.13469857001387087)
w2 ( -0.026088631418236075 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11974173530173401) - present_state_Q (-0.12280375302695337)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8436285295924589 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1258684591158754) - present_state_Q ( -0.1258684591158754)) * f1( 0.14179084075703893)
w2 ( -0.015822999154150315 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1258684591158754) - present_state_Q (-0.1258684591158754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8380929507339994 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10139174980630802) - present_state_Q ( -0.10139174980630802)) * f1( 0.11268294848986536)
w2 ( 0.0038271038388767754 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10139174980630802) - present_state_Q (-0.10139174980630802)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8339566378376178 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07358877074859753) - present_state_Q ( -0.07358877074859753)) * f1( 0.08871831155631807)
w2 ( 0.013151701712351532 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07358877074859753) - present_state_Q (-0.07358877074859753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8306567429551611 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0582024905860633) - present_state_Q ( -0.0582024905860633)) * f1( 0.07294483689975563)
w2 ( 0.02219934654290067 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0582024905860633) - present_state_Q (-0.0582024905860633)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9099164491155113 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38481054564181216) - present_state_Q ( -0.38481054564181216)) * f1( 0.47929563799260416)
w2 ( -0.0770208839924415 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.38481054564181216) - present_state_Q (-0.38481054564181216)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.909858911865595 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4458998623825781) - present_state_Q ( -0.4458998623825781)) * f1( 0.43925717836580624)
w2 ( -0.07694229142378228 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4458998623825781) - present_state_Q (-0.4458998623825781)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8795526523483465 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3978079078150771) - present_state_Q ( -0.40602841812144297)) * f1( 0.3955152151329731)
w2 ( -0.030967433783386157 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3978079078150771) - present_state_Q (-0.40602841812144297)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8573886161271072 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3068784247296418) - present_state_Q ( -0.3068784247296418)) * f1( 0.3277779490402012)
w2 ( 0.009604001152014506 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3068784247296418) - present_state_Q (-0.3068784247296418)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8425519400881306 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21015819334703367) - present_state_Q ( -0.21015819334703367)) * f1( 0.2518351538344105)
w2 ( 0.04495254359275433 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.21015819334703367) - present_state_Q (-0.21015819334703367)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8334928225087316 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12259292696755611) - present_state_Q ( -0.12259292696755611)) * f1( 0.17751362973250565)
w2 ( 0.07557256164900236 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12259292696755611) - present_state_Q (-0.12259292696755611)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8276300352258324 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05797602545601877) - present_state_Q ( -0.04963951084415478)) * f1( 0.13209179154293593)
w2 ( 0.11107991431288661 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05797602545601877) - present_state_Q (-0.04963951084415478)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.824696034137331 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010960341210004082) - present_state_Q ( 0.005296938644084086)) * f1( 0.07412854455784372)
w2 ( 0.1348278600415016 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.010960341210004082) - present_state_Q (0.005296938644084086)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8000533849189457 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2645561912091374) - present_state_Q ( -0.2645561912091374)) * f1( 0.38618754309748815)
w2 ( 0.16035188292503055 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2645561912091374) - present_state_Q (-0.2645561912091374)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7802536588976846 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20632332965413674) - present_state_Q ( -0.20632332965413674)) * f1( 0.33805754456191695)
w2 ( 0.18377952279257947 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.20632332965413674) - present_state_Q (-0.20632332965413674)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7658161227996422 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1404761998374789) - present_state_Q ( -0.1404761998374789)) * f1( 0.27425441266988165)
w2 ( 0.20483666598672873 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1404761998374789) - present_state_Q (-0.1404761998374789)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7579860732653283 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05221456467057181) - present_state_Q ( -0.05221456467057181)) * f1( 0.17517159416133146)
w2 ( 0.2227163903148693 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05221456467057181) - present_state_Q (-0.05221456467057181)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7538007763731267 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008272444187862063) - present_state_Q ( 0.008272444187862063)) * f1( 0.10661688227324093)
w2 ( 0.2384185823241063 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.008272444187862063) - present_state_Q (0.008272444187862063)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7497972145464853 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0568614759800774) - present_state_Q ( 0.05660004262719631)) * f1( 0.1146869431247106)
w2 ( 0.259363748622355 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0568614759800774) - present_state_Q (0.05660004262719631)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7470539501981807 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09280290853779659) - present_state_Q ( 0.09099424260872946)) * f1( 0.08618864582441985)
w2 ( 0.27846091151705804 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09280290853779659) - present_state_Q (0.09099424260872946)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7390884320891955 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07226793270714316) - present_state_Q ( -0.07226793270714316)) * f1( 0.17128631068292877)
w2 ( 0.2877617343057866 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07226793270714316) - present_state_Q (-0.07226793270714316)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7337707530218004 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03376580944900102) - present_state_Q ( -0.03376580944900102)) * f1( 0.12355511511934715)
w2 ( 0.29636951887586865 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03376580944900102) - present_state_Q (-0.03376580944900102)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7321129337608882 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019898081792588805) - present_state_Q ( -0.028272503852571667)) * f1( 0.038530431658853115)
w2 ( 0.29636951887586865 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.019898081792588805) - present_state_Q (-0.028272503852571667)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7256764087758871 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01453998324045061) - present_state_Q ( -0.029140799147385543)) * f1( 0.28269205600525804)
w2 ( 0.31003072692526906 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.01453998324045061) - present_state_Q (-0.029140799147385543)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7175049787366162 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1219831479112774) - present_state_Q ( 0.11230195273271779)) * f1( 0.27247513051456584)
w2 ( 0.34002036313111006 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1219831479112774) - present_state_Q (0.11230195273271779)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7211447663069696 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15506053320373717) - present_state_Q ( 0.16321235652340732)) * f1( 0.24642059894695995)
w2 ( 0.3252497328108067 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.15506053320373717) - present_state_Q (0.16321235652340732)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7165145865022231 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18322323733007267) - present_state_Q ( 0.18322323733007267)) * f1( 0.19694588675733055)
w2 ( 0.34875964145110017 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18322323733007267) - present_state_Q (0.18322323733007267)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7139461342167339 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3764634253029112) - present_state_Q ( 0.25044954481667714)) * f1( 0.13720599480652448)
w2 ( 0.3674793212224616 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3764634253029112) - present_state_Q (0.25044954481667714)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.716331898530739 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42010059635955893) - present_state_Q ( 0.4173004533789911)) * f1( 0.13610354013480375)
w2 ( 0.34293866609843665 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.42010059635955893) - present_state_Q (0.4173004533789911)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7001186400955001 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14890016107754886) - present_state_Q ( -0.14890016107754886)) * f1( 0.30361330375392104)
w2 ( 0.3536188689978325 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14890016107754886) - present_state_Q (-0.14890016107754886)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6866552353659213 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11611349965994504) - present_state_Q ( -0.11611349965994504)) * f1( 0.26686516078764294)
w2 ( 0.3637089119917115 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11611349965994504) - present_state_Q (-0.11611349965994504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6641492546350959 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1813714821379088) - present_state_Q ( -0.1951044227255598)) * f1( 0.39007378277858146)
w2 ( 0.3752482574819469 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1813714821379088) - present_state_Q (-0.1951044227255598)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6489904461600124 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12232177790710315) - present_state_Q ( -0.12232177790710315)) * f1( 0.29717932833024785)
w2 ( 0.38545004948427475 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12232177790710315) - present_state_Q (-0.12232177790710315)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6325771619230052 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12922456010778488) - present_state_Q ( -0.12922456010778488)) * f1( 0.3179007814758674)
w2 ( 0.39577609156621485 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12922456010778488) - present_state_Q (-0.12922456010778488)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6033240989270825 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22704386653247477) - present_state_Q ( -0.22704386653247477)) * f1( 0.48405017328619127)
w2 ( 0.4078628811637994 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22704386653247477) - present_state_Q (-0.22704386653247477)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5805483122847462 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16780326591100042) - present_state_Q ( -0.16780326591100042)) * f1( 0.41333645148144454)
w2 ( 0.4188833399501974 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.16780326591100042) - present_state_Q (-0.16780326591100042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5628555227867423 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11883630830921678) - present_state_Q ( -0.11883630830921678)) * f1( 0.3490027823901054)
w2 ( 0.4290223934997633 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11883630830921678) - present_state_Q (-0.11883630830921678)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.549191008094232 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07782283959547542) - present_state_Q ( -0.07782283959547542)) * f1( 0.2907092702676102)
w2 ( 0.43842320461248185 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07782283959547542) - present_state_Q (-0.07782283959547542)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5373322240019788 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04545358013024435) - present_state_Q ( -0.056437018358046556)) * f1( 0.26242538052592085)
w2 ( 0.4474610378193823 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04545358013024435) - present_state_Q (-0.056437018358046556)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.530035885444643 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007001610699796254) - present_state_Q ( -0.007001610699796254)) * f1( 0.17957943699151266)
w2 ( 0.45558706681197864 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.007001610699796254) - present_state_Q (-0.007001610699796254)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5302313679004684 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015155691235105045) - present_state_Q ( 0.015155691235105045)) * f1( 0.1433143004337659)
w2 ( 0.45531426436974676 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.015155691235105045) - present_state_Q (0.015155691235105045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.799347000474445 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1426432584823413) - present_state_Q ( -0.1447439454988387)) * f1( 0.35963270269881426)
w2 ( 0.22158870366189282 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1426432584823413) - present_state_Q (-0.1447439454988387)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8021078272154631 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12840053283303268) - present_state_Q ( -0.12840053283303268)) * f1( 0.3269590739379072)
w2 ( 0.2165223324348766 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12840053283303268) - present_state_Q (-0.12840053283303268)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7865794201974264 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08106772126895612) - present_state_Q ( -0.0869160982963077)) * f1( 0.3243129606991499)
w2 ( 0.25482707852842956 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08106772126895612) - present_state_Q (-0.0869160982963077)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.7784394999788915 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09953339676874023) - present_state_Q ( 0.09953339676874023)) * f1( 0.26222285019051933)
w2 ( 0.2920774716774056 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09953339676874023) - present_state_Q (0.09953339676874023)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7729841015236135 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17522503384144392) - present_state_Q ( 0.17522503384144392)) * f1( 0.225152927332433)
w2 ( 0.3211531680225297 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17522503384144392) - present_state_Q (0.17522503384144392)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7709168978548653 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27843461053258955) - present_state_Q ( 0.27843461053258955)) * f1( 0.1383588496628077)
w2 ( 0.33908223008501004 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.27843461053258955) - present_state_Q (0.27843461053258955)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7702317303330917 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37359815115604345) - present_state_Q ( 0.3809979660583517)) * f1( 0.12156583455549821)
w2 ( 0.3469728889530254 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.37359815115604345) - present_state_Q (0.3809979660583517)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7390282543129089 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2928158937533243) - present_state_Q ( -0.2928158937533243)) * f1( 0.4702616852557984)
w2 ( 0.36024357504058524 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2928158937533243) - present_state_Q (-0.2928158937533243)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7133827604629451 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23698639109806302) - present_state_Q ( -0.23698639109806302)) * f1( 0.41816412877677717)
w2 ( 0.37250933008035036 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23698639109806302) - present_state_Q (-0.23698639109806302)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6934770391545783 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13504883583353527) - present_state_Q ( -0.20955070184960536)) * f1( 0.5026115764965555)
w2 ( 0.38835116281100046 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13504883583353527) - present_state_Q (-0.20955070184960536)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6898387228691699 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09594651949677713) - present_state_Q ( -0.08815499764599993)) * f1( 0.46312376214234163)
w2 ( 0.3930647835527798 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.09594651949677713) - present_state_Q (-0.08815499764599993)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6779819459879775 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06425130810109334) - present_state_Q ( -0.07206690817563147)) * f1( 0.44634458475549316)
w2 ( 0.40900329019471116 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06425130810109334) - present_state_Q (-0.07206690817563147)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6696857461574128 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016449164772359193) - present_state_Q ( -0.016449164772359193)) * f1( 0.38622140373901537)
w2 ( 0.42189154509241855 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.016449164772359193) - present_state_Q (-0.016449164772359193)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.66320181772624 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024736923690975493) - present_state_Q ( 0.01789103078595472)) * f1( 0.35127505344006715)
w2 ( 0.4329665047874071 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.024736923690975493) - present_state_Q (0.01789103078595472)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6514592707964059 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.060572794922483575) - present_state_Q ( -0.012758569258333713)) * f1( 0.28037494199096424)
w2 ( 0.4497191387374304 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.060572794922483575) - present_state_Q (-0.012758569258333713)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6448392434372928 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10789612640411006) - present_state_Q ( 0.12100692161245086)) * f1( 0.22844799099730384)
w2 ( 0.46710610019910803 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10789612640411006) - present_state_Q (0.12100692161245086)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6398432170467082 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16653494611048836) - present_state_Q ( 0.1564510631447616)) * f1( 0.19200536914398164)
w2 ( 0.48271824608708525 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.16653494611048836) - present_state_Q (0.1564510631447616)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.636185025009463 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19981690152194792) - present_state_Q ( 0.11606937973167476)) * f1( 0.12036998541400036)
w2 ( 0.49487473850390606 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.19981690152194792) - present_state_Q (0.11606937973167476)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6343972376465004 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24191660363456088) - present_state_Q ( 0.15560383755290877)) * f1( 0.06656248761596328)
w2 ( 0.505618251416328 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24191660363456088) - present_state_Q (0.15560383755290877)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6337830946437312 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2801664427333773) - present_state_Q ( 0.2774881091564225)) * f1( 0.040799108440943053)
w2 ( 0.5146499635233429 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2801664427333773) - present_state_Q (0.2774881091564225)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6326149508643345 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08033743489783803) - present_state_Q ( 0.08033743489783803)) * f1( 0.035647144895100936)
w2 ( 0.5212038896951818 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08033743489783803) - present_state_Q (0.08033743489783803)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6217584219409753 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06097824049337672) - present_state_Q ( -0.06097824049337672)) * f1( 0.4259459818382251)
w2 ( 0.5313991063529434 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06097824049337672) - present_state_Q (-0.06097824049337672)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6082822931797084 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0020951028167187635) - present_state_Q ( 0.0020951028167187635)) * f1( 0.33849889651263687)
w2 ( 0.5473236826515415 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0020951028167187635) - present_state_Q (0.0020951028167187635)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5984859990853343 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05075211481280739) - present_state_Q ( 0.05075211481280739)) * f1( 0.2764791284136946)
w2 ( 0.5614966065182805 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05075211481280739) - present_state_Q (0.05075211481280739)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5914635220246679 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16117322608895332) - present_state_Q ( 0.16770757454539456)) * f1( 0.28269732228347416)
w2 ( 0.5764011914020906 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.16117322608895332) - present_state_Q (0.16770757454539456)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5867280181917057 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20561082110443113) - present_state_Q ( 0.211723316625445)) * f1( 0.22675514756464699)
w2 ( 0.5889314573311905 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20561082110443113) - present_state_Q (0.211723316625445)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5838312936431569 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24769042818330478) - present_state_Q ( 0.25389462934110846)) * f1( 0.1695235986243753)
w2 ( 0.5991839221398237 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24769042818330478) - present_state_Q (0.25389462934110846)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5819440775933558 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2913276063144638) - present_state_Q ( 0.2837323270181673)) * f1( 0.12979438939092433)
w2 ( 0.6079079481566205 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2913276063144638) - present_state_Q (0.2837323270181673)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6819059403471124 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09520233464066535) - present_state_Q ( 0.08437745821431464)) * f1( 0.48177706668847586)
w2 ( 0.4834165146716056 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.09520233464066535) - present_state_Q (0.08437745821431464)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7196568898109825 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0036060206673000916) - present_state_Q ( 0.005711607558387566)) * f1( 0.4169758384856398)
w2 ( 0.42909545434210616 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.0036060206673000916) - present_state_Q (0.005711607558387566)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.726687030200113 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01689579376827166) - present_state_Q ( -0.018062516109106808)) * f1( 0.38284881672811566)
w2 ( 0.4180778305460429 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.01689579376827166) - present_state_Q (-0.018062516109106808)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7203665810120108 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010176237189389037) - present_state_Q ( 0.010176237189389037)) * f1( 0.33118860133221534)
w2 ( 0.4295283137378159 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.010176237189389037) - present_state_Q (0.010176237189389037)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7112970417960129 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0659315609953291) - present_state_Q ( 0.0659315609953291)) * f1( 0.2662330989562699)
w2 ( 0.4499680094440682 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0659315609953291) - present_state_Q (0.0659315609953291)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7054205276637273 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12507372749851617) - present_state_Q ( 0.12473097929371732)) * f1( 0.20420417608650568)
w2 ( 0.4672345930514362 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12507372749851617) - present_state_Q (0.12473097929371732)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7019034535841154 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17011859345618013) - present_state_Q ( 0.1769791609471042)) * f1( 0.14652479029222387)
w2 ( 0.4816365549553471 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17011859345618013) - present_state_Q (0.1769791609471042)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7001860147668694 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21698584349158473) - present_state_Q ( 0.22705078224337383)) * f1( 0.08823314718512451)
w2 ( 0.4933154230816942 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21698584349158473) - present_state_Q (0.22705078224337383)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6796535322634718 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23507662902604745) - present_state_Q ( -0.23507662902604745)) * f1( 0.33573453920572444)
w2 ( 0.4933154230816942 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.23507662902604745) - present_state_Q (-0.23507662902604745)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6615775920675396 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11189530053805315) - present_state_Q ( -0.2064029004438764)) * f1( 0.30368840982329076)
w2 ( 0.4933154230816942 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11189530053805315) - present_state_Q (-0.2064029004438764)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6503753933182199 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07050364305522742) - present_state_Q ( -0.0636486127901499)) * f1( 0.24534037934875905)
w2 ( 0.5024473880513868 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07050364305522742) - present_state_Q (-0.0636486127901499)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6411938990085034 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030492537604388845) - present_state_Q ( -0.037088391585088074)) * f1( 0.21153609224580616)
w2 ( 0.5111281708078798 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.030492537604388845) - present_state_Q (-0.037088391585088074)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6349944731691556 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008884614547529035) - present_state_Q ( 0.00245824942257708)) * f1( 0.15559627889983366)
w2 ( 0.5190967750485233 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.008884614547529035) - present_state_Q (0.00245824942257708)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6307452441861241 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.031259972357370894) - present_state_Q ( 0.031259972357370894)) * f1( 0.11426773888314574)
w2 ( 0.5265340955460905 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.031259972357370894) - present_state_Q (0.031259972357370894)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.614533176513513 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07076520729784361) - present_state_Q ( -0.025678853721105926)) * f1( 0.37462429422585636)
w2 ( 0.5438443105241262 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07076520729784361) - present_state_Q (-0.025678853721105926)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.601215084445894 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12143066105181685) - present_state_Q ( 0.012661798946991598)) * f1( 0.3333846456020489)
w2 ( 0.5598235612104538 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12143066105181685) - present_state_Q (0.012661798946991598)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5968996586007078 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37719501583961673) - present_state_Q ( 0.2816349239183985)) * f1( 0.2764799642430193)
w2 ( 0.5723103274236989 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.37719501583961673) - present_state_Q (0.2816349239183985)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5967593919829651 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42434199319634036) - present_state_Q ( 0.4362793444186319)) * f1( 0.227895896814483)
w2 ( 0.5729258129137992 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.42434199319634036) - present_state_Q (0.4362793444186319)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5970157660133435 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.46637871798878694) - present_state_Q ( 0.46021143875829107)) * f1( 0.188877419726853)
w2 ( 0.5715684562178579 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.46637871798878694) - present_state_Q (0.46021143875829107)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5976136311428818 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4917325843189993) - present_state_Q ( 0.49711201018698575)) * f1( 0.12471437149485265)
w2 ( 0.5667745810423493 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4917325843189993) - present_state_Q (0.49711201018698575)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5891360241219248 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010405804481858766) - present_state_Q ( -0.010405804481858766)) * f1( 0.20709152910994935)
w2 ( 0.5749618855230229 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.010405804481858766) - present_state_Q (-0.010405804481858766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5807129688961787 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09850947119236239) - present_state_Q ( -0.0047087306634480824)) * f1( 0.2031807644872178)
w2 ( 0.5832530790786765 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09850947119236239) - present_state_Q (-0.0047087306634480824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.576465656572205 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14252982422408694) - present_state_Q ( 0.14252982422408694)) * f1( 0.15631028110138867)
w2 ( 0.5941220054066094 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14252982422408694) - present_state_Q (0.14252982422408694)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5734928315658256 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17234535176749738) - present_state_Q ( 0.1686963567158594)) * f1( 0.11961240823398084)
w2 ( 0.6040635325450451 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17234535176749738) - present_state_Q (0.1686963567158594)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.541306195684155 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28223842919782) - present_state_Q ( -0.28223842919782)) * f1( 0.49213941947140905)
w2 ( 0.6040635325450451 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.28223842919782) - present_state_Q (-0.28223842919782)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5168871568098248 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13336782976691783) - present_state_Q ( -0.13336782976691783)) * f1( 0.46956886564852446)
w2 ( 0.6144641534808496 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13336782976691783) - present_state_Q (-0.13336782976691783)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.49670492073555733 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09284071054051814) - present_state_Q ( -0.09284071054051814)) * f1( 0.41737067441987463)
w2 ( 0.6241352862705789 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09284071054051814) - present_state_Q (-0.09284071054051814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.48484001047606445 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01897485052944002) - present_state_Q ( -0.01710919338077309)) * f1( 0.28575567647829864)
w2 ( 0.6324395204371355 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.01897485052944002) - present_state_Q (-0.01710919338077309)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.47534634353997385 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002074586820544802) - present_state_Q ( 0.008868420128789578)) * f1( 0.24259442582543247)
w2 ( 0.6402663012082007 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.002074586820544802) - present_state_Q (0.008868420128789578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.46813805668064484 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02819805125923007) - present_state_Q ( 0.0349186557965334)) * f1( 0.19592999022885021)
w2 ( 0.6476243241947885 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.02819805125923007) - present_state_Q (0.0349186557965334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.462654908446947 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05620090110036663) - present_state_Q ( 0.05608761527267439)) * f1( 0.1568709241179699)
w2 ( 0.6546149736915358 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05620090110036663) - present_state_Q (0.05608761527267439)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4563651285570678 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05585118370351541) - present_state_Q ( 0.049256805588589206)) * f1( 0.17651642219436794)
w2 ( 0.661741539947171 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05585118370351541) - present_state_Q (0.049256805588589206)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4513938429631955 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06577783468807044) - present_state_Q ( 0.06577783468807044)) * f1( 0.14587107808137276)
w2 ( 0.6685575389227857 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06577783468807044) - present_state_Q (0.06577783468807044)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4349844454674335 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10292589497721116) - present_state_Q ( -0.11188676751828089)) * f1( 0.544088669199821)
w2 ( 0.6745894224831969 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10292589497721116) - present_state_Q (-0.11188676751828089)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.43117222991230747 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28741709177175) - present_state_Q ( 0.28741709177175)) * f1( 0.2697488678982038)
w2 ( 0.6830688995275224 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.28741709177175) - present_state_Q (0.28741709177175)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.43363468004123934 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7341291906171672) - present_state_Q ( 0.5975154107116626)) * f1( 0.1984206840808365)
w2 ( 0.6706586503625278 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.7341291906171672) - present_state_Q (0.5975154107116626)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43695379179957283 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7483271415861161) - present_state_Q ( 0.7530599279924644)) * f1( 0.11929500758022753)
w2 ( 0.6372713847024655 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.7483271415861161) - present_state_Q (0.7530599279924644)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.45267459899881823 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08472614995803454) - present_state_Q ( -0.08472614995803454)) * f1( 0.4855900804171373)
w2 ( 0.6307964554017101 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08472614995803454) - present_state_Q (-0.08472614995803454)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.44062589507145056 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07127034175372143) - present_state_Q ( -0.0763950853276954)) * f1( 0.44746132620656776)
w2 ( 0.6361818164247566 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07127034175372143) - present_state_Q (-0.0763950853276954)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.43210441079629375 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03497390193236341) - present_state_Q ( -0.03497390193236341)) * f1( 0.36813602430472503)
w2 ( 0.6408113466595391 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03497390193236341) - present_state_Q (-0.03497390193236341)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.42237424606069246 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11466568953162146) - present_state_Q ( 0.11466568953162146)) * f1( 0.3278347676922377)
w2 ( 0.6526833818364007 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11466568953162146) - present_state_Q (0.11466568953162146)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41523032612975547 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1479834298592601) - present_state_Q ( 0.1479834298592601)) * f1( 0.26774814972750466)
w2 ( 0.6633559783614674 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1479834298592601) - present_state_Q (0.1479834298592601)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41026608867566633 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17904478636096222) - present_state_Q ( 0.17904478636096222)) * f1( 0.20783068950666572)
w2 ( 0.6729103660524728 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17904478636096222) - present_state_Q (0.17904478636096222)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40698144041388495 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2114875966265033) - present_state_Q ( 0.20640981843067685)) * f1( 0.15296006597299433)
w2 ( 0.6814999237017517 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2114875966265033) - present_state_Q (0.20640981843067685)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4049350796410747 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22954039793617478) - present_state_Q ( 0.22954039793617478)) * f1( 0.10580229776752459)
w2 ( 0.6892364693760494 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22954039793617478) - present_state_Q (0.22954039793617478)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.41866167899763906 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.033499201711158944) - present_state_Q ( 0.02560984183577003)) * f1( 0.617592198078564)
w2 ( 0.6803460725094632 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.033499201711158944) - present_state_Q (0.02560984183577003)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4691182135025413 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04822272763761007) - present_state_Q ( 0.04822272763761007)) * f1( 0.534836868524186)
w2 ( 0.6426100543145092 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.04822272763761007) - present_state_Q (0.04822272763761007)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4761371128111726 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1687038138758444) - present_state_Q ( 0.1687038138758444)) * f1( 0.4622762716751487)
w2 ( 0.6335000483652137 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.1687038138758444) - present_state_Q (0.1687038138758444)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4745882279393207 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1810545473366828) - present_state_Q ( 0.1810545473366828)) * f1( 0.41804235865433015)
w2 ( 0.6357231028090328 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1810545473366828) - present_state_Q (0.1810545473366828)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4714979680725341 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33999249547547594) - present_state_Q ( 0.3445745216895554)) * f1( 0.3455710675121083)
w2 ( 0.6428770810376722 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.33999249547547594) - present_state_Q (0.3445745216895554)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.46980304552174124 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3741597053629537) - present_state_Q ( 0.37854808916804744)) * f1( 0.28791974696528555)
w2 ( 0.647586511547132 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3741597053629537) - present_state_Q (0.37854808916804744)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.46862677806605274 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.40032902083404764) - present_state_Q ( 0.3950953730722061)) * f1( 0.2617561493858145)
w2 ( 0.6511815138680279 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.40032902083404764) - present_state_Q (0.3950953730722061)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.46833951776358784 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42829955283001114) - present_state_Q ( 0.42829955283001114)) * f1( 0.19769604000596158)
w2 ( 0.6523439460642672 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.42829955283001114) - present_state_Q (0.42829955283001114)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4684954600815271 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45150402372315473) - present_state_Q ( 0.4562853579997566)) * f1( 0.14004754321151705)
w2 ( 0.6514531496140719 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.45150402372315473) - present_state_Q (0.4562853579997566)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4374010072373562 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14453000625731766) - present_state_Q ( -0.14453000625731766)) * f1( 0.5866025598888579)
w2 ( 0.6620546897267036 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14453000625731766) - present_state_Q (-0.14453000625731766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4021884880875699 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14395377665158396) - present_state_Q ( -0.15335937894086035)) * f1( 0.653337125790219)
w2 ( 0.6728339697522177 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.14395377665158396) - present_state_Q (-0.15335937894086035)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3718356279058481 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11005128130920122) - present_state_Q ( -0.11005128130920122)) * f1( 0.6082174962859286)
w2 ( 0.6828148928157832 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11005128130920122) - present_state_Q (-0.11005128130920122)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3461214983915426 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06996245160618822) - present_state_Q ( -0.06996245160618822)) * f1( 0.5554213062704116)
w2 ( 0.6920742169446946 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06996245160618822) - present_state_Q (-0.06996245160618822)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.325024793774132 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03190782409039858) - present_state_Q ( -0.03190782409039858)) * f1( 0.49208924689983746)
w2 ( 0.7006485577783218 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03190782409039858) - present_state_Q (-0.03190782409039858)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3052045610732005 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015495941184852063) - present_state_Q ( -0.015495941184852063)) * f1( 0.4788116344400011)
w2 ( 0.7089274847196492 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.015495941184852063) - present_state_Q (-0.015495941184852063)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2793282737829182 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03952791040157977) - present_state_Q ( -0.03952791040157977)) * f1( 0.5940717488229911)
w2 ( 0.7176389871068776 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03952791040157977) - present_state_Q (-0.03952791040157977)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2631821117616086 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12846367855245538) - present_state_Q ( 0.12846367855245538)) * f1( 0.5677617741394357)
w2 ( 0.7290142946789891 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12846367855245538) - present_state_Q (0.12846367855245538)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.25016280096826554 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15847417443043024) - present_state_Q ( 0.15847417443043024)) * f1( 0.5058533140799345)
w2 ( 0.7393092243994936 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.15847417443043024) - present_state_Q (0.15847417443043024)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24003540971062964 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1865645108273478) - present_state_Q ( 0.1865645108273478)) * f1( 0.43635256125189087)
w2 ( 0.7485929020097092 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1865645108273478) - present_state_Q (0.1865645108273478)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2321771103274294 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2100292993946538) - present_state_Q ( 0.2100292993946538)) * f1( 0.37247780032543476)
w2 ( 0.7570318472315016 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2100292993946538) - present_state_Q (0.2100292993946538)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2262061093931361 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22831193091637625) - present_state_Q ( 0.23066900626761044)) * f1( 0.3107271536080755)
w2 ( 0.7647183347044627 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22831193091637625) - present_state_Q (0.23066900626761044)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.22120398189853865 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24638183474507402) - present_state_Q ( 0.2434409254044611)) * f1( 0.27605977860127096)
w2 ( 0.7719662250272645 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24638183474507402) - present_state_Q (0.2434409254044611)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21767136778799018 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26136223775791356) - present_state_Q ( 0.26136223775791356)) * f1( 0.21439149442953867)
w2 ( 0.7785571844679796 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.26136223775791356) - present_state_Q (0.26136223775791356)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2148937361947912 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27237996140936327) - present_state_Q ( 0.27237996140936327)) * f1( 0.17936632077332276)
w2 ( 0.7847515058572425 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.27237996140936327) - present_state_Q (0.27237996140936327)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21297551336952206 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2874446052948199) - present_state_Q ( 0.28518648417049897)) * f1( 0.13362007976989165)
w2 ( 0.7904938249116018 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2874446052948199) - present_state_Q (0.28518648417049897)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21155569354322382 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29388215491039155) - present_state_Q ( 0.29388215491039155)) * f1( 0.10477906450931287)
w2 ( 0.7959140673348277 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.29388215491039155) - present_state_Q (0.29388215491039155)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.20964943895087726 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14642706047238127) - present_state_Q ( 0.14426729259175713)) * f1( 0.0705039917640456)
w2 ( 0.8013215756039374 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14642706047238127) - present_state_Q (0.14426729259175713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2229406130137036 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18600938914343132) - present_state_Q ( 0.025745074022643838)) * f1( 0.6416389272077313)
w2 ( 0.7971786929017713 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18600938914343132) - present_state_Q (0.025745074022643838)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30425333265122567 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5016141920907786) - present_state_Q ( 0.5038256410849039)) * f1( 0.6006860366364994)
w2 ( 0.6888855551517052 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.5016141920907786) - present_state_Q (0.5038256410849039)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.43246752478968215 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3788142644627608) - present_state_Q ( 0.3848992447396773)) * f1( 0.5462855507065792)
w2 ( 0.5011241296882332 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.3788142644627608) - present_state_Q (0.3848992447396773)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4507282458183822 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18210099179997313) - present_state_Q ( 0.18643115455210058)) * f1( 0.4959173507948054)
w2 ( 0.4716664452584649 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18210099179997313) - present_state_Q (0.18643115455210058)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.43964935341973793 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17296005167377995) - present_state_Q ( 0.17296005167377995)) * f1( 0.4534286600164452)
w2 ( 0.4912133215379528 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17296005167377995) - present_state_Q (0.17296005167377995)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.42790969622221164 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.215930958291226) - present_state_Q ( 0.12228554454239601)) * f1( 0.39222723072162247)
w2 ( 0.5091717746151564 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.215930958291226) - present_state_Q (0.12228554454239601)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4186019094868024 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2502252503843916) - present_state_Q ( 0.15693577483611013)) * f1( 0.3471930906090836)
w2 ( 0.5252569796272961 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2502252503843916) - present_state_Q (0.15693577483611013)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4116307254162366 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2886527906397505) - present_state_Q ( 0.19197067459214742)) * f1( 0.29427365330285)
w2 ( 0.5394706558956058 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2886527906397505) - present_state_Q (0.19197067459214742)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.40894981282929177 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3263040591078748) - present_state_Q ( 0.3270523463485861)) * f1( 0.25392705625219025)
w2 ( 0.547916900660582 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3263040591078748) - present_state_Q (0.3270523463485861)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4088921611361405 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44197181657040524) - present_state_Q ( 0.44197181657040524)) * f1( 0.25906622467241835)
w2 ( 0.5481394371692455 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.44197181657040524) - present_state_Q (0.44197181657040524)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.39357056127209566 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049476726283086886) - present_state_Q ( 0.04513882177806822)) * f1( 0.4258260970467903)
w2 ( 0.5625317912032551 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.049476726283086886) - present_state_Q (0.04513882177806822)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3765246887425716 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06635511375271541) - present_state_Q ( -0.0382805816714685)) * f1( 0.3831255554906017)
w2 ( 0.5714301130641899 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06635511375271541) - present_state_Q (-0.0382805816714685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3660516085133698 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09700525743415542) - present_state_Q ( 0.10088043940378641)) * f1( 0.3391320931658529)
w2 ( 0.583782916517775 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09700525743415542) - present_state_Q (0.10088043940378641)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3571033937947553 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.128465994892156) - present_state_Q ( 0.12120165662749488)) * f1( 0.30681878556890163)
w2 ( 0.5954487142322439 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.128465994892156) - present_state_Q (0.12120165662749488)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.34102202292329564 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015036864110396383) - present_state_Q ( -0.018595019639852065)) * f1( 0.3855599383226108)
w2 ( 0.6037905408968202 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.015036864110396383) - present_state_Q (-0.018595019639852065)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30766435639499295 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10802625181699523) - present_state_Q ( -0.10802625181699523)) * f1( 0.6708785492361546)
w2 ( 0.6137350134295261 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10802625181699523) - present_state_Q (-0.10802625181699523)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2777862115205092 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06784913011777477) - present_state_Q ( -0.07400185877001247)) * f1( 0.6394918922727691)
w2 ( 0.6230793523446908 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06784913011777477) - present_state_Q (-0.07400185877001247)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26017910520198795 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09115162096682858) - present_state_Q ( 0.09401225001718228)) * f1( 0.5587732021365434)
w2 ( 0.6356834688278709 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.09115162096682858) - present_state_Q (0.09401225001718228)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2458018051585114 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12163587923638144) - present_state_Q ( 0.12431884806681989)) * f1( 0.4994810761741967)
w2 ( 0.6471972584221436 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12163587923638144) - present_state_Q (0.12431884806681989)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23378143145718183 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14543463265233653) - present_state_Q ( 0.1480205387513963)) * f1( 0.45100712155458483)
w2 ( 0.6578581754026971 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14543463265233653) - present_state_Q (0.1480205387513963)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.22078799698261553 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14893072368448285) - present_state_Q ( 0.14893072368448285)) * f1( 0.48854413186153567)
w2 ( 0.6684966693500557 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14893072368448285) - present_state_Q (0.14893072368448285)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.20982508444034398 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17198686368901633) - present_state_Q ( 0.1696290317535567)) * f1( 0.4428213368599191)
w2 ( 0.6783994555346695 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17198686368901633) - present_state_Q (0.1696290317535567)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.19829260510138558 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17249369904563722) - present_state_Q ( 0.17249369904563722)) * f1( 0.47118333554794556)
w2 ( 0.6881896823690266 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17249369904563722) - present_state_Q (0.17249369904563722)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1888640869630132 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.192743747190461) - present_state_Q ( 0.192743747190461)) * f1( 0.41621383568465165)
w2 ( 0.6972509074701699 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.192743747190461) - present_state_Q (0.192743747190461)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18268233339764747 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21921933233978222) - present_state_Q ( 0.2208391418039779)) * f1( 0.30742330168604615)
w2 ( 0.7052942191273699 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21921933233978222) - present_state_Q (0.2208391418039779)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1779057310753051 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23407812559261448) - present_state_Q ( 0.2356442911469501)) * f1( 0.2543945856156682)
w2 ( 0.7128047599838624 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.23407812559261448) - present_state_Q (0.2356442911469501)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17138623444641585 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2457835833087382) - present_state_Q ( 0.10613794292244083)) * f1( 0.2047320727341509)
w2 ( 0.719173568292031 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2457835833087382) - present_state_Q (0.10613794292244083)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16908460652153467 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2617902583018808) - present_state_Q ( 0.2634314426193086)) * f1( 0.1414231707452671)
w2 ( 0.7256834716204662 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2617902583018808) - present_state_Q (0.2634314426193086)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1670017358383348 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2698807847848409) - present_state_Q ( 0.26810703333577324)) * f1( 0.13109623500581724)
w2 ( 0.7320387134261747 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2698807847848409) - present_state_Q (0.26810703333577324)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16492276592848634 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2689449358004064) - present_state_Q ( 0.2706013567894137)) * f1( 0.1330173514038226)
w2 ( 0.7382904388977998 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2689449358004064) - present_state_Q (0.2706013567894137)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1615851996623495 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1283117576179201) - present_state_Q ( 0.1283117576179201)) * f1( 0.11730539475689361)
w2 ( 0.7439808272606772 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1283117576179201) - present_state_Q (0.1283117576179201)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.18423512960750368 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4880659376185883) - present_state_Q ( 0.19047360671431737)) * f1( 0.6629241069961246)
w2 ( 0.7303141467425788 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.4880659376185883) - present_state_Q (0.19047360671431737)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2107293322544428 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.468521149701737) - present_state_Q ( 0.4684825734555776)) * f1( 0.6283749694486619)
w2 ( 0.6965837100637464 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.468521149701737) - present_state_Q (0.4684825734555776)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.34656527244198176 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4376950934622367) - present_state_Q ( 0.4376950934622367)) * f1( 0.5674192259309434)
w2 ( 0.5050696633344653 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.4376950934622367) - present_state_Q (0.4376950934622367)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.45817225796402794 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2287067762849938) - present_state_Q ( 0.2287067762849938)) * f1( 0.5059622770251266)
w2 ( 0.3286027754419458 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2287067762849938) - present_state_Q (0.2287067762849938)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.4604643362632386 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.056552882335722254) - present_state_Q ( 0.056552882335722254)) * f1( 0.45033136430978277)
w2 ( 0.32453096791377384 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.056552882335722254) - present_state_Q (0.056552882335722254)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.44938891027931116 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1393203394287736) - present_state_Q ( 0.13902176091500032)) * f1( 0.40287421280922275)
w2 ( 0.3520219952165615 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1393203394287736) - present_state_Q (0.13902176091500032)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4414729680311275 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19914037991415584) - present_state_Q ( 0.19432851395516693)) * f1( 0.3509064813445941)
w2 ( 0.3745805476201864 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.19914037991415584) - present_state_Q (0.19432851395516693)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43631964954681646 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24183171676060178) - present_state_Q ( 0.24652371252114882)) * f1( 0.29006721673162217)
w2 ( 0.3923464935356775 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24183171676060178) - present_state_Q (0.24652371252114882)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43287733349313784 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2822628771159178) - present_state_Q ( 0.2864262224934774)) * f1( 0.2427584252788392)
w2 ( 0.40652650005748897 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2822628771159178) - present_state_Q (0.2864262224934774)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4306330240925759 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3242912803486975) - present_state_Q ( 0.32006532377195973)) * f1( 0.19973597505746932)
w2 ( 0.41776288048377996 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3242912803486975) - present_state_Q (0.32006532377195973)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4295422774328347 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3531060029747567) - present_state_Q ( 0.35744190741668236)) * f1( 0.14007512125714267)
w2 ( 0.4255497497718593 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3531060029747567) - present_state_Q (0.35744190741668236)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4173164533244962 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0340849289339529) - present_state_Q ( 0.029696882156326154)) * f1( 0.3271459531114267)
w2 ( 0.44049821420134205 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0340849289339529) - present_state_Q (0.029696882156326154)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.375592127207666 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1996619817553065) - present_state_Q ( -0.20800664965440382)) * f1( 0.7095485695226745)
w2 ( 0.45225902323091954 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1996619817553065) - present_state_Q (-0.20800664965440382)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3421074415065633 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1461549740429399) - present_state_Q ( -0.1461549740429399)) * f1( 0.6299567044926456)
w2 ( 0.46288981276369245 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1461549740429399) - present_state_Q (-0.1461549740429399)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3211877017513296 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06726961507024654) - present_state_Q ( 0.0670435976418989)) * f1( 0.6158600031863806)
w2 ( 0.4832708145956 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06726961507024654) - present_state_Q (0.0670435976418989)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.30528261579678456 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1167489202004722) - present_state_Q ( 0.1167489202004722)) * f1( 0.5392907873259527)
w2 ( 0.5009663729047745 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1167489202004722) - present_state_Q (0.1167489202004722)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2925367966265578 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15249293868673247) - present_state_Q ( 0.15249293868673247)) * f1( 0.48508128990452665)
w2 ( 0.516731754215691 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.15249293868673247) - present_state_Q (0.15249293868673247)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2820987204938174 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18116977854357103) - present_state_Q ( 0.18116977854357103)) * f1( 0.4405232964602178)
w2 ( 0.5309485861743382 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18116977854357103) - present_state_Q (0.18116977854357103)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.27325116170619856 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2034809487269338) - present_state_Q ( 0.2034809487269338)) * f1( 0.40797137532636)
w2 ( 0.5439606149430838 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2034809487269338) - present_state_Q (0.2034809487269338)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2663722516541783 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22930069642102252) - present_state_Q ( 0.22930069642102252)) * f1( 0.3552617011348855)
w2 ( 0.5555783773363486 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22930069642102252) - present_state_Q (0.22930069642102252)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.26162278770365005 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25796595257468047) - present_state_Q ( 0.25796595257468047)) * f1( 0.2829914653610143)
w2 ( 0.5656482158973158 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25796595257468047) - present_state_Q (0.25796595257468047)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.25830948663506376 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2758783163035673) - present_state_Q ( 0.2804684203669114)) * f1( 0.22521168621679713)
w2 ( 0.5744753805731225 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2758783163035673) - present_state_Q (0.2804684203669114)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2558102081907918 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2963504587349841) - present_state_Q ( 0.2962756434171945)) * f1( 0.18740924136120282)
w2 ( 0.5824769447205008 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2963504587349841) - present_state_Q (0.2962756434171945)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2542899898612378 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3158698809524272) - present_state_Q ( 0.3158772854312382)) * f1( 0.13138209627661027)
w2 ( 0.589419526880341 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3158698809524272) - present_state_Q (0.3158772854312382)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2489055386963393 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0765330105123869) - present_state_Q ( 0.0765330105123869)) * f1( 0.1626131444900599)
w2 ( 0.596041932691118 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0765330105123869) - present_state_Q (0.0765330105123869)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24381960265629096 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0805545222590731) - present_state_Q ( 0.0805545222590731)) * f1( 0.15529531597248866)
w2 ( 0.6025919512904547 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0805545222590731) - present_state_Q (0.0805545222590731)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23810486496852332 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03695396982267951) - present_state_Q ( 0.03695396982267951)) * f1( 0.3427305250481071)
w2 ( 0.6059267798336465 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03695396982267951) - present_state_Q (0.03695396982267951)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3165927033566409 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028945132635759346) - present_state_Q ( 0.028945132635759346)) * f1( 0.38739327456901734)
w2 ( 0.5654057674462029 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.028945132635759346) - present_state_Q (0.028945132635759346)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32968703119309695 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012213897515807814) - present_state_Q ( 0.012213897515807814)) * f1( 0.31860259223917126)
w2 ( 0.5571859172909183 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.012213897515807814) - present_state_Q (0.012213897515807814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3353503946392335 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028786918352255303) - present_state_Q ( 0.028786918352255303)) * f1( 0.2506931037196919)
w2 ( 0.5526677527605777 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.028786918352255303) - present_state_Q (0.028786918352255303)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3320585864799348 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04293946869695507) - present_state_Q ( 0.0423622618348895)) * f1( 0.20328375874006058)
w2 ( 0.5559063864612738 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04293946869695507) - present_state_Q (0.0423622618348895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32744092132637237 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06614532853976472) - present_state_Q ( 0.06614532853976472)) * f1( 0.13562651467593176)
w2 ( 0.5627157705475581 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06614532853976472) - present_state_Q (0.06614532853976472)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3211486015572295 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05036166616111975) - present_state_Q ( 0.053870968154952334)) * f1( 0.17918403636568858)
w2 ( 0.5697390745167813 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05036166616111975) - present_state_Q (0.053870968154952334)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.31511984865864684 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06161049632403963) - present_state_Q ( 0.058291206523339044)) * f1( 0.1733048442687958)
w2 ( 0.5766964713789626 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06161049632403963) - present_state_Q (0.058291206523339044)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30952815499836006 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07027632818803961) - present_state_Q ( 0.06397530370028179)) * f1( 0.1629982712740851)
w2 ( 0.583557517961333 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07027632818803961) - present_state_Q (0.06397530370028179)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3061939518283497 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08482502406770456) - present_state_Q ( 0.08482502406770456)) * f1( 0.10301641065489175)
w2 ( 0.5900306675281144 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08482502406770456) - present_state_Q (0.08482502406770456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3994061404747641 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5925124804935533) - present_state_Q ( 0.4717246659426597)) * f1( 0.38637602369029245)
w2 ( 0.34878332573878396 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.5925124804935533) - present_state_Q (0.4717246659426597)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.46785051725298044 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30160629198548966) - present_state_Q ( 0.29799734243839016)) * f1( 0.3018046951027458)
w2 ( 0.07664292015000296 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.30160629198548966) - present_state_Q (0.29799734243839016)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5141016637144226 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00093874990730336) - present_state_Q ( -0.00093874990730336)) * f1( 0.23135346467679466)
w2 ( -0.20323879736167683 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.00093874990730336) - present_state_Q (-0.00093874990730336)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5003470868346275 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32835927641149576) - present_state_Q ( -0.3792682957017397)) * f1( 0.1842709060906983)
w2 ( -0.09873826583319417 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.32835927641149576) - present_state_Q (-0.3792682957017397)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4930424427187888 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18318857984819725) - present_state_Q ( -0.18318857984819725)) * f1( 0.12931555424394722)
w2 ( -0.030953899209588853 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18318857984819725) - present_state_Q (-0.18318857984819725)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4835040092403563 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09710575047680481) - present_state_Q ( -0.10695907128098446)) * f1( 0.19182428002672391)
w2 ( -0.01106395936025669 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09710575047680481) - present_state_Q (-0.10695907128098446)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4719582373434359 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11340269095409972) - present_state_Q ( -0.11340269095409972)) * f1( 0.2299668605783461)
w2 ( -0.0010227109230828944 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11340269095409972) - present_state_Q (-0.11340269095409972)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45961941240533133 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11571541801635632) - present_state_Q ( -0.11571541801635632)) * f1( 0.24474808720773417)
w2 ( 0.009060166601211523 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11571541801635632) - present_state_Q (-0.11571541801635632)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4502084445075825 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08838671869193303) - present_state_Q ( -0.08838671869193303)) * f1( 0.1962466109517377)
w2 ( 0.018651127537666318 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08838671869193303) - present_state_Q (-0.08838671869193303)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4355043561609336 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1253575860510474) - present_state_Q ( -0.1253575860510474)) * f1( 0.28672898772427746)
w2 ( 0.028907564086585173 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1253575860510474) - present_state_Q (-0.1253575860510474)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4228443834317737 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10559400048206809) - present_state_Q ( -0.10559400048206809)) * f1( 0.255739148699188)
w2 ( 0.0388082560952624 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10559400048206809) - present_state_Q (-0.10559400048206809)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.41342896970658316 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07478289882409511) - present_state_Q ( -0.07046701622173586)) * f1( 0.20336161961511645)
w2 ( 0.05732780514883545 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07478289882409511) - present_state_Q (-0.07046701622173586)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40663446753516674 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04544767630814549) - present_state_Q ( -0.041374166057818254)) * f1( 0.15554132107140647)
w2 ( 0.0748009810859156 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04544767630814549) - present_state_Q (-0.041374166057818254)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.40238374645811076 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016230550742418366) - present_state_Q ( -0.012180637390875639)) * f1( 0.103535320260575)
w2 ( 0.09122328437858096 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.016230550742418366) - present_state_Q (-0.012180637390875639)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3915279971353213 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07098160514809825) - present_state_Q ( -0.0750934081066063)) * f1( 0.23196281113218184)
w2 ( 0.10058318933041689 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07098160514809825) - present_state_Q (-0.0750934081066063)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.38307701076617984 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04934213983303436) - present_state_Q ( -0.053626501075924926)) * f1( 0.18834703898970712)
w2 ( 0.10955703507226933 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04934213983303436) - present_state_Q (-0.053626501075924926)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37642105980706425 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03694057853036033) - present_state_Q ( -0.03694057853036033)) * f1( 0.15362964597407255)
w2 ( 0.11822196548581582 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03694057853036033) - present_state_Q (-0.03694057853036033)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37240488100659885 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013070256650188627) - present_state_Q ( -0.013070256650188627)) * f1( 0.09753612023240676)
w2 ( 0.1264572301055192 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.013070256650188627) - present_state_Q (-0.013070256650188627)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3739820041959831 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004711011278986224) - present_state_Q ( -0.004711011278986224)) * f1( 0.08056408181062062)
w2 ( 0.12254202830854095 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.004711011278986224) - present_state_Q (-0.004711011278986224)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4246270752310684 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05824860961084133) - present_state_Q ( 0.054603424286556776)) * f1( 0.24719641225101854)
w2 ( -0.12331139929051584 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.05824860961084133) - present_state_Q (0.054603424286556776)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4140976683800309 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22246446570514733) - present_state_Q ( -0.22246446570514733)) * f1( 0.17542637034153508)
w2 ( -0.0512852369943599 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22246446570514733) - present_state_Q (-0.22246446570514733)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.40859195278401855 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08763936552827459) - present_state_Q ( -0.09789641292714657)) * f1( 0.11256082680960684)
w2 ( -0.002371989356927985 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08763936552827459) - present_state_Q (-0.09789641292714657)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3998715584421926 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07682143404778108) - present_state_Q ( -0.07729583191916668)) * f1( 0.18569293347075808)
w2 ( 0.025804831953935334 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07682143404778108) - present_state_Q (-0.07729583191916668)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3923382901983813 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056051921235037994) - present_state_Q ( -0.05199879604762522)) * f1( 0.16875842703812083)
w2 ( 0.05258844818938263 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.056051921235037994) - present_state_Q (-0.05199879604762522)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.38643378880264023 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01552141273570156) - present_state_Q ( -0.023365990444690045)) * f1( 0.13997884155163762)
w2 ( 0.07789727913964983 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.01552141273570156) - present_state_Q (-0.023365990444690045)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3841085365698635 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02304593464568234) - present_state_Q ( 0.02304593464568234)) * f1( 0.061310458672670015)
w2 ( 0.10065279866878299 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.02304593464568234) - present_state_Q (0.02304593464568234)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.3786248072749173 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02927401333293046) - present_state_Q ( -0.02927401333293046)) * f1( 0.1286213878709283)
w2 ( 0.10917973090877574 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02927401333293046) - present_state_Q (-0.02927401333293046)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37551385770664875 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0071451996704072415) - present_state_Q ( -0.0071451996704072415)) * f1( 0.07654317756078603)
w2 ( 0.11730834450284307 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.0071451996704072415) - present_state_Q (-0.0071451996704072415)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37341248333122806 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0035743494302744594) - present_state_Q ( 0.0035743494302744594)) * f1( 0.05296028112451211)
w2 ( 0.1252440062130981 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0035743494302744594) - present_state_Q (0.0035743494302744594)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37192255843142874 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01079369050217731) - present_state_Q ( 0.01079369050217731)) * f1( 0.038175238849199385)
w2 ( 0.1330497197840589 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.01079369050217731) - present_state_Q (0.01079369050217731)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3714017667862488 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004440371302893939) - present_state_Q ( -0.004790304174218767)) * f1( 0.012879843036200113)
w2 ( 0.1330497197840589 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.004440371302893939) - present_state_Q (-0.004790304174218767)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.37026554355910796 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011273784550175914) - present_state_Q ( 0.01566225281805171)) * f1( 0.02947668029016337)
w2 ( 0.14075902229679824 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.011273784550175914) - present_state_Q (0.01566225281805171)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37133168176267 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19918759140009218) - present_state_Q ( -0.20282836369127122)) * f1( 0.6238230161261493)
w2 ( 0.14041721438782348 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19918759140009218) - present_state_Q (-0.20282836369127122)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.340439319718678 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1481578620363362) - present_state_Q ( -0.17624130491390091)) * f1( 0.5502486262996975)
w2 ( 0.15164572476202884 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1481578620363362) - present_state_Q (-0.17624130491390091)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3167878109942865 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10281963332765459) - present_state_Q ( -0.10281963332765459)) * f1( 0.4801969507151997)
w2 ( 0.1713472315618244 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10281963332765459) - present_state_Q (-0.10281963332765459)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3019310687009274 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04155161956651557) - present_state_Q ( 0.04155161956651557)) * f1( 0.40972413549601444)
w2 ( 0.20760758580083802 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.04155161956651557) - present_state_Q (0.04155161956651557)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2892616351056739 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1016284067309631) - present_state_Q ( 0.08863509616186835)) * f1( 0.3940385802324166)
w2 ( 0.23976036025196082 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1016284067309631) - present_state_Q (0.08863509616186835)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.28175928696162783 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1561277352108212) - present_state_Q ( 0.1561277352108212)) * f1( 0.2891244980019099)
w2 ( 0.2657088640829869 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1561277352108212) - present_state_Q (0.1561277352108212)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2772114181822061 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2033086933957671) - present_state_Q ( 0.20593947507104282)) * f1( 0.2121292598958199)
w2 ( 0.28714800350984027 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2033086933957671) - present_state_Q (0.20593947507104282)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.27432472249169976 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23996255964121305) - present_state_Q ( 0.24294837064412744)) * f1( 0.15944376734388765)
w2 ( 0.30525279204183964 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.23996255964121305) - present_state_Q (0.24294837064412744)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2726052478397238 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2721336930882652) - present_state_Q ( 0.2743878866302859)) * f1( 0.11251229977089516)
w2 ( 0.3205353403096937 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2721336930882652) - present_state_Q (0.2743878866302859)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2718175734434908 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3035963522267629) - present_state_Q ( 0.3035963522267629)) * f1( 0.062137424782409134)
w2 ( 0.3332116686092851 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3035963522267629) - present_state_Q (0.3035963522267629)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2678579277610145 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10173876085854328) - present_state_Q ( 0.09872660913589697)) * f1( 0.12713695391369342)
w2 ( 0.3456695592872834 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10173876085854328) - present_state_Q (0.09872660913589697)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2595322298441204 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01728003523354285) - present_state_Q ( 0.011923129271353174)) * f1( 0.21358629578120059)
w2 ( 0.3534656567723234 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.01728003523354285) - present_state_Q (0.011923129271353174)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24629043940725762 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.007881855467765778) - present_state_Q ( -0.012732498942069695)) * f1( 0.32144612769921205)
w2 ( 0.36170454304022925 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.007881855467765778) - present_state_Q (-0.012732498942069695)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23525520678105677 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0038070497267080755) - present_state_Q ( 0.0038070497267080755)) * f1( 0.27826438998719105)
w2 ( 0.3696360161451485 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0038070497267080755) - present_state_Q (0.0038070497267080755)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.22521929667426907 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013108415210445507) - present_state_Q ( 0.013108415210445507)) * f1( 0.25852260126674254)
w2 ( 0.3774000646713605 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.013108415210445507) - present_state_Q (0.013108415210445507)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21648847777566257 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02356847041901146) - present_state_Q ( 0.02356847041901146)) * f1( 0.23049331598943518)
w2 ( 0.3849758322038183 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.02356847041901146) - present_state_Q (0.02356847041901146)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20677414665593874 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021718001065643693) - present_state_Q ( 0.021718001065643693)) * f1( 0.25533536908325094)
w2 ( 0.3925849081846367 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.021718001065643693) - present_state_Q (0.021718001065643693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20060661925007867 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04329553747689573) - present_state_Q ( 0.04320291001026825)) * f1( 0.17078572054474414)
w2 ( 0.39980744105938515 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.04329553747689573) - present_state_Q (0.04320291001026825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19508026313830257 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.048825374890506) - present_state_Q ( 0.048825374890506)) * f1( 0.1552098003434093)
w2 ( 0.40692858431135603 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.048825374890506) - present_state_Q (0.048825374890506)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1901019388626966 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05669084935673059) - present_state_Q ( 0.05378638319592656)) * f1( 0.1414768117612085)
w2 ( 0.413966238346151 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05669084935673059) - present_state_Q (0.05378638319592656)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.187938254872908 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15006171239565802) - present_state_Q ( 0.15006171239565802)) * f1( 0.08166556867163424)
w2 ( 0.4245640166999073 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.15006171239565802) - present_state_Q (0.15006171239565802)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18442699105053756 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06933953832225075) - present_state_Q ( 0.06558088971330929)) * f1( 0.10286311129016952)
w2 ( 0.4313910779822856 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06933953832225075) - present_state_Q (0.06558088971330929)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19688709358833784 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3148042409364716) - present_state_Q ( 0.2284405642614571)) * f1( 0.6326205153582987)
w2 ( 0.4156342667688608 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.3148042409364716) - present_state_Q (0.2284405642614571)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.20978770794448215 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4662270212688421) - present_state_Q ( 0.4662270212688421)) * f1( 0.5874481160730285)
w2 ( 0.3848896620889867 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.4662270212688421) - present_state_Q (0.4662270212688421)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20924895443054708 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4302672035787568) - present_state_Q ( 0.43240792897129476)) * f1( 0.50735860073105)
w2 ( 0.386376292883108 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4302672035787568) - present_state_Q (0.43240792897129476)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2091457937963658 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44201947489426313) - present_state_Q ( 0.44201947489426313)) * f1( 0.4726777986119733)
w2 ( 0.38668183904643083 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.44201947489426313) - present_state_Q (0.44201947489426313)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20951232079237472 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.45421950889293417) - present_state_Q ( 0.45421950889293417)) * f1( 0.41662356287646846)
w2 ( 0.38545018092592115 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.45421950889293417) - present_state_Q (0.45421950889293417)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2100931341169667 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4594789239439714) - present_state_Q ( 0.46152854325587406)) * f1( 0.3727786019697328)
w2 ( 0.38326888980531437 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4594789239439714) - present_state_Q (0.46152854325587406)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.21082136430477869 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.46738587463137127) - present_state_Q ( 0.46957248616245556)) * f1( 0.3189250322082441)
w2 ( 0.3800721439874098 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.46738587463137127) - present_state_Q (0.46957248616245556)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20958949830916923 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.472322458944096) - present_state_Q ( 0.4005063402770676)) * f1( 0.2636366228399576)
w2 ( 0.38567925266149083 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.472322458944096) - present_state_Q (0.4005063402770676)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2105625626099992 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.49233864349114503) - present_state_Q ( 0.4903548401682767)) * f1( 0.23663453540334584)
w2 ( 0.37992231604680815 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.49233864349114503) - present_state_Q (0.4903548401682767)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.21136760482327105 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4954179070517159) - present_state_Q ( 0.49120696225051724)) * f1( 0.1932170643761067)
w2 ( 0.37408919203045976 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.4954179070517159) - present_state_Q (0.49120696225051724)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.211993143804156 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.49419653193606156) - present_state_Q ( 0.49419653193606156)) * f1( 0.13970133659446718)
w2 ( 0.367820429006516 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.49419653193606156) - present_state_Q (0.49419653193606156)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.18052959970514984 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0025532330294018857) - present_state_Q ( -0.06867554943238897)) * f1( 0.6709633749528067)
w2 ( 0.3771990464612226 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0025532330294018857) - present_state_Q (-0.06867554943238897)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15139206232420732 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029209100057395576) - present_state_Q ( -0.04262249241053645)) * f1( 0.6539775299762829)
w2 ( 0.3861099145095481 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.029209100057395576) - present_state_Q (-0.04262249241053645)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12990456892372526 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057252015744611084) - present_state_Q ( 0.06027574796279149)) * f1( 0.6220155561350751)
w2 ( 0.3999278926540149 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.057252015744611084) - present_state_Q (0.06027574796279149)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11134775787529518 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08403664088287) - present_state_Q ( 0.08535123922942947)) * f1( 0.57442104192648)
w2 ( 0.4128499896483692 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.08403664088287) - present_state_Q (0.08535123922942947)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09558848524035894 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10730785758860328) - present_state_Q ( 0.10730785758860328)) * f1( 0.5193830515699649)
w2 ( 0.42498690677517953 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.10730785758860328) - present_state_Q (0.10730785758860328)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08124094889113453 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12456948770589832) - present_state_Q ( 0.1226685343839305)) * f1( 0.49510386326489736)
w2 ( 0.4365784433506459 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12456948770589832) - present_state_Q (0.1226685343839305)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07037693921658048 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14218658363017725) - present_state_Q ( 0.14218658363017725)) * f1( 0.39936502654047246)
w2 ( 0.4474597263399595 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.14218658363017725) - present_state_Q (0.14218658363017725)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06125197610648053 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1543841409718493) - present_state_Q ( 0.1543841409718493)) * f1( 0.3495427598581742)
w2 ( 0.45790189726497293 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1543841409718493) - present_state_Q (0.1543841409718493)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05351397248042797 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16558081970894492) - present_state_Q ( 0.1643667721669793)) * f1( 0.3068307005530463)
w2 ( 0.4679895496571295 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.16558081970894492) - present_state_Q (0.1643667721669793)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.048120767647842275 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17528183966139105) - present_state_Q ( 0.17528183966139105)) * f1( 0.22263307411570246)
w2 ( 0.47767940342931947 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17528183966139105) - present_state_Q (0.17528183966139105)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0445802912675758 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18380873174596501) - present_state_Q ( 0.18380873174596501)) * f1( 0.15093336995193274)
w2 ( 0.48706228908646476 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18380873174596501) - present_state_Q (0.18380873174596501)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.041088126271111525 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18857746601197817) - present_state_Q ( 0.1880789858029869)) * f1( 0.15132090077898322)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18857746601197817) - present_state_Q (0.1880789858029869)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03169005188717525 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009452686778321442) - present_state_Q ( -0.009452686778321442)) * f1( 0.2300588426921646)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.009452686778321442) - present_state_Q (-0.009452686778321442)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.007955210508061465 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01806933215108104) - present_state_Q ( -0.01806933215108104)) * f1( 0.5701894151329421)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.01806933215108104) - present_state_Q (-0.01806933215108104)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.015504651265915059 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004617725840685779) - present_state_Q ( -0.004617725840685779)) * f1( 0.5804655748589401)
w2 ( 0.4962934395183932 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.004617725840685779) - present_state_Q (-0.004617725840685779)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03942093796438138 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11163874029570388) - present_state_Q ( 0.11163874029570388)) * f1( 0.7984734503020494)
w2 ( 0.5022839421930706 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.11163874029570388) - present_state_Q (0.11163874029570388)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0633888457857827 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13430691062781272) - present_state_Q ( 0.13430691062781272)) * f1( 0.8586838349656652)
w2 ( 0.50786641780177 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.13430691062781272) - present_state_Q (0.13430691062781272)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08655981742398318 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1592437900113608) - present_state_Q ( 0.15866715802135867)) * f1( 0.9006927599525731)
w2 ( 0.5130115622213656 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1592437900113608) - present_state_Q (0.15866715802135867)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1090048644862839 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18619120310540443) - present_state_Q ( 0.18619120310540443)) * f1( 0.965677760752431)
w2 ( 0.5176601205654683 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.18619120310540443) - present_state_Q (0.18619120310540443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1300705770549889 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21447972707131685) - present_state_Q ( 0.21447972707131685)) * f1( 1.0178234107358004)
w2 ( 0.5217994854781846 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21447972707131685) - present_state_Q (0.21447972707131685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1491135049420526 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2369373699803818) - present_state_Q ( 0.2371143824517675)) * f1( 1.020634246129369)
w2 ( 0.5255310725691099 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2369373699803818) - present_state_Q (0.2371143824517675)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1663018089143689 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25724921374549475) - present_state_Q ( 0.25710856463093273)) * f1( 1.0193734643698489)
w2 ( 0.5289033997039823 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25724921374549475) - present_state_Q (0.25710856463093273)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18182034255538623 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2742049490494577) - present_state_Q ( 0.27404642388000394)) * f1( 1.0118094627933352)
w2 ( 0.5319708811244811 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2742049490494577) - present_state_Q (0.27404642388000394)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19583303345306036 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2911747705344102) - present_state_Q ( 0.29141295311385607)) * f1( 1.0175911797801134)
w2 ( 0.5347249716032728 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2911747705344102) - present_state_Q (0.29141295311385607)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.20848872073874583 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3063325032762877) - present_state_Q ( 0.3063325032762877)) * f1( 1.0181505409986142)
w2 ( 0.5372109865442997 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3063325032762877) - present_state_Q (0.3063325032762877)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2199103614231936 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31957871235903385) - present_state_Q ( 0.31984754772718355)) * f1( 1.0187858108855954)
w2 ( 0.539453193014474 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.31957871235903385) - present_state_Q (0.31984754772718355)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2318159747884683 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33116585981906543) - present_state_Q ( 0.25433877913766956)) * f1( 0.6659447039557687)
w2 ( 0.5430287491513588 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.33116585981906543) - present_state_Q (0.25433877913766956)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.24093991133417755 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3450530641574562) - present_state_Q ( 0.3450530641574562)) * f1( 1.0199785176277958)
w2 ( 0.5448177939965246 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3450530641574562) - present_state_Q (0.3450530641574562)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.24921900579509612 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3542471585706867) - present_state_Q ( 0.35402676476653505)) * f1( 1.0171133732482107)
w2 ( 0.5464457530183353 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3542471585706867) - present_state_Q (0.35402676476653505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.25668932081685386 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36286806460548815) - present_state_Q ( 0.36286806460548815)) * f1( 1.017494284566361)
w2 ( 0.5479141278554365 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.36286806460548815) - present_state_Q (0.36286806460548815)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.26339385039701796 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37103042387294843) - present_state_Q ( 0.37136110404242784)) * f1( 1.0198253579007193)
w2 ( 0.5492289666223338 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.37103042387294843) - present_state_Q (0.37136110404242784)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2490999353770471 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37814167708368074) - present_state_Q ( 0.37814167708368074)) * f1( 1.0186110395318915)
w2 ( 0.5464224164348275 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.37814167708368074) - present_state_Q (0.37814167708368074)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.23609185132650495 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3633790884253367) - present_state_Q ( 0.3637006200778246)) * f1( 1.0213416410797747)
w2 ( 0.5438751622101217 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.3633790884253367) - present_state_Q (0.3637006200778246)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16319674192223652 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3498168710986534) - present_state_Q ( 0.3496015503782833)) * f1( 1.0200543414910417)
w2 ( 0.5295827649447533 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.3498168710986534) - present_state_Q (0.3496015503782833)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0464176508967683 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2723455480197595) - present_state_Q ( 0.2723455480197595)) * f1( 1.019805867877635)
w2 ( 0.5066805450803976 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.2723455480197595) - present_state_Q (0.2723455480197595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.058847810175996246 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14864568466930633) - present_state_Q ( 0.14860314105984845)) * f1( 1.0182986672222947)
w2 ( 0.48600577362853925 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.14864568466930633) - present_state_Q (0.14860314105984845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05032784140854994 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.062354241724235565) - present_state_Q ( 0.062354241724235565)) * f1( 0.5921530962198179)
w2 ( 0.488883397277503 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.062354241724235565) - present_state_Q (0.062354241724235565)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04290604283901264 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07093050116401399) - present_state_Q ( 0.07044249784026134)) * f1( 0.5431224715828088)
w2 ( 0.49161640832302583 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07093050116401399) - present_state_Q (0.07044249784026134)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0357224735471057 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27529331932298595) - present_state_Q ( 0.27479040900763885)) * f1( 0.4703168749887231)
w2 ( 0.5007807436985054 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.27529331932298595) - present_state_Q (0.27479040900763885)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0820690876351184 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28616567770983103) - present_state_Q ( 0.28616567770983103)) * f1( 0.4003857261007366)
w2 ( 0.43132779710217456 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.28616567770983103) - present_state_Q (0.28616567770983103)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1197464955897118 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23112682579153324) - present_state_Q ( 0.2308834067899675)) * f1( 0.340119188304377)
w2 ( 0.36486155364952566 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.23112682579153324) - present_state_Q (0.2308834067899675)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13327749253061225 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25575160677004594) - present_state_Q ( 0.25407628583815217)) * f1( 0.31577506210308787)
w2 ( 0.3305814636366339 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.25575160677004594) - present_state_Q (0.25407628583815217)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1290191210349127 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23306694332878305) - present_state_Q ( 0.23441854869790343)) * f1( 0.22544408392514073)
w2 ( 0.3456925152874319 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.23306694332878305) - present_state_Q (0.23441854869790343)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1261113119800293 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25330568279508964) - present_state_Q ( 0.25458231105445767)) * f1( 0.17029802248879278)
w2 ( 0.359352375865436 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25330568279508964) - present_state_Q (0.25458231105445767)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.12350985053028603 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2705161586097297) - present_state_Q ( 0.20109227714683514)) * f1( 0.1151296275049911)
w2 ( 0.3729099361882843 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2705161586097297) - present_state_Q (0.20109227714683514)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1160605857477476 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1166339088810936) - present_state_Q ( 0.11784965928544149)) * f1( 0.25353698555560633)
w2 ( 0.384662485452391 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1166339088810936) - present_state_Q (0.11784965928544149)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10847613425248866 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1987149288960932) - present_state_Q ( 0.12410331128647056)) * f1( 0.25643229958507646)
w2 ( 0.39649321271651655 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1987149288960932) - present_state_Q (0.12410331128647056)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1041992587223707 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21434286815157613) - present_state_Q ( 0.2153804994883081)) * f1( 0.20756112205469116)
w2 ( 0.4088564399561275 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.21434286815157613) - present_state_Q (0.2153804994883081)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10101803468287406 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2282756752008138) - present_state_Q ( 0.2282756752008138)) * f1( 0.16351545089451577)
w2 ( 0.4205295534952836 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2282756752008138) - present_state_Q (0.2282756752008138)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09827607643981162 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1574867696184717) - present_state_Q ( 0.1574867696184717)) * f1( 0.10616967369550295)
w2 ( 0.4308600297890186 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1574867696184717) - present_state_Q (0.1574867696184717)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09408811874029803 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07383276390602816) - present_state_Q ( 0.07383276390602816)) * f1( 0.1255569259455797)
w2 ( 0.4375310400387101 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07383276390602816) - present_state_Q (0.07383276390602816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09267150213562918 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07728802373294133) - present_state_Q ( 0.07728802373294133)) * f1( 0.1086022806238151)
w2 ( 0.44013985561151714 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07728802373294133) - present_state_Q (0.07728802373294133)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09115544459479218 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07725929239839656) - present_state_Q ( 0.07725929239839656)) * f1( 0.1162026995974058)
w2 ( 0.442749188348346 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07725929239839656) - present_state_Q (0.07725929239839656)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0920496493638673 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07801222454867132) - present_state_Q ( 0.07672318113794832)) * f1( 0.12974163621595186)
w2 ( 0.44137074917468433 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.07801222454867132) - present_state_Q (0.07672318113794832)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11880524603683673 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07636906447766512) - present_state_Q ( 0.07636906447766512)) * f1( 0.12933330479306426)
w2 ( 0.39999610601408636 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.07636906447766512) - present_state_Q (0.07636906447766512)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15875240163353024 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1735947709419733) - present_state_Q ( 0.1735947709419733)) * f1( 0.5372621398279367)
w2 ( 0.4053669965691148 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.1735947709419733) - present_state_Q (0.1735947709419733)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.265031472195016 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16641768738965998) - present_state_Q ( 0.16467360756970112)) * f1( 0.49477418649128574)
w2 ( 0.2764850862392707 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.16641768738965998) - present_state_Q (0.16467360756970112)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.35694445025356725 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.043213975374475105) - present_state_Q ( -0.012220948952930591)) * f1( 0.4633977332256937)
w2 ( 0.1971467800988858 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.043213975374475105) - present_state_Q (-0.012220948952930591)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4459006692638245 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03658719569768906) - present_state_Q ( -0.04371911088958616)) * f1( 0.4538722449216693)
w2 ( 0.07955040357807483 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.03658719569768906) - present_state_Q (-0.04371911088958616)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5182416096585617 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12300466161786555) - present_state_Q ( -0.12300466161786555)) * f1( 0.3828989627815345)
w2 ( -0.033807344694560454 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.12300466161786555) - present_state_Q (-0.12300466161786555)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5762527403210735 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1768379829633055) - present_state_Q ( -0.1768379829633055)) * f1( 0.3151330229795323)
w2 ( -0.10744117730788147 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.1768379829633055) - present_state_Q (-0.1768379829633055)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6230226960566936 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21354761736477834) - present_state_Q ( -0.21354761736477834)) * f1( 0.2587109796596962)
w2 ( -0.21590960597018347 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.21354761736477834) - present_state_Q (-0.21354761736477834)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6376237414010703 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3370989438406318) - present_state_Q ( -0.3802808650346685)) * f1( 0.2638286857041363)
w2 ( -0.2712525089051229 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( -0.3370989438406318) - present_state_Q (-0.3802808650346685)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6293691529775057 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3240561311644117) - present_state_Q ( -0.3240561311644117)) * f1( 0.16789544850554028)
w2 ( -0.23192046746128528 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3240561311644117) - present_state_Q (-0.3240561311644117)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.6240613148963313 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1970165355239039) - present_state_Q ( -0.1970165355239039)) * f1( 0.09194008758354393)
w2 ( -0.19728157454299447 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1970165355239039) - present_state_Q (-0.1970165355239039)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6050236927962596 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22462341948225722) - present_state_Q ( -0.26407973439085614)) * f1( 0.29671299943406715)
w2 ( -0.17161687884528926 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22462341948225722) - present_state_Q (-0.26407973439085614)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5906384553803422 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18022500063311075) - present_state_Q ( -0.21454837640216862)) * f1( 0.24115026667754802)
w2 ( -0.14775584379173495 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18022500063311075) - present_state_Q (-0.21454837640216862)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5919956814136058 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1065596430404146) - present_state_Q ( -0.1065596430404146)) * f1( 0.13038174805681746)
w2 ( -0.1498377702170075 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1065596430404146) - present_state_Q (-0.1065596430404146)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.638668167127143 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17482242967497133) - present_state_Q ( -0.18036580544974568)) * f1( 0.25405295364184655)
w2 ( -0.18658009896736255 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( -0.17482242967497133) - present_state_Q (-0.18036580544974568)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13759126286577905 ) += alpha ( 0.1 ) * (reward ( -2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2769077005576931) - present_state_Q ( 0.2769077005576931)) * f1( 0.4209697105811708)
w2 ( 0.35666339249291035 ) += alpha ( 0.1) * (reward ( -2) + discount_factor ( 0.1) * next_state_max_Q( 0.2769077005576931) - present_state_Q (0.2769077005576931)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14818445265368843 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3018944201249379) - present_state_Q ( 0.30329454947774315)) * f1( 0.38787959281418033)
w2 ( 0.32935288174638544 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.3018944201249379) - present_state_Q (0.30329454947774315)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1496749387306943 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3392183602275418) - present_state_Q ( 0.2733849357927816)) * f1( 0.3776910799434715)
w2 ( 0.3254065717693827 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.3392183602275418) - present_state_Q (0.2733849357927816)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.19291509250129185 ) += alpha ( 0.1 ) * (reward ( -0.9 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3382847254131409) - present_state_Q ( 0.33668280610891993)) * f1( 0.3594795526267043)
w2 ( 0.18106405174126997 ) += alpha ( 0.1) * (reward ( -0.9) + discount_factor ( 0.1) * next_state_max_Q( 0.3382847254131409) - present_state_Q (0.33668280610891993)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1912549333428326 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1580161162388164) - present_state_Q ( 0.15994204905382348)) * f1( 0.29720231990307644)
w2 ( 0.18776719924967694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1580161162388164) - present_state_Q (0.15994204905382348)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.18471911278777758 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17611732068372746) - present_state_Q ( 0.14235479516434119)) * f1( 0.23744435393952465)
w2 ( 0.2152928929400801 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.17611732068372746) - present_state_Q (0.14235479516434119)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18083346243074883 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2224301642308409) - present_state_Q ( 0.2224301642308409)) * f1( 0.19446448586251605)
w2 ( 0.2392704352031493 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2224301642308409) - present_state_Q (0.2224301642308409)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.17828208052836236 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25945701198001353) - present_state_Q ( 0.25941877856586937)) * f1( 0.15321137639843563)
w2 ( 0.25925366591900517 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.25945701198001353) - present_state_Q (0.25941877856586937)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1698329618845657 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.015469996574508912) - present_state_Q ( 0.013075051734919817)) * f1( 0.21749623593108403)
w2 ( 0.2670231048774558 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.015469996574508912) - present_state_Q (0.013075051734919817)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.163413288399875 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024854705393412807) - present_state_Q ( 0.02455605513741451)) * f1( 0.16986435093609697)
w2 ( 0.2745816931854943 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.024854705393412807) - present_state_Q (0.02455605513741451)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15722258972169234 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028169491954936027) - present_state_Q ( 0.027931000010910825)) * f1( 0.1651355216606037)
w2 ( 0.282079412169186 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.028169491954936027) - present_state_Q (0.027931000010910825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15331505403101683 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.037937777271147194) - present_state_Q ( 0.03954938056045758)) * f1( 0.10727785303139879)
w2 ( 0.2893643001125191 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.037937777271147194) - present_state_Q (0.03954938056045758)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15114760883063097 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03651469065119381) - present_state_Q ( -0.03822941890355225)) * f1( 0.6268287190285556)
w2 ( 0.2900558591092878 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.03651469065119381) - present_state_Q (-0.03822941890355225)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13118474943179195 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0816285659660814) - present_state_Q ( 0.0816285659660814)) * f1( 0.6113556821334568)
w2 ( 0.3096479165471194 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0816285659660814) - present_state_Q (0.0816285659660814)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11776128605173493 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1750800943785704) - present_state_Q ( 0.1750800943785704)) * f1( 0.5537094759394465)
w2 ( 0.3290421497518623 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.1750800943785704) - present_state_Q (0.1750800943785704)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.10754757338024135 ) += alpha ( 0.1 ) * (reward ( 0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20695969884522303) - present_state_Q ( 0.20695969884522303)) * f1( 0.47786520377795894)
w2 ( 0.34614105143500623 ) += alpha ( 0.1) * (reward ( 0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20695969884522303) - present_state_Q (0.20695969884522303)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.9671473617849645 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8671055202890097) - present_state_Q ( 0.6583489089897506)) * f1( 0.2583489089897506)
w2 ( 0.949134465721566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.8671055202890097) - present_state_Q (0.6583489089897506)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.8970328911514085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.1501240652995643) - present_state_Q ( 1.1501240652995643)) * f1( 0.40409198035863086)
w2 ( 0.8103255330199973 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 1.1501240652995643) - present_state_Q (1.1501240652995643)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.7965680382457591 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.4083622657561623) - present_state_Q ( 1.4262926961204374)) * f1( 0.506003805405422)
w2 ( 0.5720707566746187 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 1.4083622657561623) - present_state_Q (1.4262926961204374)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.7026590202235096 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.1236191919652194) - present_state_Q ( 1.1236191919652194)) * f1( 0.5487720608503893)
w2 ( 0.366719883942375 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 1.1236191919652194) - present_state_Q (1.1236191919652194)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.6142734122663496 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.8688447300065194) - present_state_Q ( 0.861285409980809)) * f1( 0.5994679312819071)
w2 ( 0.1897917715047561 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.8688447300065194) - present_state_Q (0.861285409980809)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.5300134695401837 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6291790128043037) - present_state_Q ( 0.6347207655841205)) * f1( 0.6625236118830263)
w2 ( 0.03717542778831326 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.6291790128043037) - present_state_Q (0.6347207655841205)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.4542919579394976 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.41160354399794535) - present_state_Q ( 0.4174851456817025)) * f1( 0.703519162747347)
w2 ( -0.09198354716551574 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.41160354399794535) - present_state_Q (0.4174851456817025)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.3940764910188457 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1851329708247668) - present_state_Q ( 0.1864136303616257)) * f1( 0.6938062426262995)
w2 ( -0.21348959382459662 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.1851329708247668) - present_state_Q (0.1864136303616257)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3467042577029737 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02491209305620451) - present_state_Q ( -0.023815967632427937)) * f1( 0.6980103355336994)
w2 ( -0.3085041276588436 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.02491209305620451) - present_state_Q (-0.023815967632427937)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.30967832216199 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1904397639537702) - present_state_Q ( -0.18949225992477728)) * f1( 0.6991939481899379)
w2 ( -0.3826413679647276 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1904397639537702) - present_state_Q (-0.18949225992477728)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2812972008328702 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3208322697465861) - present_state_Q ( -0.3215894695310667)) * f1( 0.691389840027465)
w2 ( -0.44011049400683044 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3208322697465861) - present_state_Q (-0.3215894695310667)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.25885856114652306 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.419357880103693) - present_state_Q ( -0.4200578146410685)) * f1( 0.697116346653599)
w2 ( -0.48517341027853256 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.419357880103693) - present_state_Q (-0.4200578146410685)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.24149891025232337 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49903747619454375) - present_state_Q ( -0.4996658720921263)) * f1( 0.6937259540594158)
w2 ( -0.5202067128523585 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.49903747619454375) - present_state_Q (-0.4996658720921263)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.22778067959762563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5595841257023294) - present_state_Q ( -0.5595841257023294)) * f1( 0.698575708332206)
w2 ( -0.547699113013865 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5595841257023294) - present_state_Q (-0.5595841257023294)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.21717003652583858 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6089936134021143) - present_state_Q ( -0.6083474628187191)) * f1( 0.6955431675792723)
w2 ( -0.5690563788068739 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6089936134021143) - present_state_Q (-0.6083474628187191)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.20894294646698283 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6454445049338945) - present_state_Q ( -0.6459828276650026)) * f1( 0.693908354372318)
w2 ( -0.585655006002848 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6454445049338945) - present_state_Q (-0.6459828276650026)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2023996921449109 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6744181310464845) - present_state_Q ( -0.6738458599190396)) * f1( 0.6990958582467864)
w2 ( -0.5987584394488332 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6744181310464845) - present_state_Q (-0.6738458599190396)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1974237032846913 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6974601588617935) - present_state_Q ( -0.697961519205039)) * f1( 0.6931843350970992)
w2 ( -0.6088082689841928 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6974601588617935) - present_state_Q (-0.697961519205039)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.19337886730663428 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7155920744812267) - present_state_Q ( -0.7138815919470202)) * f1( 0.7012834949773007)
w2 ( -0.6168831351543472 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7155920744812267) - present_state_Q (-0.7138815919470202)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.19027257163800235 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7284179717052295) - present_state_Q ( -0.7284179717052295)) * f1( 0.6992409222071067)
w2 ( -0.6231024707194883 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7284179717052295) - present_state_Q (-0.7284179717052295)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18792517881870427 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7397203383537859) - present_state_Q ( -0.7401777548190648)) * f1( 0.6946124869729884)
w2 ( -0.6278336697817722 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7397203383537859) - present_state_Q (-0.7401777548190648)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1860074127134492 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7478966985206428) - present_state_Q ( -0.7473975785124618)) * f1( 0.700116716711748)
w2 ( -0.6316685625693166 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7478966985206428) - present_state_Q (-0.7473975785124618)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18460922136875013 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7548543576154431) - present_state_Q ( -0.7553261944938802)) * f1( 0.693573397001695)
w2 ( -0.6344908563467896 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7548543576154431) - present_state_Q (-0.7553261944938802)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18350367560241893 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7595955087473713) - present_state_Q ( -0.7600448443872834)) * f1( 0.694669277880019)
w2 ( -0.6367189152550331 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7595955087473713) - present_state_Q (-0.7600448443872834)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18260701696925333 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7640221223705057) - present_state_Q ( -0.7635346300971682)) * f1( 0.6968353676845519)
w2 ( -0.6385203767546166 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7640221223705057) - present_state_Q (-0.7635346300971682)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1820055569729915 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7676621069609658) - present_state_Q ( -0.7680416466256501)) * f1( 0.6893868752700205)
w2 ( -0.6397418157244791 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7676621069609658) - present_state_Q (-0.7680416466256501)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18144475786846917 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7683594381602434) - present_state_Q ( -0.7687894863301512)) * f1( 0.6969515535338476)
w2 ( -0.6408683197725014 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7683594381602434) - present_state_Q (-0.7687894863301512)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1810155636323815 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7714972049915608) - present_state_Q ( -0.7709806662963539)) * f1( 0.6957212920786444)
w2 ( -0.6417319873608937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7714972049915608) - present_state_Q (-0.7709806662963539)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18086804553529104 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730924788848237) - present_state_Q ( -0.7751432246806862)) * f1( 0.6810550162135972)
w2 ( -0.6420352306099851 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730924788848237) - present_state_Q (-0.7751432246806862)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1805613050280298 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7723794391504399) - present_state_Q ( -0.7728352947137114)) * f1( 0.6967180287005418)
w2 ( -0.6426516014981717 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7723794391504399) - present_state_Q (-0.7728352947137114)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18032530731699067 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7745462365386429) - present_state_Q ( -0.7740632665416891)) * f1( 0.695879859398701)
w2 ( -0.6431263914938762 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7745462365386429) - present_state_Q (-0.7740632665416891)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18016970256404052 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7752854270517293) - present_state_Q ( -0.7752854270517293)) * f1( 0.6936991978602383)
w2 ( -0.6434404276853583 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7752854270517293) - present_state_Q (-0.7752854270517293)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18001064680315565 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775798708209821) - present_state_Q ( -0.7752968023372412)) * f1( 0.6966753823531731)
w2 ( -0.6437600572730819 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775798708209821) - present_state_Q (-0.7752968023372412)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17995498331435877 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7763952893395345) - present_state_Q ( -0.7768342541105469)) * f1( 0.691235925660741)
w2 ( -0.6438727957483589 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7763952893395345) - present_state_Q (-0.7768342541105469)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17987952710533744 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7760858774101492) - present_state_Q ( -0.7765214246005421)) * f1( 0.6940651886754087)
w2 ( -0.6440249985880251 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7760858774101492) - present_state_Q (-0.7765214246005421)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17976597319923915 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775494894020988) - present_state_Q ( -0.7759246413411249)) * f1( 0.6988586122338106)
w2 ( -0.6442524773165614 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775494894020988) - present_state_Q (-0.7759246413411249)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17970580532215638 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7773562810875727) - present_state_Q ( -0.7768709082803232)) * f1( 0.6958077645997598)
w2 ( -0.6443735380925422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7773562810875727) - present_state_Q (-0.7768709082803232)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17983110079643694 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775673414316004) - present_state_Q ( -0.7795943726752124)) * f1( 0.6818287279851161)
w2 ( -0.6441162686980548 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775673414316004) - present_state_Q (-0.7795943726752124)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17977244752581178 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7763600592585884) - present_state_Q ( -0.7767919940661543)) * f1( 0.6949341996887708)
w2 ( -0.6442344303584134 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7763600592585884) - present_state_Q (-0.7767919940661543)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17968474845049368 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7759060472834582) - present_state_Q ( -0.7763352888859322)) * f1( 0.6986215927099397)
w2 ( -0.6444101745763513 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7759060472834582) - present_state_Q (-0.7763352888859322)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17987094069897724 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.776871066276019) - present_state_Q ( -0.7804352740579914)) * f1( 0.6775142097407434)
w2 ( -0.6440254311360968 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.776871066276019) - present_state_Q (-0.7804352740579914)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17983139769255088 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7766620777609047) - present_state_Q ( -0.7770950973815303)) * f1( 0.6923881407693858)
w2 ( -0.6441053865913352 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7766620777609047) - present_state_Q (-0.7770950973815303)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.179764673466358 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777265891881208) - present_state_Q ( -0.7767665145928573)) * f1( 0.694990019755539)
w2 ( -0.644239797034672 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777265891881208) - present_state_Q (-0.7767665145928573)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1796888025384592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7760796190042984) - present_state_Q ( -0.7765204616069612)) * f1( 0.6976635165476521)
w2 ( -0.6443920470757577 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7760796190042984) - present_state_Q (-0.7765204616069612)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17963489627651588 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777483469915576) - present_state_Q ( -0.7769745181812637)) * f1( 0.696617407186549)
w2 ( -0.6445003831091989 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777483469915576) - present_state_Q (-0.7769745181812637)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17959511307025666 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777010737003313) - present_state_Q ( -0.7771988558806092)) * f1( 0.696421926170167)
w2 ( -0.6445803583177182 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777010737003313) - present_state_Q (-0.7771988558806092)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17956426361315542 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778418595957145) - present_state_Q ( -0.7773412056823128)) * f1( 0.6964070114400361)
w2 ( -0.6446423755565344 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778418595957145) - present_state_Q (-0.7773412056823128)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17953369592211293 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778500442081882) - present_state_Q ( -0.7773464316677576)) * f1( 0.6969810784901712)
w2 ( -0.6447037757419629 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778500442081882) - present_state_Q (-0.7773464316677576)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17958244347201754 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7780732567597861) - present_state_Q ( -0.77851270536211)) * f1( 0.6910824179237335)
w2 ( -0.6446050225859045 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7780732567597861) - present_state_Q (-0.77851270536211)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17954476165685135 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767117677760106) - present_state_Q ( -0.777131181840496)) * f1( 0.6978179345204025)
w2 ( -0.6446806218770992 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767117677760106) - present_state_Q (-0.777131181840496)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17948596556323027 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7768447056528248) - present_state_Q ( -0.7768447056528248)) * f1( 0.7001494435987473)
w2 ( -0.6447981889648432 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7768447056528248) - present_state_Q (-0.7768447056528248)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1794627075088961 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769600461901902) - present_state_Q ( -0.7773629894366338)) * f1( 0.6984082277451727)
w2 ( -0.6448448110903772 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769600461901902) - present_state_Q (-0.7773629894366338)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17945852489686137 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7782764307636755) - present_state_Q ( -0.7777676005159089)) * f1( 0.696607873278754)
w2 ( -0.6448532170488414 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7782764307636755) - present_state_Q (-0.7777676005159089)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17946609231541205 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7784660933497257) - present_state_Q ( -0.7779553923721239)) * f1( 0.6956432499821443)
w2 ( -0.6448379874236402 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7784660933497257) - present_state_Q (-0.7779553923721239)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17937262831453743 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779932537956408) - present_state_Q ( -0.7764712679950755)) * f1( 0.7037647767804792)
w2 ( -0.6450239154574686 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779932537956408) - present_state_Q (-0.7764712679950755)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17960853113136613 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777026093498527) - present_state_Q ( -0.7812446778742125)) * f1( 0.6789709495290545)
w2 ( -0.6445374970859769 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777026093498527) - present_state_Q (-0.7812446778742125)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1795810003773858 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779023768771041) - present_state_Q ( -0.7773945239764186)) * f1( 0.6957240346927311)
w2 ( -0.6445928970055577 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779023768771041) - present_state_Q (-0.7773945239764186)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1796189766200095 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778902912166117) - present_state_Q ( -0.7783386086290316)) * f1( 0.6910054344166339)
w2 ( -0.6445159558745258 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778902912166117) - present_state_Q (-0.7783386086290316)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17965160376052144 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778226306707153) - present_state_Q ( -0.7782546223385348)) * f1( 0.6907272172487164)
w2 ( -0.644449825576521 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778226306707153) - present_state_Q (-0.7782546223385348)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1796182531787607 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767702566690765) - present_state_Q ( -0.7771978292197925)) * f1( 0.6959688862783916)
w2 ( -0.6445169130791171 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767702566690765) - present_state_Q (-0.7771978292197925)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17959270615343562 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7768902005009477) - present_state_Q ( -0.7773219282246087)) * f1( 0.6959301066231299)
w2 ( -0.6445683059346852 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7768902005009477) - present_state_Q (-0.7773219282246087)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17956488326669207 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7768450225130591) - present_state_Q ( -0.7772851123610609)) * f1( 0.6966347276966239)
w2 ( -0.6446242205193194 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7768450225130591) - present_state_Q (-0.7772851123610609)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.179469225237126 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7784936527656505) - present_state_Q ( -0.7764859945889823)) * f1( 0.7016289145519964)
w2 ( -0.644815092415581 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7784936527656505) - present_state_Q (-0.7764859945889823)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1795515278974381 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7786601690333955) - present_state_Q ( -0.7790602838586381)) * f1( 0.689147932520244)
w2 ( -0.6446478950418392 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7786601690333955) - present_state_Q (-0.7790602838586381)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17957519237456593 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776823963759724) - present_state_Q ( -0.7781098061215682)) * f1( 0.6928219903985658)
w2 ( -0.6446000757340832 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776823963759724) - present_state_Q (-0.7781098061215682)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1795396156536121 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777719689555055) - present_state_Q ( -0.7772668087657267)) * f1( 0.6970522799212584)
w2 ( -0.6446715300722585 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777719689555055) - present_state_Q (-0.7772668087657267)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17952884567393218 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7770942744959938) - present_state_Q ( -0.7775547183679661)) * f1( 0.6961439862627969)
w2 ( -0.6446931893436871 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7770942744959938) - present_state_Q (-0.7775547183679661)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17949476866714992 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767378284952308) - present_state_Q ( -0.7771858596574399)) * f1( 0.6984092442250253)
w2 ( -0.6447614985905787 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767378284952308) - present_state_Q (-0.7771858596574399)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17938674231766025 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777931373037366) - present_state_Q ( -0.7762455312841147)) * f1( 0.7043133773838633)
w2 ( -0.644976228133055 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777931373037366) - present_state_Q (-0.7762455312841147)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17943498485059622 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7780767971170504) - present_state_Q ( -0.778502988357283)) * f1( 0.6938290389854568)
w2 ( -0.6448788849226741 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7780767971170504) - present_state_Q (-0.778502988357283)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17942830162047294 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7782402646165735) - present_state_Q ( -0.7777281684654882)) * f1( 0.697201108972256)
w2 ( -0.6448923050421378 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7782402646165735) - present_state_Q (-0.7777281684654882)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17948646448590946 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7782226626667234) - present_state_Q ( -0.7786626196980471)) * f1( 0.6921238524768828)
w2 ( -0.6447746555617453 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7782226626667234) - present_state_Q (-0.7786626196980471)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.13292533957320515 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00828282750188293) - present_state_Q ( -0.00828282750188293)) * f1( 0.6723186840639979)
w2 ( -0.6586255646667114 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.00828282750188293) - present_state_Q (-0.00828282750188293)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1431365782169447 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17700141434824604) - present_state_Q ( -0.1796258220289611)) * f1( 0.630612674053463)
w2 ( -0.652148537442946 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17700141434824604) - present_state_Q (-0.1796258220289611)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1592520916713551 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02799981660224128) - present_state_Q ( -0.02799981660224128)) * f1( 0.715609470076197)
w2 ( -0.6476445407441056 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02799981660224128) - present_state_Q (-0.02799981660224128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1879721171811898 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15992051661340104) - present_state_Q ( -0.2926283579799709)) * f1( 0.6025563963361914)
w2 ( -0.6190463623649878 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15992051661340104) - present_state_Q (-0.2926283579799709)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.21863144362745446 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11013993480125833) - present_state_Q ( -0.2357370930581096)) * f1( 0.7218662341824257)
w2 ( -0.5935629763903087 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11013993480125833) - present_state_Q (-0.2357370930581096)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.24624886191600748 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19500477163451113) - present_state_Q ( -0.19558990660712916)) * f1( 0.7343311490941256)
w2 ( -0.570997610623688 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19500477163451113) - present_state_Q (-0.19558990660712916)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.27149464492000264 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16095920451347318) - present_state_Q ( -0.16380910292239348)) * f1( 0.7260519381113011)
w2 ( -0.5501348196754252 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16095920451347318) - present_state_Q (-0.16380910292239348)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2930967024227805 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15926100610972976) - present_state_Q ( -0.15926100610972976)) * f1( 0.6291832597503292)
w2 ( -0.5295347253454998 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15926100610972976) - present_state_Q (-0.15926100610972976)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3214253100028791 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11484614735192047) - present_state_Q ( -0.22075309242102045)) * f1( 0.692176630369388)
w2 ( -0.49679324713063355 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11484614735192047) - present_state_Q (-0.22075309242102045)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.34030888679497845 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.053317937543832544) - present_state_Q ( -0.053317937543832544)) * f1( 0.7614770931771219)
w2 ( -0.4819140785032666 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.053317937543832544) - present_state_Q (-0.053317937543832544)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3575588262014547 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021242248761016824) - present_state_Q ( -0.021242248761016824)) * f1( 0.7872442029477332)
w2 ( -0.4687669970701717 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.021242248761016824) - present_state_Q (-0.021242248761016824)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.3732841495519156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005886372033391818) - present_state_Q ( 0.00010073764021856224)) * f1( 0.7868941143793724)
w2 ( -0.45677657315180487 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0005886372033391818) - present_state_Q (0.00010073764021856224)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.34053915609452534 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022418491676037322) - present_state_Q ( 0.01913275384333729)) * f1( 0.7854571325526984)
w2 ( -0.4817900274323489 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.022418491676037322) - present_state_Q (0.01913275384333729)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.2873340163974587 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01947898222286598) - present_state_Q ( -0.022435335179030624)) * f1( 0.7829897869552669)
w2 ( -0.5225607812149442 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.01947898222286598) - present_state_Q (-0.022435335179030624)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.24579260940678643 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19133526857926494) - present_state_Q ( -0.19172820601365367)) * f1( 0.7876561981622144)
w2 ( -0.564753206882486 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.19133526857926494) - present_state_Q (-0.19172820601365367)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.20894702586179986 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25815125523716487) - present_state_Q ( -0.25815125523716487)) * f1( 0.7878646584866645)
w2 ( -0.6021663165054101 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.25815125523716487) - present_state_Q (-0.25815125523716487)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.17609316047478452 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3168185600194189) - present_state_Q ( -0.3164217752942893)) * f1( 0.7911635843018779)
w2 ( -0.6353871229620223 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3168185600194189) - present_state_Q (-0.3164217752942893)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.14716919673126772 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3696472876374574) - present_state_Q ( -0.3696472876374574)) * f1( 0.7874377991643577)
w2 ( -0.6647725182521254 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3696472876374574) - present_state_Q (-0.3696472876374574)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.12133790489340038 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41529064542738714) - present_state_Q ( -0.41529064542738714)) * f1( 0.7917918407008311)
w2 ( -0.6908715917813535 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41529064542738714) - present_state_Q (-0.41529064542738714)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09857439833560562 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.457018785220006) - present_state_Q ( -0.457018785220006)) * f1( 0.7885292587599374)
w2 ( -0.7139662392455131 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.457018785220006) - present_state_Q (-0.457018785220006)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.07839927945523581 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4933262178126793) - present_state_Q ( -0.49345138116167153)) * f1( 0.7884563491843847)
w2 ( -0.7344367384950807 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4933262178126793) - present_state_Q (-0.49345138116167153)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06069130868983474 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5253483003161821) - present_state_Q ( -0.5262089769704119)) * f1( 0.7824104284105904)
w2 ( -0.7525428067399772 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5253483003161821) - present_state_Q (-0.5262089769704119)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.044798589582265305 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5541169522266371) - present_state_Q ( -0.5541169522266371)) * f1( 0.7895247968737636)
w2 ( -0.7686463861796593 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5541169522266371) - present_state_Q (-0.5541169522266371)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0346299700861601 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.579299660888216) - present_state_Q ( -0.5880946636213542)) * f1( 0.5987341470453729)
w2 ( -0.7822332103770567 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.579299660888216) - present_state_Q (-0.5880946636213542)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.021825446256248972 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5982872174493664) - present_state_Q ( -0.5983299889510633)) * f1( 0.7928559938766827)
w2 ( -0.7951531090005666 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5982872174493664) - present_state_Q (-0.5983299889510633)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.023091092929382433 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6189453690621183) - present_state_Q ( -0.7779759908622316)) * f1( 0.7870225395009708)
w2 ( -0.7935449636049646 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6189453690621183) - present_state_Q (-0.7779759908622316)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023211504267452287 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7795469471857558) - present_state_Q ( -0.7800087641894627)) * f1( 0.5862086934082561)
w2 ( -0.7933395566578759 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7795469471857558) - present_state_Q (-0.7800087641894627)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023290012281800093 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7792108657322897) - present_state_Q ( -0.7792108657322897)) * f1( 0.6086934634993822)
w2 ( -0.7932105787419699 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7792108657322897) - present_state_Q (-0.7792108657322897)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02308004201955786 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7748227911468492) - present_state_Q ( -0.7748227911468492)) * f1( 0.7895138642537235)
w2 ( -0.7934765275387534 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7748227911468492) - present_state_Q (-0.7748227911468492)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.022900959945980386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7752571298765395) - present_state_Q ( -0.7752571298765395)) * f1( 0.789400541245763)
w2 ( -0.7937033858498649 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7752571298765395) - present_state_Q (-0.7752571298765395)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.1669117853948076 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17787555175645992) - present_state_Q ( -0.43578541398115805)) * f1( 0.44590722034374525)
w2 ( -0.6673348268573043 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.17787555175645992) - present_state_Q (-0.43578541398115805)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1673680657560419 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4546886989491476) - present_state_Q ( -0.4551428717586302)) * f1( 0.47165626765659324)
w2 ( -0.666560906708207 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4546886989491476) - present_state_Q (-0.4551428717586302)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.16697334105519282 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4368312221466278) - present_state_Q ( -0.4368312221466278)) * f1( 0.5760806446821067)
w2 ( -0.6671090587136498 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4368312221466278) - present_state_Q (-0.4368312221466278)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.14742084516313567 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42952464882771013) - present_state_Q ( -0.42952464882771013)) * f1( 0.6238277169573969)
w2 ( -0.6921832839980546 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42952464882771013) - present_state_Q (-0.42952464882771013)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.12882389615781076 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4553520150168735) - present_state_Q ( -0.45829961272774183)) * f1( 0.6474458504499845)
w2 ( -0.7151621310999703 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4553520150168735) - present_state_Q (-0.45829961272774183)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.109895033228522 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48087356617678034) - present_state_Q ( -0.48087356617678034)) * f1( 0.7083789686923154)
w2 ( -0.7365392343352422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.48087356617678034) - present_state_Q (-0.48087356617678034)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09277607812921107 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5108654838341486) - present_state_Q ( -0.5109039690294226)) * f1( 0.7127475750054384)
w2 ( -0.7557538406835616 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5108654838341486) - present_state_Q (-0.5109039690294226)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09681809795641308 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8412495795703799) - present_state_Q ( -0.8411923773872025)) * f1( 0.7082885239183401)
w2 ( -0.7489057503519418 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8412495795703799) - present_state_Q (-0.8411923773872025)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.10014777919819802 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8297813463593222) - present_state_Q ( -0.8297241615011117)) * f1( 0.7122918170966878)
w2 ( -0.7432962271281203 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8297813463593222) - present_state_Q (-0.8297241615011117)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.11238148322823752 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9696466156839703) - present_state_Q ( -0.969677356231507)) * f1( 0.7083268577276448)
w2 ( -0.7191164498752849 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9696466156839703) - present_state_Q (-0.969677356231507)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1219302038246413 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9266745182569136) - present_state_Q ( -0.9267010758913429)) * f1( 0.7124123266059478)
w2 ( -0.7003517425060937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9266745182569136) - present_state_Q (-0.9267010758913429)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.12935179437649674 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8938979455972911) - present_state_Q ( -0.8939352673385963)) * f1( 0.7098911463678068)
w2 ( -0.6857153763170523 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8938979455972911) - present_state_Q (-0.8939352673385963)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1351272357412844 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8678264163253051) - present_state_Q ( -0.86786410902572)) * f1( 0.7123010412207679)
w2 ( -0.6743639708820057 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8678264163253051) - present_state_Q (-0.86786410902572)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1397560509636658 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8484590314550441) - present_state_Q ( -0.8551805283765554)) * f1( 0.6581132987025419)
w2 ( -0.6645171233496586 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8484590314550441) - present_state_Q (-0.8551805283765554)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14315786874108954 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8309188707268026) - present_state_Q ( -0.8309188707268026)) * f1( 0.7112758358388587)
w2 ( -0.6578213456380815 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8309188707268026) - present_state_Q (-0.8309188707268026)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14580975205210878 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8192088680468516) - present_state_Q ( -0.8192502043131292)) * f1( 0.7104023025385733)
w2 ( -0.6525952411868993 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8192088680468516) - present_state_Q (-0.8192502043131292)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14788447641531996 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8102453289389064) - present_state_Q ( -0.8103002528043043)) * f1( 0.7086843191422897)
w2 ( -0.6484966403994413 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8102453289389064) - present_state_Q (-0.8103002528043043)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1498781556474834 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.803370376341645) - present_state_Q ( -0.8106582045001457)) * f1( 0.6575206162003835)
w2 ( -0.644251677038204 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.803370376341645) - present_state_Q (-0.8106582045001457)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15097732528398872 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7948138374655288) - present_state_Q ( -0.7948652673143711)) * f1( 0.7144942508565797)
w2 ( -0.6420979333387093 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7948138374655288) - present_state_Q (-0.7948652673143711)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1518323891197951 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7910233093440181) - present_state_Q ( -0.7910703615575068)) * f1( 0.7144565908409676)
w2 ( -0.6404224090514746 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7910233093440181) - present_state_Q (-0.7910703615575068)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15299272207904482 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7880666560963743) - present_state_Q ( -0.7963886324242859)) * f1( 0.6599562901478093)
w2 ( -0.6379609336974238 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7880666560963743) - present_state_Q (-0.7963886324242859)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1533821272357262 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.783833454839814) - present_state_Q ( -0.783833454839814)) * f1( 0.7144905381845719)
w2 ( -0.6371979183876072 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.783833454839814) - present_state_Q (-0.783833454839814)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1536916202198097 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.782550149701013) - present_state_Q ( -0.7825907802562119)) * f1( 0.7138139720684258)
w2 ( -0.6365909112475517 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.782550149701013) - present_state_Q (-0.7825907802562119)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15397522736520447 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7821605865996873) - present_state_Q ( -0.7822144926455732)) * f1( 0.7092955552494623)
w2 ( -0.636031130489567 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7821605865996873) - present_state_Q (-0.7822144926455732)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1540931543823626 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7813410081620278) - present_state_Q ( -0.7797748386264881)) * f1( 0.7187438262157416)
w2 ( -0.6358014271961271 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7813410081620278) - present_state_Q (-0.7797748386264881)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1543327506544156 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7798882526723695) - present_state_Q ( -0.781384160901904)) * f1( 0.7056629972208558)
w2 ( -0.6353260802072737 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7798882526723695) - present_state_Q (-0.781384160901904)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15446998340105875 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7798842719183878) - present_state_Q ( -0.7799220196854589)) * f1( 0.7097294134930288)
w2 ( -0.6350553772581669 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7798842719183878) - present_state_Q (-0.7799220196854589)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15508586448681666 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7786733483760067) - present_state_Q ( -0.7872060791055737)) * f1( 0.6594902570253122)
w2 ( -0.6337479530606506 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7786733483760067) - present_state_Q (-0.7872060791055737)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15503892400496977 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7770438006659955) - present_state_Q ( -0.7770438006659955)) * f1( 0.7105956044645408)
w2 ( -0.6338404341767351 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7770438006659955) - present_state_Q (-0.7770438006659955)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1549202639615205 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777032834741909) - present_state_Q ( -0.7761168176384557)) * f1( 0.7176248862860207)
w2 ( -0.6340719256759899 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777032834741909) - present_state_Q (-0.7761168176384557)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15541335158575473 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779445093412534) - present_state_Q ( -0.7852506996047228)) * f1( 0.6613079123536093)
w2 ( -0.6330280508621062 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779445093412534) - present_state_Q (-0.7852506996047228)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15581138588447926 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761461331195492) - present_state_Q ( -0.7836441230419242)) * f1( 0.6601437207176768)
w2 ( -0.6321839194999105 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761461331195492) - present_state_Q (-0.7836441230419242)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15561496708535444 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774641502745992) - present_state_Q ( -0.7746911767863133)) * f1( 0.7083327696949477)
w2 ( -0.6325721357882705 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774641502745992) - present_state_Q (-0.7746911767863133)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15544053054053142 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774996720634731) - present_state_Q ( -0.7750443746513658)) * f1( 0.7104497563629145)
w2 ( -0.6329158774259656 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774996720634731) - present_state_Q (-0.7750443746513658)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15543215269581262 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7759485214717485) - present_state_Q ( -0.7774749470531729)) * f1( 0.6987063217392935)
w2 ( -0.6329326641391259 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7759485214717485) - present_state_Q (-0.7774749470531729)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1553015231344652 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7756781758931268) - present_state_Q ( -0.775728307880372)) * f1( 0.7101324918945031)
w2 ( -0.6331901954983775 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7756781758931268) - present_state_Q (-0.775728307880372)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15521186099173656 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7763191659999439) - present_state_Q ( -0.7763671771273719)) * f1( 0.7089376481841019)
w2 ( -0.6333672590245446 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7763191659999439) - present_state_Q (-0.7763671771273719)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1556958648290034 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761717681451734) - present_state_Q ( -0.7850031134361027)) * f1( 0.6553046174974659)
w2 ( -0.6323332278975227 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761717681451734) - present_state_Q (-0.7850031134361027)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15604062902286747 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7748987103817038) - present_state_Q ( -0.7827246456263576)) * f1( 0.6586037050039402)
w2 ( -0.6316003594551765 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7748987103817038) - present_state_Q (-0.7827246456263576)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1563081629942199 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7727197505678187) - present_state_Q ( -0.781328465441075)) * f1( 0.6595207827641514)
w2 ( -0.6310324508013754 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7727197505678187) - present_state_Q (-0.781328465441075)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15597539139032998 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7725082600933914) - present_state_Q ( -0.7725599574597627)) * f1( 0.7094029610357798)
w2 ( -0.6316891723983161 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7725082600933914) - present_state_Q (-0.7725599574597627)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1558962921361502 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730461710876589) - present_state_Q ( -0.7761643691159643)) * f1( 0.6937022005664061)
w2 ( -0.6318488071173083 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730461710876589) - present_state_Q (-0.7761643691159643)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15579937051819778 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7745322090063911) - present_state_Q ( -0.7760609709642321)) * f1( 0.6961509957223256)
w2 ( -0.6320437221084053 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7745322090063911) - present_state_Q (-0.7760609709642321)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15610454293601975 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7738943807567578) - present_state_Q ( -0.7820123039863627)) * f1( 0.6601368582127343)
w2 ( -0.6313965208809091 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7738943807567578) - present_state_Q (-0.7820123039863627)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1558300617224592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7734687727031009) - present_state_Q ( -0.7734687727031009)) * f1( 0.7077715641847491)
w2 ( -0.6319394555203184 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7734687727031009) - present_state_Q (-0.7734687727031009)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15579232759401337 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7737098683631635) - present_state_Q ( -0.7768259732436448)) * f1( 0.6923520615486697)
w2 ( -0.6320157574232924 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7737098683631635) - present_state_Q (-0.7768259732436448)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15609796821863267 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.773517590472544) - present_state_Q ( -0.781981902086819)) * f1( 0.6601105451982626)
w2 ( -0.6313675373977534 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.773517590472544) - present_state_Q (-0.781981902086819)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15590467999420526 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7729312454313515) - present_state_Q ( -0.7745346739440376)) * f1( 0.7007130179914869)
w2 ( -0.631753720481627 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7729312454313515) - present_state_Q (-0.7745346739440376)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15562344571671727 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7733321153914525) - present_state_Q ( -0.7733856197231306)) * f1( 0.7124198513814706)
w2 ( -0.6323063833358691 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7733321153914525) - present_state_Q (-0.7733856197231306)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1559675438431759 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7751953185797253) - present_state_Q ( -0.7827446999232194)) * f1( 0.6585398252494062)
w2 ( -0.6315748598067346 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7751953185797253) - present_state_Q (-0.7827446999232194)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1556664896869908 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730827715190455) - present_state_Q ( -0.7730827715190455)) * f1( 0.7124689500920479)
w2 ( -0.6321664305953348 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730827715190455) - present_state_Q (-0.7730827715190455)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15545638985084775 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7744360548208845) - present_state_Q ( -0.7744851135393623)) * f1( 0.7101585544608395)
w2 ( -0.6325806194673165 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7744360548208845) - present_state_Q (-0.7744851135393623)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15582573059803376 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7753418012492613) - present_state_Q ( -0.7831371028775996)) * f1( 0.6591930024553099)
w2 ( -0.6317962102819422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7753418012492613) - present_state_Q (-0.7831371028775996)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15611610841014584 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7739932467352783) - present_state_Q ( -0.7818047739755181)) * f1( 0.6591332511326414)
w2 ( -0.6311794473796636 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7739932467352783) - present_state_Q (-0.7818047739755181)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15581395622553582 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7729692081763886) - present_state_Q ( -0.7730326442101881)) * f1( 0.7085661002433237)
w2 ( -0.6317764461047066 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7729692081763886) - present_state_Q (-0.7730326442101881)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18101900429740825 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13443885675900863) - present_state_Q ( -0.13600894596300328)) * f1( 0.6791649532506372)
w2 ( -0.6438720531502612 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13443885675900863) - present_state_Q (-0.13600894596300328)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.18134910183425165 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11579480578377926) - present_state_Q ( -0.11579480578377926)) * f1( 0.783089134903361)
w2 ( -0.6437034401420452 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.11579480578377926) - present_state_Q (-0.11579480578377926)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.18939625790834488 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10464849970466605) - present_state_Q ( -0.10826619222616155)) * f1( 0.8228063018863767)
w2 ( -0.6397913864518173 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.10464849970466605) - present_state_Q (-0.10826619222616155)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.20533320553172116 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08421663715182318) - present_state_Q ( -0.08421663715182318)) * f1( 0.9065644660835644)
w2 ( -0.6327595875143517 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08421663715182318) - present_state_Q (-0.08421663715182318)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.14237787515666295 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054515630718379754) - present_state_Q ( -0.054515630718379754)) * f1( 0.9671509475201846)
w2 ( -0.65879702480849 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.054515630718379754) - present_state_Q (-0.054515630718379754)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09449229708260848 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25137081996775196) - present_state_Q ( -0.25137081996775196)) * f1( 1.0107426786570328)
w2 ( -0.6872230005302314 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.25137081996775196) - present_state_Q (-0.25137081996775196)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05240663994697412 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3165352994393962) - present_state_Q ( -0.3165352994393962)) * f1( 1.0138233891699375)
w2 ( -0.712130094360504 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3165352994393962) - present_state_Q (-0.3165352994393962)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.029304464034896203 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3740857305347213) - present_state_Q ( -0.39220571083043043)) * f1( 0.6692347729478325)
w2 ( -0.7328422660938865 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3740857305347213) - present_state_Q (-0.39220571083043043)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.004237987019841235 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41000849401629347) - present_state_Q ( -0.41000849401629347)) * f1( 1.0133905061247672)
w2 ( -0.7527018074170067 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41000849401629347) - present_state_Q (-0.41000849401629347)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.041080295172927314 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.604741945912993) - present_state_Q ( -0.604741945912993)) * f1( 0.6088975655910247)
w2 ( -0.6931603873112712 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.604741945912993) - present_state_Q (-0.604741945912993)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.08712663011865748 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3879921063511181) - present_state_Q ( -0.5270439435965844)) * f1( 0.6690401355865925)
w2 ( -0.6381008086743534 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3879921063511181) - present_state_Q (-0.5270439435965844)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1277366660861809 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4513299018138477) - present_state_Q ( -0.4521965418736348)) * f1( 0.6689585605052207)
w2 ( -0.5895357245389734 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4513299018138477) - present_state_Q (-0.4521965418736348)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1649211273810678 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3846675336695) - present_state_Q ( -0.3846675336695)) * f1( 0.6807837453891914)
w2 ( -0.5458396621147694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3846675336695) - present_state_Q (-0.3846675336695)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.19835161463634227 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3246537779143852) - present_state_Q ( -0.3246537779143852)) * f1( 0.6792213560279694)
w2 ( -0.5064645901049337 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3246537779143852) - present_state_Q (-0.3246537779143852)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.22826145007802898 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27184383826765135) - present_state_Q ( -0.2717115767898892)) * f1( 0.6728460241614036)
w2 ( -0.4709024146678838 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.27184383826765135) - present_state_Q (-0.2717115767898892)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.255467681946701 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22106236010661065) - present_state_Q ( -0.22106236010661065)) * f1( 0.6819354366428746)
w2 ( -0.4389859247402078 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.22106236010661065) - present_state_Q (-0.22106236010661065)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.27993430838303573 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17740263094516456) - present_state_Q ( -0.17740263094516456)) * f1( 0.6802665116883913)
w2 ( -0.41021293531215597 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17740263094516456) - present_state_Q (-0.17740263094516456)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.30198660017503987 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13748418966767298) - present_state_Q ( -0.13748418966767298)) * f1( 0.6811818089876102)
w2 ( -0.3843140736560835 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13748418966767298) - present_state_Q (-0.13748418966767298)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3150450990764305 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10221916732744915) - present_state_Q ( -0.10524901087606839)) * f1( 0.6695735768792269)
w2 ( -0.3687119061246176 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.10221916732744915) - present_state_Q (-0.10524901087606839)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.33351826429832243 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0809343028953044) - present_state_Q ( -0.08413947047748183)) * f1( 0.6692059487364523)
w2 ( -0.3466282229095815 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0809343028953044) - present_state_Q (-0.08413947047748183)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3298099506614802 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0505161501863734) - present_state_Q ( -0.0505161501863734)) * f1( 0.6799820352220288)
w2 ( -0.3509910600961626 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0505161501863734) - present_state_Q (-0.0505161501863734)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3332659083307144 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056452342050468424) - present_state_Q ( -0.056452342050468424)) * f1( 0.6802114538282282)
w2 ( -0.3469264914685289 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.056452342050468424) - present_state_Q (-0.056452342050468424)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.2887186604451064 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05067353829018417) - present_state_Q ( -0.05067353829018417)) * f1( 0.6807406614765654)
w2 ( -0.39927799671163566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.05067353829018417) - present_state_Q (-0.05067353829018417)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.2485583983821561 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12274103032310632) - present_state_Q ( -0.12274103032310632)) * f1( 0.681221528054287)
w2 ( -0.446440642528372 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.12274103032310632) - present_state_Q (-0.12274103032310632)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.21248171746801933 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18816772070443172) - present_state_Q ( -0.18816772070443172)) * f1( 0.6798595196065492)
w2 ( -0.4888925666376529 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.18816772070443172) - present_state_Q (-0.18816772070443172)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.23436639201637705 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24672460046495204) - present_state_Q ( -0.24672460046495204)) * f1( 0.6795382424697428)
w2 ( -0.46312839540417633 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.24672460046495204) - present_state_Q (-0.24672460046495204)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.2541212368593853 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21271766084793492) - present_state_Q ( -0.210334076316936)) * f1( 0.6834112972785485)
w2 ( -0.44000341058560494 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.21271766084793492) - present_state_Q (-0.210334076316936)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.27869063648608383 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17917135843781276) - present_state_Q ( -0.17917135843781276)) * f1( 0.6801138392314107)
w2 ( -0.41110307277808245 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17917135843781276) - present_state_Q (-0.17917135843781276)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.3008139424205616 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1394863124717507) - present_state_Q ( -0.1394863124717507)) * f1( 0.6795927848123904)
w2 ( -0.3850600582801164 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1394863124717507) - present_state_Q (-0.1394863124717507)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.32064648962758635 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10373783906513656) - present_state_Q ( -0.10675932014918313)) * f1( 0.6691469313396801)
w2 ( -0.36134921538070286 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10373783906513656) - present_state_Q (-0.10675932014918313)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.34301618366318276 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14333021943858149) - present_state_Q ( -0.14333021943858149)) * f1( 0.6799357017609602)
w2 ( -0.32844949563123055 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14333021943858149) - present_state_Q (-0.14333021943858149)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.34202106383247516 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0948736000639189) - present_state_Q ( -0.0948736000639189)) * f1( 0.6809471584485541)
w2 ( -0.32991087162547783 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0948736000639189) - present_state_Q (-0.0948736000639189)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.3548214060119455 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09761921752350294) - present_state_Q ( -0.1010902364567132)) * f1( 0.6690249793528592)
w2 ( -0.3107780401550415 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.09761921752350294) - present_state_Q (-0.1010902364567132)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.37264854443044215 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06958246317304148) - present_state_Q ( -0.07315425458494296)) * f1( 0.669699689883137)
w2 ( -0.2841584393282776 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06958246317304148) - present_state_Q (-0.07315425458494296)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.38813079629705577 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030282533140306045) - present_state_Q ( -0.030282533140306045)) * f1( 0.6812743803306591)
w2 ( -0.26143301134565006 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.030282533140306045) - present_state_Q (-0.030282533140306045)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.34115441379192757 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0031010796824981757) - present_state_Q ( -0.0006165789627977092)) * f1( 0.6719807726445819)
w2 ( -0.3313403426525453 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0031010796824981757) - present_state_Q (-0.0006165789627977092)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.3336162989672151 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09915610470624844) - present_state_Q ( -0.09915610470624844)) * f1( 0.6805840070060112)
w2 ( -0.3424162932289829 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09915610470624844) - present_state_Q (-0.09915610470624844)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2931566745019818 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11584195772540334) - present_state_Q ( -0.11584195772540334)) * f1( 0.6791464811671126)
w2 ( -0.4019905170336966 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.11584195772540334) - present_state_Q (-0.11584195772540334)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.25873894688178484 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20307348149086438) - present_state_Q ( -0.20586134911378762)) * f1( 0.669025081052975)
w2 ( -0.4534351169372265 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20307348149086438) - present_state_Q (-0.20586134911378762)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2054349324019668 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18940597984230034) - present_state_Q ( -0.19180031223738397)) * f1( 1.0111921991372284)
w2 ( -0.5061491455119111 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.18940597984230034) - present_state_Q (-0.19180031223738397)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.16147235163487264 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2972969985549132) - present_state_Q ( -0.2972969985549132)) * f1( 1.0166340481391198)
w2 ( -0.549392415641969 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2972969985549132) - present_state_Q (-0.2972969985549132)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.12546209579839845 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38493242860569954) - present_state_Q ( -0.38493242860569954)) * f1( 1.0185024579821105)
w2 ( -0.584748497067456 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38493242860569954) - present_state_Q (-0.38493242860569954)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0960863844123754 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.457220721446309) - present_state_Q ( -0.45706826961837776)) * f1( 1.017679695501373)
w2 ( -0.6136138773200813 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.457220721446309) - present_state_Q (-0.45706826961837776)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07207105133639129 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.515760137940619) - present_state_Q ( -0.515760137940619)) * f1( 1.0183933965035241)
w2 ( -0.6371954649054257 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.515760137940619) - present_state_Q (-0.515760137940619)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.05240112273583149 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5636383743536254) - present_state_Q ( -0.5636383743536254)) * f1( 1.0206190861358864)
w2 ( -0.6564680112135994 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5636383743536254) - present_state_Q (-0.5636383743536254)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03644387167266776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6032375130601775) - present_state_Q ( -0.6032375130601775)) * f1( 1.015827436022155)
w2 ( -0.6721766350381835 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6032375130601775) - present_state_Q (-0.6032375130601775)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023405943983689503 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.635158648940249) - present_state_Q ( -0.635158648940249)) * f1( 1.0157533873026257)
w2 ( -0.6850123566335611 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.635158648940249) - present_state_Q (-0.635158648940249)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.012714616845467675 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6611682418696392) - present_state_Q ( -0.6611682418696392)) * f1( 1.018720491706625)
w2 ( -0.6955072148652935 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6611682418696392) - present_state_Q (-0.6611682418696392)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.004049942897442416 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6826406410464486) - present_state_Q ( -0.6826406410464486)) * f1( 1.0119513608018325)
w2 ( -0.7040695571711132 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6826406410464486) - present_state_Q (-0.6826406410464486)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0006400999490107301 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6999723730894634) - present_state_Q ( -0.7013044340695703)) * f1( 0.6827560712742505)
w2 ( -0.7109388374950508 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6999723730894634) - present_state_Q (-0.7013044340695703)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.004126545170202827 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7113119067230622) - present_state_Q ( -0.7113119067230622)) * f1( 0.5828296480697277)
w2 ( -0.7169207658899752 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7113119067230622) - present_state_Q (-0.7113119067230622)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006920601278943454 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7191041820113911) - present_state_Q ( -0.7191041820113911)) * f1( 0.5291147997559955)
w2 ( -0.72220138950895 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7191041820113911) - present_state_Q (-0.7191041820113911)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.009222223623137474 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7255079658080955) - present_state_Q ( -0.7255935405605354)) * f1( 0.490152649294558)
w2 ( -0.7268971151109773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7255079658080955) - present_state_Q (-0.7255935405605354)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.011068288987061593 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.730935444034751) - present_state_Q ( -0.730935444034751)) * f1( 0.4378910216015487)
w2 ( -0.7311129251478498 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.730935444034751) - present_state_Q (-0.730935444034751)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.012528047247819885 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7352338520375605) - present_state_Q ( -0.7353448962746616)) * f1( 0.3823509787067203)
w2 ( -0.7349307740407592 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7352338520375605) - present_state_Q (-0.7353448962746616)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.020867249187679977 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2976362454847291) - present_state_Q ( -0.444622400292881)) * f1( 0.29245865664044024)
w2 ( -0.7520392474960947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2976362454847291) - present_state_Q (-0.444622400292881)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.028164567537425987 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4562589011421503) - present_state_Q ( -0.4564901037843221)) * f1( 0.2523837828023145)
w2 ( -0.7693873946758883 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4562589011421503) - present_state_Q (-0.4564901037843221)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03083662288453948 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4676412219992018) - present_state_Q ( -0.6215187009343794)) * f1( 0.21334555148713186)
w2 ( -0.7794070283771316 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4676412219992018) - present_state_Q (-0.6215187009343794)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.033055831148909955 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6286234668150289) - present_state_Q ( -0.6286234668150289)) * f1( 0.1653178473016086)
w2 ( -0.7901461387664496 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6286234668150289) - present_state_Q (-0.6286234668150289)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.025577556305449636 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6354559618810325) - present_state_Q ( -0.6357941312319356)) * f1( 0.11124270940914306)
w2 ( -0.7363662559629429 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.6354559618810325) - present_state_Q (-0.6357941312319356)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.004111550259857474 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16308969189714803) - present_state_Q ( -0.16310525407756227)) * f1( 0.6189802769235023)
w2 ( -0.729430330265186 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16308969189714803) - present_state_Q (-0.16310525407756227)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.014953498510526516 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14823710853576272) - present_state_Q ( -0.14823710853576272)) * f1( 0.5718141173366109)
w2 ( -0.7227620623115423 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14823710853576272) - present_state_Q (-0.14823710853576272)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03643971259592599 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13454610344360238) - present_state_Q ( -0.13454610344360238)) * f1( 0.6691617357411138)
w2 ( -0.7163402324495575 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13454610344360238) - present_state_Q (-0.13454610344360238)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05878823027631499 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11655877021194362) - present_state_Q ( -0.11655877021194362)) * f1( 0.7329716503020396)
w2 ( -0.7102421745857425 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11655877021194362) - present_state_Q (-0.11655877021194362)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.034819558737144124 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3861659282464145) - present_state_Q ( -0.3861659282464145)) * f1( 0.6800574930920857)
w2 ( -0.731389214460436 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3861659282464145) - present_state_Q (-0.3861659282464145)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.020923305507355466 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5596788261153198) - present_state_Q ( -0.5603733849961352)) * f1( 0.7104623800365425)
w2 ( -0.7470367742696677 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5596788261153198) - present_state_Q (-0.5603733849961352)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.00928963035431976 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5836941365612475) - present_state_Q ( -0.5836941365612475)) * f1( 0.6660172719644072)
w2 ( -0.7610107964372579 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5836941365612475) - present_state_Q (-0.5836941365612475)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0125257185254163 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44974216465846556) - present_state_Q ( -0.44974216465846556)) * f1( 0.7389221036870721)
w2 ( -0.7787247195457008 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.44974216465846556) - present_state_Q (-0.44974216465846556)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.031177622951663146 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4758319782336577) - present_state_Q ( -0.4758319782336577)) * f1( 0.6863595480604514)
w2 ( -0.7950297927210832 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4758319782336577) - present_state_Q (-0.4758319782336577)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.023741023535207614 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4982244155268364) - present_state_Q ( -0.657230374071053)) * f1( 0.6801846287981732)
w2 ( -0.7304371581196136 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4982244155268364) - present_state_Q (-0.657230374071053)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06773190279370589 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4201988835672784) - present_state_Q ( -0.4201988835672784)) * f1( 0.7608522554936208)
w2 ( -0.6957464184069806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4201988835672784) - present_state_Q (-0.4201988835672784)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09554580459952926 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5031230262968762) - present_state_Q ( -0.5032143803477289)) * f1( 0.7881478620266407)
w2 ( -0.6675142521895373 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.5031230262968762) - present_state_Q (-0.5032143803477289)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.14140210409942203 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6098915171572422) - present_state_Q ( -0.6089354905282339)) * f1( 0.6130961155942997)
w2 ( -0.5927196183082863 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6098915171572422) - present_state_Q (-0.6089354905282339)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.18006388802298995 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5069445434625248) - present_state_Q ( -0.5097725844893565)) * f1( 0.5866039571844597)
w2 ( -0.526811805293976 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5069445434625248) - present_state_Q (-0.5097725844893565)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.21557044364953046 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38378454282634067) - present_state_Q ( -0.4161559037465659)) * f1( 0.6145368888918021)
w2 ( -0.46903406034758277 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.38378454282634067) - present_state_Q (-0.4161559037465659)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.22105527050937085 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29952649927963704) - present_state_Q ( -0.29983135148415446)) * f1( 0.7849068081824531)
w2 ( -0.4620461901919637 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.29952649927963704) - present_state_Q (-0.29983135148415446)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.23297181363768893 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.288061903291431) - present_state_Q ( -0.33311010897230314)) * f1( 0.5832753090327008)
w2 ( -0.4416157983276477 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.288061903291431) - present_state_Q (-0.33311010897230314)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.24339046518405577 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2579550660198787) - present_state_Q ( -0.2579550660198787)) * f1( 0.7883388528424853)
w2 ( -0.4283998423858586 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2579550660198787) - present_state_Q (-0.2579550660198787)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.2522824063051197 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2358657414451635) - present_state_Q ( -0.23617304671927888)) * f1( 0.7897876998641452)
w2 ( -0.4171411951283823 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2358657414451635) - present_state_Q (-0.23617304671927888)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.21255060396938455 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21780693874863105) - present_state_Q ( -0.2181239936266639)) * f1( 0.7888667482464854)
w2 ( -0.4675068651532023 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21780693874863105) - present_state_Q (-0.2181239936266639)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.18908332587920268 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29864286054704187) - present_state_Q ( -0.3396726504604798)) * f1( 0.6014295528002147)
w2 ( -0.5065260287126248 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.29864286054704187) - present_state_Q (-0.3396726504604798)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.15888483503708312 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3564629783796668) - present_state_Q ( -0.35608741746576666)) * f1( 0.7956207166726429)
w2 ( -0.5444819167498447 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3564629783796668) - present_state_Q (-0.35608741746576666)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.13328284509543 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4186341538109213) - present_state_Q ( -0.4186341538109213)) * f1( 0.7920690663117789)
w2 ( -0.5768048429068617 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4186341538109213) - present_state_Q (-0.4186341538109213)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.11145468547132115 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47111989010254085) - present_state_Q ( -0.4713165444233289)) * f1( 0.791461935014995)
w2 ( -0.6043843873655542 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.47111989010254085) - present_state_Q (-0.4713165444233289)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.09283262664438714 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5160534853126484) - present_state_Q ( -0.51621211254635)) * f1( 0.7911042451588288)
w2 ( -0.6279237109640456 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5160534853126484) - present_state_Q (-0.51621211254635)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07695326312001223 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5547439894963481) - present_state_Q ( -0.5545548798873636)) * f1( 0.7903345378531096)
w2 ( -0.6480156628702728 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5547439894963481) - present_state_Q (-0.5545548798873636)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06336549109125657 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.586987917631509) - present_state_Q ( -0.5870863250375297)) * f1( 0.7917706847300413)
w2 ( -0.6651769095428349 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.586987917631509) - present_state_Q (-0.5870863250375297)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.051784693063745835 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.615066219888959) - present_state_Q ( -0.615066219888959)) * f1( 0.7908198735761142)
w2 ( -0.6798209497528286 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.615066219888959) - present_state_Q (-0.615066219888959)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04197028223730806 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6391001190159404) - present_state_Q ( -0.6391001190159404)) * f1( 0.7863487901098829)
w2 ( -0.6923019390413939 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6391001190159404) - present_state_Q (-0.6391001190159404)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03348808032650661 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6589392223197285) - present_state_Q ( -0.6589983651645471)) * f1( 0.7935036912199442)
w2 ( -0.7029914947481365 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6589392223197285) - present_state_Q (-0.6589983651645471)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02625922556636085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6764469320512553) - present_state_Q ( -0.6764469320512553)) * f1( 0.7926570420900054)
w2 ( -0.7121112708635235 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6764469320512553) - present_state_Q (-0.6764469320512553)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02010101968208075 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6912896480519748) - present_state_Q ( -0.6913265936186928)) * f1( 0.791519048888356)
w2 ( -0.719891507982174 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6912896480519748) - present_state_Q (-0.6913265936186928)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.014826841433211054 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7039109390727387) - present_state_Q ( -0.7039379557828933)) * f1( 0.7936688014639738)
w2 ( -0.7265368217946121 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7039109390727387) - present_state_Q (-0.7039379557828933)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.010333523619675426 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7147643989696038) - present_state_Q ( -0.714785154751097)) * f1( 0.7925941001292505)
w2 ( -0.7322059503091984 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7147643989696038) - present_state_Q (-0.714785154751097)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.006546953006169547 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7240919677849723) - present_state_Q ( -0.7241054206935756)) * f1( 0.7839077853559137)
w2 ( -0.7370363279176906 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7240919677849723) - present_state_Q (-0.7241054206935756)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0032850000679503417 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7318814104890666) - present_state_Q ( -0.7318679464942642)) * f1( 0.789433102476212)
w2 ( -0.7411683473731548 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7318814104890666) - present_state_Q (-0.7318679464942642)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.000499731311048733 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7385813459678351) - present_state_Q ( -0.7385751410871878)) * f1( 0.7894082899015135)
w2 ( -0.7446966467241144 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7385813459678351) - present_state_Q (-0.7385751410871878)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.001882607346692251 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7443024739672623) - present_state_Q ( -0.7443014994636181)) * f1( 0.7907194361445954)
w2 ( -0.7477095215174252 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7443024739672623) - present_state_Q (-0.7443014994636181)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.003364978304981357 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7487788015875967) - present_state_Q ( -0.7487788015875967)) * f1( 0.567978273350612)
w2 ( -0.7503194293745414 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7487788015875967) - present_state_Q (-0.7487788015875967)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.004594388308874359 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.75204351485894) - present_state_Q ( -0.7521108126813817)) * f1( 0.5323610271686845)
w2 ( -0.7526287832549927 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.75204351485894) - present_state_Q (-0.7521108126813817)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023448477072519312 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7547606652194617) - present_state_Q ( -0.7548525499514415)) * f1( 0.4840180121809557)
w2 ( -0.6946911349120432 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.7547606652194617) - present_state_Q (-0.7548525499514415)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.01868383341495288 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6817622496117843) - present_state_Q ( -0.6817622496117843)) * f1( 0.5513742005620896)
w2 ( -0.7033325324469826 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6817622496117843) - present_state_Q (-0.6817622496117843)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.013941545188146592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6920574960163405) - present_state_Q ( -0.6918747560535783)) * f1( 0.6132454801397702)
w2 ( -0.7110656318017882 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6920574960163405) - present_state_Q (-0.6918747560535783)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.008547049361641381 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7000009196237534) - present_state_Q ( -0.7002787475342307)) * f1( 0.7737222898885467)
w2 ( -0.7180377662446026 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7000009196237534) - present_state_Q (-0.7002787475342307)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.00385560840352056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7112570424816731) - present_state_Q ( -0.7113317417902889)) * f1( 0.7846011144395536)
w2 ( -0.7240171624903904 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7112570424816731) - present_state_Q (-0.7113317417902889)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0007574345283958566 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7209905853998699) - present_state_Q ( -0.7216493878233197)) * f1( 0.6141118130432406)
w2 ( -0.7290621295620571 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7209905853998699) - present_state_Q (-0.7216493878233197)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0027503897209610077 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.728462507780427) - present_state_Q ( -0.7284634856518695)) * f1( 0.790357301845586)
w2 ( -0.7335004060746744 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.728462507780427) - present_state_Q (-0.7284634856518695)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.004916020384695175 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7350215732332043) - present_state_Q ( -0.7350494053019934)) * f1( 0.5631926324890983)
w2 ( -0.7373456812768071 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7350215732332043) - present_state_Q (-0.7350494053019934)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006575979243410415 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7397286818564539) - present_state_Q ( -0.7397286818564539)) * f1( 0.4847418019391536)
w2 ( -0.7407700999097262 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7397286818564539) - present_state_Q (-0.7397286818564539)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.007262608138289658 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.595381855488391) - present_state_Q ( -0.7436037468608827)) * f1( 0.4309087432105257)
w2 ( -0.7423635437785219 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.595381855488391) - present_state_Q (-0.7436037468608827)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.017111286178606328 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29948189682582343) - present_state_Q ( -0.4479546055815279)) * f1( 0.3492518481125203)
w2 ( -0.7592831588245852 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.29948189682582343) - present_state_Q (-0.4479546055815279)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.029025351022755795 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3085406013486344) - present_state_Q ( -0.3085406013486344)) * f1( 0.2821142588822928)
w2 ( -0.7761756971760343 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3085406013486344) - present_state_Q (-0.3085406013486344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03577871817997404 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47284800469870986) - present_state_Q ( -0.47284800469870986)) * f1( 0.24608096513594102)
w2 ( -0.7926419049223039 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.47284800469870986) - present_state_Q (-0.47284800469870986)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03840289978079497 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6413395896749136) - present_state_Q ( -0.6417876285582942)) * f1( 0.2144879696877006)
w2 ( -0.8024296113550397 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6413395896749136) - present_state_Q (-0.6417876285582942)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04283092426978645 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.487594969646026) - present_state_Q ( -0.48797852558534793)) * f1( 0.16979860399982133)
w2 ( -0.8180764696377949 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.487594969646026) - present_state_Q (-0.48797852558534793)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0385788186686811 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.49529459500622436) - present_state_Q ( -0.6589098889337833)) * f1( 0.10386685086517032)
w2 ( -0.785326035283142 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.49529459500622436) - present_state_Q (-0.6589098889337833)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.028649233102187963 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16720174008485736) - present_state_Q ( -0.1679711326243319)) * f1( 0.28269205600525804)
w2 ( -0.7783010161108251 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16720174008485736) - present_state_Q (-0.1679711326243319)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.020144072521056593 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.162693951936499) - present_state_Q ( -0.162693951936499)) * f1( 0.24551263516358482)
w2 ( -0.7713725249759682 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.162693951936499) - present_state_Q (-0.162693951936499)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.000704822739399754 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46795736605929206) - present_state_Q ( -0.6225121334354736)) * f1( 0.2687695573494197)
w2 ( -0.7093152132296047 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.46795736605929206) - present_state_Q (-0.6225121334354736)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.026827359561413095 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7090955512685636) - present_state_Q ( -0.7090955512685636)) * f1( 0.31165561035688016)
w2 ( -0.625496613615434 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7090955512685636) - present_state_Q (-0.7090955512685636)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.057566066308049196 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7408517027770329) - present_state_Q ( -0.7410845297925426)) * f1( 0.35454128551879266)
w2 ( -0.5214566904736532 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7408517027770329) - present_state_Q (-0.7410845297925426)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.09560679275139478 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8102055143754004) - present_state_Q ( -0.8107777272954959)) * f1( 0.4091468980407984)
w2 ( -0.37269554233638025 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.8102055143754004) - present_state_Q (-0.8107777272954959)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.11494307381012923 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4758817585377821) - present_state_Q ( -0.5504208670050581)) * f1( 0.4800077422582594)
w2 ( -0.3082423117521754 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.4758817585377821) - present_state_Q (-0.5504208670050581)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.10575133197072153 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.45663345026890106) - present_state_Q ( -0.45663345026890106)) * f1( 0.31802045415074254)
w2 ( -0.35448709491345365 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.45663345026890106) - present_state_Q (-0.45663345026890106)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.12481648044723884 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3840198814998861) - present_state_Q ( -0.5258147194652676)) * f1( 0.3911499895595702)
w2 ( -0.276501057903009 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.3840198814998861) - present_state_Q (-0.5258147194652676)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.10729167246244187 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27653848254037305) - present_state_Q ( -0.3318386941209748)) * f1( 0.4427523252155626)
w2 ( -0.33191517948163773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.27653848254037305) - present_state_Q (-0.3318386941209748)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09165176013351004 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34512150664695423) - present_state_Q ( -0.4125606026466597)) * f1( 0.48578465999659315)
w2 ( -0.37698839620416275 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.34512150664695423) - present_state_Q (-0.4125606026466597)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.07755582564617432 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4030494790962682) - present_state_Q ( -0.4784471583371008)) * f1( 0.5383049521019351)
w2 ( -0.41364848674431637 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4030494790962682) - present_state_Q (-0.4784471583371008)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06472151596431386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5330137203864392) - present_state_Q ( -0.5337674798316653)) * f1( 0.5846163229211162)
w2 ( -0.44438323165329335 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5330137203864392) - present_state_Q (-0.5337674798316653)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05883530814226406 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6719214729322295) - present_state_Q ( -0.6712892215873859)) * f1( 0.6137672838161956)
w2 ( -0.4597276997662273 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6719214729322295) - present_state_Q (-0.6712892215873859)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.05254488634698355 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6891602787153404) - present_state_Q ( -0.6891602787153404)) * f1( 0.788710765284311)
w2 ( -0.4724886196312183 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6891602787153404) - present_state_Q (-0.6891602787153404)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.04804136477111965 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7145691608143971) - present_state_Q ( -0.7144624743830196)) * f1( 0.7901685570838295)
w2 ( -0.4816077303029655 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7145691608143971) - present_state_Q (-0.7144624743830196)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.044800720179711734 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7324280649645161) - present_state_Q ( -0.7324280649645161)) * f1( 0.7939887574376213)
w2 ( -0.4881380889480752 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7324280649645161) - present_state_Q (-0.7324280649645161)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.042492434880062074 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.745463226990012) - present_state_Q ( -0.745463226990012)) * f1( 0.7936862439771862)
w2 ( -0.4927913842615135 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.745463226990012) - present_state_Q (-0.745463226990012)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.040852527997054744 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7547878596665616) - present_state_Q ( -0.7547878596665616)) * f1( 0.792572966150789)
w2 ( -0.4961019324695286 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7547878596665616) - present_state_Q (-0.7547878596665616)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03969802188293028 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7615231155607685) - present_state_Q ( -0.7615231155607685)) * f1( 0.7891794699412914)
w2 ( -0.4984426038287779 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7615231155607685) - present_state_Q (-0.7615231155607685)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.039557040262784345 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7663542656081489) - present_state_Q ( -0.7742310514796799)) * f1( 0.5863545220215046)
w2 ( -0.4988273038417595 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7663542656081489) - present_state_Q (-0.7742310514796799)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.038802343216658874 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7670289549393258) - present_state_Q ( -0.7670847792554178)) * f1( 0.7846620142761062)
w2 ( -0.5003662024399218 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7670289549393258) - present_state_Q (-0.7670847792554178)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03881903853295989 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7698255985640153) - present_state_Q ( -0.7772602874955894)) * f1( 0.6011398919401155)
w2 ( -0.5003217660176518 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7698255985640153) - present_state_Q (-0.7772602874955894)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.038243275670255164 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7696568644105539) - present_state_Q ( -0.7697101194968782)) * f1( 0.7935463446682094)
w2 ( -0.5014826567287202 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7696568644105539) - present_state_Q (-0.7697101194968782)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.038346908713538 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7723401520726456) - present_state_Q ( -0.7789242520090957)) * f1( 0.6131273628083592)
w2 ( -0.5012122188404272 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7723401520726456) - present_state_Q (-0.7789242520090957)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03792367735498215 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7717381644449541) - present_state_Q ( -0.7717906613949171)) * f1( 0.7862143197770367)
w2 ( -0.5020735236483598 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7717381644449541) - present_state_Q (-0.7717906613949171)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03764511743782363 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7733463451337604) - present_state_Q ( -0.7737605308541916)) * f1( 0.7793839902843968)
w2 ( -0.5026453802338293 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7733463451337604) - present_state_Q (-0.7737605308541916)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03742040499890215 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7746918541389005) - present_state_Q ( -0.774613179577017)) * f1( 0.7868066515141208)
w2 ( -0.503102341167729 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7746918541389005) - present_state_Q (-0.774613179577017)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.037253261399210945 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775425086489328) - present_state_Q ( -0.775425086489328)) * f1( 0.789373054083867)
w2 ( -0.5034411287132657 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775425086489328) - present_state_Q (-0.775425086489328)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03758369424577063 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7760943748070698) - present_state_Q ( -0.7831045257548375)) * f1( 0.6013240007722463)
w2 ( -0.5025619145894049 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7760943748070698) - present_state_Q (-0.7831045257548375)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03785349302423527 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774596294837352) - present_state_Q ( -0.7820607162968423)) * f1( 0.586380543170945)
w2 ( -0.5018257406993077 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774596294837352) - present_state_Q (-0.7820607162968423)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03751912397199917 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7730670778820333) - present_state_Q ( -0.7730670778820333)) * f1( 0.788675095789583)
w2 ( -0.5025040814842949 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7730670778820333) - present_state_Q (-0.7730670778820333)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.0377715056511194 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774484352370388) - present_state_Q ( -0.7816919026217161)) * f1( 0.5947534321379514)
w2 ( -0.5018251267027466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774484352370388) - present_state_Q (-0.7816919026217161)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.037436826721354954 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7731587243987031) - present_state_Q ( -0.7730795897890137)) * f1( 0.7900297438764281)
w2 ( -0.5025029319268837 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7731587243987031) - present_state_Q (-0.7730795897890137)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.0371921786754632 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7742903087365315) - present_state_Q ( -0.7743414252922661)) * f1( 0.7923552391748804)
w2 ( -0.5029969488199055 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7742903087365315) - present_state_Q (-0.7743414252922661)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.037061758872345 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7754570528317503) - present_state_Q ( -0.7758688238360911)) * f1( 0.7777520786874819)
w2 ( -0.503265249851439 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7754570528317503) - present_state_Q (-0.7758688238360911)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036920388235531613 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7757993272742577) - present_state_Q ( -0.7757993272742577)) * f1( 0.7939470058449182)
w2 ( -0.5035501467239458 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7757993272742577) - present_state_Q (-0.7757993272742577)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036822625479824386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.776407725218264) - present_state_Q ( -0.776407725218264)) * f1( 0.7928548679744938)
w2 ( -0.5037474342925158 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.776407725218264) - present_state_Q (-0.776407725218264)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03678131900558946 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7767617248686141) - present_state_Q ( -0.7771489033504191)) * f1( 0.7834039844174554)
w2 ( -0.5038317973543466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7767617248686141) - present_state_Q (-0.7771489033504191)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03675096331342496 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.776918250100674) - present_state_Q ( -0.7773044983676414)) * f1( 0.783723318756801)
w2 ( -0.5038937696171347 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.776918250100674) - present_state_Q (-0.7773044983676414)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03670777011769673 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7771708210267938) - present_state_Q ( -0.7771708210267938)) * f1( 0.7907060860635076)
w2 ( -0.5039811713892764 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7771708210267938) - present_state_Q (-0.7771708210267938)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03667424489861787 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7773072851560834) - present_state_Q ( -0.7773072851560834)) * f1( 0.7917285352276933)
w2 ( -0.5040489223268004 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7773072851560834) - present_state_Q (-0.7773072851560834)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036680738652547784 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7774205979902918) - present_state_Q ( -0.77782517594485)) * f1( 0.7812867001689959)
w2 ( -0.5040356237434691 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7774205979902918) - present_state_Q (-0.77782517594485)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036672162635184774 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775937959697112) - present_state_Q ( -0.7776501781463131)) * f1( 0.7853391425975736)
w2 ( -0.5040530959755744 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775937959697112) - present_state_Q (-0.7776501781463131)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03664783480487691 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7773874250196) - present_state_Q ( -0.777431667376636)) * f1( 0.792243601047079)
w2 ( -0.5041022279956262 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7773874250196) - present_state_Q (-0.777431667376636)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03663344358813071 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775213038875104) - present_state_Q ( -0.7775702241570484)) * f1( 0.7911337952247948)
w2 ( -0.5041313329926986 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775213038875104) - present_state_Q (-0.7775702241570484)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03661801171137196 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777561541327718) - present_state_Q ( -0.777561541327718)) * f1( 0.7929527943698862)
w2 ( -0.5041624710415072 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777561541327718) - present_state_Q (-0.777561541327718)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036614012333857346 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777215475240956) - present_state_Q ( -0.7777215475240956)) * f1( 0.7902779203418361)
w2 ( -0.5041705681980374 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777215475240956) - present_state_Q (-0.7777215475240956)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036618196532859464 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779174439719032) - present_state_Q ( -0.7778448872436)) * f1( 0.7873494336102113)
w2 ( -0.5041620653426119 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779174439719032) - present_state_Q (-0.7778448872436)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03661107816352295 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777595068614324) - present_state_Q ( -0.7776859845217134)) * f1( 0.7912273888329342)
w2 ( -0.5041764599289207 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777595068614324) - present_state_Q (-0.7776859845217134)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036601601045713056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776450109179283) - present_state_Q ( -0.7776450109179283)) * f1( 0.793129468590081)
w2 ( -0.504195578356739 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776450109179283) - present_state_Q (-0.7776450109179283)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03660545272640044 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779125792453478) - present_state_Q ( -0.7778400850153716)) * f1( 0.7888409121598404)
w2 ( -0.504187766022205 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779125792453478) - present_state_Q (-0.7778400850153716)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036601151454856 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777174167447259) - present_state_Q ( -0.7777174167447259)) * f1( 0.7917675300297329)
w2 ( -0.5041964580109645 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777174167447259) - present_state_Q (-0.7777174167447259)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03664107316740671 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779115494197024) - present_state_Q ( -0.778305495114088)) * f1( 0.7761733326476037)
w2 ( -0.5041141635834256 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779115494197024) - present_state_Q (-0.778305495114088)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03666968927701091 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777476531973855) - present_state_Q ( -0.7781434552714693)) * f1( 0.7761564824282825)
w2 ( -0.5040551731911487 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777476531973855) - present_state_Q (-0.7781434552714693)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03665432056676713 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776409601357144) - present_state_Q ( -0.7775692190476297)) * f1( 0.788636572285827)
w2 ( -0.5040863535056994 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776409601357144) - present_state_Q (-0.7775692190476297)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036641665344662094 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775388218533898) - present_state_Q ( -0.7775936210463613)) * f1( 0.789662558607088)
w2 ( -0.5041119952879358 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775388218533898) - present_state_Q (-0.7775936210463613)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03662450127529658 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776156935596702) - present_state_Q ( -0.7775449560868779)) * f1( 0.7923830999687113)
w2 ( -0.5041466534109901 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776156935596702) - present_state_Q (-0.7775449560868779)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03704304289977718 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777171748796406) - present_state_Q ( -0.7847884414133166)) * f1( 0.5964915093329302)
w2 ( -0.5030239775829337 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777171748796406) - present_state_Q (-0.7847884414133166)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.03689073582846109 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7757198594749631) - present_state_Q ( -0.7756397319496602)) * f1( 0.7882352500585036)
w2 ( -0.5033331382225875 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7757198594749631) - present_state_Q (-0.7756397319496602)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036780315774585286 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761697751823707) - present_state_Q ( -0.7762178849602329)) * f1( 0.7892262255567404)
w2 ( -0.5035569930318682 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761697751823707) - present_state_Q (-0.7762178849602329)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.036688025979739664 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7765651273592873) - present_state_Q ( -0.7764939218018259)) * f1( 0.7938286127858138)
w2 ( -0.5037430075813246 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7765651273592873) - present_state_Q (-0.7764939218018259)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.03681862705142815 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3169286169560931) - present_state_Q ( -0.31777957909257526)) * f1( 0.09458985799067265)
w2 ( -0.7778825665872634 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.3169286169560931) - present_state_Q (-0.31777957909257526)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.02507712704420416 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3200115653460611) - present_state_Q ( -0.3200115653460611)) * f1( 0.24059937647273363)
w2 ( -0.7583621502348052 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3200115653460611) - present_state_Q (-0.3200115653460611)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.015327160501790331 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3084640418489483) - present_state_Q ( -0.3084640418489483)) * f1( 0.2041374893544395)
w2 ( -0.7392574447282431 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3084640418489483) - present_state_Q (-0.3084640418489483)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0015548060392794605 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44765848404062014) - present_state_Q ( -0.4478449917805238)) * f1( 0.2799295370513504)
w2 ( -0.7030726961256554 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.44765848404062014) - present_state_Q (-0.4478449917805238)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.020067639069564158 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42135949334410516) - present_state_Q ( -0.4213466678835842)) * f1( 0.3196217272473178)
w2 ( -0.6683200530127049 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.42135949334410516) - present_state_Q (-0.4213466678835842)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04129924347884324 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.39348739916924513) - present_state_Q ( -0.3933006022242195)) * f1( 0.38327526007126644)
w2 ( -0.6350829412742672 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.39348739916924513) - present_state_Q (-0.3933006022242195)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06972505995309733 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.48974521685470396) - present_state_Q ( -0.48974521685470396)) * f1( 0.44361917123482936)
w2 ( -0.5838212856607286 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.48974521685470396) - present_state_Q (-0.48974521685470396)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09773339827085735 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.433055019358015) - present_state_Q ( -0.43399600847182396)) * f1( 0.4741626623053231)
w2 ( -0.5365660451378468 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.433055019358015) - present_state_Q (-0.43399600847182396)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.10933504332359775 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3828984488008277) - present_state_Q ( -0.3828984488008277)) * f1( 0.47429423441292495)
w2 ( -0.5169973568241872 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.3828984488008277) - present_state_Q (-0.3828984488008277)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.08978700502621886 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3571414704492897) - present_state_Q ( -0.3571414704492897)) * f1( 0.5163615735072854)
w2 ( -0.5472831709518384 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3571414704492897) - present_state_Q (-0.3571414704492897)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06964149852494446 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38646634567387245) - present_state_Q ( -0.38646634567387245)) * f1( 0.5720225446054308)
w2 ( -0.5754575940633196 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38646634567387245) - present_state_Q (-0.38646634567387245)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04958380124321263 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3008138688287525) - present_state_Q ( -0.4159053876414165)) * f1( 0.6384223279358958)
w2 ( -0.6005916740026364 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3008138688287525) - present_state_Q (-0.4159053876414165)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.029689799747563913 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3259303124139126) - present_state_Q ( -0.4460486472144399)) * f1( 0.6942729505310269)
w2 ( -0.6235152247247925 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3259303124139126) - present_state_Q (-0.4460486472144399)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.010809333396103269 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47811898461747593) - present_state_Q ( -0.47803372818941714)) * f1( 0.6998515236574389)
w2 ( -0.6450974783465789 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.47811898461747593) - present_state_Q (-0.47803372818941714)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.006069049946306276 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5085215715150407) - present_state_Q ( -0.5085484217950527)) * f1( 0.6965795767687969)
w2 ( -0.664481777175095 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5085215715150407) - present_state_Q (-0.5085484217950527)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.019936133443185424 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.535324981594253) - present_state_Q ( -0.535444413975581)) * f1( 0.6358478295031217)
w2 ( -0.6819288239098026 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.535324981594253) - present_state_Q (-0.535444413975581)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.031343220246500776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5567371888077559) - present_state_Q ( -0.5569889920026092)) * f1( 0.5741300291446253)
w2 ( -0.6978236020600559 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5567371888077559) - present_state_Q (-0.5569889920026092)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04061281105617881 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5741091860797227) - present_state_Q ( -0.5741091860797227)) * f1( 0.5057012108845972)
w2 ( -0.7124877406623159 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5741091860797227) - present_state_Q (-0.5741091860797227)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04970830678571123 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5918547887760842) - present_state_Q ( -0.5920979909060314)) * f1( 0.5443552859613072)
w2 ( -0.7258547397000421 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5918547887760842) - present_state_Q (-0.5920979909060314)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05006635281488157 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6100689144444894) - present_state_Q ( -0.7548737096315034)) * f1( 0.5837851218018714)
w2 ( -0.7264680578813366 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6100689144444894) - present_state_Q (-0.7548737096315034)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.044385656254551674 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.898038909642595) - present_state_Q ( -0.898038909642595)) * f1( 0.5248483004573968)
w2 ( -0.7134798556399363 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.898038909642595) - present_state_Q (-0.898038909642595)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04027687405261641 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8766669011505632) - present_state_Q ( -0.8766669011505632)) * f1( 0.46165982688468626)
w2 ( -0.7027998303156755 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8766669011505632) - present_state_Q (-0.8766669011505632)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03736113040165598 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.859355120089889) - present_state_Q ( -0.859355120089889)) * f1( 0.39713418896864816)
w2 ( -0.6939894773459675 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.859355120089889) - present_state_Q (-0.859355120089889)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03491556871309493 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7062643429472353) - present_state_Q ( -0.8450622384164289)) * f1( 0.3285464189467823)
w2 ( -0.6850571808513628 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7062643429472353) - present_state_Q (-0.8450622384164289)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04327458906276099 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42010083743918014) - present_state_Q ( -0.42010083743918014)) * f1( 0.2596700916677904)
w2 ( -0.7043717356296471 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42010083743918014) - present_state_Q (-0.42010083743918014)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04422319811081129 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5729384334340217) - present_state_Q ( -0.7138127805599511)) * f1( 0.21816602155624765)
w2 ( -0.7087198419079922 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5729384334340217) - present_state_Q (-0.7138127805599511)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.05028102389071824 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2895851758106581) - present_state_Q ( -0.2895851758106581)) * f1( 0.13787422230710664)
w2 ( -0.7262947755788085 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2895851758106581) - present_state_Q (-0.2895851758106581)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05386970281892531 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2946679314672811) - present_state_Q ( -0.2946679314672811)) * f1( 0.08253652997952876)
w2 ( -0.7436867300459864 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2946679314672811) - present_state_Q (-0.2946679314672811)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06535781611713654 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16407092008290625) - present_state_Q ( -0.3128082660921035)) * f1( 0.28464189092058684)
w2 ( -0.7598306830826339 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.16407092008290625) - present_state_Q (-0.3128082660921035)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0770755778983067 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16587256273487094) - present_state_Q ( -0.16587256273487094)) * f1( 0.2127737269161589)
w2 ( -0.7708449769534063 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.16587256273487094) - present_state_Q (-0.16587256273487094)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08598964459647593 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3247959200456069) - present_state_Q ( -0.32520772569353934)) * f1( 0.2188726360824007)
w2 ( -0.7871358516058471 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3247959200456069) - present_state_Q (-0.32520772569353934)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09406305326473031 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3321645577354742) - present_state_Q ( -0.3321645577354742)) * f1( 0.2013058336776142)
w2 ( -0.80317792752737 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3321645577354742) - present_state_Q (-0.3321645577354742)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10150255220313048 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3389071580155412) - present_state_Q ( -0.3389917010387572)) * f1( 0.18838990881931744)
w2 ( -0.818973888117882 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3389071580155412) - present_state_Q (-0.3389917010387572)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10678246480532398 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34123002833570965) - present_state_Q ( -0.34123002833570965)) * f1( 0.13438551831937234)
w2 ( -0.8346896070977964 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.34123002833570965) - present_state_Q (-0.34123002833570965)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10749237950960243 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3421063434118387) - present_state_Q ( -0.3421063434118387)) * f1( 0.07707726720604591)
w2 ( -0.8383737787349702 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3421063434118387) - present_state_Q (-0.3421063434118387)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11317822099547213 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19488665013549172) - present_state_Q ( -0.19488665013549172)) * f1( 0.25315184678804875)
w2 ( -0.8428658190325313 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19488665013549172) - present_state_Q (-0.19488665013549172)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1185959283486275 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19599509465847217) - present_state_Q ( -0.19599509465847217)) * f1( 0.24228982052176762)
w2 ( -0.8473379073286789 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19599509465847217) - present_state_Q (-0.19599509465847217)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12325214259857685 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19396298761264752) - present_state_Q ( -0.19396298761264752)) * f1( 0.2065450853835759)
w2 ( -0.8518465735516512 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.19396298761264752) - present_state_Q (-0.19396298761264752)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12372629860310265 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19129739128262205) - present_state_Q ( -0.19158925596313411)) * f1( 0.1721669157664518)
w2 ( -0.8523973832149537 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19129739128262205) - present_state_Q (-0.19158925596313411)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1215246717056534 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18669155041565386) - present_state_Q ( -0.18669155041565386)) * f1( 0.1310317527938764)
w2 ( -0.8490369353074719 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.18669155041565386) - present_state_Q (-0.18669155041565386)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1205465975844583 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17725793197326103) - present_state_Q ( -0.17725793197326103)) * f1( 0.061308907954202886)
w2 ( -0.8458462925319532 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17725793197326103) - present_state_Q (-0.17725793197326103)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11366143173139547 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5368213845907144) - present_state_Q ( -0.5368213845907144)) * f1( 0.24317243007215056)
w2 ( -0.8288579377640546 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5368213845907144) - present_state_Q (-0.5368213845907144)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10274115720935444 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3531373296334255) - present_state_Q ( -0.3554083341413755)) * f1( 0.20996708093693364)
w2 ( -0.8080541537169332 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3531373296334255) - present_state_Q (-0.3554083341413755)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09813125314188985 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17486368309090028) - present_state_Q ( -0.17486368309090028)) * f1( 0.12899263262636237)
w2 ( -0.800906607421297 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17486368309090028) - present_state_Q (-0.17486368309090028)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08697570769553417 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3419234199556287) - present_state_Q ( -0.3419234199556287)) * f1( 0.21971366202706782)
w2 ( -0.7805973643028944 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3419234199556287) - present_state_Q (-0.3419234199556287)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07868421248690764 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.326832879934316) - present_state_Q ( -0.326832879934316)) * f1( 0.1677932218067773)
w2 ( -0.760831380625259 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.326832879934316) - present_state_Q (-0.326832879934316)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07307809109965709 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31348174397407214) - present_state_Q ( -0.31348174397407214)) * f1( 0.11627735011633789)
w2 ( -0.7415460378421924 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.31348174397407214) - present_state_Q (-0.31348174397407214)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06922055328479561 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3025867560324397) - present_state_Q ( -0.3025867560324397)) * f1( 0.08167072792615276)
w2 ( -0.7226529146250246 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3025867560324397) - present_state_Q (-0.3025867560324397)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06049009193223638 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16135109895640745) - present_state_Q ( -0.16200334574662595)) * f1( 0.2524216001240624)
w2 ( -0.7157355499080048 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16135109895640745) - present_state_Q (-0.16200334574662595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05300787401100444 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15642819770478364) - present_state_Q ( -0.15642819770478364)) * f1( 0.21955806808924547)
w2 ( -0.7089198423493187 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15642819770478364) - present_state_Q (-0.15642819770478364)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04779352066399717 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15003401413061768) - present_state_Q ( -0.15003401413061768)) * f1( 0.15563811631157337)
w2 ( -0.7022192300949676 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15003401413061768) - present_state_Q (-0.15003401413061768)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0435411558579271 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14656705031550002) - present_state_Q ( -0.14656705031550002)) * f1( 0.12811787479634473)
w2 ( -0.6955810231892886 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14656705031550002) - present_state_Q (-0.14656705031550002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.007836095711667969 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5839946271219039) - present_state_Q ( -0.7231108317597615)) * f1( 0.6322709635982439)
w2 ( -0.6391098862845315 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.5839946271219039) - present_state_Q (-0.7231108317597615)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.022915327809909333 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6439029251365763) - present_state_Q ( -0.7716820120009604)) * f1( 0.6061881623586602)
w2 ( -0.5782348799460552 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6439029251365763) - present_state_Q (-0.7716820120009604)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01786132679102108 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5632732001456238) - present_state_Q ( -0.6789201761348349)) * f1( 0.6529114453235721)
w2 ( -0.5875237372116224 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5632732001456238) - present_state_Q (-0.6789201761348349)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.02064946159369213 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6929218974449953) - present_state_Q ( -0.8104266448873197)) * f1( 0.6778100725998567)
w2 ( -0.5817649134916276 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6929218974449953) - present_state_Q (-0.8104266448873197)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.022800850525219563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6845687767216448) - present_state_Q ( -0.8006873183018427)) * f1( 0.667502177908918)
w2 ( -0.5772526518034726 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6845687767216448) - present_state_Q (-0.8006873183018427)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.016768222963681842 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5601480198148041) - present_state_Q ( -0.6755985501754987)) * f1( 0.7501751730598553)
w2 ( -0.5869026020201904 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5601480198148041) - present_state_Q (-0.6755985501754987)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.010346701681167688 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.69056479083587) - present_state_Q ( -0.69056479083587)) * f1( 0.8181148126471758)
w2 ( -0.5963216046099165 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.69056479083587) - present_state_Q (-0.69056479083587)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0038100181531799115 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4678820354969089) - present_state_Q ( -0.5871463564188921)) * f1( 0.8867800071712166)
w2 ( -0.6122857893229964 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4678820354969089) - present_state_Q (-0.5871463564188921)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006969788128973433 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7381155304909717) - present_state_Q ( -0.7381155304909717)) * f1( 0.8851882504971882)
w2 ( -0.6165693120299715 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7381155304909717) - present_state_Q (-0.7381155304909717)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.009493957822099315 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7461030321929246) - present_state_Q ( -0.7460412005203096)) * f1( 0.8835313169341373)
w2 ( -0.6199976043538494 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7461030321929246) - present_state_Q (-0.7460412005203096)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0018403174361156845 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8761997947250076) - present_state_Q ( -0.8761997947250076)) * f1( 0.8640388743379123)
w2 ( -0.6075964302184984 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8761997947250076) - present_state_Q (-0.8761997947250076)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.003771970628087408 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.852140773101061) - present_state_Q ( -0.8521773976834672)) * f1( 0.8381137662993977)
w2 ( -0.5982215653662278 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.852140773101061) - present_state_Q (-0.8521773976834672)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01917074344127595 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8341415087625996) - present_state_Q ( -0.9537444405946469)) * f1( 0.9040536970052641)
w2 ( -0.5709687190112859 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8341415087625996) - present_state_Q (-0.9537444405946469)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.01959137047235456 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7829419783093828) - present_state_Q ( -0.7833238637916178)) * f1( 0.8362921799716864)
w2 ( -0.5702645657767907 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7829419783093828) - present_state_Q (-0.7833238637916178)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03805086184220825 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-1.009105648040035) - present_state_Q ( -1.009105648040035)) * f1( 0.8866439631009965)
w2 ( -0.5327894507943051 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -1.009105648040035) - present_state_Q (-1.009105648040035)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.05024036311620418 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9229720999393294) - present_state_Q ( -0.923731932579087)) * f1( 0.9274186481504973)
w2 ( -0.5091312007289773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9229720999393294) - present_state_Q (-0.923731932579087)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04897868699947644 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7638913351327323) - present_state_Q ( -0.7638913351327323)) * f1( 1.0095186994632406)
w2 ( -0.5111308484698639 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7638913351327323) - present_state_Q (-0.7638913351327323)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.056326241957732705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7850633733540074) - present_state_Q ( -0.8868061418151221)) * f1( 0.6784458193211285)
w2 ( -0.49163688366351405 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7850633733540074) - present_state_Q (-0.8868061418151221)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.06051215756161575 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.84675462835985) - present_state_Q ( -0.8472927449797321)) * f1( 0.6684920617080874)
w2 ( -0.4803657728776396 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.84675462835985) - present_state_Q (-0.8472927449797321)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0633111536947372 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.823484082279646) - present_state_Q ( -0.823484082279646)) * f1( 0.6804303558037919)
w2 ( -0.47296135154833696 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.823484082279646) - present_state_Q (-0.823484082279646)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( 0.07078763400467838 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.9031832466762598) - present_state_Q ( -0.9044486669674495)) * f1( 0.6550826151296625)
w2 ( -0.45013528308837225 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.9031832466762598) - present_state_Q (-0.9044486669674495)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07534094852066793 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8520551226347486) - present_state_Q ( -0.8520551226347486)) * f1( 0.681128056050147)
w2 ( -0.4367653610141175 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8520551226347486) - present_state_Q (-0.8520551226347486)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07808562899359352 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8224718877624092) - present_state_Q ( -0.8239771069161274)) * f1( 0.6577248639033774)
w2 ( -0.4284193773861402 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8224718877624092) - present_state_Q (-0.8239771069161274)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07970243133919587 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.8036818216901737) - present_state_Q ( -0.804480903639112)) * f1( 0.6705184015033561)
w2 ( -0.4235968330921213 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.8036818216901737) - present_state_Q (-0.804480903639112)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08061622673577637 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7935446831316648) - present_state_Q ( -0.7927260249232796)) * f1( 0.6833874493635063)
w2 ( -0.4209225217700987 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7935446831316648) - present_state_Q (-0.7927260249232796)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08118587646228817 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7870980235910825) - present_state_Q ( -0.7870980235910825)) * f1( 0.6791067030282983)
w2 ( -0.4192448775237039 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7870980235910825) - present_state_Q (-0.7870980235910825)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08150771846288188 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7839413115330499) - present_state_Q ( -0.7831124986966667)) * f1( 0.6821045576377371)
w2 ( -0.4183012040150315 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7839413115330499) - present_state_Q (-0.7831124986966667)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08171333927602066 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7811350439264334) - present_state_Q ( -0.7811350439264334)) * f1( 0.680516706266157)
w2 ( -0.4176968961082735 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7811350439264334) - present_state_Q (-0.7811350439264334)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07938526747869001 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7522510923132502) - present_state_Q ( -0.7523244263360376)) * f1( 1.01659492338097)
w2 ( -0.42227703268733097 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7522510923132502) - present_state_Q (-0.7523244263360376)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07813886160397138 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7642211390160113) - present_state_Q ( -0.7641205003068177)) * f1( 1.0132051906159505)
w2 ( -0.42473735540628765 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7642211390160113) - present_state_Q (-0.7641205003068177)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07739585395606148 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7696922152896654) - present_state_Q ( -0.7696922152896654)) * f1( 1.021034781991948)
w2 ( -0.4261927566541479 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7696922152896654) - present_state_Q (-0.7696922152896654)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0770427724858414 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7740272251752717) - present_state_Q ( -0.7739200389393641)) * f1( 1.0138201254743893)
w2 ( -0.4268892933697805 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7740272251752717) - present_state_Q (-0.7739200389393641)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07868137680160563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.775460681077772) - present_state_Q ( -0.8018629423178425)) * f1( 0.6738548308507376)
w2 ( -0.42202591852776744 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.775460681077772) - present_state_Q (-0.8018629423178425)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07740639349849243 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7638754518272591) - present_state_Q ( -0.7638754518272591)) * f1( 1.0190007913872647)
w2 ( -0.4245283371988608 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7638754518272591) - present_state_Q (-0.7638754518272591)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07674889948645004 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7705021927432515) - present_state_Q ( -0.7705660981655076)) * f1( 1.0140063718863566)
w2 ( -0.4258251614206243 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7705021927432515) - present_state_Q (-0.7705660981655076)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0763524247345688 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7734540425740793) - present_state_Q ( -0.7734540425740793)) * f1( 1.0188586519207983)
w2 ( -0.42660343375729004 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7734540425740793) - present_state_Q (-0.7734540425740793)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07628173832918861 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7752982347107481) - present_state_Q ( -0.7768232480969699)) * f1( 1.0004085617863452)
w2 ( -0.426744748832111 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7752982347107481) - present_state_Q (-0.7768232480969699)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07611197734621457 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7758533517659686) - present_state_Q ( -0.7759159941900771)) * f1( 1.0169341335587003)
w2 ( -0.42707861702941496 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7758533517659686) - present_state_Q (-0.7759159941900771)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0778160747775712 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769995488503453) - present_state_Q ( -0.8031064794454112)) * f1( 0.6707322079046953)
w2 ( -0.4219973121173396 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769995488503453) - present_state_Q (-0.8031064794454112)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07664960202573282 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7649375785365845) - present_state_Q ( -0.7650026719563038)) * f1( 1.015110984512716)
w2 ( -0.4242955292968105 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7649375785365845) - present_state_Q (-0.7650026719563038)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07801573030647466 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.770692089783159) - present_state_Q ( -0.7976080494100043)) * f1( 0.6651438211838431)
w2 ( -0.42018776121047285 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.770692089783159) - present_state_Q (-0.7976080494100043)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07649411826386639 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7611327683762165) - present_state_Q ( -0.7611327683762165)) * f1( 1.015727901712057)
w2 ( -0.4231838629027539 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7611327683762165) - present_state_Q (-0.7611327683762165)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07581618791358237 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7685301009721802) - present_state_Q ( -0.770057380466779)) * f1( 0.9975975548276327)
w2 ( -0.42454298882884167 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7685301009721802) - present_state_Q (-0.770057380466779)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0752753291829999 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7718766690617115) - present_state_Q ( -0.7718766690617115)) * f1( 1.018374976647169)
w2 ( -0.4256051883977336 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7718766690617115) - present_state_Q (-0.7718766690617115)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07499444826759867 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7747069769435134) - present_state_Q ( -0.7747069769435134)) * f1( 1.0163143845703864)
w2 ( -0.4261579325479012 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7747069769435134) - present_state_Q (-0.7747069769435134)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07663282750030487 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7761761592824994) - present_state_Q ( -0.8020724145864295)) * f1( 0.6699622661412465)
w2 ( -0.4212669728162653 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7761761592824994) - present_state_Q (-0.8020724145864295)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07756720280587412 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7650056488990391) - present_state_Q ( -0.7901765306896598)) * f1( 0.6832243654674306)
w2 ( -0.4185317796563141 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7650056488990391) - present_state_Q (-0.7901765306896598)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07578588125330046 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7582887353103561) - present_state_Q ( -0.7582887353103561)) * f1( 1.0155687088449006)
w2 ( -0.42203980730045 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7582887353103561) - present_state_Q (-0.7582887353103561)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07477979097173085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7669138809644944) - present_state_Q ( -0.76682212620458)) * f1( 1.0194179591064054)
w2 ( -0.4240136596788239 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7669138809644944) - present_state_Q (-0.76682212620458)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07425297701827438 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7720189068519905) - present_state_Q ( -0.7720189068519905)) * f1( 1.0164298605005582)
w2 ( -0.42505025644546557 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7720189068519905) - present_state_Q (-0.7720189068519905)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07396422760377076 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.774728311379785) - present_state_Q ( -0.7746318501454459)) * f1( 1.0163722153107977)
w2 ( -0.4256184526439721 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.774728311379785) - present_state_Q (-0.7746318501454459)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07377218664188274 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7756887265478628) - present_state_Q ( -0.7756887265478628)) * f1( 1.0214150973737723)
w2 ( -0.42599448186535677 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7756887265478628) - present_state_Q (-0.7756887265478628)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07370102536123732 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769999272891429) - present_state_Q ( -0.7769999272891429)) * f1( 1.0164946961053887)
w2 ( -0.426134494953311 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769999272891429) - present_state_Q (-0.7769999272891429)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07368447022184607 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775962255287677) - present_state_Q ( -0.7775962255287677)) * f1( 1.0131848778474128)
w2 ( -0.4261671743581328 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775962255287677) - present_state_Q (-0.7775962255287677)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07362025621937676 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7771858332314465) - present_state_Q ( -0.7770897574272604)) * f1( 1.0211729970027865)
w2 ( -0.4262929395373096 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7771858332314465) - present_state_Q (-0.7770897574272604)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07361960286131174 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7777706341968392) - present_state_Q ( -0.7777706341968392)) * f1( 1.0162317916259673)
w2 ( -0.42629422538187856 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7777706341968392) - present_state_Q (-0.7777706341968392)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07533433101436647 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778974051554628) - present_state_Q ( -0.8035128073908565)) * f1( 0.6666110854380961)
w2 ( -0.4211496120068165 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778974051554628) - present_state_Q (-0.8035128073908565)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07425472000743996 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7659426890498827) - present_state_Q ( -0.7659426890498827)) * f1( 1.0135688992736773)
w2 ( -0.4232799279778376 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7659426890498827) - present_state_Q (-0.7659426890498827)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07364110382712108 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7710712609881699) - present_state_Q ( -0.7710712609881699)) * f1( 1.0166167882653339)
w2 ( -0.424487100999967 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7710712609881699) - present_state_Q (-0.7710712609881699)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07329503252789457 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7740008685876807) - present_state_Q ( -0.7740008685876807)) * f1( 1.018090842150598)
w2 ( -0.4251669446541845 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7740008685876807) - present_state_Q (-0.7740008685876807)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07311064859824681 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7757640900420922) - present_state_Q ( -0.7757640900420922)) * f1( 1.0173922664935986)
w2 ( -0.42552940844660786 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7757640900420922) - present_state_Q (-0.7757640900420922)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07303079137545199 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7769029813572714) - present_state_Q ( -0.7769029813572714)) * f1( 1.0142959604070934)
w2 ( -0.42568687180229897 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7769029813572714) - present_state_Q (-0.7769029813572714)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07298481133771369 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7772742567405565) - present_state_Q ( -0.7772742567405565)) * f1( 1.0146334918253224)
w2 ( -0.4257775055889988 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7772742567405565) - present_state_Q (-0.7772742567405565)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07292641820167961 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7771414226737454) - present_state_Q ( -0.7771414226737454)) * f1( 1.019576363086387)
w2 ( -0.42589204950772463 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7771414226737454) - present_state_Q (-0.7771414226737454)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07288585657096686 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777336302960516) - present_state_Q ( -0.777336302960516)) * f1( 1.0208618205962918)
w2 ( -0.42597151497483177 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777336302960516) - present_state_Q (-0.777336302960516)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07287814400372337 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7776936562945128) - present_state_Q ( -0.7776936562945128)) * f1( 1.0187075675354966)
w2 ( -0.42598665684181947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7776936562945128) - present_state_Q (-0.7776936562945128)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07289426697649705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7778860259387614) - present_state_Q ( -0.7779473323141486)) * f1( 1.0157500905306862)
w2 ( -0.42595491089776494 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7778860259387614) - present_state_Q (-0.7779473323141486)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07291204596070418 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777972535326635) - present_state_Q ( -0.777972535326635)) * f1( 1.0143086628847526)
w2 ( -0.42591985453897063 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777972535326635) - present_state_Q (-0.777972535326635)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0729490392728677 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7774369678621745) - present_state_Q ( -0.7781095249894221)) * f1( 1.0112208911029041)
w2 ( -0.4258466888983297 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7774369678621745) - present_state_Q (-0.7781095249894221)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07293211883832935 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.777592694846688) - present_state_Q ( -0.777592694846688)) * f1( 1.0157869615362023)
w2 ( -0.4258800038259259 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.777592694846688) - present_state_Q (-0.777592694846688)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07291325392200974 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7775049184640356) - present_state_Q ( -0.7775650535863189)) * f1( 1.0173152137538037)
w2 ( -0.4259170914779428 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7775049184640356) - present_state_Q (-0.7775650535863189)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07460289760017383 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7779331590709669) - present_state_Q ( -0.8030435194890837)) * f1( 0.6691604179260726)
w2 ( -0.4208670507615454 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7779331590709669) - present_state_Q (-0.8030435194890837)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07350423381893935 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7657865365417194) - present_state_Q ( -0.7657865365417194)) * f1( 1.0180243318215894)
w2 ( -0.4230254741840359 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7657865365417194) - present_state_Q (-0.7657865365417194)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07306380402929039 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7712411197286824) - present_state_Q ( -0.7727100079210438)) * f1( 0.9977784494385243)
w2 ( -0.4239082949944008 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7712411197286824) - present_state_Q (-0.7727100079210438)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.07267745738311085 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7735543150332561) - present_state_Q ( -0.7735543150332561)) * f1( 1.0164030732067364)
w2 ( -0.4246685182884147 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7735543150332561) - present_state_Q (-0.7735543150332561)) * f2(2.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06883372784721851 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04014884435358565) - present_state_Q ( -0.04076630804501646)) * f1( 0.6077179527600028)
w2 ( -0.42593348981622153 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.04014884435358565) - present_state_Q (-0.04076630804501646)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08247836080288024 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046327280489330915) - present_state_Q ( -0.046327280489330915)) * f1( 0.5645403596353914)
w2 ( -0.4210995987674136 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.046327280489330915) - present_state_Q (-0.046327280489330915)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09598770338546712 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03633820839195061) - present_state_Q ( -0.03633820839195061)) * f1( 0.5805366510128319)
w2 ( -0.41644551101635846 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03633820839195061) - present_state_Q (-0.03633820839195061)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10998961733069733 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.022189484697615766) - present_state_Q ( -0.022189484697615766)) * f1( 0.6365358827296063)
w2 ( -0.4120461002918014 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.022189484697615766) - present_state_Q (-0.022189484697615766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1355718052330098 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16687768686690174) - present_state_Q ( -0.16687768686690174)) * f1( 0.730523255359613)
w2 ( -0.3910347052009887 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16687768686690174) - present_state_Q (-0.16687768686690174)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1369147173133834 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13054857029800754) - present_state_Q ( -0.13054857029800754)) * f1( 0.7676541050974044)
w2 ( -0.3899850824048963 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13054857029800754) - present_state_Q (-0.13054857029800754)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.11347964419128732 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12306076644231242) - present_state_Q ( -0.12306076644231242)) * f1( 0.8102144544966459)
w2 ( -0.40733980101701145 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12306076644231242) - present_state_Q (-0.12306076644231242)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06784742868645308 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15343767421186447) - present_state_Q ( -0.15241486469579668)) * f1( 0.8106212930959551)
w2 ( -0.4411155351805348 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.15343767421186447) - present_state_Q (-0.15241486469579668)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.026298268197360128 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21027511275526575) - present_state_Q ( -0.2095539819616884)) * f1( 0.8123423424245)
w2 ( -0.4718039469393651 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21027511275526575) - present_state_Q (-0.2095539819616884)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.011303537565763244 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2617885892612889) - present_state_Q ( -0.2617885892612889)) * f1( 0.8097027052324195)
w2 ( -0.4996673631192555 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2617885892612889) - present_state_Q (-0.2617885892612889)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03561212610598583 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3083184001277999) - present_state_Q ( -0.4082518727516509)) * f1( 0.7535678283625328)
w2 ( -0.5254737605001458 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3083184001277999) - present_state_Q (-0.4082518727516509)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05850246435804646 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3357762311631896) - present_state_Q ( -0.3357762311631896)) * f1( 0.5754212708928296)
w2 ( -0.5493418440173335 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3357762311631896) - present_state_Q (-0.3357762311631896)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04670771697111542 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24920553594815414) - present_state_Q ( -0.35907390475162093)) * f1( 0.5037189230331554)
w2 ( -0.5352926429479252 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.24920553594815414) - present_state_Q (-0.35907390475162093)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.028560062230701875 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23472799488390975) - present_state_Q ( -0.23472799488390975)) * f1( 0.44127478372547485)
w2 ( -0.5188424351321045 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23472799488390975) - present_state_Q (-0.23472799488390975)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.017501812298539813 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11419932588845294) - present_state_Q ( -0.11419932588845294)) * f1( 0.3652246545464093)
w2 ( -0.5127868472661123 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11419932588845294) - present_state_Q (-0.11419932588845294)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.00865468701640929 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10777096947638547) - present_state_Q ( -0.10777096947638547)) * f1( 0.29788915194788007)
w2 ( -0.5068469698155375 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10777096947638547) - present_state_Q (-0.10777096947638547)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.00881685098912512 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10338875144485711) - present_state_Q ( -0.10338875144485711)) * f1( 0.23332530430284912)
w2 ( -0.50698597228953 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.10338875144485711) - present_state_Q (-0.10338875144485711)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.00032598649630154533 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2045472132458368) - present_state_Q ( -0.30585230519035334)) * f1( 0.18835770488620718)
w2 ( -0.4778621172575838 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2045472132458368) - present_state_Q (-0.30585230519035334)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.009950950831767622 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2866474077848789) - present_state_Q ( -0.2866487612306371)) * f1( 0.21015939215441917)
w2 ( -0.45038307603045485 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2866474077848789) - present_state_Q (-0.2866487612306371)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.019441773401974664 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26808964977461) - present_state_Q ( -0.26808964977461)) * f1( 0.21507450693360508)
w2 ( -0.4239062349426259 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.26808964977461) - present_state_Q (-0.26808964977461)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.028618715022202527 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25014702360042523) - present_state_Q ( -0.25014702360042523)) * f1( 0.21586083112789112)
w2 ( -0.39839829566820295 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.25014702360042523) - present_state_Q (-0.25014702360042523)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03750656125251609 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23296560194811436) - present_state_Q ( -0.23282800776883947)) * f1( 0.21702475555816678)
w2 ( -0.3738264088137613 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23296560194811436) - present_state_Q (-0.23282800776883947)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04575882874625263 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21631250118695716) - present_state_Q ( -0.21645657674717814)) * f1( 0.20901059119496782)
w2 ( -0.35013688921605235 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21631250118695716) - present_state_Q (-0.21645657674717814)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05394440563589928 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2002306086650841) - present_state_Q ( -0.2002306086650841)) * f1( 0.21529233012447077)
w2 ( -0.3273244363481378 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2002306086650841) - present_state_Q (-0.2002306086650841)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06183369253719069 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18477619194745173) - present_state_Q ( -0.18477619194745173)) * f1( 0.215378586981762)
w2 ( -0.3053465219829754 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18477619194745173) - present_state_Q (-0.18477619194745173)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06906579381761596 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16995265923241587) - present_state_Q ( -0.1705599436632456)) * f1( 0.20454818413007442)
w2 ( -0.2841326413185752 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16995265923241587) - present_state_Q (-0.1705599436632456)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0755873744201198 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025431282689401934) - present_state_Q ( 0.025431282689401934)) * f1( 0.3682182059118738)
w2 ( -0.2841326413185752 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.025431282689401934) - present_state_Q (0.025431282689401934)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.08065572941765507 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029999022958442655) - present_state_Q ( -0.027461166622092668)) * f1( 0.388495590260983)
w2 ( -0.28152341994021646 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.029999022958442655) - present_state_Q (-0.027461166622092668)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08145218255916833 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01927579154940967) - present_state_Q ( -0.01927579154940967)) * f1( 0.45909810382953664)
w2 ( -0.2811764556923271 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.01927579154940967) - present_state_Q (-0.01927579154940967)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07656806757691424 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01174906465894826) - present_state_Q ( -0.01174906465894826)) * f1( 0.5461637132584085)
w2 ( -0.28296497252846603 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.01174906465894826) - present_state_Q (-0.01174906465894826)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05270521965531741 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009876547021645865) - present_state_Q ( -0.009876547021645865)) * f1( 0.6101296397107017)
w2 ( -0.2907871946820764 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.009876547021645865) - present_state_Q (-0.009876547021645865)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02676638440352726 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02269163466464129) - present_state_Q ( -0.022188447313141166)) * f1( 0.6824559665722828)
w2 ( -0.29838880900514286 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02269163466464129) - present_state_Q (-0.022188447313141166)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.017560767762550198 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04148934801222825) - present_state_Q ( -0.04176551404229886)) * f1( 0.669206848735582)
w2 ( -0.31163647742032136 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.04148934801222825) - present_state_Q (-0.04176551404229886)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04976361310810863 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0712190352718967) - present_state_Q ( -0.07122028572939644)) * f1( 0.5064123827374573)
w2 ( -0.3243545097762772 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0712190352718967) - present_state_Q (-0.07122028572939644)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07840453168736441 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08781876842864499) - present_state_Q ( -0.08782374729904688)) * f1( 0.46123751693687665)
w2 ( -0.33677367236715355 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.08781876842864499) - present_state_Q (-0.08782374729904688)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10414968749663867 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10039410968846552) - present_state_Q ( -0.10046881333968838)) * f1( 0.42234904225050424)
w2 ( -0.3489650843197367 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.10039410968846552) - present_state_Q (-0.10046881333968838)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08278570252391922 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13679193935640066) - present_state_Q ( -0.1369661817368687)) * f1( 0.4527019348662979)
w2 ( -0.41573703877636553 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13679193935640066) - present_state_Q (-0.1369661817368687)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09835787084863486 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12504757433717012) - present_state_Q ( -0.12504757433717012)) * f1( 0.4982411203366737)
w2 ( -0.4032353261002274 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12504757433717012) - present_state_Q (-0.12504757433717012)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11533531284124646 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10450684768879329) - present_state_Q ( -0.10450684768879329)) * f1( 0.5773537212765506)
w2 ( -0.3914730795834308 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10450684768879329) - present_state_Q (-0.10450684768879329)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.12617434304637937 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08538118781257312) - present_state_Q ( -0.08623964083706602)) * f1( 0.6099570830760148)
w2 ( -0.38436501870119844 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08538118781257312) - present_state_Q (-0.08623964083706602)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1303732020049852 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07208527322043332) - present_state_Q ( -0.07208527322043332)) * f1( 0.6472055434441937)
w2 ( -0.38176994886526283 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.07208527322043332) - present_state_Q (-0.07208527322043332)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.12005061727711872 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06033814782483099) - present_state_Q ( -0.06033814782483099)) * f1( 0.7085032069530833)
w2 ( -0.3875977755435689 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06033814782483099) - present_state_Q (-0.06033814782483099)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11676808008705042 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06335438436791539) - present_state_Q ( -0.06335438436791539)) * f1( 0.7637172380202912)
w2 ( -0.38931701770632393 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.06335438436791539) - present_state_Q (-0.06335438436791539)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.1212516150018599 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062189610485789776) - present_state_Q ( -0.062189610485789776)) * f1( 0.8010510794303373)
w2 ( -0.3870781917288355 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.062189610485789776) - present_state_Q (-0.062189610485789776)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11747803363276654 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05707463130051128) - present_state_Q ( -0.05830538156842226)) * f1( 0.7960792532258752)
w2 ( -0.38897427499130066 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.05707463130051128) - present_state_Q (-0.05830538156842226)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.06710879599816447 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06116550608385152) - present_state_Q ( -0.06350759747611447)) * f1( 0.7838240875588047)
w2 ( -0.4146786331165915 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06116550608385152) - present_state_Q (-0.06350759747611447)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04281827240325643 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11147512394073797) - present_state_Q ( -0.11147512394073797)) * f1( 0.810569292695796)
w2 ( -0.4266655286547249 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11147512394073797) - present_state_Q (-0.11147512394073797)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.02029265005554711 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13631846906460893) - present_state_Q ( -0.13593392959672512)) * f1( 0.8111556098775105)
w2 ( -0.4377734453471143 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13631846906460893) - present_state_Q (-0.13593392959672512)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.00022377926674871026 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15872973042158484) - present_state_Q ( -0.1589074856273196)) * f1( 0.7984118617911746)
w2 ( -0.44805206484370785 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.15872973042158484) - present_state_Q (-0.1589074856273196)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01703741857822743 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17937854536958714) - present_state_Q ( -0.17937854536958714)) * f1( 0.7047991281565282)
w2 ( -0.4575944372104027 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.17937854536958714) - present_state_Q (-0.17937854536958714)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.026515319723336726 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2858713889122021) - present_state_Q ( -0.2858713889122021)) * f1( 0.6641103835072668)
w2 ( -0.4661573822091438 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2858713889122021) - present_state_Q (-0.2858713889122021)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.015935143295568027 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29597816248629155) - present_state_Q ( -0.29650261913544845)) * f1( 0.6339048514345795)
w2 ( -0.45614309403593467 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.29597816248629155) - present_state_Q (-0.29650261913544845)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0071882088875172626 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28180868134066545) - present_state_Q ( -0.28180868134066545)) * f1( 0.5097428224171532)
w2 ( -0.4289254252435387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.28180868134066545) - present_state_Q (-0.28180868134066545)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.00410850850377887 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25265093850481013) - present_state_Q ( -0.25265093850481013)) * f1( 0.6544490727700593)
w2 ( -0.439282274564279 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25265093850481013) - present_state_Q (-0.25265093850481013)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03510769586588475 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2662913823619289) - present_state_Q ( -0.2663363082272368)) * f1( 0.6734666573342609)
w2 ( -0.46689984436481635 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2662913823619289) - present_state_Q (-0.2663363082272368)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05652367659090132 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3026340619840447) - present_state_Q ( -0.3960140308570079)) * f1( 0.6407186461647884)
w2 ( -0.4936397943921281 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3026340619840447) - present_state_Q (-0.3960140308570079)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07996425845171289 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32898676857893283) - present_state_Q ( -0.32898676857893283)) * f1( 0.5803389645205114)
w2 ( -0.5178745088888657 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32898676857893283) - present_state_Q (-0.32898676857893283)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09971540805365726 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3519340311017036) - present_state_Q ( -0.3519340311017036)) * f1( 0.5153468132674405)
w2 ( -0.5408700712093737 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3519340311017036) - present_state_Q (-0.3519340311017036)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08479974807680872 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.47776764011390693) - present_state_Q ( -0.47776764011390693)) * f1( 0.45200219330351377)
w2 ( -0.5144708011211724 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.47776764011390693) - present_state_Q (-0.47776764011390693)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.08085041852025848 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3402185141759284) - present_state_Q ( -0.3402185141759284)) * f1( 0.3718882923409234)
w2 ( -0.5080990013556722 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3402185141759284) - present_state_Q (-0.3402185141759284)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06433953118260063 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3315811062110684) - present_state_Q ( -0.33163900589071477)) * f1( 0.3312240748710697)
w2 ( -0.47819014763949574 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3315811062110684) - present_state_Q (-0.33163900589071477)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05199741429396999 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3036909601910256) - present_state_Q ( -0.3036909601910256)) * f1( 0.2607552666138334)
w2 ( -0.44979083578918033 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3036909601910256) - present_state_Q (-0.3036909601910256)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04529136711659286 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18933046877615717) - present_state_Q ( -0.18933046877615717)) * f1( 0.18105005005175337)
w2 ( -0.4349749389132387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18933046877615717) - present_state_Q (-0.18933046877615717)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.043369484516852366 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0037921320609013917) - present_state_Q ( -0.004269198905538686)) * f1( 0.09426076485058518)
w2 ( -0.4349749389132387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0037921320609013917) - present_state_Q (-0.004269198905538686)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.029204477403658884 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27763352041784806) - present_state_Q ( -0.2785003403173678)) * f1( 0.40386408011417585)
w2 ( -0.4139307196167037 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.27763352041784806) - present_state_Q (-0.2785003403173678)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.013882470046179365 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2583988348292332) - present_state_Q ( -0.25869607341764855)) * f1( 0.35397454659918526)
w2 ( -0.38795934822062017 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2583988348292332) - present_state_Q (-0.25869607341764855)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0057438606505103695 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23831262919932192) - present_state_Q ( -0.31590449884344596)) * f1( 0.3988497903133343)
w2 ( -0.34859348934673906 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23831262919932192) - present_state_Q (-0.31590449884344596)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.025461144581880917 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27635084779985825) - present_state_Q ( -0.27635084779985825)) * f1( 0.4394158965727558)
w2 ( -0.31269622830514926 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.27635084779985825) - present_state_Q (-0.27635084779985825)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.03644587554659488 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17490200677639348) - present_state_Q ( -0.23744125243742328)) * f1( 0.49941706924460477)
w2 ( -0.29510014416436653 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.17490200677639348) - present_state_Q (-0.23744125243742328)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.042043999424977135 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15701050712759024) - present_state_Q ( -0.21567116293193814)) * f1( 0.5599797533595509)
w2 ( -0.2871025351868322 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.15701050712759024) - present_state_Q (-0.21567116293193814)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.058265301669218346 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20584567300193368) - present_state_Q ( -0.2057672360929822)) * f1( 0.5688039288259645)
w2 ( -0.2642879216834091 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.20584567300193368) - present_state_Q (-0.2057672360929822)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.061802234733905524 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1766456505212308) - present_state_Q ( -0.17626070410041622)) * f1( 0.6036119652477697)
w2 ( -0.2596002305595456 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1766456505212308) - present_state_Q (-0.17626070410041622)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.024684987706236478 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21456117348380635) - present_state_Q ( -0.21436336233745956)) * f1( 0.7319616906549902)
w2 ( -0.31030950606063773 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21456117348380635) - present_state_Q (-0.21436336233745956)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.006903122330766967 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29217105475428695) - present_state_Q ( -0.2924564310564828)) * f1( 0.7232361310694324)
w2 ( -0.35398557350253235 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.29217105475428695) - present_state_Q (-0.2924564310564828)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.011326556932755454 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3577853619390448) - present_state_Q ( -0.35790683227064213)) * f1( 0.568041326840299)
w2 ( -0.3617727438948586 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.3577853619390448) - present_state_Q (-0.35790683227064213)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.030013667400602354 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36738816490829695) - present_state_Q ( -0.36750517443516256)) * f1( 0.5061053040510745)
w2 ( -0.3986961081004253 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.36738816490829695) - present_state_Q (-0.36750517443516256)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.033974153989218284 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25192734005043593) - present_state_Q ( -0.331666561670521)) * f1( 0.4234629184278106)
w2 ( -0.40617820188718706 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.25192734005043593) - present_state_Q (-0.331666561670521)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.029881127907435478 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2561483742961679) - present_state_Q ( -0.33738401467360524)) * f1( 0.36620347243389517)
w2 ( -0.39723666770766797 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2561483742961679) - present_state_Q (-0.33738401467360524)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.01397163512406854 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32739984885371043) - present_state_Q ( -0.32739984885371043)) * f1( 0.32162489707038816)
w2 ( -0.3576638785902008 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32739984885371043) - present_state_Q (-0.32739984885371043)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0032082720939278844 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4329857829570973) - present_state_Q ( -0.43326521312066296)) * f1( 0.2912013358703549)
w2 ( -0.2868678824112064 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4329857829570973) - present_state_Q (-0.43326521312066296)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01777989211539393 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34332297995984895) - present_state_Q ( -0.34332297995984895)) * f1( 0.2862846126228409)
w2 ( -0.22578900057554271 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.34332297995984895) - present_state_Q (-0.34332297995984895)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.001559514836887245 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2647013256014551) - present_state_Q ( -0.2647013256014551)) * f1( 0.35126619715475393)
w2 ( -0.28120125741058555 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2647013256014551) - present_state_Q (-0.2647013256014551)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014387918846081114 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33679989460166504) - present_state_Q ( -0.33681484113052856)) * f1( 0.40183507546806185)
w2 ( -0.3288250752101421 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33679989460166504) - present_state_Q (-0.33681484113052856)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.025817799431819523 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3994194985230809) - present_state_Q ( -0.3994194985230809)) * f1( 0.33565718034514147)
w2 ( -0.36968776936964937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3994194985230809) - present_state_Q (-0.3994194985230809)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03479432839552472 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4515179496585037) - present_state_Q ( -0.4515179496585037)) * f1( 0.30570484660272934)
w2 ( -0.404923830806531 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4515179496585037) - present_state_Q (-0.4515179496585037)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04081724726554909 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4985325339728083) - present_state_Q ( -0.5791713752621804)) * f1( 0.35287395099185803)
w2 ( -0.42881929374544503 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4985325339728083) - present_state_Q (-0.5791713752621804)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.046388793264365444 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5312963211756909) - present_state_Q ( -0.6170601799247799)) * f1( 0.40946339601061554)
w2 ( -0.4478690170524355 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5312963211756909) - present_state_Q (-0.6170601799247799)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05569343517307377 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5584887362942307) - present_state_Q ( -0.5594163227099607)) * f1( 0.4736812643910146)
w2 ( -0.471440923162771 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5584887362942307) - present_state_Q (-0.5594163227099607)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.05930036132261619 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6837569665423431) - present_state_Q ( -0.6837569665423431)) * f1( 0.42625623721521083)
w2 ( -0.4832875453784357 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6837569665423431) - present_state_Q (-0.6837569665423431)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06192046182290122 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6983329294791042) - present_state_Q ( -0.6983329294791042)) * f1( 0.3664457595978688)
w2 ( -0.49329759626406855 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6983329294791042) - present_state_Q (-0.6983329294791042)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06349829291233114 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.610925083300081) - present_state_Q ( -0.7095846025528947)) * f1( 0.3063279443465539)
w2 ( -0.5005087030728644 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.610925083300081) - present_state_Q (-0.7095846025528947)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06890902688764043 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41562003471101344) - present_state_Q ( -0.5157217753255863)) * f1( 0.23958238174569205)
w2 ( -0.523092725887416 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41562003471101344) - present_state_Q (-0.5157217753255863)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0710842287054944 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5364022439264947) - present_state_Q ( -0.641020789103978)) * f1( 0.19314621959152806)
w2 ( -0.5366070581220566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5364022439264947) - present_state_Q (-0.641020789103978)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07243852373380107 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5461228102621651) - present_state_Q ( -0.6534442218865766)) * f1( 0.13386587029779537)
w2 ( -0.5487472252188134 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5461228102621651) - present_state_Q (-0.6534442218865766)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07339583082043705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5558217248958003) - present_state_Q ( -0.6662602653707212)) * f1( 0.10717494929459111)
w2 ( -0.5594658540730764 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5558217248958003) - present_state_Q (-0.6662602653707212)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07113058519834321 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11733359800224163) - present_state_Q ( -0.11733359800224163)) * f1( 0.07412447174194894)
w2 ( -0.553353849309036 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11733359800224163) - present_state_Q (-0.11733359800224163)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.052922034344414975 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2587146916821534) - present_state_Q ( -0.260093741078989)) * f1( 0.5448036347137669)
w2 ( -0.5399849584326051 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2587146916821534) - present_state_Q (-0.260093741078989)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.032057607445980484 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24148268953642482) - present_state_Q ( -0.24239441587039762)) * f1( 0.498855208882228)
w2 ( -0.5232551125559348 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.24148268953642482) - present_state_Q (-0.24239441587039762)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.00939568496628243 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22366102972966612) - present_state_Q ( -0.32831205224085314)) * f1( 0.44791192641210564)
w2 ( -0.4928983555998616 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.22366102972966612) - present_state_Q (-0.32831205224085314)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.005162493390810454 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2007525186510569) - present_state_Q ( -0.2007525186510569)) * f1( 0.38242836195623725)
w2 ( -0.47767126492842354 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2007525186510569) - present_state_Q (-0.2007525186510569)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.02300731966288137 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28458305276489965) - present_state_Q ( -0.28458305276489965)) * f1( 0.39122688190744037)
w2 ( -0.450303780079119 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.28458305276489965) - present_state_Q (-0.28458305276489965)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04356174299291975 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2592693697888378) - present_state_Q ( -0.2592693697888378)) * f1( 0.47432288587009175)
w2 ( -0.42430323411052173 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2592693697888378) - present_state_Q (-0.2592693697888378)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.06647813584315332 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14572498345486917) - present_state_Q ( -0.2305856302769735)) * f1( 0.5508574391350629)
w2 ( -0.39934244619463255 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14572498345486917) - present_state_Q (-0.2305856302769735)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.07494023725808614 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03793057947471578) - present_state_Q ( -0.03793057947471578)) * f1( 0.630852674075547)
w2 ( -0.39665969576408766 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.03793057947471578) - present_state_Q (-0.03793057947471578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08348753532876853 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028269967036485356) - present_state_Q ( -0.028269967036485356)) * f1( 0.681369234800795)
w2 ( -0.3941508363574309 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.028269967036485356) - present_state_Q (-0.028269967036485356)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10800770618848875 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1799301673008547) - present_state_Q ( -0.1799301673008547)) * f1( 0.6774704067005083)
w2 ( -0.3724346073231848 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1799301673008547) - present_state_Q (-0.1799301673008547)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.12353426495491464 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22521859319627918) - present_state_Q ( -0.14961491248479242)) * f1( 0.6837091029435162)
w2 ( -0.3588090241332749 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22521859319627918) - present_state_Q (-0.14961491248479242)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14930160789189378 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2028591889658113) - present_state_Q ( -0.20411599646677311)) * f1( 0.671321619715094)
w2 ( -0.32810261792765955 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2028591889658113) - present_state_Q (-0.20411599646677311)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1727487511717309 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16097717340166195) - present_state_Q ( -0.16097717340166195)) * f1( 0.6798648880858893)
w2 ( -0.3005122614427399 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16097717340166195) - present_state_Q (-0.16097717340166195)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.1870869968531053 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12426019758996759) - present_state_Q ( -0.12250697142241958)) * f1( 0.6825105069185954)
w2 ( -0.28370578530966606 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.12426019758996759) - present_state_Q (-0.12250697142241958)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.16599256542206398 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09975051747450223) - present_state_Q ( -0.09975051747450223)) * f1( 0.6799730227810278)
w2 ( -0.3085237480515019 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09975051747450223) - present_state_Q (-0.09975051747450223)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.12728761780366232 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13393717549135487) - present_state_Q ( -0.13562075942142143)) * f1( 0.6698989122617625)
w2 ( -0.354745584701719 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13393717549135487) - present_state_Q (-0.13562075942142143)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.09174824109181584 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1972185869265387) - present_state_Q ( -0.1972185869265387)) * f1( 0.6801751995106119)
w2 ( -0.3965458464430082 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1972185869265387) - present_state_Q (-0.1972185869265387)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0656762058054047 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33447978964630093) - present_state_Q ( -0.33631286097190694)) * f1( 0.6565028904567655)
w2 ( -0.43625935824228057 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33447978964630093) - present_state_Q (-0.33631286097190694)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04244438246777929 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3916027784148615) - present_state_Q ( -0.39227435073597694)) * f1( 0.6697251609910138)
w2 ( -0.4709479509528315 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3916027784148615) - present_state_Q (-0.39227435073597694)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.021874320708990317 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4420525130459137) - present_state_Q ( -0.4420525130459137)) * f1( 0.6807835625563196)
w2 ( -0.5011632247786992 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4420525130459137) - present_state_Q (-0.4420525130459137)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.010108013212171807 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5866728506666308) - present_state_Q ( -0.586450733433368)) * f1( 0.6832274473752555)
w2 ( -0.5218292109746947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5866728506666308) - present_state_Q (-0.586450733433368)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.00039671013921653564 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6193122401587666) - present_state_Q ( -0.6193122401587666)) * f1( 0.6809263963544311)
w2 ( -0.538943489037548 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6193122401587666) - present_state_Q (-0.6193122401587666)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0076249232874886705 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6464629219470619) - present_state_Q ( -0.6464629219470619)) * f1( 0.6787446837822009)
w2 ( -0.5531254934672653 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6464629219470619) - present_state_Q (-0.6464629219470619)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012629352422838922 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5574793107899884) - present_state_Q ( -0.6681044094834415)) * f1( 0.5709981803839261)
w2 ( -0.5636427160587321 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5574793107899884) - present_state_Q (-0.6681044094834415)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.016392829711441778 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5700458054477819) - present_state_Q ( -0.6827743486595285)) * f1( 0.5070006105357054)
w2 ( -0.5725503438849621 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5700458054477819) - present_state_Q (-0.6827743486595285)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.011276099895959284 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.35072787580402237) - present_state_Q ( -0.46523794458101475)) * f1( 0.43907425378922)
w2 ( -0.522137131324913 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.35072787580402237) - present_state_Q (-0.46523794458101475)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.049886227468090925 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7258897777452152) - present_state_Q ( -0.7258897777452152)) * f1( 0.4524796832893827)
w2 ( -0.4026750193290159 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.7258897777452152) - present_state_Q (-0.7258897777452152)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.08792180567051422 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6192246223181885) - present_state_Q ( -0.6192246223181885)) * f1( 0.5022510195677427)
w2 ( -0.2815066737151967 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6192246223181885) - present_state_Q (-0.6192246223181885)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.09494660417799951 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22978105263398246) - present_state_Q ( -0.3423837221200612)) * f1( 0.5883139078723579)
w2 ( -0.26478988735526393 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.22978105263398246) - present_state_Q (-0.3423837221200612)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05426057495739241 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04246564687973148) - present_state_Q ( -0.09542362435078429)) * f1( 0.6682735903163186)
w2 ( -0.3013192637754953 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.04246564687973148) - present_state_Q (-0.09542362435078429)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.030617951022009312 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3246697580601263) - present_state_Q ( -0.38493361081522537)) * f1( 0.6802979603414434)
w2 ( -0.3499739348742055 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3246697580601263) - present_state_Q (-0.38493361081522537)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01596642661130301 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5393966202697652) - present_state_Q ( -0.5390796055413437)) * f1( 0.6819101069949693)
w2 ( -0.38435154391190673 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5393966202697652) - present_state_Q (-0.5390796055413437)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.005336080164098563 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6041037707821685) - present_state_Q ( -0.6041037707821685)) * f1( 0.6800957873188243)
w2 ( -0.40936060091927445 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6041037707821685) - present_state_Q (-0.6041037707821685)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.002425471098040881 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6513886843263306) - present_state_Q ( -0.6513376115321343)) * f1( 0.6820268486951511)
w2 ( -0.42756880202335423 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6513886843263306) - present_state_Q (-0.6513376115321343)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.0075188670314679275 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6855508474859964) - present_state_Q ( -0.6855992963306305)) * f1( 0.6139892140816261)
w2 ( -0.44084172817022926 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6855508474859964) - present_state_Q (-0.6855992963306305)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.01488719212938296 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6211075104595957) - present_state_Q ( -0.6211075104595957)) * f1( 0.5225642380468531)
w2 ( -0.4605821818523202 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6211075104595957) - present_state_Q (-0.6211075104595957)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.013014481756776292 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5596701512015569) - present_state_Q ( -0.6517865875720209)) * f1( 0.4682906567056985)
w2 ( -0.377167441709059 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.5596701512015569) - present_state_Q (-0.6517865875720209)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0021914393368761744 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5949094541964978) - present_state_Q ( -0.5949094541964978)) * f1( 0.6576099377557241)
w2 ( -0.4035004803047633 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5949094541964978) - present_state_Q (-0.5949094541964978)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.006094038407563392 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6440916678156468) - present_state_Q ( -0.6440916678156468)) * f1( 0.6886344725954265)
w2 ( -0.4227512801393102 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6440916678156468) - present_state_Q (-0.6440916678156468)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.011100839942471168 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6798644812500755) - present_state_Q ( -0.6798644812500755)) * f1( 0.5681672473350181)
w2 ( -0.4368507948392993 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6798644812500755) - present_state_Q (-0.6798644812500755)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.014528889256193283 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7047512906565085) - present_state_Q ( -0.7047512906565085)) * f1( 0.521583857044671)
w2 ( -0.44736660898476205 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.7047512906565085) - present_state_Q (-0.7047512906565085)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.020226095926944604 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5434146481667551) - present_state_Q ( -0.7223612917606599)) * f1( 0.45252718697941563)
w2 ( -0.32448343667372453 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.5434146481667551) - present_state_Q (-0.7223612917606599)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.034549275178313216 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5079061111628326) - present_state_Q ( -0.5079061111628326)) * f1( 0.5570717925903138)
w2 ( -0.2833449566662766 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5079061111628326) - present_state_Q (-0.5079061111628326)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( 0.005256923049940035 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20463504899790885) - present_state_Q ( -0.26130404033116417)) * f1( 0.637955969303451)
w2 ( -0.32926090312313927 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20463504899790885) - present_state_Q (-0.26130404033116417)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.013395676016932437 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32569743294801445) - present_state_Q ( -0.45740179419727023)) * f1( 0.6778623429090282)
w2 ( -0.3677844159967936 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32569743294801445) - present_state_Q (-0.45740179419727023)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.02348935446725266 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5967542730847651) - present_state_Q ( -0.5967542730847651)) * f1( 0.6195437601958262)
w2 ( -0.39385180067258746 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5967542730847651) - present_state_Q (-0.5967542730847651)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.10268530103382158 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27050426625548757) - present_state_Q ( -0.27050426625548757)) * f1( 0.6911615642523673)
w2 ( -0.5716156957238384 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.27050426625548757) - present_state_Q (-0.27050426625548757)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12526865892103423 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3026703727310281) - present_state_Q ( -0.4169935118757958)) * f1( 0.7208830640435216)
w2 ( -0.5904121072476768 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3026703727310281) - present_state_Q (-0.4169935118757958)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14599498405264388 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.43712576921135887) - present_state_Q ( -0.43963085622038284)) * f1( 0.6816037834778774)
w2 ( -0.608657010489722 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.43712576921135887) - present_state_Q (-0.43963085622038284)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14542631797727978 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4547326000933923) - present_state_Q ( -0.4547443146337433)) * f1( 0.6133779795312656)
w2 ( -0.6081007472122577 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4547326000933923) - present_state_Q (-0.4547443146337433)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15682035069595915 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5791906663946144) - present_state_Q ( -0.5791906663946144)) * f1( 0.6375054385911942)
w2 ( -0.6223990192318455 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5791906663946144) - present_state_Q (-0.5791906663946144)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11375623588091988 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.7257862076709145) - present_state_Q ( -0.7257862076709145)) * f1( 0.6592715038593076)
w2 ( -0.5570782605414631 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.7257862076709145) - present_state_Q (-0.7257862076709145)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06691758958039812 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6245143196620851) - present_state_Q ( -0.6267882133607406)) * f1( 0.6128011557296074)
w2 ( -0.4806445824020099 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.6245143196620851) - present_state_Q (-0.6267882133607406)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0326084984253671 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5152360921278074) - present_state_Q ( -0.5152360921278074)) * f1( 0.5169270134011252)
w2 ( -0.41427333411050726 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.5152360921278074) - present_state_Q (-0.5152360921278074)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.008903312088328768 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2634308489000288) - present_state_Q ( -0.3462855157221303)) * f1( 0.4559194428333157)
w2 ( -0.3726779396439371 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2634308489000288) - present_state_Q (-0.3462855157221303)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.014568181063258898 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37646257532786065) - present_state_Q ( -0.3765556606025488)) * f1( 0.43553690133978307)
w2 ( -0.3187869993369608 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.37646257532786065) - present_state_Q (-0.3765556606025488)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03921318107564048 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3113099362968758) - present_state_Q ( -0.3113099362968758)) * f1( 0.5132461635133179)
w2 ( -0.270769105070242 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3113099362968758) - present_state_Q (-0.3113099362968758)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.06334908738608158 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2484252856204448) - present_state_Q ( -0.2484252856204448)) * f1( 0.5698037965014102)
w2 ( -0.22841082936440196 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2484252856204448) - present_state_Q (-0.2484252856204448)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.030186919210388394 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.142775921698888) - present_state_Q ( -0.18845808757176838)) * f1( 0.6306758856546938)
w2 ( -0.280992779824214 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.142775921698888) - present_state_Q (-0.18845808757176838)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.001122484720162159 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.204246895444949) - present_state_Q ( -0.2604454514097918)) * f1( 0.6806699375718718)
w2 ( -0.32699070363768434 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.204246895444949) - present_state_Q (-0.2604454514097918)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.025491724648212 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32765472647132243) - present_state_Q ( -0.3276659475658453)) * f1( 0.6015617994901359)
w2 ( -0.36750065614581306 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32765472647132243) - present_state_Q (-0.3276659475658453)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.04510926268873469 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38152320379399624) - present_state_Q ( -0.38152320379399624)) * f1( 0.5500823440428428)
w2 ( -0.4031635678043534 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38152320379399624) - present_state_Q (-0.38152320379399624)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06041169864444206 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42489846501982875) - present_state_Q ( -0.42489846501982875)) * f1( 0.48182780918969104)
w2 ( -0.4349227059525688 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42489846501982875) - present_state_Q (-0.42489846501982875)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.03394651387254661 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4609265859575262) - present_state_Q ( -0.4609265859575262)) * f1( 0.43044444351756)
w2 ( -0.37343931321639146 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4609265859575262) - present_state_Q (-0.4609265859575262)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.012143804066890217 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38626307240129726) - present_state_Q ( -0.38693759436179836)) * f1( 0.3976337952134557)
w2 ( -0.31860818450422457 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.38626307240129726) - present_state_Q (-0.38693759436179836)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.005226098639738559 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3229076026429813) - present_state_Q ( -0.3229076026429813)) * f1( 0.3540421201688342)
w2 ( -0.26954650026635624 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3229076026429813) - present_state_Q (-0.3229076026429813)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023988373527820343 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26732099024499867) - present_state_Q ( -0.26732099024499867)) * f1( 0.4258453915192324)
w2 ( -0.22548761114430635 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.26732099024499867) - present_state_Q (-0.26732099024499867)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0004350213358661967 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21394213631048367) - present_state_Q ( -0.21394213631048367)) * f1( 0.4812946080080373)
w2 ( -0.2762328188763628 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.21394213631048367) - present_state_Q (-0.21394213631048367)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.019854509959482132 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27644617540640115) - present_state_Q ( -0.33169273918167375)) * f1( 0.4904507260857191)
w2 ( -0.3237470442794388 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.27644617540640115) - present_state_Q (-0.33169273918167375)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04203555801697811 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33439728204135905) - present_state_Q ( -0.33479430105389907)) * f1( 0.5564104476516832)
w2 ( -0.3636115869944625 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33439728204135905) - present_state_Q (-0.33479430105389907)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.05845101232779571 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38887341984953727) - present_state_Q ( -0.46118152120290645)) * f1( 0.5911094792536236)
w2 ( -0.39693628548830817 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.38887341984953727) - present_state_Q (-0.46118152120290645)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07874698450557771 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4348708216752954) - present_state_Q ( -0.4354483265305088)) * f1( 0.658877229126905)
w2 ( -0.42774016105201024 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4348708216752954) - present_state_Q (-0.4354483265305088)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09182978678592892 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5670323024221264) - present_state_Q ( -0.5678361422750902)) * f1( 0.6926988932358941)
w2 ( -0.45040421160806493 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5670323024221264) - present_state_Q (-0.5678361422750902)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1021239573883991 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5984497067784066) - present_state_Q ( -0.5994050048834284)) * f1( 0.6416213411352345)
w2 ( -0.4696570075033944 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5984497067784066) - present_state_Q (-0.5994050048834284)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11032965673483512 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6232667875759531) - present_state_Q ( -0.6243021424189242)) * f1( 0.594510191021521)
w2 ( -0.48621995186403494 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6232667875759531) - present_state_Q (-0.6243021424189242)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11681219784225957 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.64198646755597) - present_state_Q ( -0.64198646755597)) * f1( 0.5304333127744628)
w2 ( -0.5008854133679902 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.64198646755597) - present_state_Q (-0.64198646755597)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12209074508614527 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6585012757104305) - present_state_Q ( -0.6585012757104305)) * f1( 0.4917190218987761)
w2 ( -0.5137672755912637 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6585012757104305) - present_state_Q (-0.6585012757104305)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12638023095368536 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.669829412914938) - present_state_Q ( -0.6711932867506375)) * f1( 0.4478026242082882)
w2 ( -0.5252620341361665 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.669829412914938) - present_state_Q (-0.6711932867506375)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12976720471401776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6779638085848898) - present_state_Q ( -0.6779638085848898)) * f1( 0.377031813139763)
w2 ( -0.5360419428089983 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6779638085848898) - present_state_Q (-0.6779638085848898)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.13214273484822206 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5790888242702151) - present_state_Q ( -0.6862972128320148)) * f1( 0.33172388629379773)
w2 ( -0.5446353431603991 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5790888242702151) - present_state_Q (-0.6862972128320148)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1173774851001619 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6897903266064158) - present_state_Q ( -0.6909427938693405)) * f1( 0.2828788288648359)
w2 ( -0.4819996918153552 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.6897903266064158) - present_state_Q (-0.6909427938693405)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.10288403608979076 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41112747199325517) - present_state_Q ( -0.5075274103563262)) * f1( 0.21748394523180867)
w2 ( -0.4153582254996551 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.41112747199325517) - present_state_Q (-0.5075274103563262)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09312253630502099 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3491016563017296) - present_state_Q ( -0.4321733014016606)) * f1( 0.16343717199556934)
w2 ( -0.3556319119225063 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3491016563017296) - present_state_Q (-0.4321733014016606)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.08353233829359843 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.101754870325119) - present_state_Q ( -0.101754870325119)) * f1( 0.32890521624426916)
w2 ( -0.3498003242566542 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.101754870325119) - present_state_Q (-0.101754870325119)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07522473093479 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09431897486872758) - present_state_Q ( -0.09431897486872758)) * f1( 0.2916105368890829)
w2 ( -0.3441025827090171 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09431897486872758) - present_state_Q (-0.09431897486872758)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0685806695619404 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08679158443987135) - present_state_Q ( -0.08679158443987135)) * f1( 0.2388984004960615)
w2 ( -0.33854033418909946 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08679158443987135) - present_state_Q (-0.08679158443987135)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06180060011309686 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08134707624065535) - present_state_Q ( -0.14905514307847526)) * f1( 0.19887541912254203)
w2 ( -0.32490351677092305 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08134707624065535) - present_state_Q (-0.14905514307847526)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05392176331301864 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08272225173507897) - present_state_Q ( -0.08272225173507897)) * f1( 0.2870772832048042)
w2 ( -0.31941451623969164 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08272225173507897) - present_state_Q (-0.08272225173507897)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.046610609774402983 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0784513171241595) - present_state_Q ( -0.0784513171241595)) * f1( 0.270176881858458)
w2 ( -0.3140023925314568 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0784513171241595) - present_state_Q (-0.0784513171241595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04038571021102096 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0736949085948949) - present_state_Q ( -0.0736949085948949)) * f1( 0.23373283768079778)
w2 ( -0.3086758841767487 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0736949085948949) - present_state_Q (-0.0736949085948949)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.035197427139202535 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06936085941910924) - present_state_Q ( -0.06970907467180448)) * f1( 0.1974435461154459)
w2 ( -0.30342042440215083 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06936085941910924) - present_state_Q (-0.06970907467180448)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03182282170922699 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06527456815691716) - present_state_Q ( -0.06527456815691716)) * f1( 0.1304209895323332)
w2 ( -0.2982454821753263 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06527456815691716) - present_state_Q (-0.06527456815691716)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.02858671871014707 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06365171267028479) - present_state_Q ( -0.06365171267028479)) * f1( 0.12577816862980942)
w2 ( -0.2930997513472612 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06365171267028479) - present_state_Q (-0.06365171267028479)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.024513972822287906 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06315302431405592) - present_state_Q ( -0.06315302431405592)) * f1( 0.15857273059445717)
w2 ( -0.2879629969096082 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06315302431405592) - present_state_Q (-0.06315302431405592)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09022050662425121 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13411072261236698) - present_state_Q ( -0.13411072261236698)) * f1( 0.32953409120983507)
w2 ( -0.5649398563020134 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13411072261236698) - present_state_Q (-0.13411072261236698)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10960902282918514 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3895824608955484) - present_state_Q ( -0.38893958506653414)) * f1( 0.5539280719566765)
w2 ( -0.5859409759633947 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3895824608955484) - present_state_Q (-0.38893958506653414)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.126862586217234 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4063715334874307) - present_state_Q ( -0.40850398172580965)) * f1( 0.519477271834703)
w2 ( -0.6058689662607707 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4063715334874307) - present_state_Q (-0.40850398172580965)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14421311620547056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4334933219319954) - present_state_Q ( -0.4348774068630257)) * f1( 0.5624670695612037)
w2 ( -0.6243772817805812 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4334933219319954) - present_state_Q (-0.4348774068630257)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14900936074232965 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5305933863402967) - present_state_Q ( -0.5305933863402967)) * f1( 0.21559454322818639)
w2 ( -0.6421745579640799 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5305933863402967) - present_state_Q (-0.5305933863402967)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1530615878602274 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5392901632009552) - present_state_Q ( -0.5422672213394257)) * f1( 0.19144820718674388)
w2 ( -0.6591075015625334 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.5392901632009552) - present_state_Q (-0.5422672213394257)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1562996623389592 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41045667655960266) - present_state_Q ( -0.41045667655960266)) * f1( 0.09794864819887447)
w2 ( -0.6789428410283148 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41045667655960266) - present_state_Q (-0.41045667655960266)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15115884813934055 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16779844771733587) - present_state_Q ( -0.16779844771733587)) * f1( 0.20479813604622332)
w2 ( -0.6739224689694028 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.16779844771733587) - present_state_Q (-0.16779844771733587)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14530964634594667 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15822533139024028) - present_state_Q ( -0.29300982518412083)) * f1( 0.15507420098062402)
w2 ( -0.6588349772875989 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.15822533139024028) - present_state_Q (-0.29300982518412083)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.14074424085516377 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.278261896522285) - present_state_Q ( -0.278261896522285)) * f1( 0.10135531933084435)
w2 ( -0.6408175490127966 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.278261896522285) - present_state_Q (-0.278261896522285)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13708838335528312 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27127807268292026) - present_state_Q ( -0.27127807268292026)) * f1( 0.10622852478338574)
w2 ( -0.6270515383962115 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.27127807268292026) - present_state_Q (-0.27127807268292026)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13416231879789656 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2627423817262819) - present_state_Q ( -0.2627423817262819)) * f1( 0.08696408897682029)
w2 ( -0.6135928126540654 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2627423817262819) - present_state_Q (-0.2627423817262819)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13086012147296053 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2572080860007322) - present_state_Q ( -0.2587405539549929)) * f1( 0.09915920515213486)
w2 ( -0.6002720228398686 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2572080860007322) - present_state_Q (-0.2587405539549929)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13080696027753186 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00034729127404160235) - present_state_Q ( -0.00034729127404160235)) * f1( 0.002653912208948719)
w2 ( -0.6002720228398686 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.00034729127404160235) - present_state_Q (-0.00034729127404160235)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.11584186722507861 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2734064654345547) - present_state_Q ( -0.3946677910275055)) * f1( 0.2637824260297488)
w2 ( -0.5662323941708256 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2734064654345547) - present_state_Q (-0.3946677910275055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10110112468339395 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3711708160633264) - present_state_Q ( -0.37168301949714705)) * f1( 0.2757516238285925)
w2 ( -0.5341584378973767 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3711708160633264) - present_state_Q (-0.37168301949714705)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08714477864926277 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4506273565909369) - present_state_Q ( -0.4506273565909369)) * f1( 0.23046831918045585)
w2 ( -0.48571326822282923 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4506273565909369) - present_state_Q (-0.4506273565909369)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.06922577601474353 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32323699007613294) - present_state_Q ( -0.32323699007613294)) * f1( 0.3650135973201465)
w2 ( -0.45625847075871806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32323699007613294) - present_state_Q (-0.32323699007613294)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.055093940661695054 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.38296784235917714) - present_state_Q ( -0.38296784235917714)) * f1( 0.2594563295090747)
w2 ( -0.4126847861088573 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.38296784235917714) - present_state_Q (-0.38296784235917714)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04232149808562076 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3433407932066465) - present_state_Q ( -0.3439557998782216)) * f1( 0.25062594588983517)
w2 ( -0.3719150484642527 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3433407932066465) - present_state_Q (-0.3439557998782216)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.03276627934882375 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37938378249188115) - present_state_Q ( -0.37938378249188115)) * f1( 0.17647612597546505)
w2 ( -0.3177705080399834 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.37938378249188115) - present_state_Q (-0.37938378249188115)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.023536264955278146 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13647642701474547) - present_state_Q ( -0.13647642701474547)) * f1( 0.2859105148625427)
w2 ( -0.30485735666745256 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13647642701474547) - present_state_Q (-0.13647642701474547)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01691851455302088 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06671209825743009) - present_state_Q ( -0.06695558493275622)) * f1( 0.25425077473576535)
w2 ( -0.2996516691653123 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06671209825743009) - present_state_Q (-0.06695558493275622)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.011058733607485837 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12305101657962683) - present_state_Q ( -0.12305101657962683)) * f1( 0.1885714554610391)
w2 ( -0.2872218325684457 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12305101657962683) - present_state_Q (-0.12305101657962683)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.006483125186182592 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11654834312600877) - present_state_Q ( -0.11654834312600877)) * f1( 0.15007234621395224)
w2 ( -0.27502609221590935 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11654834312600877) - present_state_Q (-0.11654834312600877)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.002532795875755789 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.057328444481535935) - present_state_Q ( -0.057328444481535935)) * f1( 0.3583497112327133)
w2 ( -0.2699941802152417 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.057328444481535935) - present_state_Q (-0.057328444481535935)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.015210657423241469 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05272410165198314) - present_state_Q ( -0.05272410165198314)) * f1( 0.5032914034909388)
w2 ( -0.277045146385506 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.05272410165198314) - present_state_Q (-0.05272410165198314)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0261838215135149 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11633889417921417) - present_state_Q ( -0.1164729062248025)) * f1( 0.37176878771588423)
w2 ( -0.2888515857132307 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.11633889417921417) - present_state_Q (-0.1164729062248025)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03595348685635984 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12416086041441621) - present_state_Q ( -0.12442304235707126)) * f1( 0.3392326848544344)
w2 ( -0.30037130746060553 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.12416086041441621) - present_state_Q (-0.12442304235707126)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05143773806215227 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12969310611512286) - present_state_Q ( -0.12969310611512286)) * f1( 0.26547030525892623)
w2 ( -0.3237023556404611 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.12969310611512286) - present_state_Q (-0.12969310611512286)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.046457452684342225 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1407809589936028) - present_state_Q ( -0.1407809589936028)) * f1( 0.21968339128296296)
w2 ( -0.3146342411166914 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1407809589936028) - present_state_Q (-0.1407809589936028)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.042619936202605194 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07364836933836748) - present_state_Q ( -0.07364836933836748)) * f1( 0.23078151072718442)
w2 ( -0.3113085704686008 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.07364836933836748) - present_state_Q (-0.07364836933836748)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.035396426332424964 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1336958606068975) - present_state_Q ( -0.13412166768345243)) * f1( 0.2252053933254225)
w2 ( -0.29847848720369025 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1336958606068975) - present_state_Q (-0.13412166768345243)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03168448473796018 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06478231722619697) - present_state_Q ( -0.06478231722619697)) * f1( 0.143704331552796)
w2 ( -0.2933124054936187 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06478231722619697) - present_state_Q (-0.06478231722619697)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.028264630174314853 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12083446734863565) - present_state_Q ( -0.12083446734863565)) * f1( 0.11076415413451689)
w2 ( -0.2809623646690678 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12083446734863565) - present_state_Q (-0.12083446734863565)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.024935203852082085 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11525734432456049) - present_state_Q ( -0.11548096118201667)) * f1( 0.10953673532240321)
w2 ( -0.2688041555990854 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11525734432456049) - present_state_Q (-0.11548096118201667)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.022695776492491052 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05599095861316432) - present_state_Q ( -0.05599095861316432)) * f1( 0.0894369064145838)
w2 ( -0.26379631834404843 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05599095861316432) - present_state_Q (-0.05599095861316432)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.014698718488947373 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2200245095067797) - present_state_Q ( -0.22019479687749963)) * f1( 0.4034998408312106)
w2 ( -0.2479409306699027 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.2200245095067797) - present_state_Q (-0.22019479687749963)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0026493959811257593 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15333408085994088) - present_state_Q ( -0.2029222669939214)) * f1( 0.31087896958059774)
w2 ( -0.2169338219572685 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15333408085994088) - present_state_Q (-0.2029222669939214)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.007807034477034116 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21763362760807833) - present_state_Q ( -0.21763362760807833)) * f1( 0.264137809446083)
w2 ( -0.17734679547254145 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21763362760807833) - present_state_Q (-0.21763362760807833)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0182347486095477 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2103093002800933) - present_state_Q ( -0.24577865937460155)) * f1( 0.3211019874871684)
w2 ( -0.13188211336401853 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2103093002800933) - present_state_Q (-0.24577865937460155)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01676284406167121 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1511227717169046) - present_state_Q ( -0.1774991943897083)) * f1( 0.3913278144224806)
w2 ( -0.13714794495349603 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1511227717169046) - present_state_Q (-0.1774991943897083)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.006993849957510319 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18465798052763707) - present_state_Q ( -0.18499318986253965)) * f1( 0.4184214233903408)
w2 ( -0.1698341101001274 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.18465798052763707) - present_state_Q (-0.18499318986253965)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.01884943320490732 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2003231400413786) - present_state_Q ( -0.2003231400413786)) * f1( 0.4972643250717273)
w2 ( -0.2321992109756585 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2003231400413786) - present_state_Q (-0.2003231400413786)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.042797487471073065 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2887120606546992) - present_state_Q ( -0.2888989559058798)) * f1( 0.5443082889313843)
w2 ( -0.2849958809948093 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2887120606546992) - present_state_Q (-0.2888989559058798)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.05763318270558128 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.41863777951837783) - present_state_Q ( -0.41863777951837783)) * f1( 0.4589883025007447)
w2 ( -0.33024752077549374 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.41863777951837783) - present_state_Q (-0.41863777951837783)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06949689171958456 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28623358402843063) - present_state_Q ( -0.4183325923386282)) * f1( 0.38234167147429987)
w2 ( -0.36748241270319953 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.28623358402843063) - present_state_Q (-0.4183325923386282)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04567385877298358 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4649071483898337) - present_state_Q ( -0.5384036309304736)) * f1( 0.34430681076418734)
w2 ( -0.27061460445039087 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4649071483898337) - present_state_Q (-0.5384036309304736)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.021625609785252794 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34620751642723635) - present_state_Q ( -0.34620751642723635)) * f1( 0.4700717579716948)
w2 ( -0.20922419267624934 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.34620751642723635) - present_state_Q (-0.34620751642723635)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.011292939807083322 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13404731545373813) - present_state_Q ( -0.17589215398898797)) * f1( 0.39364438425193754)
w2 ( -0.1882251988807602 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13404731545373813) - present_state_Q (-0.17589215398898797)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.0003999670668485226 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11665957949639907) - present_state_Q ( -0.1544325558753623)) * f1( 0.3411332068145596)
w2 ( -0.1608038710467024 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11665957949639907) - present_state_Q (-0.1544325558753623)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.009516523679182409 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.096367979122454) - present_state_Q ( -0.12852875333179448)) * f1( 0.28588230143152155)
w2 ( -0.1352925146131385 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.096367979122454) - present_state_Q (-0.12852875333179448)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.020343246509842705 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18660932438631614) - present_state_Q ( -0.18660932438631614)) * f1( 0.2942456895476705)
w2 ( -0.08377973974046266 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18660932438631614) - present_state_Q (-0.18660932438631614)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.02989831870229565 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05984078744066942) - present_state_Q ( -0.07659673538876195)) * f1( 0.35309036579905495)
w2 ( -0.05671847407599315 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05984078744066942) - present_state_Q (-0.07659673538876195)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.014182492092173024 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02131363347278119) - present_state_Q ( -0.03265732828797982)) * f1( 0.42535672655861534)
w2 ( -0.08627639688073702 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02131363347278119) - present_state_Q (-0.03265732828797982)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.019630498337902355 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02740956397010466) - present_state_Q ( -0.02740956397010466)) * f1( 0.5006873782153569)
w2 ( -0.11328965257781325 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.02740956397010466) - present_state_Q (-0.02740956397010466)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.047985051257065606 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12350637052191593) - present_state_Q ( -0.14577170417613713)) * f1( 0.5004519454196884)
w2 ( -0.1812791245229398 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.12350637052191593) - present_state_Q (-0.14577170417613713)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07668825197314796 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20794037937865822) - present_state_Q ( -0.20814598883693805)) * f1( 0.5599007109540645)
w2 ( -0.23254392943303256 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20794037937865822) - present_state_Q (-0.20814598883693805)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09482841817937179 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36281041852284396) - present_state_Q ( -0.36281041852284396)) * f1( 0.4857186903887296)
w2 ( -0.28482981669915425 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.36281041852284396) - present_state_Q (-0.36281041852284396)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10737122781079131 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3814826760091251) - present_state_Q ( -0.43844863934895595)) * f1( 0.41851268567056144)
w2 ( -0.32678776465442816 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3814826760091251) - present_state_Q (-0.43844863934895595)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12042503653973026 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36446736693279913) - present_state_Q ( -0.36446736693279913)) * f1( 0.3509282984522598)
w2 ( -0.3639857016304762 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.36446736693279913) - present_state_Q (-0.36446736693279913)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1297970302274853 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2529303368057258) - present_state_Q ( -0.3985246174579163)) * f1( 0.2868084313683814)
w2 ( -0.39666254325274186 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2529303368057258) - present_state_Q (-0.3985246174579163)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13756223281603547 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4287483523108929) - present_state_Q ( -0.4287483523108929)) * f1( 0.2471998704586438)
w2 ( -0.4280751915447615 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4287483523108929) - present_state_Q (-0.4287483523108929)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.14166018799710398 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4556706522470421) - present_state_Q ( -0.5412856905559945)) * f1( 0.2006034660631351)
w2 ( -0.45258895650500663 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4556706522470421) - present_state_Q (-0.5412856905559945)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1444429073799221 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4741609285935593) - present_state_Q ( -0.5646787198945608)) * f1( 0.15227970817738626)
w2 ( -0.47451744126078205 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4741609285935593) - present_state_Q (-0.5646787198945608)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1447147347570076 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2945191753312398) - present_state_Q ( -0.38942266358339617)) * f1( 0.0679071804403043)
w2 ( -0.47771978157676026 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2945191753312398) - present_state_Q (-0.38942266358339617)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.14239412094849882 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4876300693854038) - present_state_Q ( -0.4876300693854038)) * f1( 0.06848153939047073)
w2 ( -0.44383307533207395 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.4876300693854038) - present_state_Q (-0.4876300693854038)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13998009156110416 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18686945825230045) - present_state_Q ( -0.18686945825230045)) * f1( 0.06556610664317797)
w2 ( -0.4291057748349911 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18686945825230045) - present_state_Q (-0.18686945825230045)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12577803605719856 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1692876409832722) - present_state_Q ( -0.2551087959502704)) * f1( 0.5962739778594812)
w2 ( -0.4195785735609134 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1692876409832722) - present_state_Q (-0.2551087959502704)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09595997812121163 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32754590346749396) - present_state_Q ( -0.32754590346749396)) * f1( 0.6026390752076601)
w2 ( -0.3898910947736687 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32754590346749396) - present_state_Q (-0.32754590346749396)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06784943763604162 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2914757731268523) - present_state_Q ( -0.29219033239952913)) * f1( 0.6070830431176459)
w2 ( -0.36210852946845806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2914757731268523) - present_state_Q (-0.29219033239952913)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03969567126455298 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32824436073757324) - present_state_Q ( -0.32824436073757324)) * f1( 0.5682808657845834)
w2 ( -0.3224749354953528 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.32824436073757324) - present_state_Q (-0.32824436073757324)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0157703294743345 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.27823919498290905) - present_state_Q ( -0.27902875860998394)) * f1( 0.5302545477420268)
w2 ( -0.28637854836641735 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.27823919498290905) - present_state_Q (-0.27902875860998394)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.004231196998573095 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23669370577528537) - present_state_Q ( -0.23673909579955416)) * f1( 0.4842167133443813)
w2 ( -0.2533329703486553 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23669370577528537) - present_state_Q (-0.23673909579955416)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.014073304370776592 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20035694827884512) - present_state_Q ( -0.20035694827884512)) * f1( 0.5458096138889204)
w2 ( -0.23890727007257848 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.20035694827884512) - present_state_Q (-0.20035694827884512)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.019504581535300644 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1823082768861092) - present_state_Q ( -0.1823082768861092)) * f1( 0.6265436275409012)
w2 ( -0.2817810741367786 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1823082768861092) - present_state_Q (-0.1823082768861092)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04553132421695548 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23583265195910982) - present_state_Q ( -0.23583265195910982)) * f1( 0.5336075850102302)
w2 ( -0.32080112319572274 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.23583265195910982) - present_state_Q (-0.23583265195910982)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.03836630004275421 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2783193280471257) - present_state_Q ( -0.2783193280471257)) * f1( 0.4761212168407504)
w2 ( -0.3087621315763297 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2783193280471257) - present_state_Q (-0.2783193280471257)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.032379959665715885 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2637181129214805) - present_state_Q ( -0.2637304407522555)) * f1( 0.43581829554996504)
w2 ( -0.2977734412195211 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2637181129214805) - present_state_Q (-0.2637304407522555)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.01590791515763038 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2504421349024558) - present_state_Q ( -0.250747756904181)) * f1( 0.38693698379834274)
w2 ( -0.26371715774640625 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2504421349024558) - present_state_Q (-0.250747756904181)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.002663916920683306 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21599816849403514) - present_state_Q ( -0.21631140581412733)) * f1( 0.3355360877972781)
w2 ( -0.23214023062922834 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21599816849403514) - present_state_Q (-0.21631140581412733)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.007236430122700647 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13999230717919617) - present_state_Q ( -0.1864203533050418)) * f1( 0.26583742013902334)
w2 ( -0.20234654082225856 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13999230717919617) - present_state_Q (-0.1864203533050418)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.019236723756126072 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20006162996377003) - present_state_Q ( -0.20006162996377003)) * f1( 0.3157511120463633)
w2 ( -0.16434099412551925 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.20006162996377003) - present_state_Q (-0.20006162996377003)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.03273745050418758 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1568905391119436) - present_state_Q ( -0.15672569057634667)) * f1( 0.39587320823003636)
w2 ( -0.130237330459004 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1568905391119436) - present_state_Q (-0.15672569057634667)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04654410318805684 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11536049567298931) - present_state_Q ( -0.11536049567298931)) * f1( 0.45442863011314044)
w2 ( -0.09985488584843498 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11536049567298931) - present_state_Q (-0.11536049567298931)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.05988921851665242 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03523342544109867) - present_state_Q ( -0.055204402610785665)) * f1( 0.5302391576489771)
w2 ( -0.07972040104310091 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03523342544109867) - present_state_Q (-0.055204402610785665)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.016915783569324556 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0046592661417533265) - present_state_Q ( 0.0046592661417533265)) * f1( 0.6102505169412344)
w2 ( -0.10788813462420402 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0046592661417533265) - present_state_Q (0.0046592661417533265)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.024782366750164056 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05390596556438851) - present_state_Q ( -0.05390596556438851)) * f1( 0.6400481045269268)
w2 ( -0.14697721248372703 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.05390596556438851) - present_state_Q (-0.05390596556438851)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05704688392168389 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19152717330537444) - present_state_Q ( -0.19152717330537444)) * f1( 0.6115040777855347)
w2 ( -0.21029227776674658 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.19152717330537444) - present_state_Q (-0.19152717330537444)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.08046472979659662 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28230924860320955) - present_state_Q ( -0.28230924860320955)) * f1( 0.5251560334871538)
w2 ( -0.26380287891759996 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.28230924860320955) - present_state_Q (-0.28230924860320955)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09961228354008766 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24732421364954818) - present_state_Q ( -0.30008478943306816)) * f1( 0.4509045218592506)
w2 ( -0.3062676421107886 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24732421364954818) - present_state_Q (-0.30008478943306816)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09579439944093567 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22190149579048538) - present_state_Q ( -0.22190149579048538)) * f1( 0.38289364693324074)
w2 ( -0.3002849613381024 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22190149579048538) - present_state_Q (-0.22190149579048538)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0851507095743976 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08995593786181985) - present_state_Q ( -0.15001293012944034)) * f1( 0.3121157997616998)
w2 ( -0.2866442678843721 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08995593786181985) - present_state_Q (-0.15001293012944034)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07867039585371963 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0777667636196176) - present_state_Q ( -0.0777667636196176)) * f1( 0.240020431361012)
w2 ( -0.28124446613921894 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0777667636196176) - present_state_Q (-0.0777667636196176)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07329751628687818 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07220050602594491) - present_state_Q ( -0.07220050602594491)) * f1( 0.20276512689425988)
w2 ( -0.27594485703075194 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07220050602594491) - present_state_Q (-0.07220050602594491)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06993421338643128 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17250619307366163) - present_state_Q ( -0.17250619307366163)) * f1( 0.09467276937530716)
w2 ( -0.2546295226047742 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17250619307366163) - present_state_Q (-0.17250619307366163)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06561940212273937 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21004159219717242) - present_state_Q ( -0.21143238641104728)) * f1( 0.11051483891756213)
w2 ( -0.22339526442946783 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21004159219717242) - present_state_Q (-0.21143238641104728)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0631057796686474 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051377500329935516) - present_state_Q ( -0.051377500329935516)) * f1( 0.10208028764895895)
w2 ( -0.218470469423529 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.051377500329935516) - present_state_Q (-0.051377500329935516)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08060610408089916 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19586331623138215) - present_state_Q ( -0.19586331623138215)) * f1( 0.33415228847945777)
w2 ( -0.2603683106548695 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.19586331623138215) - present_state_Q (-0.19586331623138215)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0856347474562325 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22922063522751626) - present_state_Q ( -0.22922063522751626)) * f1( 0.25960796570218314)
w2 ( -0.27586442491848834 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.22922063522751626) - present_state_Q (-0.22922063522751626)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09939001100776672 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24425142587857243) - present_state_Q ( -0.24527525696738922)) * f1( 0.2870764235646651)
w2 ( -0.3141964157681258 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24425142587857243) - present_state_Q (-0.24527525696738922)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11432478670114686 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2837218532630051) - present_state_Q ( -0.2848227975791849)) * f1( 0.3367105469187351)
w2 ( -0.34968036678789505 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2837218532630051) - present_state_Q (-0.2848227975791849)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1250611963787001 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3843449839619931) - present_state_Q ( -0.3843449839619931)) * f1( 0.3032117371424788)
w2 ( -0.38508931823131565 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3843449839619931) - present_state_Q (-0.3843449839619931)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1106211200640019 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4152547719164774) - present_state_Q ( -0.41649767545568683)) * f1( 0.25114390501481315)
w2 ( -0.3275920984049117 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.4152547719164774) - present_state_Q (-0.41649767545568683)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10078891195843571 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.34761584938203943) - present_state_Q ( -0.3487529569815115)) * f1( 0.19129130643729478)
w2 ( -0.27619296120058096 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.34761584938203943) - present_state_Q (-0.3487529569815115)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.09501693526902297 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23345577150274094) - present_state_Q ( -0.2886943637428571)) * f1( 0.12403549457336739)
w2 ( -0.22965808254132264 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.23345577150274094) - present_state_Q (-0.2886943637428571)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0934773836284514 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14225431892996887) - present_state_Q ( -0.14225431892996887)) * f1( 0.046933416580413924)
w2 ( -0.2099763493191043 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14225431892996887) - present_state_Q (-0.14225431892996887)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08562586349039947 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0699102869973473) - present_state_Q ( -0.0699102869973473)) * f1( 0.29862856714605396)
w2 ( -0.20471796415315205 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0699102869973473) - present_state_Q (-0.0699102869973473)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07017102393646531 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08829579215367736) - present_state_Q ( -0.08829579215367736)) * f1( 0.5530128093640325)
w2 ( -0.19912863989438587 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08829579215367736) - present_state_Q (-0.08829579215367736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05688164791076303 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07471955407329989) - present_state_Q ( -0.07471955407329989)) * f1( 0.4972683044502314)
w2 ( -0.19378368792106646 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07471955407329989) - present_state_Q (-0.07471955407329989)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04532479410364017 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06425330698954493) - present_state_Q ( -0.06425330698954493)) * f1( 0.4482389371934356)
w2 ( -0.18862712839525464 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06425330698954493) - present_state_Q (-0.06425330698954493)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03591845206467255 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054824072839927504) - present_state_Q ( -0.054824072839927504)) * f1( 0.37724710059969874)
w2 ( -0.18364029508413593 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.054824072839927504) - present_state_Q (-0.054824072839927504)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.028401156427431257 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04783712434054713) - present_state_Q ( -0.04783712434054713)) * f1( 0.3092857482755003)
w2 ( -0.17877922684600608 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04783712434054713) - present_state_Q (-0.04783712434054713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.021339766345192385 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04336920849609761) - present_state_Q ( -0.0788182583238791)) * f1( 0.2572630309665708)
w2 ( -0.1677999733470353 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04336920849609761) - present_state_Q (-0.0788182583238791)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.015903192988256414 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03849526473674815) - present_state_Q ( -0.03850407964819824)) * f1( 0.2316841196297836)
w2 ( -0.16310688228354484 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03849526473674815) - present_state_Q (-0.03850407964819824)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.011804386412060606 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03543237965964816) - present_state_Q ( -0.03543237965964816)) * f1( 0.17675715845333428)
w2 ( -0.15846909944967116 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03543237965964816) - present_state_Q (-0.03543237965964816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.009504847348649554 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0328761371620142) - present_state_Q ( -0.0328761371620142)) * f1( 0.10015914684663174)
w2 ( -0.1538773289807549 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0328761371620142) - present_state_Q (-0.0328761371620142)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.007677096378823286 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03611959326814658) - present_state_Q ( -0.03611959326814658)) * f1( 0.5622528459391716)
w2 ( -0.15322717630192825 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.03611959326814658) - present_state_Q (-0.03611959326814658)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05197529165648547 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03548377482045447) - present_state_Q ( -0.03573792087799532)) * f1( 0.6633348555655654)
w2 ( -0.16658338543400925 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.03548377482045447) - present_state_Q (-0.03573792087799532)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08934394617467695 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06526439511944562) - present_state_Q ( -0.09858107220624747)) * f1( 0.614671260409509)
w2 ( -0.19090120012623712 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06526439511944562) - present_state_Q (-0.09858107220624747)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12770057095780776 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13475676479551085) - present_state_Q ( -0.13566989373995147)) * f1( 0.6638324836637548)
w2 ( -0.2140134314358211 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13475676479551085) - present_state_Q (-0.13566989373995147)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1602468191386135 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16040585266211238) - present_state_Q ( -0.16040585266211238)) * f1( 0.585748987077732)
w2 ( -0.23623882073998506 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.16040585266211238) - present_state_Q (-0.16040585266211238)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1622392122613749 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1792097348791092) - present_state_Q ( -0.1810218263345218)) * f1( 0.5399564153824639)
w2 ( -0.23771478662612064 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1792097348791092) - present_state_Q (-0.1810218263345218)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1473533721378017 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14797363410763176) - present_state_Q ( -0.15015541987573466)) * f1( 0.6324763361473742)
w2 ( -0.23300762549682122 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.14797363410763176) - present_state_Q (-0.15015541987573466)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11739389098807029 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3199937409404918) - present_state_Q ( -0.3229311330750449)) * f1( 0.6102575480534587)
w2 ( -0.18391444959872164 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3199937409404918) - present_state_Q (-0.3229311330750449)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.08781237251145385 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25905568590273415) - present_state_Q ( -0.2947642160516823)) * f1( 0.6309261573137829)
w2 ( -0.12765141190335255 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.25905568590273415) - present_state_Q (-0.2947642160516823)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07026595613168463 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18539101795699775) - present_state_Q ( -0.18539101795699775)) * f1( 0.6575338349514916)
w2 ( -0.10096622028722275 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.18539101795699775) - present_state_Q (-0.18539101795699775)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.06860686429256316 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1420407325278824) - present_state_Q ( -0.142364263084914)) * f1( 0.5891621643930621)
w2 ( -0.09815020130401018 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1420407325278824) - present_state_Q (-0.142364263084914)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.04892861214754925 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1563302371887655) - present_state_Q ( -0.15729484596388568)) * f1( 0.5759570096451231)
w2 ( -0.05715078263460907 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1563302371887655) - present_state_Q (-0.15729484596388568)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03467599051940343 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09315016530365106) - present_state_Q ( -0.09315016530365106)) * f1( 0.5021443499772515)
w2 ( -0.023090564781814746 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09315016530365106) - present_state_Q (-0.09315016530365106)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.023396617031999065 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04361220812066732) - present_state_Q ( -0.04402813702345783)) * f1( 0.47062705465178695)
w2 ( 0.005669465163552191 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04361220812066732) - present_state_Q (-0.04402813702345783)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014717093499802504 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0029581239391460717) - present_state_Q ( -0.003204657379286411)) * f1( 0.42775481437599666)
w2 ( 0.030018526561796814 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0029581239391460717) - present_state_Q (-0.003204657379286411)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.008309288381386466 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03084945350236279) - present_state_Q ( 0.030556227813911166)) * f1( 0.3714051324277016)
w2 ( 0.050721972666155836 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03084945350236279) - present_state_Q (0.030556227813911166)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0034230163846469793 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05821134178637736) - present_state_Q ( 0.05811752156106636)) * f1( 0.3308160112096132)
w2 ( 0.0684464061802644 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05821134178637736) - present_state_Q (0.05811752156106636)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 3.755925210222142e-05 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08123100513694258) - present_state_Q ( 0.08120237983333133)) * f1( 0.2726564754910518)
w2 ( 0.08367689266190795 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08123100513694258) - present_state_Q (0.08120237983333133)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.003925995970918139 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10042599541247328) - present_state_Q ( 0.10042559456277916)) * f1( 0.3547293341557466)
w2 ( 0.09683093325932413 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10042599541247328) - present_state_Q (0.10042559456277916)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.007983029400936134 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11785162888404008) - present_state_Q ( 0.1178935327736197)) * f1( 0.43209745374089203)
w2 ( 0.10809792887309824 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11785162888404008) - present_state_Q (0.1178935327736197)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01290531410803684 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13343482307989815) - present_state_Q ( 0.11197432937560348)) * f1( 0.4855801360384149)
w2 ( 0.11823484416633688 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13343482307989815) - present_state_Q (0.11197432937560348)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.016415348825385458 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1486130589412985) - present_state_Q ( 0.14873171143425834)) * f1( 0.5307812252619455)
w2 ( 0.12617039550152145 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1486130589412985) - present_state_Q (0.14873171143425834)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.021128760128462435 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16071680981446473) - present_state_Q ( 0.13581045376939524)) * f1( 0.5872588131033781)
w2 ( 0.13419651822272657 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16071680981446473) - present_state_Q (0.13581045376939524)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.023867241924478158 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.174107062193841) - present_state_Q ( 0.174528889385428)) * f1( 0.6386114204581105)
w2 ( 0.13934233624280132 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.174107062193841) - present_state_Q (0.174528889385428)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.026153951085173058 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18253592097466953) - present_state_Q ( 0.18245760974040287)) * f1( 0.6388172666655799)
w2 ( 0.143637854125649 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18253592097466953) - present_state_Q (0.18245760974040287)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012840649727492714 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19022022351565274) - present_state_Q ( 0.19022022351565274)) * f1( 0.6826807355694706)
w2 ( 0.07509406998595848 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.19022022351565274) - present_state_Q (0.19022022351565274)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.029852798879266892 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08217112008321957) - present_state_Q ( 0.08213806324063123)) * f1( 0.6210605313408963)
w2 ( 0.04222355583808136 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08217112008321957) - present_state_Q (0.08213806324063123)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.0538335632380853 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.034044332805340055) - present_state_Q ( 0.034044332805340055)) * f1( 0.5568635044100704)
w2 ( -0.009453232104895383 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.034044332805340055) - present_state_Q (0.034044332805340055)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04748458263836751 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031851225332333276) - present_state_Q ( -0.03374187175331235)) * f1( 0.4863004507729695)
w2 ( 0.0009913078327109404 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.031851225332333276) - present_state_Q (-0.03374187175331235)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.03803574383970052 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019469778960897546) - present_state_Q ( -0.0198020310680177)) * f1( 0.43372135101268944)
w2 ( 0.01841971208646518 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.019469778960897546) - present_state_Q (-0.0198020310680177)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.030010670591427084 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005985830750098054) - present_state_Q ( 0.00638680968826303)) * f1( 0.4132125003715699)
w2 ( 0.041725124892874804 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.005985830750098054) - present_state_Q (0.00638680968826303)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.023944039351240003 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0390276312297185) - present_state_Q ( 0.0390276312297185)) * f1( 0.3679530788254263)
w2 ( 0.061510140720065214 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0390276312297185) - present_state_Q (0.0390276312297185)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.019569370301933054 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06608602913163177) - present_state_Q ( 0.06634431991316206)) * f1( 0.3118875993047292)
w2 ( 0.07834185468006535 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06608602913163177) - present_state_Q (0.06634431991316206)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.016426695274867394 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08871580251017971) - present_state_Q ( 0.08888465296141361)) * f1( 0.2619181187530869)
w2 ( 0.09274028595481788 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08871580251017971) - present_state_Q (0.08888465296141361)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014245378811521728 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10787425501613439) - present_state_Q ( 0.10780880340144722)) * f1( 0.21182226163639306)
w2 ( 0.10509772060683784 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10787425501613439) - present_state_Q (0.10780880340144722)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012788949001203089 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12406070044667403) - present_state_Q ( 0.10319791789504427)) * f1( 0.13336273727287537)
w2 ( 0.11601853582180015 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12406070044667403) - present_state_Q (0.10319791789504427)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.011798159099521517 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1376922899694308) - present_state_Q ( 0.13755957161884935)) * f1( 0.1300084445683877)
w2 ( 0.1251636947071714 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1376922899694308) - present_state_Q (0.13755957161884935)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.010934591698306309 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09921383280171996) - present_state_Q ( 0.09921067392023177)) * f1( 0.07800215590775375)
w2 ( 0.1340205514559666 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09921383280171996) - present_state_Q (0.09921067392023177)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.007824728755188325 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07780868293942297) - present_state_Q ( 0.07779624591685042)) * f1( 0.23924852696006732)
w2 ( 0.14181962879859214 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07780868293942297) - present_state_Q (0.07779624591685042)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.041915605587297594 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11011757360953153) - present_state_Q ( 0.11011757360953153)) * f1( 0.4266127981917303)
w2 ( 0.07789116349870587 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.11011757360953153) - present_state_Q (0.11011757360953153)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.06952074508236214 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04671978258793348) - present_state_Q ( 0.04671978258793348)) * f1( 0.3720129529932564)
w2 ( 0.01852733915237466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.04671978258793348) - present_state_Q (0.04671978258793348)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09299626949528347 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00860628270068505) - present_state_Q ( -0.00875900006872033)) * f1( 0.3391918680198765)
w2 ( -0.03684079110373319 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.00860628270068505) - present_state_Q (-0.00875900006872033)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11835893808031236 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06502575754398576) - present_state_Q ( -0.06631551370110197)) * f1( 0.39617590058259256)
w2 ( -0.08805575606799693 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06502575754398576) - present_state_Q (-0.06631551370110197)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.14178949799767096 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11987406774209372) - present_state_Q ( -0.13622144062548527)) * f1( 0.4069458998086444)
w2 ( -0.14563235268286934 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.11987406774209372) - present_state_Q (-0.13622144062548527)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1665305570615257 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18192097117268463) - present_state_Q ( -0.18192097117268463)) * f1( 0.4613535554478347)
w2 ( -0.18853404275843605 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.18192097117268463) - present_state_Q (-0.18192097117268463)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18821995877139078 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.26931877976308094) - present_state_Q ( -0.2670799358671761)) * f1( 0.47166054383473177)
w2 ( -0.23451923696934926 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.26931877976308094) - present_state_Q (-0.2670799358671761)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.20733648277136818 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32229137414533393) - present_state_Q ( -0.32229137414533393)) * f1( 0.46632746999265606)
w2 ( -0.2755130132962692 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.32229137414533393) - present_state_Q (-0.32229137414533393)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.22080959897977995 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4165378411518833) - present_state_Q ( -0.4165378411518833)) * f1( 0.4144095821819618)
w2 ( -0.3145269264518658 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4165378411518833) - present_state_Q (-0.4165378411518833)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23135802489399074 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4583924920673832) - present_state_Q ( -0.4584903446416569)) * f1( 0.3670946973045341)
w2 ( -0.3490087949996756 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4583924920673832) - present_state_Q (-0.4584903446416569)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23973706972605996 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.491976979875213) - present_state_Q ( -0.49510348707761265)) * f1( 0.3297613433247438)
w2 ( -0.3795001003088646 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.491976979875213) - present_state_Q (-0.49510348707761265)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.24505482847593046 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3606002276466901) - present_state_Q ( -0.512400267770236)) * f1( 0.23776109161895828)
w2 ( -0.40633927090819655 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3606002276466901) - present_state_Q (-0.512400267770236)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23947243667077725 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20223931406748769) - present_state_Q ( -0.3647750224307663)) * f1( 0.16201927524194357)
w2 ( -0.37877518362627516 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.20223931406748769) - present_state_Q (-0.3647750224307663)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.2348100727813137 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1773870808891602) - present_state_Q ( -0.25292118982322964)) * f1( 0.1071358357736014)
w2 ( -0.3526642347222163 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1773870808891602) - present_state_Q (-0.25292118982322964)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23186771920471816 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15781205951948785) - present_state_Q ( -0.22834490646393113)) * f1( 0.07131877023945962)
w2 ( -0.32791041269149734 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15781205951948785) - present_state_Q (-0.22834490646393113)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.22456938515722485 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06537808215885538) - present_state_Q ( -0.06537808215885538)) * f1( 0.28196284667436805)
w2 ( -0.32791041269149734 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06537808215885538) - present_state_Q (-0.06537808215885538)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.21570205143663376 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12864305980483054) - present_state_Q ( -0.12864305980483054)) * f1( 0.2808084335377283)
w2 ( -0.3215948376150104 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.12864305980483054) - present_state_Q (-0.12864305980483054)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20926557165441995 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11066036228002021) - present_state_Q ( -0.11066036228002021)) * f1( 0.21483984249742627)
w2 ( -0.31560295109397 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11066036228002021) - present_state_Q (-0.11066036228002021)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20440947061328846 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09537508974407707) - present_state_Q ( -0.09831127818106411)) * f1( 0.16816281667384747)
w2 ( -0.3098274757098369 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09537508974407707) - present_state_Q (-0.09831127818106411)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.198098756261311 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.212927096734084) - present_state_Q ( -0.27621764291190554)) * f1( 0.13871990499735992)
w2 ( -0.2734334810507571 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.212927096734084) - present_state_Q (-0.27621764291190554)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1946489986144209 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18126216751615482) - present_state_Q ( -0.1827999285775172)) * f1( 0.09459847351259133)
w2 ( -0.251553058341203 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.18126216751615482) - present_state_Q (-0.1827999285775172)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19110959505936745 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17042734599925577) - present_state_Q ( -0.17042734599925577)) * f1( 0.10015726324465965)
w2 ( -0.23034998165724319 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17042734599925577) - present_state_Q (-0.17042734599925577)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.18786949383314083 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0696430647219564) - present_state_Q ( -0.0696430647219564)) * f1( 0.12334842938254818)
w2 ( -0.22509640649224796 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0696430647219564) - present_state_Q (-0.0696430647219564)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.181317743935904 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0869576667385078) - present_state_Q ( -0.08894065160985729)) * f1( 0.2337866005559004)
w2 ( -0.21949150879352783 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0869576667385078) - present_state_Q (-0.08894065160985729)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17527171461368052 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08003995514725568) - present_state_Q ( -0.08366625537776627)) * f1( 0.2193274235373164)
w2 ( -0.213978263596267 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08003995514725568) - present_state_Q (-0.08366625537776627)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15935638847070005 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1292432340585551) - present_state_Q ( -0.13060433946904465)) * f1( 0.5009860657969366)
w2 ( -0.20762466327500323 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1292432340585551) - present_state_Q (-0.13060433946904465)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14674447746359154 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10895420205416727) - present_state_Q ( -0.10895420205416727)) * f1( 0.423135024872658)
w2 ( -0.20166348763802822 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10895420205416727) - present_state_Q (-0.10895420205416727)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13674666769541735 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09218126598108939) - present_state_Q ( -0.09218126598108939)) * f1( 0.35332551759126873)
w2 ( -0.1960042248503686 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09218126598108939) - present_state_Q (-0.09218126598108939)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1293320274437365 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07686794547420417) - present_state_Q ( -0.07686794547420417)) * f1( 0.27545168843183987)
w2 ( -0.19062060183183294 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07686794547420417) - present_state_Q (-0.07686794547420417)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12519282734686724 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05925586739400639) - present_state_Q ( -0.05925586739400639)) * f1( 0.16339144638270495)
w2 ( -0.18555399621874083 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05925586739400639) - present_state_Q (-0.05925586739400639)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12359982923011709 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.050797019998638976) - present_state_Q ( -0.050797019998638976)) * f1( 0.10932112521886653)
w2 ( -0.18263964985876532 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.050797019998638976) - present_state_Q (-0.050797019998638976)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12198536856586871 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049980709457248765) - present_state_Q ( -0.05026455124770235)) * f1( 0.11113786614036947)
w2 ( -0.1797343202527258 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.049980709457248765) - present_state_Q (-0.05026455124770235)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11791170738273557 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.054394811578919636) - present_state_Q ( -0.05579566703113412)) * f1( 0.16271462072823234)
w2 ( -0.17472719653526095 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.054394811578919636) - present_state_Q (-0.05579566703113412)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11365532517218187 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05371643058175604) - present_state_Q ( -0.05504666118667158)) * f1( 0.1704768960250216)
w2 ( -0.16973369617269102 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05371643058175604) - present_state_Q (-0.05504666118667158)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11098010016641609 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05436305581096014) - present_state_Q ( -0.05436305581096014)) * f1( 0.17963361193584448)
w2 ( -0.16675516116809375 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.05436305581096014) - present_state_Q (-0.05436305581096014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10790810267220118 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.056016985685180126) - present_state_Q ( -0.056016985685180126)) * f1( 0.20423439353157447)
w2 ( -0.1637468554257605 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.056016985685180126) - present_state_Q (-0.056016985685180126)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10525174589722633 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04444626525027221) - present_state_Q ( -0.04468105129883603)) * f1( 0.1105726068590929)
w2 ( -0.15894212693028434 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04444626525027221) - present_state_Q (-0.04468105129883603)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1006940118172381 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19099194642940132) - present_state_Q ( -0.19250449550698556)) * f1( 0.6208998568685998)
w2 ( -0.1530697028611607 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.19099194642940132) - present_state_Q (-0.19250449550698556)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09145568582699715 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1789138576163385) - present_state_Q ( -0.1798816788943549)) * f1( 0.5703012082749834)
w2 ( -0.14011047941054303 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.1789138576163385) - present_state_Q (-0.1798816788943549)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07886304313274446 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15753681061892616) - present_state_Q ( -0.15936351779348146)) * f1( 0.5169184817494606)
w2 ( -0.12062169247201592 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.15753681061892616) - present_state_Q (-0.15936351779348146)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07334309658819778 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13128054350258772) - present_state_Q ( -0.13285633205512987)) * f1( 0.46103950130756033)
w2 ( -0.11104343025562623 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.13128054350258772) - present_state_Q (-0.13285633205512987)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.061916050516877895 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09394685023843369) - present_state_Q ( -0.11615553628955894)) * f1( 0.3725066619215304)
w2 ( -0.08650256215436897 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09394685023843369) - present_state_Q (-0.11615553628955894)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05128637185470114 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09244191588618952) - present_state_Q ( -0.09244191588618952)) * f1( 0.37534477681775436)
w2 ( -0.06384674421056331 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09244191588618952) - present_state_Q (-0.09244191588618952)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04303119200194583 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05526114376286825) - present_state_Q ( -0.05526114376286825)) * f1( 0.330557546253416)
w2 ( -0.048862642447368425 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05526114376286825) - present_state_Q (-0.05526114376286825)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.036008446582477524 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0415193885033162) - present_state_Q ( -0.04202188507907166)) * f1( 0.29523466628756473)
w2 ( -0.03459044567364402 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0415193885033162) - present_state_Q (-0.04202188507907166)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.030874164008108048 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028933244797157254) - present_state_Q ( -0.028933244797157254)) * f1( 0.22714052310579003)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.028933244797157254) - present_state_Q (-0.028933244797157254)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.016101247469421702 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02084900740979341) - present_state_Q ( -0.02084900740979341)) * f1( 0.6752897796461186)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02084900740979341) - present_state_Q (-0.02084900740979341)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0030588389200692633 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009603819072565318) - present_state_Q ( -0.010043792889355726)) * f1( 0.6237897348283202)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.009603819072565318) - present_state_Q (-0.010043792889355726)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.009034908749766132 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001784910109888343) - present_state_Q ( -0.001834452391525477)) * f1( 0.5997218027695091)
w2 ( -0.021028050454597526 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.001784910109888343) - present_state_Q (-0.001834452391525477)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.021573555879511347 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0014970849266015603) - present_state_Q ( 0.0014970849266015603)) * f1( 0.6311845725800694)
w2 ( -0.01705499798327635 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0014970849266015603) - present_state_Q (0.0014970849266015603)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03436939738446339 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007431441514679838) - present_state_Q ( 0.007460269878416641)) * f1( 0.6620266566853366)
w2 ( -0.009323683012354297 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.007431441514679838) - present_state_Q (0.007460269878416641)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04654237272831509 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019120944462622493) - present_state_Q ( 0.019164248138231716)) * f1( 0.6661077320349671)
w2 ( -0.0020137691600330757 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.019120944462622493) - present_state_Q (0.019164248138231716)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.05805163337109642 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030131167841807936) - present_state_Q ( 0.03018959631270977)) * f1( 0.6659545304587886)
w2 ( 0.0048991716588257656 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.030131167841807936) - present_state_Q (0.03018959631270977)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.06897486978373896 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04079429476559325) - present_state_Q ( 0.04079429476559325)) * f1( 0.6689669841641094)
w2 ( 0.011430577047264408 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04079429476559325) - present_state_Q (0.04079429476559325)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.07929490180555122 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050677718020557344) - present_state_Q ( 0.050677718020557344)) * f1( 0.6684389161764114)
w2 ( 0.017606179198524346 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.050677718020557344) - present_state_Q (0.050677718020557344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.08887681555259166 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06444242192332719) - present_state_Q ( 0.06384608787975798)) * f1( 0.6719521576721748)
w2 ( 0.026162068457278833 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06444242192332719) - present_state_Q (0.06384608787975798)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09782293114843459 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07619675302921276) - present_state_Q ( 0.07619675302921276)) * f1( 0.6807119672176564)
w2 ( 0.034047443793701344 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07619675302921276) - present_state_Q (0.07619675302921276)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.10604623040919016 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08676733332549053) - present_state_Q ( 0.08600299142792864)) * f1( 0.6703389929321004)
w2 ( 0.041407868307978574 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08676733332549053) - present_state_Q (0.08600299142792864)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.11355986948272255 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09549679749012707) - present_state_Q ( 0.09351071806178599)) * f1( 0.6475100228649724)
w2 ( 0.04837020600921218 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09549679749012707) - present_state_Q (0.09351071806178599)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.12063798786140265 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10628978030135525) - present_state_Q ( 0.10542553689983472)) * f1( 0.6728029333102723)
w2 ( 0.05468241247703023 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10628978030135525) - present_state_Q (0.10542553689983472)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1271984086467269 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11426566937048055) - present_state_Q ( 0.11426566937048055)) * f1( 0.675212039990629)
w2 ( 0.06051206633102428 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11426566937048055) - present_state_Q (0.11426566937048055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1332689707093093 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12188467702593545) - present_state_Q ( 0.12079919564401853)) * f1( 0.6642532461240671)
w2 ( 0.06599542265453878 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12188467702593545) - present_state_Q (0.12079919564401853)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1389044118530296 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1300802335607672) - present_state_Q ( 0.12904660646158914)) * f1( 0.6711941451395746)
w2 ( 0.07103310766820804 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1300802335607672) - present_state_Q (0.12904660646158914)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14411477239224957 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13718268199252315) - present_state_Q ( 0.13718268199252315)) * f1( 0.6807761980350362)
w2 ( 0.0756252428406118 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13718268199252315) - present_state_Q (0.13718268199252315)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.148941070405695 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1433113369030328) - present_state_Q ( 0.1433113369030328)) * f1( 0.6795708002237575)
w2 ( 0.07988643064784803 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1433113369030328) - present_state_Q (0.1433113369030328)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.15346107801575973 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14861124030964054) - present_state_Q ( 0.14669999457071303)) * f1( 0.6631356677709741)
w2 ( 0.0839760984154631 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14861124030964054) - present_state_Q (0.14669999457071303)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1575966515101251 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15447780260861782) - present_state_Q ( 0.15447780260861782)) * f1( 0.678296704970691)
w2 ( 0.08763429707459773 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15447780260861782) - present_state_Q (0.15447780260861782)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.16152921153591845 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1584954977318756) - present_state_Q ( 0.15581613682287998)) * f1( 0.6550618784656651)
w2 ( 0.09123630185161619 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1584954977318756) - present_state_Q (0.15581613682287998)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.16509894616682633 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16461628500776457) - present_state_Q ( 0.16338952689965164)) * f1( 0.6726197989552029)
w2 ( 0.09442062794768367 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16461628500776457) - present_state_Q (0.16338952689965164)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1684533564528398 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16763676120480453) - present_state_Q ( 0.1662175336026835)) * f1( 0.6636332900838856)
w2 ( 0.0974533964987515 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16763676120480453) - present_state_Q (0.1662175336026835)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17149409583861852 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17216169849999893) - present_state_Q ( 0.17216169849999893)) * f1( 0.6749029107803891)
w2 ( 0.10015666477975156 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17216169849999893) - present_state_Q (0.17216169849999893)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17429987597018273 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17615817267140113) - present_state_Q ( 0.17615817267140113)) * f1( 0.6767823302370148)
w2 ( 0.1026441234554959 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17615817267140113) - present_state_Q (0.17615817267140113)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17694594229333105 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17991208482021082) - present_state_Q ( 0.17856409004577903)) * f1( 0.6711285095377388)
w2 ( 0.10500975056167042 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17991208482021082) - present_state_Q (0.17856409004577903)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1793258301606257 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18333806590254692) - present_state_Q ( 0.18333806590254692)) * f1( 0.680050720609714)
w2 ( 0.10710949500293289 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18333806590254692) - present_state_Q (0.18333806590254692)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18155105110293845 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18571504050145055) - present_state_Q ( 0.18571504050145055)) * f1( 0.6772551583389087)
w2 ( 0.10908088281585455 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18571504050145055) - present_state_Q (0.18571504050145055)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18373718687122118 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18818808389990793) - present_state_Q ( 0.18585618190567726)) * f1( 0.6632164974241544)
w2 ( 0.11105864040491337 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18818808389990793) - present_state_Q (0.18585618190567726)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.185691466063596 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1914698590783301) - present_state_Q ( 0.1900527729171993)) * f1( 0.6717071855505925)
w2 ( 0.1128042931843514 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1914698590783301) - present_state_Q (0.1900527729171993)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1874115984600572 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19416197297968452) - present_state_Q ( 0.19416197297968452)) * f1( 0.6811266007548066)
w2 ( 0.11431954664344843 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19416197297968452) - present_state_Q (0.19416197297968452)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.18900840186056672 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19615613156862527) - present_state_Q ( 0.19615613156862527)) * f1( 0.6806644019406506)
w2 ( 0.11572711553874267 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19615613156862527) - present_state_Q (0.19615613156862527)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.1905036615331047 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19774939552214682) - present_state_Q ( 0.19774939552214682)) * f1( 0.6788752507074212)
w2 ( 0.11704864818054674 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19774939552214682) - present_state_Q (0.19774939552214682)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.17162297077003671 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1990720750057905) - present_state_Q ( 0.1990720750057905)) * f1( 0.6763276099817789)
w2 ( 0.10029875613023403 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1990720750057905) - present_state_Q (0.1990720750057905)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14828983318869324 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1766063157625806) - present_state_Q ( 0.172904652400104)) * f1( 0.6568199945274695)
w2 ( 0.07898411488080327 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1766063157625806) - present_state_Q (0.172904652400104)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.14618041876597251 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14756313438628552) - present_state_Q ( 0.14636187700113343)) * f1( 0.6674187025803316)
w2 ( 0.07708778106705298 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.14756313438628552) - present_state_Q (0.14636187700113343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.13092697928178254 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1447873254321761) - present_state_Q ( 0.14357916288215)) * f1( 0.6657970681951119)
w2 ( 0.06334175524671704 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1447873254321761) - present_state_Q (0.14357916288215)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.09612263130721385 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12666028749456837) - present_state_Q ( 0.12666028749456837)) * f1( 0.677134955933974)
w2 ( 0.03250209972201033 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12666028749456837) - present_state_Q (0.12666028749456837)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04473338077770984 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0844885227418714) - present_state_Q ( 0.08325489266867739)) * f1( 0.6632530962631542)
w2 ( -0.013986262701659084 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0844885227418714) - present_state_Q (0.08325489266867739)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.024666506257713116 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007449201495424474) - present_state_Q ( 0.007113577721413445)) * f1( 0.2840850072393674)
w2 ( -0.042241009004533925 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.007449201495424474) - present_state_Q (0.007113577721413445)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.031244995528035717 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-3.8392964848674854e-05) - present_state_Q ( -0.00034875249822794753)) * f1( 0.3283581881461698)
w2 ( -0.03823411074049907 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -3.8392964848674854e-05) - present_state_Q (-0.00034875249822794753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03967169708029156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034377525399451304) - present_state_Q ( -0.034485347837053426)) * f1( 0.36471712858209093)
w2 ( -0.010508399304846063 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.034377525399451304) - present_state_Q (-0.034485347837053426)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.04794133900695029 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004101939953180259) - present_state_Q ( 0.004101939953180259)) * f1( 0.4212579836242467)
w2 ( 0.013048591180210473 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.004101939953180259) - present_state_Q (0.004101939953180259)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.05566816786274267 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04003391271188424) - present_state_Q ( 0.04099256219546621)) * f1( 0.4740070889525439)
w2 ( 0.03587010725081159 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04003391271188424) - present_state_Q (0.04099256219546621)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.040464416863205566 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08127129008323394) - present_state_Q ( 0.0812110072944228)) * f1( 0.5567428987370951)
w2 ( -0.0023616357092423254 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08127129008323394) - present_state_Q (0.0812110072944228)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0053179728870659596 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019292812649438648) - present_state_Q ( 0.016593684782764677)) * f1( 0.4917894861299496)
w2 ( -0.10241465220173726 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.019292812649438648) - present_state_Q (0.016593684782764677)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0323384843476342 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13989232119727946) - present_state_Q ( -0.13989232119727946)) * f1( 0.6559250976319391)
w2 ( -0.18278821973088005 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.13989232119727946) - present_state_Q (-0.13989232119727946)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05511617560023471 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2720881384671995) - present_state_Q ( -0.2720881384671995)) * f1( 0.5004758624425584)
w2 ( -0.24650511428401292 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2720881384671995) - present_state_Q (-0.2720881384671995)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.050435652590258374 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12233399279034737) - present_state_Q ( -0.22093603850395255)) * f1( 0.43058043883293556)
w2 ( -0.2378089031460195 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.12233399279034737) - present_state_Q (-0.22093603850395255)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.030989231579279554 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3030896568732092) - present_state_Q ( -0.3977778864516081)) * f1( 0.3426869789589127)
w2 ( -0.14701387582373354 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3030896568732092) - present_state_Q (-0.3977778864516081)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.019004604286089407 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1857809751714606) - present_state_Q ( -0.21518375033620732)) * f1( 0.3021799414104103)
w2 ( -0.09148908442906496 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.1857809751714606) - present_state_Q (-0.21518375033620732)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.010728295304329935 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15106431176847254) - present_state_Q ( -0.15106431176847254)) * f1( 0.24634960094357083)
w2 ( -0.03773582353440491 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15106431176847254) - present_state_Q (-0.15106431176847254)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.005934698092766598 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.062385040545243076) - present_state_Q ( -0.062385040545243076)) * f1( 0.1871427690273309)
w2 ( 0.0032476223041101038 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.062385040545243076) - present_state_Q (-0.062385040545243076)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.003337250050781431 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004409837486120182) - present_state_Q ( 0.004409837486120182)) * f1( 0.1325018034892834)
w2 ( 0.0346126057061088 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.004409837486120182) - present_state_Q (0.004409837486120182)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.002001936734464543 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05508864245141058) - present_state_Q ( 0.05508392341360138)) * f1( 0.08876940944336488)
w2 ( 0.058680596239155156 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05508864245141058) - present_state_Q (0.05508392341360138)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.009109760443685513 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.022267248894712648) - present_state_Q ( 0.022287049850688464)) * f1( 0.5920210287217691)
w2 ( 0.053878183240706465 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.022267248894712648) - present_state_Q (0.022287049850688464)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0008108401461516656 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02736111700244347) - present_state_Q ( 0.02717905120405366)) * f1( 0.5650926577260867)
w2 ( 0.06441160687047791 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02736111700244347) - present_state_Q (0.02717905120405366)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.011621132648235577 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052107063685400384) - present_state_Q ( 0.05210178113233624)) * f1( 0.7060524058546348)
w2 ( 0.07666032088937422 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.052107063685400384) - present_state_Q (0.05210178113233624)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.02273094305760669 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06956865434153733) - present_state_Q ( 0.05446265010969373)) * f1( 0.7285397931805419)
w2 ( 0.08580997380884182 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06956865434153733) - present_state_Q (0.05446265010969373)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.008037518857063717 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08734740292350501) - present_state_Q ( 0.08734740292350501)) * f1( 0.822641798408534)
w2 ( 0.07152096079834946 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.08734740292350501) - present_state_Q (0.08734740292350501)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.01170321759883184 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06427730199877199) - present_state_Q ( 0.06419277248910099)) * f1( 0.8679300135377719)
w2 ( 0.07489975741521156 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.06427730199877199) - present_state_Q (0.06419277248910099)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.023986014023726507 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07056408175505557) - present_state_Q ( 0.07044199268426282)) * f1( 0.8990849450790218)
w2 ( 0.08582891065451098 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07056408175505557) - present_state_Q (0.07044199268426282)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.001655953352000715 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09134766795174322) - present_state_Q ( 0.07466075129379372)) * f1( 0.9657046342995687)
w2 ( 0.06989735158459381 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09134766795174322) - present_state_Q (0.07466075129379372)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04115645361405206 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05446111511756052) - present_state_Q ( 0.05446111511756052)) * f1( 0.8797144849246351)
w2 ( 0.03397615129612945 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05446111511756052) - present_state_Q (0.05446111511756052)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04071121500365371 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0061150090424393275) - present_state_Q ( -0.0061150090424393275)) * f1( 0.8090087253770244)
w2 ( 0.034416431947185085 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.0061150090424393275) - present_state_Q (-0.0061150090424393275)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.026389398461907323 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0010951402625697243) - present_state_Q ( -0.0014287449570423438)) * f1( 0.7113983336579655)
w2 ( 0.05052197042164792 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0010951402625697243) - present_state_Q (-0.0014287449570423438)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.015131829264541255 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02378327049518505) - present_state_Q ( 0.02378327049518505)) * f1( 0.6303404704788796)
w2 ( 0.0648095749459946 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02378327049518505) - present_state_Q (0.02378327049518505)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0056673749443135815 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06850794599133211) - present_state_Q ( 0.055356138716116964)) * f1( 0.6247384942433942)
w2 ( 0.07995904053429623 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06850794599133211) - present_state_Q (0.055356138716116964)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0009623378106376054 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09283971980565643) - present_state_Q ( 0.09272726178023363)) * f1( 0.5687971755170816)
w2 ( 0.09394584575833607 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09283971980565643) - present_state_Q (0.09272726178023363)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0008273987269102453 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1133729324002108) - present_state_Q ( 0.1133729324002108)) * f1( 0.6628831197901704)
w2 ( 0.09370156905911331 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1133729324002108) - present_state_Q (0.1133729324002108)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.05746705488283182 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11303584551767543) - present_state_Q ( 0.11304348392310327)) * f1( 0.7270993205583531)
w2 ( -0.0025072188654469973 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.11303584551767543) - present_state_Q (0.11304348392310327)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09803951868846883 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.038029273482983) - present_state_Q ( -0.038029273482983)) * f1( 0.6094032644590762)
w2 ( -0.08240005732928485 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.038029273482983) - present_state_Q (-0.038029273482983)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.12980297478121935 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1536237954910869) - present_state_Q ( -0.15439237659832358)) * f1( 0.5662237896085364)
w2 ( -0.14971645768337905 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.1536237954910869) - present_state_Q (-0.15439237659832358)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.11861253402321803 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24542190072042108) - present_state_Q ( -0.24542190072042108)) * f1( 0.5066305422599686)
w2 ( -0.12321089240557356 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.24542190072042108) - present_state_Q (-0.24542190072042108)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.10267826256311927 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15099105532023108) - present_state_Q ( -0.17563323380134577)) * f1( 0.4419629158711904)
w2 ( -0.08715747957864128 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.15099105532023108) - present_state_Q (-0.17563323380134577)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10209594204670902 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12755604548856264) - present_state_Q ( -0.12755604548856264)) * f1( 0.39344808629856987)
w2 ( -0.08567743548467065 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.12755604548856264) - present_state_Q (-0.12755604548856264)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.10287375737056093 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08422663053968343) - present_state_Q ( -0.08422663053968343)) * f1( 0.3214639934843422)
w2 ( -0.08712919743552774 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08422663053968343) - present_state_Q (-0.08422663053968343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09640319915888806 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0609687481799742) - present_state_Q ( -0.0609687481799742)) * f1( 0.2538749421943146)
w2 ( -0.07693432250104867 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0609687481799742) - present_state_Q (-0.0609687481799742)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.020531283228363402 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01124490975935563) - present_state_Q ( 0.011213726038920216)) * f1( 0.2609439149187399)
w2 ( 0.044478811537895464 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.01124490975935563) - present_state_Q (0.011213726038920216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04345745392125077 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019877019711174355) - present_state_Q ( 0.020132850050240104)) * f1( 0.3192414618995858)
w2 ( 0.0013901026531480953 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.019877019711174355) - present_state_Q (0.020132850050240104)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06436277795300234 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012353868873098868) - present_state_Q ( -0.012353868873098868)) * f1( 0.303467628105539)
w2 ( -0.039942788427704566 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.012353868873098868) - present_state_Q (-0.012353868873098868)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08321123362746677 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02591649488070017) - present_state_Q ( -0.02591649488070017)) * f1( 0.27854511202500026)
w2 ( -0.05347629151985196 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.02591649488070017) - present_state_Q (-0.02591649488070017)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09970514622285967 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04211994253666179) - present_state_Q ( -0.04211994253666179)) * f1( 0.24911811813205148)
w2 ( -0.07995997358853214 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.04211994253666179) - present_state_Q (-0.04211994253666179)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11424689060232254 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07076167855533769) - present_state_Q ( -0.07076167855533769)) * f1( 0.22853077564610455)
w2 ( -0.11813884294654392 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.07076167855533769) - present_state_Q (-0.07076167855533769)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12380818423013767 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0884916698508929) - present_state_Q ( -0.0884916698508929)) * f1( 0.15412554328728986)
w2 ( -0.1553602927745957 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0884916698508929) - present_state_Q (-0.0884916698508929)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12127737441048855 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0816950787019774) - present_state_Q ( -0.08034291839130282)) * f1( 0.14699190844795984)
w2 ( -0.1484733563537515 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0816950787019774) - present_state_Q (-0.08034291839130282)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11872154412044914 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07112967481608254) - present_state_Q ( -0.07112967481608254)) * f1( 0.09680562703183476)
w2 ( -0.13791268806037252 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07112967481608254) - present_state_Q (-0.07112967481608254)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11318747458402363 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07785977323336069) - present_state_Q ( -0.07935803817389504)) * f1( 0.20377904557239432)
w2 ( -0.12704980562635015 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07785977323336069) - present_state_Q (-0.07935803817389504)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1101036014587217 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06435351026989795) - present_state_Q ( -0.06435351026989795)) * f1( 0.11956789449623559)
w2 ( -0.11673307925663383 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06435351026989795) - present_state_Q (-0.06435351026989795)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1053591093494036 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04296769072970294) - present_state_Q ( -0.045045034618525055)) * f1( 0.19707274312305867)
w2 ( -0.11191811394572274 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04296769072970294) - present_state_Q (-0.045045034618525055)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10065887506626946 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.041923198838770534) - present_state_Q ( -0.04311087433871607)) * f1( 0.19672956308726477)
w2 ( -0.10713974285662596 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.041923198838770534) - present_state_Q (-0.04311087433871607)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09477130212474076 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04492045359515344) - present_state_Q ( -0.045970001165216406)) * f1( 0.24381409565459364)
w2 ( -0.10231018374051194 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04492045359515344) - present_state_Q (-0.045970001165216406)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08896924945085759 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06239058927875818) - present_state_Q ( -0.06239058927875818)) * f1( 0.2265086086323742)
w2 ( -0.09206412252647665 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06239058927875818) - present_state_Q (-0.06239058927875818)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08491991630966264 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05145235300926296) - present_state_Q ( -0.05145235300926296)) * f1( 0.16440179150607978)
w2 ( -0.08221183781814319 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05145235300926296) - present_state_Q (-0.05145235300926296)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08213253552025443 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04280829438742998) - present_state_Q ( -0.04280829438742998)) * f1( 0.11685785492282151)
w2 ( -0.07267073922019571 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04280829438742998) - present_state_Q (-0.04280829438742998)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08467465419992357 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059332457674734204) - present_state_Q ( -0.06097007154602327)) * f1( 0.5653779395442227)
w2 ( -0.07357000270462472 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.059332457674734204) - present_state_Q (-0.06097007154602327)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07128673510599143 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07125510188692409) - present_state_Q ( -0.07219482982306161)) * f1( 0.5050723754978185)
w2 ( -0.06296722991924994 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07125510188692409) - present_state_Q (-0.07219482982306161)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.05805598146146966 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07181078590882878) - present_state_Q ( -0.07323141940558749)) * f1( 0.497302638440766)
w2 ( -0.04700420947036766 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07181078590882878) - present_state_Q (-0.07323141940558749)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04586482475743838 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0557706348245519) - present_state_Q ( -0.0564183571813481)) * f1( 0.4860107570113798)
w2 ( -0.031953731848434086 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0557706348245519) - present_state_Q (-0.0564183571813481)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03329934109326514 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04915889953338788) - present_state_Q ( -0.04915889953338788)) * f1( 0.5144664605049823)
w2 ( -0.012414291082030154 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04915889953338788) - present_state_Q (-0.04915889953338788)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.02183238491693934 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02658097041804618) - present_state_Q ( -0.026955362441267974)) * f1( 0.5112392322707843)
w2 ( 0.005529490149926916 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.02658097041804618) - present_state_Q (-0.026955362441267974)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.012635397214318542 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.003886067462798376) - present_state_Q ( -0.0043167326180980105)) * f1( 0.45099162576532925)
w2 ( 0.025922302737108736 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.003886067462798376) - present_state_Q (-0.0043167326180980105)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0059919080161648175 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021280648107047908) - present_state_Q ( 0.021280648107047908)) * f1( 0.3673532815257176)
w2 ( 0.044007044407474424 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.021280648107047908) - present_state_Q (0.021280648107047908)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.0005525686922305704 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.050797125052401464) - present_state_Q ( 0.042008408225488056)) * f1( 0.3335558851361703)
w2 ( 0.06031417483544964 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.050797125052401464) - present_state_Q (0.042008408225488056)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.003118216651404736 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07222675629363556) - present_state_Q ( 0.07222675629363556)) * f1( 0.2719182447660695)
w2 ( 0.076513685155737 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07222675629363556) - present_state_Q (0.07222675629363556)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.007577146803278397 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09301205481185379) - present_state_Q ( 0.09301205481185379)) * f1( 0.3834347508954327)
w2 ( 0.0904683832360568 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09301205481185379) - present_state_Q (0.09301205481185379)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.011996066696832102 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11187249453423742) - present_state_Q ( 0.11193558567317508)) * f1( 0.44522376001046743)
w2 ( 0.10237858288968664 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11187249453423742) - present_state_Q (0.11193558567317508)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.016100818272103423 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12870476680696621) - present_state_Q ( 0.12870476680696621)) * f1( 0.487698800548284)
w2 ( 0.1124784680745343 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12870476680696621) - present_state_Q (0.12870476680696621)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01983823289859737 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14346474820012795) - present_state_Q ( 0.14346357666465745)) * f1( 0.5272660576465971)
w2 ( 0.12098441585317694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14346474820012795) - present_state_Q (0.14346357666465745)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.023155127453993993 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1566519111444754) - present_state_Q ( 0.15625753601351564)) * f1( 0.558327803001366)
w2 ( 0.12811333446528878 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1566519111444754) - present_state_Q (0.15625753601351564)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.01991976013628476 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16850695773159285) - present_state_Q ( 0.1682976644660803)) * f1( 0.6288742368905428)
w2 ( 0.12193969822213825 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16850695773159285) - present_state_Q (0.1682976644660803)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.037965891880429814 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16038845367962193) - present_state_Q ( 0.1599903693241754)) * f1( 0.6858883522759993)
w2 ( 0.020665515347392638 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.16038845367962193) - present_state_Q (0.1599903693241754)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07826230967054496 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0030278064373142205) - present_state_Q ( 0.0030278064373142205)) * f1( 0.573430805948723)
w2 ( -0.06366148774783731 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0030278064373142205) - present_state_Q (0.0030278064373142205)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.10931097074411258 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09107347581207034) - present_state_Q ( -0.10380577336163781)) * f1( 0.5129453217365156)
w2 ( -0.12419164516979422 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.09107347581207034) - present_state_Q (-0.10380577336163781)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13373253442444344 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20040322795090093) - present_state_Q ( -0.20040322795090093)) * f1( 0.4699734472892767)
w2 ( -0.18654809655109694 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.20040322795090093) - present_state_Q (-0.20040322795090093)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.15365935175255063 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24179127413505772) - present_state_Q ( -0.24179127413505772)) * f1( 0.4130870458839035)
w2 ( -0.23478688187894176 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24179127413505772) - present_state_Q (-0.24179127413505772)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16949381455842497 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24395938219197155) - present_state_Q ( -0.2909167585677599)) * f1( 0.36528773581713636)
w2 ( -0.2781347998440855 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24395938219197155) - present_state_Q (-0.2909167585677599)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18198497587026094 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33026938493465197) - present_state_Q ( -0.33076677173617525)) * f1( 0.31052444025293524)
w2 ( -0.3183608165198145 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33026938493465197) - present_state_Q (-0.33076677173617525)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18376986040690743 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.36229412209319195) - present_state_Q ( -0.36229412209319195)) * f1( 0.24141171744143294)
w2 ( -0.32575434553142724 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.36229412209319195) - present_state_Q (-0.36229412209319195)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.17735967599816418 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22731340687267296) - present_state_Q ( -0.2924642759789584)) * f1( 0.17337336755477573)
w2 ( -0.29617571070809195 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22731340687267296) - present_state_Q (-0.2924642759789584)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17384566153834566 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13770985462728458) - present_state_Q ( -0.13770985462728458)) * f1( 0.10847770349020552)
w2 ( -0.28321815594150973 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13770985462728458) - present_state_Q (-0.13770985462728458)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1712921862946677 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17851377741755892) - present_state_Q ( -0.23716122096679376)) * f1( 0.06089709757439553)
w2 ( -0.24967336848350669 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17851377741755892) - present_state_Q (-0.23716122096679376)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.16041681907509944 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1317926327022797) - present_state_Q ( -0.1342139855124907)) * f1( 0.49202076077659923)
w2 ( -0.24525267403866144 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1317926327022797) - present_state_Q (-0.1342139855124907)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15101833469956885 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11884196893025352) - present_state_Q ( -0.12111017846705424)) * f1( 0.44920254668300763)
w2 ( -0.24106815440718085 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.11884196893025352) - present_state_Q (-0.12111017846705424)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14783243450092676 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20288895506546797) - present_state_Q ( -0.20288895506546797)) * f1( 0.3857019251141678)
w2 ( -0.2361121508336456 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.20288895506546797) - present_state_Q (-0.20288895506546797)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1446145316323902 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1985140411659382) - present_state_Q ( -0.20058833394795328)) * f1( 0.3985664150541777)
w2 ( -0.23126793504376403 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1985140411659382) - present_state_Q (-0.20058833394795328)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.16404138602779386 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.24332784692757348) - present_state_Q ( -0.24343427187794026)) * f1( 0.40396994121885565)
w2 ( -0.26973981606894937 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.24332784692757348) - present_state_Q (-0.24343427187794026)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.179502937686188 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2714505036428742) - present_state_Q ( -0.2714505036428742)) * f1( 0.33929639425434)
w2 ( -0.30619537980666245 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.2714505036428742) - present_state_Q (-0.2714505036428742)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1751108048947271 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29309300572537167) - present_state_Q ( -0.29309300572537167)) * f1( 0.2681666523151595)
w2 ( -0.29309268339443567 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.29309300572537167) - present_state_Q (-0.29309300572537167)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.16478125066665206 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21149172435273772) - present_state_Q ( -0.32872879771051194)) * f1( 0.20350608483297172)
w2 ( -0.24233472086691185 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21149172435273772) - present_state_Q (-0.32872879771051194)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1608217909145627 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11822744945442186) - present_state_Q ( -0.11822744945442186)) * f1( 0.12922320362001258)
w2 ( -0.23007853268655265 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11822744945442186) - present_state_Q (-0.11822744945442186)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.15605342111692583 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2454743501801276) - present_state_Q ( -0.24818046787845494)) * f1( 0.11255897032958061)
w2 ( -0.18771522940050844 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2454743501801276) - present_state_Q (-0.24818046787845494)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1481311468245179 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0826065267990787) - present_state_Q ( -0.0826065267990787)) * f1( 0.2887695802914336)
w2 ( -0.18222831191812502 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0826065267990787) - present_state_Q (-0.0826065267990787)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1419691551934694 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07103029959422191) - present_state_Q ( -0.07103029959422191)) * f1( 0.23347309429506588)
w2 ( -0.17694976652542904 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07103029959422191) - present_state_Q (-0.07103029959422191)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13767094595528157 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.059460136370648585) - present_state_Q ( -0.059460136370648585)) * f1( 0.1695451595296244)
w2 ( -0.17187948407075737 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.059460136370648585) - present_state_Q (-0.059460136370648585)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13154604727394226 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06679325338111108) - present_state_Q ( -0.06679325338111108)) * f1( 0.23546984690211584)
w2 ( -0.16667720550989737 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06679325338111108) - present_state_Q (-0.06679325338111108)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12592426069438237 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05914208674311498) - present_state_Q ( -0.09247752784509446)) * f1( 0.1961795597506145)
w2 ( -0.15521467274306605 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05914208674311498) - present_state_Q (-0.09247752784509446)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12190120103072599 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.049429387828709745) - present_state_Q ( -0.08047232237732296)) * f1( 0.14601200101321524)
w2 ( -0.14419349739928797 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.049429387828709745) - present_state_Q (-0.08047232237732296)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11915196060884667 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0704026668469619) - present_state_Q ( -0.0704026668469619)) * f1( 0.10439001240060984)
w2 ( -0.13365900139279735 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0704026668469619) - present_state_Q (-0.0704026668469619)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11574499276965619 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0436939894507265) - present_state_Q ( -0.0436939894507265)) * f1( 0.14235761699172278)
w2 ( -0.12887250958268426 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0436939894507265) - present_state_Q (-0.0436939894507265)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11338021533046024 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03748475699890583) - present_state_Q ( -0.03748475699890583)) * f1( 0.10117288706971125)
w2 ( -0.12419778395670396 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03748475699890583) - present_state_Q (-0.03748475699890583)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11080138470004458 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035097074093147176) - present_state_Q ( -0.037343659370338754)) * f1( 0.11028469598997723)
w2 ( -0.11952110491748348 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.035097074093147176) - present_state_Q (-0.037343659370338754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.111328664872137 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09710767840272061) - present_state_Q ( -0.09809198073287911)) * f1( 0.45381688055623626)
w2 ( -0.1199858564017792 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.09710767840272061) - present_state_Q (-0.09809198073287911)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09877764544452361 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11766912511922462) - present_state_Q ( -0.11766912511922462)) * f1( 0.410295150225853)
w2 ( -0.10163172364534107 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11766912511922462) - present_state_Q (-0.11766912511922462)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08883919145377651 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09532863557035509) - present_state_Q ( -0.09532863557035509)) * f1( 0.3477467116022945)
w2 ( -0.08448397732454188 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09532863557035509) - present_state_Q (-0.09532863557035509)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07950084383482005 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09471480638525576) - present_state_Q ( -0.09649148280457079)) * f1( 0.32535529051923356)
w2 ( -0.06152237715125826 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09471480638525576) - present_state_Q (-0.09649148280457079)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07268236129923883 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06984038692612446) - present_state_Q ( -0.06984038692612446)) * f1( 0.2593995762858249)
w2 ( -0.0404938692925773 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06984038692612446) - present_state_Q (-0.06984038692612446)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0679110348903925 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046722346406765335) - present_state_Q ( -0.046722346406765335)) * f1( 0.19712142969209695)
w2 ( -0.02112986035129019 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.046722346406765335) - present_state_Q (-0.046722346406765335)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0644812807395913 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027276600498474846) - present_state_Q ( -0.027276600498474846)) * f1( 0.15273971651564594)
w2 ( -0.0031659451153999985 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.027276600498474846) - present_state_Q (-0.027276600498474846)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05244565099034786 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03425431973450566) - present_state_Q ( -0.03425431973450566)) * f1( 0.5214091644240929)
w2 ( 0.0014506326398211035 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03425431973450566) - present_state_Q (-0.03425431973450566)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04225913578658057 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.023570141110588937) - present_state_Q ( -0.023570141110588937)) * f1( 0.4604842100436895)
w2 ( 0.010299157719802308 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.023570141110588937) - present_state_Q (-0.023570141110588937)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.034223766463945704 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011989581165407631) - present_state_Q ( -0.011989581165407631)) * f1( 0.38120145983780535)
w2 ( 0.018730782641756982 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.011989581165407631) - present_state_Q (-0.011989581165407631)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.027584682116040184 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0036831841896906815) - present_state_Q ( -0.0036831841896906815)) * f1( 0.3265420028554343)
w2 ( 0.02686337727258585 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0036831841896906815) - present_state_Q (-0.0036831841896906815)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.021943367259009708 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002863102474168879) - present_state_Q ( 0.002863102474168879)) * f1( 0.2857472999582628)
w2 ( 0.03476030558351577 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.002863102474168879) - present_state_Q (0.002863102474168879)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01772168455373595 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009075016568117128) - present_state_Q ( 0.009075016568117128)) * f1( 0.22007131395508137)
w2 ( 0.042433604987063556 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.009075016568117128) - present_state_Q (0.009075016568117128)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.014648870462713732 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01389511488863712) - present_state_Q ( 0.014066409102478424)) * f1( 0.1640381806555833)
w2 ( 0.04992652908251897 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01389511488863712) - present_state_Q (0.014066409102478424)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.012220936269262885 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.018035212987900336) - present_state_Q ( 0.018035212987900336)) * f1( 0.1321193091326386)
w2 ( 0.05727726141495455 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.018035212987900336) - present_state_Q (0.018035212987900336)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.007773781769432169 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01652645448027494) - present_state_Q ( 0.01652645448027494)) * f1( 0.5224190638948456)
w2 ( 0.06068230905366465 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.01652645448027494) - present_state_Q (0.01652645448027494)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0025575400284336683 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03188510642600263) - present_state_Q ( 0.0317253790058759)) * f1( 0.602538965621753)
w2 ( 0.07097009695186812 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03188510642600263) - present_state_Q (0.0317253790058759)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01251984570216328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07265549696550674) - present_state_Q ( 0.05848864141500861)) * f1( 0.6696137047610351)
w2 ( 0.08287224961439149 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07265549696550674) - present_state_Q (0.05848864141500861)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.02052963330012951 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09138869607245184) - present_state_Q ( 0.09138869607245184)) * f1( 0.6802357361791452)
w2 ( 0.09464726696787083 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09138869607245184) - present_state_Q (0.09138869607245184)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02068194458731082 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10862560223584171) - present_state_Q ( 0.10862560223584171)) * f1( 0.6808857744128678)
w2 ( 0.09487096276664508 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.10862560223584171) - present_state_Q (0.10862560223584171)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.03465422661608197 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12788592784220426) - present_state_Q ( 0.12788592784220426)) * f1( 0.6788903462609939)
w2 ( -0.002940717440312976 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.12788592784220426) - present_state_Q (0.12788592784220426)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07655633649665114 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.024958978975393193) - present_state_Q ( -0.02496074082928726)) * f1( 0.6184492338653376)
w2 ( -0.08424493628850324 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.024958978975393193) - present_state_Q (-0.02496074082928726)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.058000928416204266 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14417237490763224) - present_state_Q ( -0.14417237490763224)) * f1( 0.5627026230978638)
w2 ( -0.044674319798478954 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14417237490763224) - present_state_Q (-0.14417237490763224)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.04376866980653407 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08360056203733804) - present_state_Q ( -0.08360056203733804)) * f1( 0.5170844518892963)
w2 ( -0.011645459098446435 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08360056203733804) - present_state_Q (-0.08360056203733804)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.03295023557984872 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0338724551521897) - present_state_Q ( -0.034465786236384674)) * f1( 0.4681713062979557)
w2 ( 0.016083965788093457 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0338724551521897) - present_state_Q (-0.034465786236384674)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.02494658113490472 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005763543487705768) - present_state_Q ( 0.005763543487705768)) * f1( 0.41083819947816397)
w2 ( 0.03946150309142124 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.005763543487705768) - present_state_Q (0.005763543487705768)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01901556002783453 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03864437481713866) - present_state_Q ( 0.038411175588200924)) * f1( 0.35847108961124285)
w2 ( 0.059315894518642796 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03864437481713866) - present_state_Q (0.038411175588200924)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01424441894384262 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0649941157309887) - present_state_Q ( 0.06477739215529521)) * f1( 0.33665488987468806)
w2 ( 0.07632253684877924 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0649941157309887) - present_state_Q (0.06477739215529521)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.011142389299183328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10261358051297315) - present_state_Q ( 0.1027419126598286)) * f1( 0.2885087095980712)
w2 ( 0.09137525920358486 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10261358051297315) - present_state_Q (0.1027419126598286)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.008471440403141125 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12489069632432062) - present_state_Q ( 0.10683353919389327)) * f1( 0.2527978313067037)
w2 ( 0.10405392285620951 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12489069632432062) - present_state_Q (0.10683353919389327)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.006841105373444647 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14366293627242094) - present_state_Q ( 0.1437204876154299)) * f1( 0.23077591179636073)
w2 ( 0.11394433569786322 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14366293627242094) - present_state_Q (0.1437204876154299)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.005861781999811987 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15835649022021367) - present_state_Q ( 0.15835649022021367)) * f1( 0.17037886323448775)
w2 ( 0.1219914179301163 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15835649022021367) - present_state_Q (0.15835649022021367)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.004910388862028252 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1699985283521091) - present_state_Q ( 0.1456085338269476)) * f1( 0.13326454126356937)
w2 ( 0.13055837621110788 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1699985283521091) - present_state_Q (0.1456085338269476)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.0008661583694440532 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02451754150232469) - present_state_Q ( 0.02451754150232469)) * f1( 0.32464511155607867)
w2 ( 0.13411706046406605 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02451754150232469) - present_state_Q (0.02451754150232469)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.013010542591551641 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0005271987250166057) - present_state_Q ( 0.0005271987250166057)) * f1( 0.6086632001893488)
w2 ( 0.13411706046406605 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0005271987250166057) - present_state_Q (0.0005271987250166057)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.02461714997809615 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03582499194594185) - present_state_Q ( 0.03582499194594185)) * f1( 0.6918681361508933)
w2 ( 0.13747221060903908 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03582499194594185) - present_state_Q (0.03582499194594185)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03647896733774462 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04677193830072077) - present_state_Q ( 0.045883321537596694)) * f1( 0.7469946534083325)
w2 ( 0.1406480880548886 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04677193830072077) - present_state_Q (0.045883321537596694)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04897402713868215 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05919513543725207) - present_state_Q ( 0.05919513543725207)) * f1( 0.8516008015975551)
w2 ( 0.14358257561701807 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05919513543725207) - present_state_Q (0.05919513543725207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06096468425411333 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07221217788925653) - present_state_Q ( 0.07221217788925653)) * f1( 0.88813735171674)
w2 ( 0.14628275641501146 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07221217788925653) - present_state_Q (0.07221217788925653)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07246695490087053 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08678470076461443) - present_state_Q ( 0.08678470076461443)) * f1( 0.9436307295847378)
w2 ( 0.1487206318012484 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08678470076461443) - present_state_Q (0.08678470076461443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08328735442392121 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10254187954283528) - present_state_Q ( 0.10254187954283528)) * f1( 1.0045648155378901)
w2 ( 0.15087487796947735 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10254187954283528) - present_state_Q (0.10254187954283528)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0931062472591512 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11455602371324052) - present_state_Q ( 0.11466953150111629)) * f1( 1.0144944150483532)
w2 ( 0.15281059938688152 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11455602371324052) - present_state_Q (0.11466953150111629)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10110070528741562 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12501847235679867) - present_state_Q ( 0.09268451852095633)) * f1( 0.6672205192705171)
w2 ( 0.15520694596117598 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12501847235679867) - present_state_Q (0.09268451852095633)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10919029138706328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13371436291823297) - present_state_Q ( 0.13371436291823297)) * f1( 1.0155515081137407)
w2 ( 0.1568000874286478 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13371436291823297) - present_state_Q (0.13371436291823297)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11656701765702127 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1425327370680003) - present_state_Q ( 0.14035135658828052)) * f1( 0.9981779306384752)
w2 ( 0.15827812577101819 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1425327370680003) - present_state_Q (0.14035135658828052)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12316699911568062 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14977823686433564) - present_state_Q ( 0.14993167447469602)) * f1( 1.014661365606004)
w2 ( 0.15957904875525294 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14977823686433564) - present_state_Q (0.14993167447469602)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12912953127530832 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1569734463328783) - present_state_Q ( 0.1569734463328783)) * f1( 1.0153501950986998)
w2 ( 0.16075352672126114 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1569734463328783) - present_state_Q (0.1569734463328783)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13450522649140775 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1633004580221094) - present_state_Q ( 0.16346915745011142)) * f1( 1.016951357361346)
w2 ( 0.16181074448830313 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1633004580221094) - present_state_Q (0.16346915745011142)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13945188027299552 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16919961567627131) - present_state_Q ( 0.16779066336192114)) * f1( 1.006864327855035)
w2 ( 0.16279333045241726 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16919961567627131) - present_state_Q (0.16779066336192114)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14380711273652041 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17476959987998203) - present_state_Q ( 0.17476959987998203)) * f1( 1.0197849861264106)
w2 ( 0.16364747765457757 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17476959987998203) - present_state_Q (0.17476959987998203)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14774703513652526 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17904284668814446) - present_state_Q ( 0.17922919839361112)) * f1( 1.0187236227397771)
w2 ( 0.16442097938008163 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17904284668814446) - present_state_Q (0.17922919839361112)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1512956446123156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18371322592442868) - present_state_Q ( 0.18357949135092214)) * f1( 1.0199547851207722)
w2 ( 0.16511681600491204 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18371322592442868) - present_state_Q (0.18357949135092214)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15450280644168646 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18726851244119036) - present_state_Q ( 0.18726851244119036)) * f1( 1.0194949738008008)
w2 ( 0.1657459827809706 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18726851244119036) - present_state_Q (0.18726851244119036)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15742330139209837 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19048450957428414) - present_state_Q ( 0.19034356312401207)) * f1( 1.017420784697185)
w2 ( 0.16632008053763894 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19048450957428414) - present_state_Q (0.19034356312401207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1601353063095108 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1924161699507923) - present_state_Q ( 0.1924161699507923)) * f1( 1.0109821890144461)
w2 ( 0.1668565894785247 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1924161699507923) - present_state_Q (0.1924161699507923)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16250347577619156 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19653620732582477) - present_state_Q ( 0.19639092563764157)) * f1( 1.0180116521390419)
w2 ( 0.1673218433804235 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19653620732582477) - present_state_Q (0.19639092563764157)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16462876924606082 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19880748501152354) - present_state_Q ( 0.19901957585808874)) * f1( 1.018779483892489)
w2 ( 0.16773906683328477 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19880748501152354) - present_state_Q (0.19901957585808874)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16650984706030397 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20152750344949777) - present_state_Q ( 0.20174057075909813)) * f1( 1.0216486350636167)
w2 ( 0.1681073104250018 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20152750344949777) - present_state_Q (0.20174057075909813)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16823286603475396 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20345188955050514) - present_state_Q ( 0.20345188955050514)) * f1( 1.0199422464426275)
w2 ( 0.1684451764130927 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20345188955050514) - present_state_Q (0.20345188955050514)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16982206549852832 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2050407734005427) - present_state_Q ( 0.20488736026536308)) * f1( 1.0176271082927277)
w2 ( 0.16875751075458653 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2050407734005427) - present_state_Q (0.20488736026536308)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17130013050876855 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2060338013616811) - present_state_Q ( 0.2060338013616811)) * f1( 1.0144871263048958)
w2 ( 0.16904890233007627 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2060338013616811) - present_state_Q (0.2060338013616811)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17264731374288306 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20763300600770418) - present_state_Q ( 0.20747493142923049)) * f1( 1.0138062968628363)
w2 ( 0.16931466971350706 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20763300600770418) - present_state_Q (0.20747493142923049)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17746145031899713 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20912850324779875) - present_state_Q ( 0.14825509908774173)) * f1( 0.6625771502903319)
w2 ( 0.17076782473824784 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20912850324779875) - present_state_Q (0.14825509908774173)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16807141447787863 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21403895491530464) - present_state_Q ( 0.21403895491530464)) * f1( 1.0136589644922926)
w2 ( 0.16891512354977234 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.21403895491530464) - present_state_Q (0.21403895491530464)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1390219676084861 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20525656349880997) - present_state_Q ( 0.20525656349880997)) * f1( 1.0202421352943671)
w2 ( 0.16322050540679375 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20525656349880997) - present_state_Q (0.20525656349880997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10290242490274736 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17373053645224365) - present_state_Q ( 0.17360363951611874)) * f1( 1.0139371558294332)
w2 ( 0.15609589368937588 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17373053645224365) - present_state_Q (0.17360363951611874)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01920810553456656 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13594734850986628) - present_state_Q ( 0.13594734850986628)) * f1( 1.0177424863502416)
w2 ( 0.13964884141619827 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.13594734850986628) - present_state_Q (0.13594734850986628)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05645006421782367 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04751300627868213) - present_state_Q ( 0.04749571953330335)) * f1( 1.018629932808999)
w2 ( 0.12479395303808956 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.04751300627868213) - present_state_Q (0.04749571953330335)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.055997734307367486 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008483595931084425) - present_state_Q ( -0.008483595931084425)) * f1( 0.5924242425953372)
w2 ( 0.12494665776484908 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.008483595931084425) - present_state_Q (-0.008483595931084425)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.050272863785771595 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005544872026694427) - present_state_Q ( -0.005544872026694427)) * f1( 0.545275696549868)
w2 ( 0.12704646546132958 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.005544872026694427) - present_state_Q (-0.005544872026694427)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04069690816091502 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001207274314575242) - present_state_Q ( 0.001207274314575242)) * f1( 0.4814131711458303)
w2 ( 0.13102473452366722 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.001207274314575242) - present_state_Q (0.001207274314575242)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.032665813098903464 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009160306413003527) - present_state_Q ( 0.009160306413003527)) * f1( 0.41881905191263286)
w2 ( 0.13485984900823317 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.009160306413003527) - present_state_Q (0.009160306413003527)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.026127215522921517 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01549222900430704) - present_state_Q ( 0.01549222900430704)) * f1( 0.3514298193827893)
w2 ( 0.13858098888615564 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01549222900430704) - present_state_Q (0.01549222900430704)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.02098964080183043 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020100854753246078) - present_state_Q ( 0.02032798921057851)) * f1( 0.28277826085871693)
w2 ( 0.14221463081145055 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.020100854753246078) - present_state_Q (0.02032798921057851)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03474368015227225 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.078838720213755) - present_state_Q ( 0.05036140018486967)) * f1( 0.31084153375038154)
w2 ( 0.12451552968491078 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.078838720213755) - present_state_Q (0.05036140018486967)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.04104587312214452 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06629391017020067) - present_state_Q ( 0.06627627309057459)) * f1( 0.24272168876216063)
w2 ( 0.1089367167604975 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06629391017020067) - present_state_Q (0.06627627309057459)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03961161538279629 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08246887634917825) - present_state_Q ( 0.08246887634917825)) * f1( 0.11403088065130235)
w2 ( 0.11899895766335666 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08246887634917825) - present_state_Q (0.08246887634917825)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.05758552915577679 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04653194269987533) - present_state_Q ( 0.04653194269987533)) * f1( 0.49328665137528066)
w2 ( 0.11253948087425504 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.04653194269987533) - present_state_Q (0.04653194269987533)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08329489186012454 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03361565783360791) - present_state_Q ( 0.03307042502415996)) * f1( 0.5982972459485829)
w2 ( 0.08675694931980708 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.03361565783360791) - present_state_Q (0.03307042502415996)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07286994598890732 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0063232610321446725) - present_state_Q ( 0.0071697837794807776)) * f1( 0.5388612051718242)
w2 ( 0.0983647018592311 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0063232610321446725) - present_state_Q (0.0071697837794807776)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06426890374000242 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023197681581495128) - present_state_Q ( 0.023892119186809897)) * f1( 0.48204649327002325)
w2 ( 0.10907036079751148 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.023197681581495128) - present_state_Q (0.023892119186809897)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05702853976797205 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03693315048814684) - present_state_Q ( 0.03745111366123163)) * f1( 0.4355310451616272)
w2 ( 0.11904489288076646 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03693315048814684) - present_state_Q (0.03745111366123163)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.051189824068878315 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049950901488385185) - present_state_Q ( 0.049950901488385185)) * f1( 0.3765839758032156)
w2 ( 0.12834754420039365 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.049950901488385185) - present_state_Q (0.049950901488385185)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0457526582219271 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0591874248960909) - present_state_Q ( 0.05817055810174823)) * f1( 0.36800221061788774)
w2 ( 0.1372124352636653 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0591874248960909) - present_state_Q (0.05817055810174823)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.04166909766629138 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0684100047916875) - present_state_Q ( 0.06879344725719744)) * f1( 0.29580825304955805)
w2 ( 0.1454952884569836 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0684100047916875) - present_state_Q (0.06879344725719744)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03649031530163034 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07317933648795305) - present_state_Q ( 0.0449108166411178)) * f1( 0.3188765652687634)
w2 ( 0.1519915731372907 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07317933648795305) - present_state_Q (0.0449108166411178)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03222877379224026 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08058523170684381) - present_state_Q ( 0.05090173420326995)) * f1( 0.2711649644530269)
w2 ( 0.15827784469598727 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08058523170684381) - present_state_Q (0.05090173420326995)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.029510919021593342 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08752200065039865) - present_state_Q ( 0.08772899356128225)) * f1( 0.2245730260470773)
w2 ( 0.16553923708621274 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08752200065039865) - present_state_Q (0.08772899356128225)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.02757764242206066 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09436549528247883) - present_state_Q ( 0.09436549528247883)) * f1( 0.16800720321928964)
w2 ( 0.17244350034095887 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09436549528247883) - present_state_Q (0.09436549528247883)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0136844509412437 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014039686890186101) - present_state_Q ( 0.014039686890186101)) * f1( 0.7415069375780847)
w2 ( 0.17619078597693552 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.014039686890186101) - present_state_Q (0.014039686890186101)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0010794026964448863 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025439854690706542) - present_state_Q ( -0.00952678352454749)) * f1( 0.6961757958322334)
w2 ( 0.17619078597693552 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.025439854690706542) - present_state_Q (-0.00952678352454749)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.014770404041575575 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.036120476446634214) - present_state_Q ( 0.036120476446634214)) * f1( 0.8174143479102896)
w2 ( 0.1795406174008961 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.036120476446634214) - present_state_Q (0.036120476446634214)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02847609041123586 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04888463344658606) - present_state_Q ( 0.04888463344658606)) * f1( 0.8785480701733477)
w2 ( 0.18266069399885757 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04888463344658606) - present_state_Q (0.04888463344658606)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0416184429108157 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06313491698126869) - present_state_Q ( 0.06256706983649775)) * f1( 0.91427336620808)
w2 ( 0.18553562243609015 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06313491698126869) - present_state_Q (0.06256706983649775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05439675201031679 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07811020820075132) - present_state_Q ( 0.07811020820075132)) * f1( 0.9852142666989953)
w2 ( 0.18812963868847662 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07811020820075132) - present_state_Q (0.07811020820075132)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06621962412335441 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09286863959978042) - present_state_Q ( 0.09286863959978042)) * f1( 1.0155516610919686)
w2 ( 0.19045800317568057 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09286863959978042) - present_state_Q (0.09286863959978042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0769142049211454 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10544033172470542) - present_state_Q ( 0.10552821429636797)) * f1( 1.0183780798213293)
w2 ( 0.19255831955320263 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10544033172470542) - present_state_Q (0.10552821429636797)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08657876788600502 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11702397511472712) - present_state_Q ( 0.11702397511472712)) * f1( 1.0207777781045726)
w2 ( 0.19445188800113755 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11702397511472712) - present_state_Q (0.11702397511472712)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0953007644752685 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12711281254999554) - present_state_Q ( 0.12722541877078475)) * f1( 1.0202852653997643)
w2 ( 0.19616160525082185 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12711281254999554) - present_state_Q (0.12722541877078475)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10317852907808246 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.136338654603606) - present_state_Q ( 0.136251686999868)) * f1( 1.018033448985408)
w2 ( 0.1977092488200317 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.136338654603606) - present_state_Q (0.136251686999868)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11028883613336349 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14453099503826486) - present_state_Q ( 0.14466598019642385)) * f1( 1.0188566494572013)
w2 ( 0.19910499120617978 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14453099503826486) - present_state_Q (0.14466598019642385)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11672584535238886 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1515560125632857) - present_state_Q ( 0.15170096594138965)) * f1( 1.0144269503838648)
w2 ( 0.20037408391247855 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1515560125632857) - present_state_Q (0.15170096594138965)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11669180974644418 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15845371151854662) - present_state_Q ( 0.11636611662143792)) * f1( 0.6535938943823709)
w2 ( 0.20036366900308689 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.15845371151854662) - present_state_Q (0.11636611662143792)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09208417477180006 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15855043903091415) - present_state_Q ( 0.1584427053611075)) * f1( 1.0143811448094975)
w2 ( 0.19551191577392657 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.15855043903091415) - present_state_Q (0.1584427053611075)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06986338199388856 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13232378523876392) - present_state_Q ( 0.13244485258026287)) * f1( 1.0136646134560663)
w2 ( 0.19112766629279884 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.13232378523876392) - present_state_Q (0.13244485258026287)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0801933545070333 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10912996887930251) - present_state_Q ( 0.10912996887930251)) * f1( 1.0149012772806396)
w2 ( 0.19316332685297138 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10912996887930251) - present_state_Q (0.10912996887930251)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08952195414891093 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11978633050444432) - present_state_Q ( 0.1197119790054435)) * f1( 1.011047787354228)
w2 ( 0.1950086599338714 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11978633050444432) - present_state_Q (0.1197119790054435)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08805527792564523 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1295359360124036) - present_state_Q ( 0.127748471896881)) * f1( 0.9913405125460653)
w2 ( 0.1947127623679586 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1295359360124036) - present_state_Q (0.127748471896881)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08648170434492398 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12833393244953364) - present_state_Q ( 0.12833393244953364)) * f1( 1.01517344651873)
w2 ( 0.194402751583867 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12833393244953364) - present_state_Q (0.12833393244953364)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08504459166254645 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12681519294067345) - present_state_Q ( 0.12681519294067345)) * f1( 1.0168005278108438)
w2 ( 0.19412007811093487 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12681519294067345) - present_state_Q (0.12681519294067345)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08375922436790216 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12526097675171424) - present_state_Q ( 0.12518399111338338)) * f1( 1.0154669897630804)
w2 ( 0.19386692024217064 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12526097675171424) - present_state_Q (0.12518399111338338)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08258836763680771 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1239101342977002) - present_state_Q ( 0.1239101342977002)) * f1( 1.0164462588062337)
w2 ( 0.19363653782481205 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1239101342977002) - present_state_Q (0.1239101342977002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -3.778082000750749e-05 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12298400560142385) - present_state_Q ( 0.12291013443504578)) * f1( 1.0193060993805747)
w2 ( 0.17742430314731397 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.12298400560142385) - present_state_Q (0.12291013443504578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0039907008748238364 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.035462505904040875) - present_state_Q ( 0.035462505904040875)) * f1( 0.5916950827821057)
w2 ( 0.17878597804104124 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.035462505904040875) - present_state_Q (0.035462505904040875)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.015228180930284465 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.038468887682960175) - present_state_Q ( 0.038468887682960175)) * f1( 0.679502713886476)
w2 ( 0.18209353806274794 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.038468887682960175) - present_state_Q (0.038468887682960175)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.025654692649631796 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.051818418107795014) - present_state_Q ( 0.04641962671258541)) * f1( 0.6567376067975969)
w2 ( 0.18526878236471184 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.051818418107795014) - present_state_Q (0.04641962671258541)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.019897068773276644 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0630742488393955) - present_state_Q ( 0.0630742488393955)) * f1( 1.0142585889379807)
w2 ( 0.1841334458856027 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.0630742488393955) - present_state_Q (0.0630742488393955)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.035038193307969485 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05707325293612339) - present_state_Q ( 0.05709904519773873)) * f1( 1.018861433893498)
w2 ( 0.1871056114875202 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05707325293612339) - present_state_Q (0.05709904519773873)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.038522536047116196 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07302510144599351) - present_state_Q ( 0.07232510921059317)) * f1( 0.9961697113289846)
w2 ( 0.18780515950620033 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.07302510144599351) - present_state_Q (0.07232510921059317)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05179564129030785 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07659944439091462) - present_state_Q ( 0.07656406312152891)) * f1( 1.0124730929600525)
w2 ( 0.19042707713255158 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07659944439091462) - present_state_Q (0.07656406312152891)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06381622581804447 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09061115888313417) - present_state_Q ( 0.0906789850889552)) * f1( 1.0154053189082983)
w2 ( 0.19279471974853873 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09061115888313417) - present_state_Q (0.0906789850889552)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07469336460750035 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10347869410892355) - present_state_Q ( 0.10356132025132325)) * f1( 1.0185869732715476)
w2 ( 0.19493045073173013 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10347869410892355) - present_state_Q (0.10356132025132325)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08452254234785851 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11515991394653458) - present_state_Q ( 0.11525613513499239)) * f1( 1.0211087074391572)
w2 ( 0.19685564785692336 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11515991394653458) - present_state_Q (0.11525613513499239)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09339186313700858 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1253905190203688) - present_state_Q ( 0.1255002159166626)) * f1( 1.0190072843621714)
w2 ( 0.19859642457663085 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1253905190203688) - present_state_Q (0.1255002159166626)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09123421090025993 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1347802235689266) - present_state_Q ( 0.1346947762361624)) * f1( 1.0169568111250167)
w2 ( 0.19817208949904544 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1347802235689266) - present_state_Q (0.1346947762361624)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08930639587653227 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1320886955068306) - present_state_Q ( 0.1322080830133588)) * f1( 1.0146814906390116)
w2 ( 0.19779210522979193 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1320886955068306) - present_state_Q (0.1322080830133588)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07732785274998015 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13058284362055955) - present_state_Q ( 0.13058284362055955)) * f1( 1.0192374429760225)
w2 ( 0.19544161404462185 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.13058284362055955) - present_state_Q (0.13058284362055955)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07676690899787823 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1173540771640113) - present_state_Q ( 0.11728268439752229)) * f1( 1.0112056498118394)
w2 ( 0.19533066851099942 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1173540771640113) - present_state_Q (0.11728268439752229)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07619940610600172 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11737546941381014) - present_state_Q ( 0.11730575426920266)) * f1( 1.0191841978314542)
w2 ( 0.19521930436444299 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.11737546941381014) - present_state_Q (0.11730575426920266)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06562073195635346 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11622704403062453) - present_state_Q ( 0.1161565551591569)) * f1( 1.0119855026034732)
w2 ( 0.1931286273493211 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.11622704403062453) - present_state_Q (0.1161565551591569)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04586862546042164 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1052031307735726) - present_state_Q ( 0.1052031307735726)) * f1( 1.0145788277398555)
w2 ( 0.1892349709953968 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1052031307735726) - present_state_Q (0.1052031307735726)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03604872496351936 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0665554190786198) - present_state_Q ( 0.06609716656842914)) * f1( 0.6158931532344657)
w2 ( 0.18604613850218546 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.0665554190786198) - present_state_Q (0.06609716656842914)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04637107862750385 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06321067126998278) - present_state_Q ( 0.06321067126998278)) * f1( 0.7212860814316917)
w2 ( 0.18890834641932577 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06321067126998278) - present_state_Q (0.06321067126998278)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.056796555314450395 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07402858441977207) - present_state_Q ( 0.07402858441977207)) * f1( 0.7816707354831285)
w2 ( 0.19157583189976987 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07402858441977207) - present_state_Q (0.07402858441977207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06686332682746077 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08392868480912186) - present_state_Q ( 0.08444352093887825)) * f1( 0.8121681729382647)
w2 ( 0.19405481885061054 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08392868480912186) - present_state_Q (0.08444352093887825)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07628597384986119 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09297202815523802) - present_state_Q ( 0.09297202815523802)) * f1( 0.8100264667487642)
w2 ( 0.19638132234381625 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09297202815523802) - present_state_Q (0.09297202815523802)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08511628334361175 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10104163700824534) - present_state_Q ( 0.10104163700824534)) * f1( 0.8096556866540475)
w2 ( 0.19856257287766785 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10104163700824534) - present_state_Q (0.10104163700824534)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09339332309736299 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10798209190894213) - present_state_Q ( 0.10874869304183872)) * f1( 0.8110807445340191)
w2 ( 0.20060356320064895 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10798209190894213) - present_state_Q (0.10874869304183872)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10112666466084957 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11551775897777503) - present_state_Q ( 0.1146680891033848)) * f1( 0.7982088439613504)
w2 ( 0.2025412369365368 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11551775897777503) - present_state_Q (0.1146680891033848)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10839791735766907 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12170241253227841) - present_state_Q ( 0.12262646865370783)) * f1( 0.8120333202108653)
w2 ( 0.20433211238852722 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12170241253227841) - present_state_Q (0.12262646865370783)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1152187122459969 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12843046515143666) - present_state_Q ( 0.12747939251716411)) * f1( 0.7990279901196907)
w2 ( 0.20603938546848682 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12843046515143666) - present_state_Q (0.12747939251716411)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12160218683807004 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1336084321947289) - present_state_Q ( 0.13464667738273978)) * f1( 0.8109689690815721)
w2 ( 0.2076136687852215 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1336084321947289) - present_state_Q (0.13464667738273978)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12757908658029046 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13944805955762324) - present_state_Q ( 0.14055180221967878)) * f1( 0.8143691411940251)
w2 ( 0.20908152885994316 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13944805955762324) - present_state_Q (0.14055180221967878)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13327014425954015 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14444491954664884) - present_state_Q ( 0.14189714365779316)) * f1( 0.784461157141298)
w2 ( 0.21053247582588058 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14444491954664884) - present_state_Q (0.14189714365779316)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13853033355991629 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15008749817729788) - present_state_Q ( 0.15008749817729788)) * f1( 0.8102415106704736)
w2 ( 0.21183090085868922 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15008749817729788) - present_state_Q (0.15008749817729788)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14351932766151607 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15408667633235373) - present_state_Q ( 0.15285939600502554)) * f1( 0.7976102633543277)
w2 ( 0.2130818862912534 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15408667633235373) - present_state_Q (0.15285939600502554)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14812116769330244 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.157728494639151) - present_state_Q ( 0.15904860962623088)) * f1( 0.8112651742807799)
w2 ( 0.2142163710880071 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.157728494639151) - present_state_Q (0.15904860962623088)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15252031623711532 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16279266761820593) - present_state_Q ( 0.1612462935414788)) * f1( 0.7993659594220927)
w2 ( 0.2153170305524139 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16279266761820593) - present_state_Q (0.1612462935414788)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15658045160017636 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1664641095203545) - present_state_Q ( 0.1664641095203545)) * f1( 0.8090771541414006)
w2 ( 0.21632067658104753 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1664641095203545) - present_state_Q (0.1664641095203545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16040079032481666 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16962053809622923) - present_state_Q ( 0.16962053809622923)) * f1( 0.8069743156870383)
w2 ( 0.2172675068953154 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16962053809622923) - present_state_Q (0.16962053809622923)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1640605864644918 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17284689966786115) - present_state_Q ( 0.17140540397592727)) * f1( 0.7977011979664036)
w2 ( 0.21818509261513258 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17284689966786115) - present_state_Q (0.17140540397592727)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1674915991820491 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17609240681235291) - present_state_Q ( 0.1746418067127676)) * f1( 0.7985146890724723)
w2 ( 0.21904444129450193 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17609240681235291) - present_state_Q (0.1746418067127676)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17057015174549364 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17846696115325353) - present_state_Q ( 0.17998033760130522)) * f1( 0.8130046522177992)
w2 ( 0.21980176846478233 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17846696115325353) - present_state_Q (0.17998033760130522)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17347167657280937 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1807637750924295) - present_state_Q ( 0.18230152206121616)) * f1( 0.8110514468831419)
w2 ( 0.22051726557374288 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1807637750924295) - present_state_Q (0.18230152206121616)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17619377276469747 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18317426800853923) - present_state_Q ( 0.18474137732006737)) * f1( 0.8107255719425203)
w2 ( 0.2211887865633586 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18317426800853923) - present_state_Q (0.18474137732006737)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17890788903530266 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1864011466957165) - present_state_Q ( 0.18456088727786524)) * f1( 0.7964136743504078)
w2 ( 0.22187037111119273 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1864011466957165) - present_state_Q (0.18456088727786524)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18143473804077231 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18902942940043044) - present_state_Q ( 0.18726538418263267)) * f1( 0.7986864678292546)
w2 ( 0.22250312228634095 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18902942940043044) - present_state_Q (0.18726538418263267)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18368653354769532 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1912989197477921) - present_state_Q ( 0.1912989197477921)) * f1( 0.8090969616718886)
w2 ( 0.2230597417308807 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1912989197477921) - present_state_Q (0.1912989197477921)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18592574213215152 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19289651955623394) - present_state_Q ( 0.191237817337443)) * f1( 0.7982396213775494)
w2 ( 0.2236207784232443 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19289651955623394) - present_state_Q (0.191237817337443)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18801982907236114 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19498363218089454) - present_state_Q ( 0.19329180554453793)) * f1( 0.7990698230172494)
w2 ( 0.22414490957671532 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19498363218089454) - present_state_Q (0.19329180554453793)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18985021638977276 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19711185388838845) - present_state_Q ( 0.19711185388838845)) * f1( 0.8099298500821311)
w2 ( 0.22459689620672432 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19711185388838845) - present_state_Q (0.19711185388838845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19156159029620506 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19875532526234652) - present_state_Q ( 0.19875532526234652)) * f1( 0.8103016627864575)
w2 ( 0.22501930035200207 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19875532526234652) - present_state_Q (0.19875532526234652)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19320080315855304 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19966290485544386) - present_state_Q ( 0.19966290485544386)) * f1( 0.8073593696204939)
w2 ( 0.2254253680646041 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19966290485544386) - present_state_Q (0.19966290485544386)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1946598733111175 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19998414089938177) - present_state_Q ( 0.20203803091632216)) * f1( 0.8123825301833535)
w2 ( 0.22578457572807642 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.19998414089938177) - present_state_Q (0.20203803091632216)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1962580634488516 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20214604235627073) - present_state_Q ( 0.20014140237527595)) * f1( 0.7961809724490821)
w2 ( 0.22618603976528345 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20214604235627073) - present_state_Q (0.20014140237527595)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19757925396369974 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20408507118286118) - present_state_Q ( 0.20408507118286118)) * f1( 0.8093826079721973)
w2 ( 0.22651250848399196 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20408507118286118) - present_state_Q (0.20408507118286118)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1988193891988585 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20519517611348775) - present_state_Q ( 0.20519517611348775)) * f1( 0.8092584176173965)
w2 ( 0.22681899531394917 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20519517611348775) - present_state_Q (0.20519517611348775)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19995120776346662 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20470916874993034) - present_state_Q ( 0.20650642590789361)) * f1( 0.8104975450051778)
w2 ( 0.22709828513329117 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.20470916874993034) - present_state_Q (0.20650642590789361)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.193588708763774 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20588138072689705) - present_state_Q ( 0.20189236936050942)) * f1( 0.7825544745843769)
w2 ( 0.22547220050753478 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20588138072689705) - present_state_Q (0.20189236936050942)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1707259473943722 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20029308053207961) - present_state_Q ( 0.20203862987699217)) * f1( 0.8107094198711552)
w2 ( 0.2198320140710591 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20029308053207961) - present_state_Q (0.20203862987699217)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14978017543684602 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18187360632565963) - present_state_Q ( 0.1803630063697986)) * f1( 0.7989213452159936)
w2 ( 0.21458850115631445 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.18187360632565963) - present_state_Q (0.1803630063697986)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14610118090240065 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16379763785616092) - present_state_Q ( 0.1624712721978118)) * f1( 0.7981935634529818)
w2 ( 0.21366667098807055 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16379763785616092) - present_state_Q (0.1624712721978118)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1424680301921085 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16098614129976002) - present_state_Q ( 0.16098614129976002)) * f1( 0.8093898103475415)
w2 ( 0.21276892044467485 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16098614129976002) - present_state_Q (0.16098614129976002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1471595819932443 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15775654858013008) - present_state_Q ( 0.15775654858013008)) * f1( 0.8086218665047307)
w2 ( 0.21392930257023252 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15775654858013008) - present_state_Q (0.15775654858013008)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14363274838614878 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16187876355917863) - present_state_Q ( 0.16033881693476543)) * f1( 0.798812791042825)
w2 ( 0.21304628375865556 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.16187876355917863) - present_state_Q (0.16033881693476543)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14830985349701803 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.158529979747982) - present_state_Q ( 0.15725779060723138)) * f1( 0.7982060856154751)
w2 ( 0.21421818790600688 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.158529979747982) - present_state_Q (0.15725779060723138)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.15269878029782685 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1626320869940145) - present_state_Q ( 0.1613241014984066)) * f1( 0.7988711547044136)
w2 ( 0.21531697005002678 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1626320869940145) - present_state_Q (0.1613241014984066)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1567188149908044 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16568881013001902) - present_state_Q ( 0.16706522557162357)) * f1( 0.8120682517552693)
w2 ( 0.21630704315885435 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16568881013001902) - present_state_Q (0.16706522557162357)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1605282162846059 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1698006322565399) - present_state_Q ( 0.1698006322565399)) * f1( 0.8074284101254456)
w2 ( 0.21725063177823664 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1698006322565399) - present_state_Q (0.1698006322565399)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1641755768965308 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1730334718127394) - present_state_Q ( 0.1716222536893416)) * f1( 0.7984398649671256)
w2 ( 0.2181642536480753 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1730334718127394) - present_state_Q (0.1716222536893416)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1596160593653878 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17616702925477346) - present_state_Q ( 0.1747205621045037)) * f1( 0.7984604887821085)
w2 ( 0.21702217646449476 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.17616702925477346) - present_state_Q (0.1747205621045037)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13897126672545052 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17250205036866506) - present_state_Q ( 0.17250205036866506)) * f1( 0.8088009163303557)
w2 ( 0.2119171395578588 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.17250205036866506) - present_state_Q (0.17250205036866506)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11150718244189758 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15484992643099127) - present_state_Q ( 0.15484992643099127)) * f1( 0.8092787895616335)
w2 ( 0.20512984088210096 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15484992643099127) - present_state_Q (0.15484992643099127)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.046278749528718494 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13107725876977894) - present_state_Q ( 0.13005760803686223)) * f1( 0.798438610955247)
w2 ( 0.18879084323890327 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.13107725876977894) - present_state_Q (0.13005760803686223)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.01590122216930802 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07524088804087586) - present_state_Q ( 0.07524088804087586)) * f1( 0.8099337120125756)
w2 ( 0.1734365072541675 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.07524088804087586) - present_state_Q (0.07524088804087586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.069187247106902 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023104555554315457) - present_state_Q ( 0.02292917050764594)) * f1( 0.7394482523414267)
w2 ( 0.15902413295512322 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.023104555554315457) - present_state_Q (0.02292917050764594)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11781093483262664 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015986815675535018) - present_state_Q ( -0.01736144054933674)) * f1( 0.7106261514408)
w2 ( 0.14533938813475888 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.015986815675535018) - present_state_Q (-0.01736144054933674)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16013249455169604 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046710910538854206) - present_state_Q ( -0.046710910538854206)) * f1( 0.6432237234469321)
w2 ( 0.13218018452445826 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.046710910538854206) - present_state_Q (-0.046710910538854206)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19901727413353656 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0715258010787768) - present_state_Q ( -0.0715258010787768)) * f1( 0.6117548987038552)
w2 ( 0.11946764894387624 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0715258010787768) - present_state_Q (-0.0715258010787768)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2333253806706045 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0857315457896226) - present_state_Q ( -0.0857315457896226)) * f1( 0.5508319619775399)
w2 ( 0.10701081676808945 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.0857315457896226) - present_state_Q (-0.0857315457896226)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26220880214603326 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08699757345722486) - present_state_Q ( -0.08699757345722486)) * f1( 0.46458613503292784)
w2 ( 0.09457677309031949 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.08699757345722486) - present_state_Q (-0.08699757345722486)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.028261155261721366 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17452595048479286) - present_state_Q ( 0.17403704244530113)) * f1( 0.4937812158449513)
w2 ( 0.16070821520365106 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17452595048479286) - present_state_Q (0.17403704244530113)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.021233519131748684 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20821938906464255) - present_state_Q ( 0.2086056848635567)) * f1( 0.5575082289900606)
w2 ( 0.05417416568879993 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.20821938906464255) - present_state_Q (0.2086056848635567)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01776769534907839 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08107989485821361) - present_state_Q ( 0.07049606603312877)) * f1( 0.25185490440890124)
w2 ( 0.0734398349721769 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.08107989485821361) - present_state_Q (0.07049606603312877)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.015577163060207604 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1137226204931017) - present_state_Q ( 0.11352599500170311)) * f1( 0.2238748963008456)
w2 ( 0.08909523769979404 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1137226204931017) - present_state_Q (0.11352599500170311)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.014487953247148138 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1572996358572354) - present_state_Q ( 0.1574596862904532)) * f1( 0.18692373944612667)
w2 ( 0.09958388761294269 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1572996358572354) - present_state_Q (0.1574596862904532)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.013922572752118816 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1772282085677046) - present_state_Q ( 0.1772282085677046)) * f1( 0.13961869569053348)
w2 ( 0.10687291782497454 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1772282085677046) - present_state_Q (0.1772282085677046)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.007501803701976086 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01654534602618661) - present_state_Q ( 0.01654534602618661)) * f1( 0.34686387529010093)
w2 ( 0.11057510159650319 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01654534602618661) - present_state_Q (0.01654534602618661)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.002058817375956779 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01987290121951812) - present_state_Q ( 0.01987290121951812)) * f1( 0.29887733521898385)
w2 ( 0.11421738937455186 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01987290121951812) - present_state_Q (0.01987290121951812)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0027434399267236348 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02229400450827326) - present_state_Q ( 0.02229400450827326)) * f1( 0.2668878614752139)
w2 ( 0.11781609729340295 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02229400450827326) - present_state_Q (0.02229400450827326)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.00870267849494364 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024481865917120312) - present_state_Q ( 0.024481865917120312)) * f1( 0.33485204085981907)
w2 ( 0.12137542370689479 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.024481865917120312) - present_state_Q (0.024481865917120312)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.015507416739552384 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02774049675215959) - present_state_Q ( 0.02765678728612862)) * f1( 0.3885818080852313)
w2 ( 0.12487776895467653 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02774049675215959) - present_state_Q (0.02765678728612862)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02334971900041139 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032082238925886505) - present_state_Q ( 0.032082238925886505)) * f1( 0.45827653014736286)
w2 ( 0.12830028865401058 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.032082238925886505) - present_state_Q (0.032082238925886505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03171400074214884 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03740137563182358) - present_state_Q ( 0.03740137563182358)) * f1( 0.5028462184411981)
w2 ( 0.13162706389263776 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03740137563182358) - present_state_Q (0.03740137563182358)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04055210059466013 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04377822800089403) - present_state_Q ( 0.04377822800089403)) * f1( 0.5503189384482535)
w2 ( 0.13483905578862165 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.04377822800089403) - present_state_Q (0.04377822800089403)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.048939013753725835 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049433176652038745) - present_state_Q ( 0.04874134889050931)) * f1( 0.5369274936068811)
w2 ( 0.13796309516411553 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.049433176652038745) - present_state_Q (0.04874134889050931)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.057409059430618144 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.055161877323364514) - present_state_Q ( 0.055161877323364514)) * f1( 0.5633390658274656)
w2 ( 0.14097018137229497 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.055161877323364514) - present_state_Q (0.055161877323364514)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06629906196726257 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06404446926461065) - present_state_Q ( 0.06404446926461065)) * f1( 0.6244734427930975)
w2 ( 0.14381738092553198 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06404446926461065) - present_state_Q (0.06404446926461065)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07548557989837858 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07460802033250227) - present_state_Q ( 0.07460802033250227)) * f1( 0.6914810373943628)
w2 ( 0.14647443655954695 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07460802033250227) - present_state_Q (0.07460802033250227)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08428885294306265 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0817104044178357) - present_state_Q ( 0.081936661999917)) * f1( 0.6973752438396298)
w2 ( 0.14899912412838429 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0817104044178357) - present_state_Q (0.081936661999917)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07182737132254255 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0883198738605232) - present_state_Q ( 0.0883198738605232)) * f1( 0.6942798127099534)
w2 ( 0.14540936639889487 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.0883198738605232) - present_state_Q (0.0883198738605232)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.053139234332072606 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07880946197465505) - present_state_Q ( 0.0786552530618347)) * f1( 0.6901739388379573)
w2 ( 0.1399938802616075 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07880946197465505) - present_state_Q (0.0786552530618347)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.021678672759364623 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06449697678178348) - present_state_Q ( 0.06449697678178348)) * f1( 0.6868409224976959)
w2 ( 0.13083293467953538 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06449697678178348) - present_state_Q (0.06449697678178348)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0296305176721735 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.041256381506798584) - present_state_Q ( 0.041256381506798584)) * f1( 0.6960663477137041)
w2 ( 0.11609031981241301 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.041256381506798584) - present_state_Q (0.041256381506798584)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07254316269243208 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00517346238307544) - present_state_Q ( 0.00517346238307544)) * f1( 0.6089870510886533)
w2 ( 0.10199719748951765 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.00517346238307544) - present_state_Q (0.00517346238307544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1095762804849304 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018906490466636318) - present_state_Q ( -0.018937046603318373)) * f1( 0.5422493952738237)
w2 ( 0.08833812544065074 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.018906490466636318) - present_state_Q (-0.018937046603318373)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12686017476510658 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03354541146223456) - present_state_Q ( -0.03354541146223456)) * f1( 0.46737337974716014)
w2 ( 0.08094194284697097 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03354541146223456) - present_state_Q (-0.03354541146223456)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12967122598592384 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03802990693569579) - present_state_Q ( -0.03802990693569579)) * f1( 0.42738625896961124)
w2 ( 0.07962648117181349 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.03802990693569579) - present_state_Q (-0.03802990693569579)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1286795029373272 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030667260043242748) - present_state_Q ( -0.030667260043242748)) * f1( 0.3593129927117616)
w2 ( 0.08017849185259186 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.030667260043242748) - present_state_Q (-0.030667260043242748)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12463353651883352 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02469499819853701) - present_state_Q ( -0.026082334753739604)) * f1( 0.327309572720151)
w2 ( 0.08265074855126958 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.02469499819853701) - present_state_Q (-0.026082334753739604)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12427585632589938 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015475883423902975) - present_state_Q ( -0.015475883423902975)) * f1( 0.25680113096462137)
w2 ( 0.08292931445289983 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( -0.015475883423902975) - present_state_Q (-0.015475883423902975)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12201990700294452 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009287462613477543) - present_state_Q ( -0.009287462613477543)) * f1( 0.20819269541951607)
w2 ( 0.08509648877994243 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.009287462613477543) - present_state_Q (-0.009287462613477543)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11851617121778481 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0015183335277122213) - present_state_Q ( -0.003957788704967522)) * f1( 0.1719152798604395)
w2 ( 0.08917260788698636 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0015183335277122213) - present_state_Q (-0.003957788704967522)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11578365465851231 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015299014770073435) - present_state_Q ( 0.0015299014770073435)) * f1( 0.13757295677759138)
w2 ( 0.09314506966040023 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0015299014770073435) - present_state_Q (0.0015299014770073435)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11308468169252071 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002804459303211951) - present_state_Q ( 0.002804459303211951)) * f1( 0.13667347671431176)
w2 ( 0.09709458939294241 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.002804459303211951) - present_state_Q (0.002804459303211951)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11038928738534168 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.003905864058347225) - present_state_Q ( 0.003905864058347225)) * f1( 0.13718085940606467)
w2 ( 0.10102428383989216 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.003905864058347225) - present_state_Q (0.003905864058347225)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10904258092708917 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004684330657123328) - present_state_Q ( 0.004684330657123328)) * f1( 0.14059811851739554)
w2 ( 0.10293996588806394 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.004684330657123328) - present_state_Q (0.004684330657123328)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10910652084681458 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00495566658755606) - present_state_Q ( 0.00495566658755606)) * f1( 0.143359836654171)
w2 ( 0.10285076388948793 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.00495566658755606) - present_state_Q (0.00495566658755606)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11052598971729823 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005858517110546484) - present_state_Q ( 0.005858517110546484)) * f1( 0.1348373640105913)
w2 ( 0.10074531058149809 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.005858517110546484) - present_state_Q (0.005858517110546484)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.03350031965321713 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13969337792456) - present_state_Q ( 0.13877668721730724)) * f1( 0.5501735525483342)
w2 ( 0.17574653002990573 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.13969337792456) - present_state_Q (0.13877668721730724)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.025992091138875725 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12738701562423138) - present_state_Q ( 0.12738701562423138)) * f1( 0.6548921871012974)
w2 ( 0.16886763118619724 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.12738701562423138) - present_state_Q (0.12738701562423138)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03230404960271178 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11897651929295254) - present_state_Q ( 0.11897651929295254)) * f1( 0.6792812662474327)
w2 ( 0.1744428991443778 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11897651929295254) - present_state_Q (0.11897651929295254)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.023659385025481688 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12876317977968282) - present_state_Q ( 0.12876317977968282)) * f1( 0.7459572589014738)
w2 ( 0.16748968743627493 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.12876317977968282) - present_state_Q (0.12876317977968282)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.029781219608351162 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1161433141826252) - present_state_Q ( 0.1162082226116001)) * f1( 0.6641935169874599)
w2 ( 0.11921405396467466 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.1161433141826252) - present_state_Q (0.1162082226116001)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0769750267517415 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052729148725046165) - present_state_Q ( 0.05272468900248543)) * f1( 0.6313960147906935)
w2 ( 0.07436694751687581 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.052729148725046165) - present_state_Q (0.05272468900248543)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11688597544571615 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0007761138625260663) - present_state_Q ( 0.0007761138625260663)) * f1( 0.5695880404043833)
w2 ( 0.0323250373682994 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.0007761138625260663) - present_state_Q (0.0007761138625260663)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10415908641038511 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.042939812151538165) - present_state_Q ( -0.042939812151538165)) * f1( 0.5332960976269319)
w2 ( 0.046643787224482466 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.042939812151538165) - present_state_Q (-0.042939812151538165)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09576478153463416 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03647262256540058) - present_state_Q ( -0.03738914434512874)) * f1( 0.627649674481976)
w2 ( 0.05466830014979779 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.03647262256540058) - present_state_Q (-0.03738914434512874)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11758862939515924 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010453536706820726) - present_state_Q ( -0.009664970974953828)) * f1( 0.5576122060642892)
w2 ( 0.02335786953413952 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.010453536706820726) - present_state_Q (-0.009664970974953828)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.12146002171999082 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027111042527242314) - present_state_Q ( -0.029366471309659005)) * f1( 0.5278358033145802)
w2 ( 0.013089620922110388 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.027111042527242314) - present_state_Q (-0.029366471309659005)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10995509951865666 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03879612461568653) - present_state_Q ( -0.04068460771266348)) * f1( 0.4858395064316517)
w2 ( 0.04624232025726367 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03879612461568653) - present_state_Q (-0.04068460771266348)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1021901494596884 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020030193814714548) - present_state_Q ( 0.010829908492074262)) * f1( 0.40617375648925047)
w2 ( 0.06918309356399134 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.020030193814714548) - present_state_Q (0.010829908492074262)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09618170789154502 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05663377140779358) - present_state_Q ( 0.044840778121278076)) * f1( 0.3736067943669289)
w2 ( 0.0884818054463315 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05663377140779358) - present_state_Q (0.044840778121278076)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09223251371362408 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09267619317851016) - present_state_Q ( 0.09159509441473207)) * f1( 0.3356088586670813)
w2 ( 0.10495595893276816 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.09267619317851016) - present_state_Q (0.09159509441473207)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08925080746770311 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12033852402628906) - present_state_Q ( 0.10114632219496997)) * f1( 0.2688946394907635)
w2 ( 0.11826246255768723 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12033852402628906) - present_state_Q (0.10114632219496997)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.08768568388254991 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14537127564308777) - present_state_Q ( 0.14537127564308777)) * f1( 0.22628559349429625)
w2 ( 0.12794568182665816 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14537127564308777) - present_state_Q (0.14537127564308777)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08671141916429966 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16307552149065657) - present_state_Q ( 0.16307552149065657)) * f1( 0.18302227177883298)
w2 ( 0.13539816611883543 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.16307552149065657) - present_state_Q (0.16307552149065657)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08604471346679059 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17751438425370145) - present_state_Q ( 0.17578266727866323)) * f1( 0.1588575694004747)
w2 ( 0.1412737940793744 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17751438425370145) - present_state_Q (0.17578266727866323)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07272940266041282 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0235431571848049) - present_state_Q ( -0.0235431571848049)) * f1( 0.6019883606291683)
w2 ( 0.1456975709087009 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0235431571848049) - present_state_Q (-0.0235431571848049)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06119298820703288 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.010857995019480458) - present_state_Q ( -0.010857995019480458)) * f1( 0.5499496453721266)
w2 ( 0.14989301481905154 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.010857995019480458) - present_state_Q (-0.010857995019480458)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05120937839312959 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0004992680885715431) - present_state_Q ( -0.0004992680885715431)) * f1( 0.49806149275251516)
w2 ( 0.15390200164464582 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.0004992680885715431) - present_state_Q (-0.0004992680885715431)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04277024418095152 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00833066402231751) - present_state_Q ( 0.00833066402231751)) * f1( 0.4383911113754816)
w2 ( 0.1577520496922441 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.00833066402231751) - present_state_Q (0.00833066402231751)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03468360926400676 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013167779528683247) - present_state_Q ( 0.013167779528683247)) * f1( 0.42979951977811254)
w2 ( 0.1615150296607278 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.013167779528683247) - present_state_Q (0.013167779528683247)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.027941739531656296 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01948754885021284) - present_state_Q ( 0.01948754885021284)) * f1( 0.3694960632379193)
w2 ( 0.165164253781424 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.01948754885021284) - present_state_Q (0.01948754885021284)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.022560112431573714 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02457922344630746) - present_state_Q ( 0.02457922344630746)) * f1( 0.302544775367328)
w2 ( 0.16872182775939046 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02457922344630746) - present_state_Q (0.02457922344630746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.018166327504048577 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028071550210516997) - present_state_Q ( 0.028071550210516997)) * f1( 0.25145332757392547)
w2 ( 0.17221653985560115 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.028071550210516997) - present_state_Q (0.028071550210516997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.005828595336303087 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01069211792506888) - present_state_Q ( -0.01069211792506888)) * f1( 0.5885679382740412)
w2 ( 0.17221653985560115 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.01069211792506888) - present_state_Q (-0.01069211792506888)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( 0.0049931544239719475 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030782418293760203) - present_state_Q ( 0.030782418293760203)) * f1( 0.6280912408789777)
w2 ( 0.17566245632631347 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.030782418293760203) - present_state_Q (0.030782418293760203)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01717270172198039 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03891629045627462) - present_state_Q ( 0.03881653338126279)) * f1( 0.7378185818393975)
w2 ( 0.17896395823960076 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.03891629045627462) - present_state_Q (0.03881653338126279)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02930700014230133 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.049172274127595395) - present_state_Q ( 0.049172274127595395)) * f1( 0.7791134264301596)
w2 ( 0.18207885730530404 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.049172274127595395) - present_state_Q (0.049172274127595395)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.041404422322838516 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06082311844302177) - present_state_Q ( 0.06082311844302177)) * f1( 0.8328162849643463)
w2 ( 0.18498404117332964 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06082311844302177) - present_state_Q (0.06082311844302177)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05295217401475773 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07327065287292531) - present_state_Q ( 0.07244454304823014)) * f1( 0.8561340268720856)
w2 ( 0.18768169161811088 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.07327065287292531) - present_state_Q (0.07244454304823014)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06413681137547736 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0857583807543047) - present_state_Q ( 0.0857583807543047)) * f1( 0.9106716263857849)
w2 ( 0.1901380407645334 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0857583807543047) - present_state_Q (0.0857583807543047)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0748176069714995 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10062160384965586) - present_state_Q ( 0.10062160384965586)) * f1( 0.9759449270139727)
w2 ( 0.1923268518952396 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.10062160384965586) - present_state_Q (0.10062160384965586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.08467631476041171 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11458944543678926) - present_state_Q ( 0.11468378552868469)) * f1( 1.0187229749097813)
w2 ( 0.1942623550755395 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11458944543678926) - present_state_Q (0.11468378552868469)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.09357776887713223 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12496284357714985) - present_state_Q ( 0.1248868677419406)) * f1( 1.0160385105358403)
w2 ( 0.19601454340785499 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12496284357714985) - present_state_Q (0.1248868677419406)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10161975280755717 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13443325684322038) - present_state_Q ( 0.1343494235422864)) * f1( 1.016764088334302)
w2 ( 0.1975964214506957 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.13443325684322038) - present_state_Q (0.1343494235422864)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10888222751728706 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14304545059269752) - present_state_Q ( 0.14295445181177777)) * f1( 1.0178647818354707)
w2 ( 0.19902342331564554 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.14304545059269752) - present_state_Q (0.14295445181177777)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11543902120897356 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1506759537593101) - present_state_Q ( 0.1506759537593101)) * f1( 1.0182678259275888)
w2 ( 0.20031125614797796 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1506759537593101) - present_state_Q (0.1506759537593101)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12134798812362078 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15772257719982746) - present_state_Q ( 0.15787135311557865)) * f1( 1.0205310184735459)
w2 ( 0.20146927424006603 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.15772257719982746) - present_state_Q (0.15787135311557865)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12770641787672887 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1630681989573535) - present_state_Q ( 0.12288606137481795)) * f1( 0.680622792383386)
w2 ( 0.20333768941048438 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1630681989573535) - present_state_Q (0.12288606137481795)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13241610056814296 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17104929825519996) - present_state_Q ( 0.17093348418464482)) * f1( 1.020042285018829)
w2 ( 0.20426111832330188 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17104929825519996) - present_state_Q (0.17093348418464482)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13666896158917646 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17588332960128755) - present_state_Q ( 0.17588332960128755)) * f1( 1.0197483943211159)
w2 ( 0.2050952183904787 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17588332960128755) - present_state_Q (0.17588332960128755)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13619225527271003 ) += alpha ( 0.1 ) * (reward ( 0 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057070760699664914) - present_state_Q ( 0.054399221631472545)) * f1( 0.0979020971389048)
w2 ( 0.20412137547924858 ) += alpha ( 0.1) * (reward ( 0) + discount_factor ( 0.1) * next_state_max_Q( 0.057070760699664914) - present_state_Q (0.054399221631472545)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11119699339058872 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29470760801361673) - present_state_Q ( 0.296017271702565)) * f1( 0.3749965152218186)
w2 ( 0.12413579417110417 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.29470760801361673) - present_state_Q (0.296017271702565)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.07716585751096242 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1923144150842654) - present_state_Q ( 0.19230587810823446)) * f1( 0.38978504527243646)
w2 ( 0.01936686177912718 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.1923144150842654) - present_state_Q (0.19230587810823446)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.042797761125799805 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05847727265957611) - present_state_Q ( 0.05847727265957611)) * f1( 0.45664027668736284)
w2 ( -0.07094868366810705 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( 0.05847727265957611) - present_state_Q (0.05847727265957611)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( 0.011181285972981077 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06335944077875055) - present_state_Q ( -0.06407058101555221)) * f1( 0.49226498844763)
w2 ( -0.1480205272355858 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.06335944077875055) - present_state_Q (-0.06407058101555221)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.01963809829381101 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14167301240582983) - present_state_Q ( -0.17127711785294703)) * f1( 0.5676909476328907)
w2 ( -0.21316734924210212 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.14167301240582983) - present_state_Q (-0.17127711785294703)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.047673510516633225 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22527782467670343) - present_state_Q ( -0.2679112945251239)) * f1( 0.6166826977548001)
w2 ( -0.2677213277952077 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.22527782467670343) - present_state_Q (-0.2679112945251239)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.07372816319804423 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.300777826750882) - present_state_Q ( -0.35432209230992356)) * f1( 0.6933934295470214)
w2 ( -0.3128120106390274 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.300777826750882) - present_state_Q (-0.35432209230992356)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.09038797466329884 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4232606915180262) - present_state_Q ( -0.48582309364583165)) * f1( 0.6494977858401805)
w2 ( -0.34872242720986335 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4232606915180262) - present_state_Q (-0.48582309364583165)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11004650891817092 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.40114259305437) - present_state_Q ( -0.40114259305437)) * f1( 0.5799462377575693)
w2 ( -0.38261959383497 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.40114259305437) - present_state_Q (-0.40114259305437)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12213421347367955 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.44142899073589825) - present_state_Q ( -0.5179529095028923)) * f1( 0.5344049300524206)
w2 ( -0.4097623925834537 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.44142899073589825) - present_state_Q (-0.5179529095028923)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.13531265971450845 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.46737759165724213) - present_state_Q ( -0.46737759165724213)) * f1( 0.4717367675700858)
w2 ( -0.43769840933430193 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.46737759165724213) - present_state_Q (-0.46737759165724213)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.14851124597503482 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31724485066105557) - present_state_Q ( -0.404784532527916)) * f1( 0.403700623250829)
w2 ( -0.4638536055373571 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.31724485066105557) - present_state_Q (-0.404784532527916)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.15940034465064554 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.330518363094981) - present_state_Q ( -0.42328908420245237)) * f1( 0.35153027927152863)
w2 ( -0.48863462570592076 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.330518363094981) - present_state_Q (-0.42328908420245237)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.16814112095603223 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3402424090612364) - present_state_Q ( -0.4379693342024205)) * f1( 0.2952417307555258)
w2 ( -0.512319018242217 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.3402424090612364) - present_state_Q (-0.4379693342024205)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17283037101298904 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4524704812421898) - present_state_Q ( -0.5534218418047695)) * f1( 0.2444543210420284)
w2 ( -0.531501538874162 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4524704812421898) - present_state_Q (-0.5534218418047695)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.17077390335225442 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4557223742376256) - present_state_Q ( -0.562022682012458)) * f1( 0.17659594757221353)
w2 ( -0.5198564944152925 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.4557223742376256) - present_state_Q (-0.562022682012458)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1677715409707126 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22489736651344258) - present_state_Q ( -0.22489736651344258)) * f1( 0.09928196530328796)
w2 ( -0.5077601892208086 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.22489736651344258) - present_state_Q (-0.22489736651344258)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16531599603068545 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21360693125366953) - present_state_Q ( -0.21360693125366953)) * f1( 0.06260212849317232)
w2 ( -0.4920703396956765 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21360693125366953) - present_state_Q (-0.21360693125366953)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16245059968410913 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11032476152285264) - present_state_Q ( -0.20873882946198793)) * f1( 0.07204804053871779)
w2 ( -0.4761620855632884 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.11032476152285264) - present_state_Q (-0.20873882946198793)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17579819320938334 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.33935025872554436) - present_state_Q ( -0.3408604346331182)) * f1( 0.33956897298262895)
w2 ( -0.4997465610376545 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.33935025872554436) - present_state_Q (-0.3408604346331182)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17271133711962616 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3478454175702257) - present_state_Q ( -0.3478454175702257)) * f1( 0.27302601961594614)
w2 ( -0.4929629084888623 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3478454175702257) - present_state_Q (-0.3478454175702257)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17850685169775934 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.42366438340081164) - present_state_Q ( -0.42601033856227744)) * f1( 0.1831959169494042)
w2 ( -0.5182713964710866 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.42366438340081164) - present_state_Q (-0.42601033856227744)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18277660935275247 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4396639411182215) - present_state_Q ( -0.4396639411182215)) * f1( 0.1403129555147859)
w2 ( -0.5426155927105746 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4396639411182215) - present_state_Q (-0.4396639411182215)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1847825392231982 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4463852846551267) - present_state_Q ( -0.4463852846551267)) * f1( 0.06725592804351874)
w2 ( -0.5664758522154054 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.4463852846551267) - present_state_Q (-0.4463852846551267)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18975625838736243 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17731376572266008) - present_state_Q ( -0.17731376572266008)) * f1( 0.5052695943850775)
w2 ( -0.4869704377802805 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.17731376572266008) - present_state_Q (-0.17731376572266008)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1941679133775682 ) += alpha ( 0.1 ) * (reward ( -0.7 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.6739559911293398) - present_state_Q ( -0.6739559911293398)) * f1( 0.47213971520303644)
w2 ( -0.4981831907383118 ) += alpha ( 0.1) * (reward ( -0.7) + discount_factor ( 0.1) * next_state_max_Q( -0.6739559911293398) - present_state_Q (-0.6739559911293398)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.0331243458552501 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4635788118682782) - present_state_Q ( 0.06163769801530028)) * f1( 0.5383623019846998)
w2 ( 0.9630832109902916 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.4635788118682782) - present_state_Q (0.06163769801530028)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0066556937787217 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24851154261751643) - present_state_Q ( 0.0394771843670442)) * f1( 0.1605228156329558)
w2 ( 0.9917074793978942 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.24851154261751643) - present_state_Q (0.0394771843670442)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1118497337073538 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(1.2559975898285642) - present_state_Q ( 1.2663083677723623)) * f1( 0.4522605762652576)
w2 ( 0.6497556614081805 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 1.2559975898285642) - present_state_Q (1.2663083677723623)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1484262521282702 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7114542770684613) - present_state_Q ( 0.6033636425456921)) * f1( 0.39236004873858193)
w2 ( 0.5006007470339652 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.7114542770684613) - present_state_Q (0.6033636425456921)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -1.1678758349266543 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.49273560715661296) - present_state_Q ( 0.48103798982091317)) * f1( 0.3657556191020515)
w2 ( 0.4048831497950198 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.49273560715661296) - present_state_Q (0.48103798982091317)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1690743043290217 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39138207270984954) - present_state_Q ( 0.3791723657133703)) * f1( 0.29936170735104073)
w2 ( 0.39767700127539046 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.39138207270984954) - present_state_Q (0.3791723657133703)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1713282126900546 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4541039031878408) - present_state_Q ( 0.44144781316254694)) * f1( 0.2346906335355888)
w2 ( 0.38039026516351315 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4541039031878408) - present_state_Q (0.44144781316254694)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1734684042543442 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4541871568075149) - present_state_Q ( 0.4541532888346092)) * f1( 0.1968271454251398)
w2 ( 0.36081804199581874 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4541871568075149) - present_state_Q (0.4541532888346092)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1753803287754296 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4838339461279929) - present_state_Q ( 0.4838339461279929)) * f1( 0.14115295210673562)
w2 ( 0.3364369427230839 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4838339461279929) - present_state_Q (0.4838339461279929)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -1.17534865387823 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.35630824334732625) - present_state_Q ( 0.3329345032464024)) * f1( 0.11747450011331301)
w2 ( 0.3368144276754501 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.35630824334732625) - present_state_Q (0.3329345032464024)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1873496858730435 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20407675909314202) - present_state_Q ( -0.20407675909314202)) * f1( 0.28825704529919266)
w2 ( 0.3201611910028032 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.20407675909314202) - present_state_Q (-0.20407675909314202)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.2021761481992244 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3474929135632745) - present_state_Q ( -0.33888977224071826)) * f1( 0.501131833462735)
w2 ( 0.2964924294735545 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.3474929135632745) - present_state_Q (-0.33888977224071826)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2238872315761062 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08064840352952884) - present_state_Q ( -0.1230524454356553)) * f1( 0.44763976352777457)
w2 ( 0.2285906941851328 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08064840352952884) - present_state_Q (-0.1230524454356553)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2415868249885442 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10863584305581386) - present_state_Q ( -0.1413880365704096)) * f1( 0.3770077802310195)
w2 ( 0.16286411750220875 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.10863584305581386) - present_state_Q (-0.1413880365704096)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.2571626527060407 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1956684365424935) - present_state_Q ( -0.19617537356863374)) * f1( 0.36788241658120213)
w2 ( 0.09512148228851025 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1956684365424935) - present_state_Q (-0.19617537356863374)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -1.2691901941741124 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2566135350848771) - present_state_Q ( -0.24442263389563573)) * f1( 0.31548583208667125)
w2 ( 0.03412328715045393 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2566135350848771) - present_state_Q (-0.24442263389563573)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -1.278404653742072 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2936000386668953) - present_state_Q ( -0.2867753812368045)) * f1( 0.2689688608094462)
w2 ( -0.02069025247032768 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2936000386668953) - present_state_Q (-0.2867753812368045)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -1.0396718802509404 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47809785967014207) - present_state_Q ( 0.4673890514087954)) * f1( 0.32382167420708463)
w2 ( 0.9101411381625517 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.47809785967014207) - present_state_Q (0.4673890514087954)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.2770501287311171 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2910401438078068) - present_state_Q ( -0.2951781943018723)) * f1( 0.20500065419992083)
w2 ( -0.010118383682953022 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.2910401438078068) - present_state_Q (-0.2951781943018723)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -1.2715135541697042 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19597565097084485) - present_state_Q ( -0.20002300444402607)) * f1( 0.14553639133380025)
w2 ( 0.04314117782561882 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.19597565097084485) - present_state_Q (-0.20002300444402607)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0604917526404176 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6390198703959838) - present_state_Q ( 0.6261778537541516)) * f1( 0.27312779137573795)
w2 ( 0.8339135514910964 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.6390198703959838) - present_state_Q (0.6261778537541516)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2681686196992974 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08196608204777164) - present_state_Q ( -0.07333784648264788)) * f1( 0.09160659273060386)
w2 ( 0.0796553016534059 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08196608204777164) - present_state_Q (-0.07333784648264788)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0688195857629297 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7721967349844474) - present_state_Q ( 0.7831109314783143)) * f1( 0.20517399571402273)
w2 ( 0.785206600533512 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7721967349844474) - present_state_Q (0.7831109314783143)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.075186512792529 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7688163018595032) - present_state_Q ( 0.7684628814200767)) * f1( 0.16259529815416804)
w2 ( 0.7382168503854168 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7688163018595032) - present_state_Q (0.7684628814200767)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.0776960629695396 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7595768842707835) - present_state_Q ( 0.6334127570935998)) * f1( 0.09747526782084955)
w2 ( 0.7124713435187647 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7595768842707835) - present_state_Q (0.6334127570935998)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.2318728711476092 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5579118777400766) - present_state_Q ( -0.5579118777400766)) * f1( 0.45249734866237645)
w2 ( 0.09569771545272727 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.5579118777400766) - present_state_Q (-0.5579118777400766)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.2044084630421361 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4566915756724656) - present_state_Q ( -0.4566915756724656)) * f1( 0.38626641588407423)
w2 ( 0.10991816381483166 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.4566915756724656) - present_state_Q (-0.4566915756724656)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1829940546461992 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.37984402331517797) - present_state_Q ( -0.37984402331517797)) * f1( 0.3336307145029472)
w2 ( 0.12275535623450487 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.37984402331517797) - present_state_Q (-0.37984402331517797)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1649063405385505 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31844564454910734) - present_state_Q ( -0.3319900899162816)) * f1( 0.30138880221998593)
w2 ( 0.13475826674373229 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.31844564454910734) - present_state_Q (-0.3319900899162816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.152184044143479 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2536054204933083) - present_state_Q ( -0.2536054204933083)) * f1( 0.2408408848666321)
w2 ( 0.14532316431261183 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2536054204933083) - present_state_Q (-0.2536054204933083)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1429429716269928 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19489872511701192) - present_state_Q ( -0.19489872511701192)) * f1( 0.19438158262816962)
w2 ( 0.15483134136471804 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.19489872511701192) - present_state_Q (-0.19489872511701192)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1365386955498789 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1171082590015288) - present_state_Q ( -0.13995697931208181)) * f1( 0.14954661066047256)
w2 ( 0.16339626443295663 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1171082590015288) - present_state_Q (-0.13995697931208181)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1325822218765187 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08636592231172713) - present_state_Q ( -0.08636592231172713)) * f1( 0.10474361820186172)
w2 ( 0.17095085103456772 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08636592231172713) - present_state_Q (-0.08636592231172713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1420289134043708 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.23868749867174938) - present_state_Q ( -0.2592185546847072)) * f1( 0.25906174335447946)
w2 ( 0.16365784713091838 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.23868749867174938) - present_state_Q (-0.2592185546847072)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0027653822932914 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3501322543419597) - present_state_Q ( 0.34586749142808515)) * f1( 0.0541325085719149)
w2 ( 0.9795658293602444 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.3501322543419597) - present_state_Q (0.34586749142808515)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0197539210157127 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3928392840711805) - present_state_Q ( 0.3928392840711805)) * f1( 0.20716071592881954)
w2 ( 0.9427866786601562 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.3928392840711805) - present_state_Q (0.3928392840711805)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0485889569767404 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1796035581816347) - present_state_Q ( 0.1796035581816347)) * f1( 0.3785898156978113)
w2 ( 0.8970880865183479 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1796035581816347) - present_state_Q (0.1796035581816347)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0752032945350385 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17528082053366245) - present_state_Q ( 0.16478882929253363)) * f1( 0.3561586455146688)
w2 ( 0.8522524416839978 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.17528082053366245) - present_state_Q (0.16478882929253363)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.098184641925072 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19137834763683192) - present_state_Q ( 0.19137834763683192)) * f1( 0.29759313331711484)
w2 ( 0.8059180109116089 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.19137834763683192) - present_state_Q (0.19137834763683192)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1187766051995234 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20099794746254518) - present_state_Q ( 0.18972073035073883)) * f1( 0.2675598118738527)
w2 ( 0.7597407547753398 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.20099794746254518) - present_state_Q (0.18972073035073883)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1208277412986207 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2185709129045921) - present_state_Q ( 0.2185709129045921)) * f1( 0.21208303682601254)
w2 ( 0.7539379254784918 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.2185709129045921) - present_state_Q (0.2185709129045921)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1198491060726814 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2674304492357181) - present_state_Q ( 0.2674304492357181)) * f1( 0.16499618918881281)
w2 ( 0.757496681219763 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2674304492357181) - present_state_Q (0.2674304492357181)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1199615178963263 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.346255358230089) - present_state_Q ( 0.346255358230089)) * f1( 0.09665824611083236)
w2 ( 0.7567988918753382 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.346255358230089) - present_state_Q (0.346255358230089)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1128525551051809 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02020375668757196) - present_state_Q ( 0.02020375668757196)) * f1( 0.2522549172878952)
w2 ( 0.7680715566345856 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.02020375668757196) - present_state_Q (0.02020375668757196)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.1083944534941619 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08513226554563955) - present_state_Q ( 0.08513226554563955)) * f1( 0.19957392925894246)
w2 ( 0.7770067950749425 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08513226554563955) - present_state_Q (0.08513226554563955)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.1057286207111499 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1405247674538408) - present_state_Q ( 0.1405247674538408)) * f1( 0.15362576927315266)
w2 ( 0.7839479034466043 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1405247674538408) - present_state_Q (0.1405247674538408)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0956903523288548 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09853898018042145) - present_state_Q ( -0.11629972181548603)) * f1( 0.24697678742291157)
w2 ( 0.7920768199225532 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09853898018042145) - present_state_Q (-0.11629972181548603)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0900279917645723 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.030864808545451178) - present_state_Q ( -0.030864808545451178)) * f1( 0.17274969349474772)
w2 ( 0.7986323864763714 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.030864808545451178) - present_state_Q (-0.030864808545451178)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.085558796869627 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017907363427788758) - present_state_Q ( -0.0017907363427788758)) * f1( 0.14817712467785704)
w2 ( 0.8046646197305414 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0017907363427788758) - present_state_Q (-0.0017907363427788758)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0824073388844777 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.033960384009829786) - present_state_Q ( 0.033960384009829786)) * f1( 0.11696514302350368)
w2 ( 0.8100533328183644 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.033960384009829786) - present_state_Q (0.033960384009829786)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0795437103636079 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04327531889851528) - present_state_Q ( 0.04327531889851528)) * f1( 0.1096956232646441)
w2 ( 0.8152743770781912 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04327531889851528) - present_state_Q (0.04327531889851528)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.077845723341806 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0820137085070618) - present_state_Q ( 0.0820137085070618)) * f1( 0.0750698337923533)
w2 ( 0.8197981303250641 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0820137085070618) - present_state_Q (0.0820137085070618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0773233123616406 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01781695021503835) - present_state_Q ( -0.01781695021503835)) * f1( 0.016530148822966797)
w2 ( 0.8197981303250641 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.01781695021503835) - present_state_Q (-0.01781695021503835)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0762620221376866 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.287848227995208) - present_state_Q ( 0.275857151312599)) * f1( 0.2005170819230582)
w2 ( 0.8229737906142794 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.287848227995208) - present_state_Q (0.275857151312599)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0811055812624204 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6237207541151525) - present_state_Q ( 0.6228241789678813)) * f1( 0.1859673643866557)
w2 ( 0.7969285802586429 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.6237207541151525) - present_state_Q (0.6228241789678813)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0848406182047963 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6516804089167128) - present_state_Q ( 0.6599444321045258)) * f1( 0.12670746551336734)
w2 ( 0.7674509411373575 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.6516804089167128) - present_state_Q (0.6599444321045258)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0881153117916613 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6432440989434168) - present_state_Q ( 0.6373197684149622)) * f1( 0.1199541854707998)
w2 ( 0.7401514052852954 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.6432440989434168) - present_state_Q (0.6373197684149622)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0778272854455613 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03754084218884324) - present_state_Q ( -0.038429484070279185)) * f1( 0.3074031240619481)
w2 ( 0.7535384212793512 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03754084218884324) - present_state_Q (-0.038429484070279185)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.071629201462616 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.044913640872365934) - present_state_Q ( 0.04448285389969919)) * f1( 0.2383800429637745)
w2 ( 0.7639387616868527 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.044913640872365934) - present_state_Q (0.04448285389969919)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.070439237459221 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26139751873731343) - present_state_Q ( 0.26139751873731343)) * f1( 0.18380027159204787)
w2 ( 0.7678232956750378 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26139751873731343) - present_state_Q (0.26139751873731343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0720999489820804 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4739883893942506) - present_state_Q ( 0.48598917688458154)) * f1( 0.11982880967621039)
w2 ( 0.7567360686394253 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4739883893942506) - present_state_Q (0.48598917688458154)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0558001269046553 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3044435872091524) - present_state_Q ( -0.3044435872091524)) * f1( 0.28396940742158455)
w2 ( 0.7567360686394253 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.3044435872091524) - present_state_Q (-0.3044435872091524)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0405025372310164 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2885646191011448) - present_state_Q ( -0.2885646191011448)) * f1( 0.27331368101569076)
w2 ( 0.7567360686394253 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2885646191011448) - present_state_Q (-0.2885646191011448)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.031785795963817 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09766858478047832) - present_state_Q ( -0.08827330983983589)) * f1( 0.23029306992888138)
w2 ( 0.7643061976666611 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09766858478047832) - present_state_Q (-0.08827330983983589)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0254040814516905 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03008984160306949) - present_state_Q ( -0.041605243787115526)) * f1( 0.18847563523472594)
w2 ( 0.7710781228591973 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03008984160306949) - present_state_Q (-0.041605243787115526)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.02210498904709 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01887225094687986) - present_state_Q ( 0.02985769746561566)) * f1( 0.12127699641117787)
w2 ( 0.7765187134117787 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.01887225094687986) - present_state_Q (0.02985769746561566)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0198516525186312 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06938718404979362) - present_state_Q ( 0.061475256400459505)) * f1( 0.09179926454460681)
w2 ( 0.7814279826518691 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06938718404979362) - present_state_Q (0.061475256400459505)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.018311843334119 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08579654734698469) - present_state_Q ( 0.08579654734698469)) * f1( 0.06911696324588874)
w2 ( 0.7858836447996234 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08579654734698469) - present_state_Q (0.08579654734698469)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0392659219395186 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11688093024053481) - present_state_Q ( -0.11688093024053481)) * f1( 0.42347969434240545)
w2 ( 0.7660913582882826 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.11688093024053481) - present_state_Q (-0.11688093024053481)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.069993645917683 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19659008428678526) - present_state_Q ( 0.20756998165576646)) * f1( 0.3899897960845932)
w2 ( 0.7030584804301155 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.19659008428678526) - present_state_Q (0.20756998165576646)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0924574409014394 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15166772127652534) - present_state_Q ( 0.032448706463589216)) * f1( 0.3639146674189096)
w2 ( 0.6660215643699593 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.15166772127652534) - present_state_Q (0.032448706463589216)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1126276251951461 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1652567709172832) - present_state_Q ( 0.05389270030685783)) * f1( 0.31646105868421504)
w2 ( 0.6277795429770515 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1652567709172832) - present_state_Q (0.05389270030685783)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1340319714481741 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20703139278173477) - present_state_Q ( 0.19442470535424872)) * f1( 0.2766414585233824)
w2 ( 0.5658818176909655 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.20703139278173477) - present_state_Q (0.19442470535424872)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1423279996239506 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2119315327218796) - present_state_Q ( 0.2119315327218796)) * f1( 0.2123166960834634)
w2 ( 0.5346227473349902 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2119315327218796) - present_state_Q (0.2119315327218796)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1416516474340168 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26381429744680646) - present_state_Q ( 0.27558818830225396)) * f1( 0.13315791052640932)
w2 ( 0.5386862066503842 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26381429744680646) - present_state_Q (0.27558818830225396)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.1389569502044803 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15577844558041187) - present_state_Q ( 0.14395640665065304)) * f1( 0.15701402239682558)
w2 ( 0.5489834929248275 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15577844558041187) - present_state_Q (0.14395640665065304)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.1366954292865101 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18619960812727432) - present_state_Q ( 0.16342249926144853)) * f1( 0.14571893736953917)
w2 ( 0.5582953406179042 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18619960812727432) - present_state_Q (0.16342249926144853)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.131285835234563 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0005950720671828724) - present_state_Q ( 0.010714279673545324)) * f1( 0.18703678320151707)
w2 ( 0.5698643891426937 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0005950720671828724) - present_state_Q (0.010714279673545324)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.1267822019611713 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.055924780460043316) - present_state_Q ( -0.03542842109714378)) * f1( 0.13206326312280342)
w2 ( 0.5766848071255566 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.055924780460043316) - present_state_Q (-0.03542842109714378)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.112388903360772 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21393629437499237) - present_state_Q ( -0.21393629437499237)) * f1( 0.2922244025748735)
w2 ( 0.5865356604243065 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.21393629437499237) - present_state_Q (-0.21393629437499237)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.1015259363055043 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15668992860768627) - present_state_Q ( -0.15668992860768627)) * f1( 0.2463140902113839)
w2 ( 0.5953560791392448 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15668992860768627) - present_state_Q (-0.15668992860768627)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0932050873544548 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11042423571622562) - present_state_Q ( -0.11042423571622562)) * f1( 0.208343211884595)
w2 ( 0.6033437153821369 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11042423571622562) - present_state_Q (-0.11042423571622562)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0889071765294933 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.036493115809040244) - present_state_Q ( -0.025372768137376378)) * f1( 0.133590222825639)
w2 ( 0.6097781845132664 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.036493115809040244) - present_state_Q (-0.025372768137376378)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.085314892982992 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0060926718970266275) - present_state_Q ( -0.0060926718970266275)) * f1( 0.11759341067783999)
w2 ( 0.6158878526074129 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0060926718970266275) - present_state_Q (-0.0060926718970266275)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0932967197788495 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05277014847480907) - present_state_Q ( 0.05277014847480907)) * f1( 0.17836758144549827)
w2 ( 0.5979881272623198 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05277014847480907) - present_state_Q (0.05277014847480907)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0981128911745959 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.32683392735627104) - present_state_Q ( 0.3269680638912297)) * f1( 0.24789250572883056)
w2 ( 0.5785596601467595 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.32683392735627104) - present_state_Q (0.3269680638912297)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0986600019070887 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47061285947438974) - present_state_Q ( 0.37684547533449153)) * f1( 0.18369166452139954)
w2 ( 0.5755812412080543 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.47061285947438974) - present_state_Q (0.37684547533449153)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.101256137202117 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5254441895388551) - present_state_Q ( 0.5136100435442306)) * f1( 0.1611849394699367)
w2 ( 0.5562533662572129 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.5254441895388551) - present_state_Q (0.5136100435442306)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.103247638308248 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5708597132989504) - present_state_Q ( 0.5587526862615746)) * f1( 0.09875209733076064)
w2 ( 0.5320533604654113 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.5708597132989504) - present_state_Q (0.5587526862615746)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -1.0875647261859724 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1328050026599499) - present_state_Q ( -0.2290511449074531)) * f1( 0.3040675595870228)
w2 ( 0.5423687733582404 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1328050026599499) - present_state_Q (-0.2290511449074531)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0792254352990862 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.047596071804705625) - present_state_Q ( -0.047596071804705625)) * f1( 0.24324398794703567)
w2 ( 0.5560822319432098 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.047596071804705625) - present_state_Q (-0.047596071804705625)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0724139585664396 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013241991122324326) - present_state_Q ( -0.013241991122324326)) * f1( 0.2183741007126056)
w2 ( 0.5685589436236135 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.013241991122324326) - present_state_Q (-0.013241991122324326)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0673308982303262 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028639512668538952) - present_state_Q ( 0.028639512668538952)) * f1( 0.18536131798082253)
w2 ( 0.579527921167546 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.028639512668538952) - present_state_Q (0.028639512668538952)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0363737320669495 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.32546139073035074) - present_state_Q ( -0.32546139073035074)) * f1( 0.5221178925123854)
w2 ( 0.6032445312338387 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.32546139073035074) - present_state_Q (-0.32546139073035074)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.012220941361511 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2414008064811436) - present_state_Q ( -0.24203257345906448)) * f1( 0.4663668819438748)
w2 ( 0.6239602309462767 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2414008064811436) - present_state_Q (-0.24203257345906448)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9984005751555163 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.040933049629344354) - present_state_Q ( -0.040933049629344354)) * f1( 0.41029499709667067)
w2 ( 0.6441706156262613 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.040933049629344354) - present_state_Q (-0.040933049629344354)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9895808082663836 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04599523830219393) - present_state_Q ( 0.04599523830219393)) * f1( 0.341052619105837)
w2 ( 0.6596868727579428 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04599523830219393) - present_state_Q (0.04599523830219393)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.982684280580932 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09780447065231973) - present_state_Q ( 0.08803775998978797)) * f1( 0.3110148874089002)
w2 ( 0.6729914339824694 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09780447065231973) - present_state_Q (0.08803775998978797)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.979289992658365 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17285393735217414) - present_state_Q ( 0.17285393735217414)) * f1( 0.2350102953725713)
w2 ( 0.681657321365452 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17285393735217414) - present_state_Q (0.17285393735217414)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9774003928145442 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22286725243699224) - present_state_Q ( 0.22286725243699224)) * f1( 0.19006335383558984)
w2 ( 0.6876224897338544 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22286725243699224) - present_state_Q (0.22286725243699224)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9924647281417792 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12556077635076945) - present_state_Q ( 0.12556077635076945)) * f1( 0.2936490711478589)
w2 ( 0.6568422078109128 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12556077635076945) - present_state_Q (0.12556077635076945)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0169779538055064 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22092182482489137) - present_state_Q ( 0.22092182482489137)) * f1( 0.30686424694816156)
w2 ( 0.5929358364235207 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.22092182482489137) - present_state_Q (0.22092182482489137)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0382053021971316 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21553267547447796) - present_state_Q ( 0.19537183666807184)) * f1( 0.27431944952869464)
w2 ( 0.5310303508938707 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.21553267547447796) - present_state_Q (0.19537183666807184)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.050563840928837 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20546576658965746) - present_state_Q ( 0.20546576658965746)) * f1( 0.21128625875943363)
w2 ( 0.4842368156994154 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.20546576658965746) - present_state_Q (0.20546576658965746)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -1.0689276588760548 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2505234484876249) - present_state_Q ( 0.2505234484876249)) * f1( 0.22246469762861537)
w2 ( 0.40168970533552917 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2505234484876249) - present_state_Q (0.2505234484876249)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0794567352961149 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2312475922655616) - present_state_Q ( 0.2098727577511815)) * f1( 0.17944801595464133)
w2 ( 0.3430149054830666 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2312475922655616) - present_state_Q (0.2098727577511815)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0572207193028091 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3271710756288054) - present_state_Q ( -0.37418550112536386)) * f1( 0.3466424256667572)
w2 ( 0.3430149054830666 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.3271710756288054) - present_state_Q (-0.37418550112536386)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0554447205127888 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17741993877895187) - present_state_Q ( -0.17741993877895187)) * f1( 0.2975971764719675)
w2 ( 0.3454020232791089 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.17741993877895187) - present_state_Q (-0.17741993877895187)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.055227703019188 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1209337773278) - present_state_Q ( -0.1209337773278)) * f1( 0.24548380564503863)
w2 ( 0.3457556392629097 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1209337773278) - present_state_Q (-0.1209337773278)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0525475656002454 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05308191812356655) - present_state_Q ( -0.05308191812356655)) * f1( 0.18136765484942008)
w2 ( 0.3516665883153581 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.05308191812356655) - present_state_Q (-0.05308191812356655)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0433256206635417 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08344973660723745) - present_state_Q ( -0.10440855399287396)) * f1( 0.2328400134384958)
w2 ( 0.36750913152864406 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08344973660723745) - present_state_Q (-0.10440855399287396)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0385690553079459 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012458117199102947) - present_state_Q ( -0.012458117199102947)) * f1( 0.1528398868506123)
w2 ( 0.3799576237478118 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.012458117199102947) - present_state_Q (-0.012458117199102947)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0353457626588012 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.029543972584193362) - present_state_Q ( 0.029543972584193362)) * f1( 0.11789209036141268)
w2 ( 0.39089404073478085 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.029543972584193362) - present_state_Q (0.029543972584193362)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0337163430784782 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0818280334302244) - present_state_Q ( 0.0818280334302244)) * f1( 0.07198521069163752)
w2 ( 0.3999482315312928 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0818280334302244) - present_state_Q (0.0818280334302244)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0327332093145025 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10952527399827452) - present_state_Q ( 0.10952527399827452)) * f1( 0.04880837857703504)
w2 ( 0.40800532166735487 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10952527399827452) - present_state_Q (0.10952527399827452)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0279275071244227 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.059225958038888554) - present_state_Q ( 0.05049854016281921)) * f1( 0.18814602947315634)
w2 ( 0.42333076500581907 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.059225958038888554) - present_state_Q (0.05049854016281921)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -1.0268820050055976 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3399819936779296) - present_state_Q ( 0.26578365703106394)) * f1( 0.1532667497297407)
w2 ( 0.430152219239492 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3399819936779296) - present_state_Q (0.26578365703106394)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0266338119939782 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39321337117543487) - present_state_Q ( 0.31683087518943176)) * f1( 0.11035478613674073)
w2 ( 0.4324012654323032 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.39321337117543487) - present_state_Q (0.31683087518943176)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -1.0229222649361283 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026059529264435555) - present_state_Q ( -0.02993027467509715)) * f1( 0.11339050633395718)
w2 ( 0.43894775186727625 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.026059529264435555) - present_state_Q (-0.02993027467509715)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0191751719624087 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.046101895092355016) - present_state_Q ( 0.03395851587647267)) * f1( 0.13844706457658412)
w2 ( 0.44977381881258677 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.046101895092355016) - present_state_Q (0.03395851587647267)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0178603081471498 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11248796607256908) - present_state_Q ( 0.11248796607256908)) * f1( 0.06615306505421074)
w2 ( 0.45772425203397427 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11248796607256908) - present_state_Q (0.11248796607256908)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -1.0091848302337842 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13387801033209526) - present_state_Q ( -0.12367867161854793)) * f1( 0.2114470132125718)
w2 ( 0.46593006944568105 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13387801033209526) - present_state_Q (-0.12367867161854793)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -1.0022578547324998 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06992729023798877) - present_state_Q ( -0.08954822245777269)) * f1( 0.18107112876892664)
w2 ( 0.4735811793143605 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06992729023798877) - present_state_Q (-0.08954822245777269)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9987670612487748 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009806470908645198) - present_state_Q ( -0.016255861719622372)) * f1( 0.11072210315788711)
w2 ( 0.47988668360693565 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.009806470908645198) - present_state_Q (-0.016255861719622372)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9806912656747888 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2481257505720964) - present_state_Q ( -0.24865736421863388)) * f1( 0.34506013895684406)
w2 ( 0.4903635793901641 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2481257505720964) - present_state_Q (-0.24865736421863388)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.965658947212577 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2029514209230952) - present_state_Q ( -0.2056593000900821)) * f1( 0.30971216589670003)
w2 ( 0.5000708625501196 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2029514209230952) - present_state_Q (-0.2056593000900821)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.955651528812542 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13115449827354736) - present_state_Q ( -0.13115449827354736)) * f1( 0.23938956031096825)
w2 ( 0.5084316435190435 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13115449827354736) - present_state_Q (-0.13115449827354736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9482533319197496 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07426066012350702) - present_state_Q ( -0.08537739275457212)) * f1( 0.1957446996300204)
w2 ( 0.5159906700538879 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07426066012350702) - present_state_Q (-0.08537739275457212)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.944024355627073 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02214573269349239) - present_state_Q ( -0.02214573269349239)) * f1( 0.1321839454552824)
w2 ( 0.5223892932423707 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02214573269349239) - present_state_Q (-0.02214573269349239)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.941185622106688 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011855757144306367) - present_state_Q ( 0.011855757144306367)) * f1( 0.09811410156112241)
w2 ( 0.5281758896137732 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.011855757144306367) - present_state_Q (0.011855757144306367)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9375933674233407 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005300174027546289) - present_state_Q ( -0.005300174027546289)) * f1( 0.11786766536232303)
w2 ( 0.5342712927462689 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.005300174027546289) - present_state_Q (-0.005300174027546289)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9272015107116762 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04858433948072749) - present_state_Q ( -0.1642512295653637)) * f1( 0.2891503902802337)
w2 ( 0.5414591486586148 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04858433948072749) - present_state_Q (-0.1642512295653637)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9367988464496905 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30249200480620414) - present_state_Q ( 0.3022451245326697)) * f1( 0.25799572300344464)
w2 ( 0.5042595562534098 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.30249200480620414) - present_state_Q (0.3022451245326697)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.9402928767856183 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.41760732793389155) - present_state_Q ( 0.4068860783322079)) * f1( 0.2115986691520007)
w2 ( 0.48444451478875156 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.41760732793389155) - present_state_Q (0.4068860783322079)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9416017301816203 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4167928138692307) - present_state_Q ( 0.41621318210857783)) * f1( 0.17560511167795498)
w2 ( 0.47550044670215297 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4167928138692307) - present_state_Q (0.41621318210857783)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.9442238582422458 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5086084764702264) - present_state_Q ( 0.5178210837196353)) * f1( 0.1570510513344694)
w2 ( 0.4521260136519872 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.5086084764702264) - present_state_Q (0.5178210837196353)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9461628991313045 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5290164862508479) - present_state_Q ( 0.5290164862508479)) * f1( 0.11010093841037305)
w2 ( 0.42746993638438036 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.5290164862508479) - present_state_Q (0.5290164862508479)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9478293274886038 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47322573611911706) - present_state_Q ( 0.47322573611911706)) * f1( 0.13235794273268833)
w2 ( 0.4098434936333716 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.47322573611911706) - present_state_Q (0.47322573611911706)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9453326646883102 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06569495789671571) - present_state_Q ( 0.06569495789671571)) * f1( 0.10364992589640475)
w2 ( 0.4194784751490898 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06569495789671571) - present_state_Q (0.06569495789671571)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9415198645246943 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.027188960610225008) - present_state_Q ( -0.027188960610225008)) * f1( 0.11750853407426601)
w2 ( 0.4259678764400739 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.027188960610225008) - present_state_Q (-0.027188960610225008)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.9502670902757829 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06906281371328024) - present_state_Q ( -0.08727475297170859)) * f1( 0.2736659238494274)
w2 ( 0.4131826153040891 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.06906281371328024) - present_state_Q (-0.08727475297170859)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9582381647702563 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04590496797742663) - present_state_Q ( -0.04590496797742663)) * f1( 0.2222301669289368)
w2 ( 0.3988351941512765 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.04590496797742663) - present_state_Q (-0.04590496797742663)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.9558032854191713 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06952487609946068) - present_state_Q ( 0.06952487609946068)) * f1( 0.17717541069970869)
w2 ( 0.4070808508419056 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06952487609946068) - present_state_Q (0.06952487609946068)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9538727524413032 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17558074486988287) - present_state_Q ( 0.18571241917906423)) * f1( 0.1464237083398219)
w2 ( 0.4176285032665395 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17558074486988287) - present_state_Q (0.18571241917906423)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.952957759317236 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24591513092391018) - present_state_Q ( 0.23580312408113757)) * f1( 0.10305324088617673)
w2 ( 0.42473157438743975 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.24591513092391018) - present_state_Q (0.23580312408113757)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.9465539951425878 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021304510866974463) - present_state_Q ( -0.021304510866974463)) * f1( 0.20063548331768352)
w2 ( 0.4374985367786508 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.021304510866974463) - present_state_Q (-0.021304510866974463)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.928287303266657 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13745269827823142) - present_state_Q ( -0.14161007363045136)) * f1( 0.42692672343194465)
w2 ( 0.4631704250068085 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13745269827823142) - present_state_Q (-0.14161007363045136)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.914523659849834 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07217916821148535) - present_state_Q ( -0.07217916821148535)) * f1( 0.37712615693829776)
w2 ( 0.4850681000902287 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07217916821148535) - present_state_Q (-0.07217916821148535)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.9042520652266407 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011411996869822805) - present_state_Q ( -0.011565166383375347)) * f1( 0.33088922651514663)
w2 ( 0.5036935380920123 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.011411996869822805) - present_state_Q (-0.011565166383375347)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8970796454487296 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03957441069399509) - present_state_Q ( 0.04841540152122287)) * f1( 0.2806747488825168)
w2 ( 0.519026060464903 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03957441069399509) - present_state_Q (0.04841540152122287)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.891938541125709 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09701683593886298) - present_state_Q ( 0.09580173957864224)) * f1( 0.24035089614863236)
w2 ( 0.5318600571058176 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09701683593886298) - present_state_Q (0.09580173957864224)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8894289231283494 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1491968361136132) - present_state_Q ( 0.16738947762834233)) * f1( 0.17010875709402054)
w2 ( 0.5407118694647988 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1491968361136132) - present_state_Q (0.16738947762834233)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.888164022391506 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20769847457376261) - present_state_Q ( 0.2165148528616691)) * f1( 0.12132759123422154)
w2 ( 0.5469671691405412 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20769847457376261) - present_state_Q (0.2165148528616691)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8877291610928161 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2653150703443002) - present_state_Q ( 0.2652031791939899)) * f1( 0.07090708551868623)
w2 ( 0.5506468688109676 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2653150703443002) - present_state_Q (0.2652031791939899)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8876977666834216 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.314201954268473) - present_state_Q ( 0.314201954268473)) * f1( 0.018233226672628354)
w2 ( 0.5516799632804701 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.314201954268473) - present_state_Q (0.314201954268473)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8890023304658166 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17107493221211803) - present_state_Q ( -0.1888276550482448)) * f1( 0.4613052501983731)
w2 ( 0.5505487697535514 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17107493221211803) - present_state_Q (-0.1888276550482448)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.8965367069414949 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21149924216770571) - present_state_Q ( 0.20163931841950095)) * f1( 0.2686153783851328)
w2 ( 0.5281096182173329 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.21149924216770571) - present_state_Q (0.20163931841950095)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.8945884565993392 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23164138608997803) - present_state_Q ( 0.23164138608997803)) * f1( 0.2128706019577873)
w2 ( 0.5354314384188545 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23164138608997803) - present_state_Q (0.23164138608997803)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.8936326524692211 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27082508452668785) - present_state_Q ( 0.2722906628028905)) * f1( 0.1744427695003062)
w2 ( 0.5398147860708368 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.27082508452668785) - present_state_Q (0.2722906628028905)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.8920496183923962 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30566733097006143) - present_state_Q ( 0.20824184576720756)) * f1( 0.12941226527001568)
w2 ( 0.5471542793106247 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.30566733097006143) - present_state_Q (0.20824184576720756)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.8931911524574716 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.42950374425584537) - present_state_Q ( 0.42950374425584537)) * f1( 0.13188788227587916)
w2 ( 0.5384989423275985 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.42950374425584537) - present_state_Q (0.42950374425584537)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.8816422719126168 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13633454555922736) - present_state_Q ( -0.13633454555922736)) * f1( 0.2732162464365281)
w2 ( 0.5469529641476646 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13633454555922736) - present_state_Q (-0.13633454555922736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8541625804182899 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.30941985585868415) - present_state_Q ( -0.30941985585868415)) * f1( 0.4750344465444678)
w2 ( 0.5585225215531209 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.30941985585868415) - present_state_Q (-0.30941985585868415)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8402751295999745 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15706163811069696) - present_state_Q ( -0.15706163811069696)) * f1( 0.3146545500620085)
w2 ( 0.5673496310391134 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15706163811069696) - present_state_Q (-0.15706163811069696)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.8141123338011163 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2653745223560216) - present_state_Q ( -0.28217950938819425)) * f1( 0.4708570105893432)
w2 ( 0.5784624721821653 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2653745223560216) - present_state_Q (-0.28217950938819425)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7939228040266845 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21647479991492172) - present_state_Q ( -0.21647479991492172)) * f1( 0.4080116226718433)
w2 ( 0.5883590185806339 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.21647479991492172) - present_state_Q (-0.21647479991492172)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7808868527660497 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16455775707677822) - present_state_Q ( -0.17278058154009157)) * f1( 0.3658446183723122)
w2 ( 0.5954855146972822 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.16455775707677822) - present_state_Q (-0.17278058154009157)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7689413405782864 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14694324143139578) - present_state_Q ( -0.1550067289056222)) * f1( 0.3510160670193777)
w2 ( 0.6022917627925318 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.14694324143139578) - present_state_Q (-0.1550067289056222)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7567735795662545 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11264468002962269) - present_state_Q ( -0.11264468002962269)) * f1( 0.3031480039983579)
w2 ( 0.610319367033065 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11264468002962269) - present_state_Q (-0.11264468002962269)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7482908532416862 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05950604338954313) - present_state_Q ( -0.05950604338954313)) * f1( 0.23992634216990388)
w2 ( 0.6173904758140768 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05950604338954313) - present_state_Q (-0.05950604338954313)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7421054938561481 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.021468261197146385) - present_state_Q ( -0.021468261197146385)) * f1( 0.1937032314801613)
w2 ( 0.6237769045156254 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.021468261197146385) - present_state_Q (-0.021468261197146385)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7396888768078977 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02415212176091104) - present_state_Q ( 0.02415212176091104)) * f1( 0.13556463329688714)
w2 ( 0.627342166323929 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02415212176091104) - present_state_Q (0.02415212176091104)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7385197393142788 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026689596326715952) - present_state_Q ( -0.026689596326715952)) * f1( 0.0360821923426698)
w2 ( 0.627342166323929 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.026689596326715952) - present_state_Q (-0.026689596326715952)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7369149439655499 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07457664898057365) - present_state_Q ( 0.07457664898057365)) * f1( 0.06891052679440306)
w2 ( 0.6319997866422787 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07457664898057365) - present_state_Q (0.07457664898057365)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7587467513913828 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01994714184204449) - present_state_Q ( -0.026994510512921843)) * f1( 0.3796834729177557)
w2 ( 0.6089997784954274 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.01994714184204449) - present_state_Q (-0.026994510512921843)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7761915940827112 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36136073627517573) - present_state_Q ( 0.3528268964617741)) * f1( 0.3376263312678253)
w2 ( 0.5573306962120017 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.36136073627517573) - present_state_Q (0.3528268964617741)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7723412592508797 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3016640812761955) - present_state_Q ( 0.20571767477221253)) * f1( 0.3093912431262412)
w2 ( 0.5672865948804343 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3016640812761955) - present_state_Q (0.20571767477221253)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.7725906210735831 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47955105606591936) - present_state_Q ( 0.3571188537200002)) * f1( 0.2721177182276692)
w2 ( 0.5663702200690934 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.47955105606591936) - present_state_Q (0.3571188537200002)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7803430323021366 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7381321052759506) - present_state_Q ( 0.7447046702193751)) * f1( 0.20902102288890448)
w2 ( 0.5070275865184086 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7381321052759506) - present_state_Q (0.7447046702193751)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.7852871139027817 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6841685226706169) - present_state_Q ( 0.6920224963097876)) * f1( 0.15278106830523416)
w2 ( 0.4552506834715725 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.6841685226706169) - present_state_Q (0.6920224963097876)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.7872129608345854 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6359240351461265) - present_state_Q ( 0.5605758280768954)) * f1( 0.09776695354358117)
w2 ( 0.4276730040328529 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.6359240351461265) - present_state_Q (0.5605758280768954)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7755525954705136 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0773035433604573) - present_state_Q ( -0.0773035433604573)) * f1( 0.31550896305147125)
w2 ( 0.4424559315938294 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0773035433604573) - present_state_Q (-0.0773035433604573)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7687131169882062 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00011097703025816918) - present_state_Q ( 0.00011097703025816918)) * f1( 0.22805854385667934)
w2 ( 0.4544519364207401 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.00011097703025816918) - present_state_Q (0.00011097703025816918)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7640371898796913 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04431145945668233) - present_state_Q ( 0.043844214245076174)) * f1( 0.17943828103734066)
w2 ( 0.4648754136887638 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04431145945668233) - present_state_Q (0.043844214245076174)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7609178204982456 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08100323560343635) - present_state_Q ( 0.08100323560343635)) * f1( 0.13735840514333417)
w2 ( 0.47395929720704005 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08100323560343635) - present_state_Q (0.08100323560343635)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7593408443446059 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1254483888210408) - present_state_Q ( 0.1254483888210408)) * f1( 0.08428680250881718)
w2 ( 0.4814431552094826 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1254483888210408) - present_state_Q (0.1254483888210408)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7584465532676423 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06962312214638443) - present_state_Q ( 0.06788479943228892)) * f1( 0.03740590516255348)
w2 ( 0.4862247054651296 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06962312214638443) - present_state_Q (0.06788479943228892)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7818902148037983 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.001484650796811593) - present_state_Q ( -0.00909890542805325)) * f1( 0.39664459863524776)
w2 ( 0.4507617318860319 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.001484650796811593) - present_state_Q (-0.00909890542805325)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7954493864616646 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.012873393042629244) - present_state_Q ( 0.012873393042629244)) * f1( 0.3294371015419678)
w2 ( 0.4260665686617299 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.012873393042629244) - present_state_Q (0.012873393042629244)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7949609670888016 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1968767591967117) - present_state_Q ( 0.20232343820282842)) * f1( 0.2812789025511235)
w2 ( 0.4278029924334142 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.1968767591967117) - present_state_Q (0.20232343820282842)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7948461478691493 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33180693154722185) - present_state_Q ( 0.328249843913581)) * f1( 0.23285891341862308)
w2 ( 0.4283946943423511 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.33180693154722185) - present_state_Q (0.328249843913581)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7956339920302421 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.388935384319513) - present_state_Q ( 0.388935384319513)) * f1( 0.1574370703396919)
w2 ( 0.4223896728358437 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.388935384319513) - present_state_Q (0.388935384319513)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7963795461575244 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4004887500085703) - present_state_Q ( 0.391438867310122)) * f1( 0.14507768804390533)
w2 ( 0.4162228737587319 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4004887500085703) - present_state_Q (0.391438867310122)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7969949919849593 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4471194810576708) - present_state_Q ( 0.4550446909345293)) * f1( 0.05578088712886433)
w2 ( 0.4029829446192804 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4471194810576708) - present_state_Q (0.4550446909345293)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7831937093826216 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11277190260735145) - present_state_Q ( -0.11277190260735145)) * f1( 0.34374755576912563)
w2 ( 0.41904273311314505 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11277190260735145) - present_state_Q (-0.11277190260735145)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7730656298652315 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0566178946019866) - present_state_Q ( -0.05770390632268274)) * f1( 0.28769510897317796)
w2 ( 0.43312441778764443 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0566178946019866) - present_state_Q (-0.05770390632268274)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.76634251064368 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012017373276235144) - present_state_Q ( -0.00043894233395208593)) * f1( 0.2246752445575533)
w2 ( 0.44509390598789755 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.012017373276235144) - present_state_Q (-0.00043894233395208593)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7605778208663864 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.033961328594361156) - present_state_Q ( -0.039730417414150496)) * f1( 0.1680047717874723)
w2 ( 0.4519564369933693 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.033961328594361156) - present_state_Q (-0.039730417414150496)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7580788087022318 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07728188762492409) - present_state_Q ( 0.09248012197925247)) * f1( 0.1160991688102455)
w2 ( 0.4605663596646989 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07728188762492409) - present_state_Q (0.09248012197925247)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7566945941259744 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12756414869074742) - present_state_Q ( 0.12756414869074742)) * f1( 0.07474472907656325)
w2 ( 0.467974050311832 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12756414869074742) - present_state_Q (0.12756414869074742)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.746139422863957 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.046470322414532) - present_state_Q ( -0.046470322414532)) * f1( 0.3087902891775716)
w2 ( 0.48164698191875516 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.046470322414532) - present_state_Q (-0.046470322414532)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7362960776395059 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009188044660657368) - present_state_Q ( -0.09164250904904978)) * f1( 0.25192598014898565)
w2 ( 0.48946145601041485 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.009188044660657368) - present_state_Q (-0.09164250904904978)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7316785593987039 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.059266326879613) - present_state_Q ( 0.058420102762010345)) * f1( 0.1865614714158642)
w2 ( 0.49936171720745287 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.059266326879613) - present_state_Q (0.058420102762010345)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7289013569485296 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10210216531620812) - present_state_Q ( 0.10210216531620812)) * f1( 0.13345002434814549)
w2 ( 0.5076860392560694 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10210216531620812) - present_state_Q (0.10210216531620812)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7270232976806971 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14368240094741808) - present_state_Q ( 0.12916138685006362)) * f1( 0.10140333551002488)
w2 ( 0.5150943133858565 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14368240094741808) - present_state_Q (0.12916138685006362)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7264420246872819 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18184966205766628) - present_state_Q ( 0.17626123807539992)) * f1( 0.0409567167571297)
w2 ( 0.5207712625110712 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18184966205766628) - present_state_Q (0.17626123807539992)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7421751944225037 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.4107737287560387) - present_state_Q ( -0.4251465533857378)) * f1( 0.7286208505294073)
w2 ( 0.5164526461212738 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.4107737287560387) - present_state_Q (-0.4251465533857378)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7747578099901163 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025323239630739736) - present_state_Q ( 0.024135887782291254)) * f1( 0.5241703468915238)
w2 ( 0.46672436101573644 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.025323239630739736) - present_state_Q (0.024135887782291254)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.7701042435690846 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019741761170868066) - present_state_Q ( 0.00429071413331128)) * f1( 0.47639245441615674)
w2 ( 0.47453903797443847 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.019741761170868066) - present_state_Q (0.00429071413331128)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.76430231188057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17020507560920095) - present_state_Q ( 0.17020507560920095)) * f1( 0.39518541146428093)
w2 ( 0.48922058116961037 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17020507560920095) - present_state_Q (0.17020507560920095)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7612643343585326 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23274273988408428) - present_state_Q ( 0.23274273988408428)) * f1( 0.33557119650006156)
w2 ( 0.49827373458004276 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23274273988408428) - present_state_Q (0.23274273988408428)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7599564673655507 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28214756784223183) - present_state_Q ( 0.28214756784223183)) * f1( 0.28390423271297244)
w2 ( 0.5028804534742419 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28214756784223183) - present_state_Q (0.28214756784223183)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7600704289563114 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3305634619678183) - present_state_Q ( 0.3383191790367631)) * f1( 0.21654039606761097)
w2 ( 0.5023541701902438 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3305634619678183) - present_state_Q (0.3383191790367631)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7607509936099227 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.380500909838328) - present_state_Q ( 0.380500909838328)) * f1( 0.16031838065222218)
w2 ( 0.4981090883047942 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.380500909838328) - present_state_Q (0.380500909838328)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7605092241550072 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.40193894962917803) - present_state_Q ( 0.317487010673927)) * f1( 0.10647407712942338)
w2 ( 0.4999256390479135 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.40193894962917803) - present_state_Q (0.317487010673927)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.7176513516458763 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.450234271262193) - present_state_Q ( -0.45748294096420833)) * f1( 0.6015481817101064)
w2 ( 0.4999256390479135 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.450234271262193) - present_state_Q (-0.45748294096420833)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6641479293844219 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.5074319356132209) - present_state_Q ( -0.5074319356132209)) * f1( 0.7070730577591279)
w2 ( 0.4999256390479135 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.5074319356132209) - present_state_Q (-0.5074319356132209)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.6570216581667098 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.3659315137607345) - present_state_Q ( -0.3659315137607345)) * f1( 0.5509789273903256)
w2 ( 0.4999256390479135 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.3659315137607345) - present_state_Q (-0.3659315137607345)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7120794710300344 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06074681156881889) - present_state_Q ( -0.07520877343874305)) * f1( 0.5336579076799285)
w2 ( 0.5369193080479828 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06074681156881889) - present_state_Q (-0.07520877343874305)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.7001764063563227 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18502224197974598) - present_state_Q ( 0.07763838037014942)) * f1( 0.4941822933881412)
w2 ( 0.5561884155542088 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18502224197974598) - present_state_Q (0.07763838037014942)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.7006651375163414 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.34513058183588247) - present_state_Q ( 0.34513058183588247)) * f1( 0.460306165565297)
w2 ( 0.5549143127159335 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.34513058183588247) - present_state_Q (0.34513058183588247)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7019176086002632 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3815888430590669) - present_state_Q ( 0.3675755526884862)) * f1( 0.4257691821632505)
w2 ( 0.551384312510024 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3815888430590669) - present_state_Q (0.3675755526884862)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7042582746016219 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39614022845244967) - present_state_Q ( 0.4031745534568934)) * f1( 0.3682577818080378)
w2 ( 0.5437570488366261 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.39614022845244967) - present_state_Q (0.4031745534568934)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7066367562152267 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.426498484597595) - present_state_Q ( 0.4124179182400134)) * f1( 0.34091262967375174)
w2 ( 0.5353848804629957 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.426498484597595) - present_state_Q (0.4124179182400134)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7068275283514615 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.44480193286757386) - present_state_Q ( 0.35182417256584625)) * f1( 0.25976671363701426)
w2 ( 0.5346504825350868 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.44480193286757386) - present_state_Q (0.35182417256584625)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.7098510486482346 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4845494512056807) - present_state_Q ( 0.4845494512056807)) * f1( 0.22216328812584354)
w2 ( 0.5183191418048734 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4845494512056807) - present_state_Q (0.4845494512056807)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7124088311144358 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.49736760949200104) - present_state_Q ( 0.5051744701286488)) * f1( 0.16455353592790647)
w2 ( 0.4996666167033395 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.49736760949200104) - present_state_Q (0.5051744701286488)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.7144302674005641 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5015847049072258) - present_state_Q ( 0.5088552419034317)) * f1( 0.1273772785755926)
w2 ( 0.4806230041338144 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.5015847049072258) - present_state_Q (0.5088552419034317)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.6940054129711751 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15437477546789896) - present_state_Q ( -0.14636911038549344)) * f1( 0.4739697175360068)
w2 ( 0.49786026944736256 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15437477546789896) - present_state_Q (-0.14636911038549344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6770182924662032 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11058043489572039) - present_state_Q ( -0.10223351805997705)) * f1( 0.43425832162989125)
w2 ( 0.5135072884301788 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11058043489572039) - present_state_Q (-0.10223351805997705)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6594391509841784 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08738885575507424) - present_state_Q ( -0.09931279101423715)) * f1( 0.4500848939787253)
w2 ( 0.529130244647728 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08738885575507424) - present_state_Q (-0.09931279101423715)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6455990540396583 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051713049785900495) - present_state_Q ( -0.051713049785900495)) * f1( 0.39937748198894923)
w2 ( 0.5429919144400204 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.051713049785900495) - present_state_Q (-0.051713049785900495)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6350372465495946 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00014530088855363443) - present_state_Q ( -0.005783506093950452)) * f1( 0.3453850659704672)
w2 ( 0.5552238358873326 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.00014530088855363443) - present_state_Q (-0.005783506093950452)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6270399049020378 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03373849743902688) - present_state_Q ( 0.03373849743902688)) * f1( 0.2965984088953694)
w2 ( 0.5660092499795276 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03373849743902688) - present_state_Q (0.03373849743902688)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6023024356755078 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13870902357563172) - present_state_Q ( -0.13870902357563172)) * f1( 0.5822798847618545)
w2 ( 0.5830027748282504 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13870902357563172) - present_state_Q (-0.13870902357563172)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5764288920816987 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09039192609517055) - present_state_Q ( -0.20049318350711115)) * f1( 0.5264692946445204)
w2 ( 0.5928318546462022 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09039192609517055) - present_state_Q (-0.20049318350711115)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.559027362981971 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.045552262498197965) - present_state_Q ( -0.051763933837588066)) * f1( 0.5011835452118922)
w2 ( 0.606720202949713 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.045552262498197965) - present_state_Q (-0.051763933837588066)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5444453887699315 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004634118657278197) - present_state_Q ( -0.01581128600530393)) * f1( 0.46240914900175634)
w2 ( 0.619334117915296 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.004634118657278197) - present_state_Q (-0.01581128600530393)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5314159849136153 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01888342092958642) - present_state_Q ( 0.007096274777267514)) * f1( 0.44198624389587404)
w2 ( 0.6311258006079237 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.01888342092958642) - present_state_Q (0.007096274777267514)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5226274451855507 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.061529511452203756) - present_state_Q ( 0.061529511452203756)) * f1( 0.3592680954488055)
w2 ( 0.6409107381956444 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.061529511452203756) - present_state_Q (0.061529511452203756)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.516268518178962 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10543241139755694) - present_state_Q ( 0.09915146952715506)) * f1( 0.3008124184815567)
w2 ( 0.6493664090601484 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10543241139755694) - present_state_Q (0.09915146952715506)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5120883042820586 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13320814818133644) - present_state_Q ( 0.13720626896612292)) * f1( 0.23735767404561842)
w2 ( 0.6564109908942288 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13320814818133644) - present_state_Q (0.13720626896612292)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5090151512544963 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16106240834481286) - present_state_Q ( 0.16106240834481286)) * f1( 0.19821188487244043)
w2 ( 0.6626127441938156 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16106240834481286) - present_state_Q (0.16106240834481286)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5073688230599922 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19194096679611028) - present_state_Q ( 0.19665709944971027)) * f1( 0.1343535611057351)
w2 ( 0.6675142240830115 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19194096679611028) - present_state_Q (0.19665709944971027)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5062970441307498 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2102472489120804) - present_state_Q ( 0.2154824995433638)) * f1( 0.10154977552443865)
w2 ( 0.6717359130969253 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2102472489120804) - present_state_Q (0.2154824995433638)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5047790738130774 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09805274550539458) - present_state_Q ( 0.09805274550539458)) * f1( 0.07168605374006001)
w2 ( 0.6759709636778282 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09805274550539458) - present_state_Q (0.09805274550539458)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5036048613464856 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17238282012060605) - present_state_Q ( 0.16236757149525888)) * f1( 0.21399622048491082)
w2 ( 0.6781657920985003 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17238282012060605) - present_state_Q (0.16236757149525888)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5036429607075424 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.330753753431649) - present_state_Q ( 0.3357729664032444)) * f1( 0.14123475429860866)
w2 ( 0.6780039366348954 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.330753753431649) - present_state_Q (0.3357729664032444)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5095869532320083 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7356523350518155) - present_state_Q ( 0.7294974077306277)) * f1( 0.16699789889466318)
w2 ( 0.6352920757278419 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7356523350518155) - present_state_Q (0.7294974077306277)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5132435078012683 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.7109067966159823) - present_state_Q ( 0.7068552881237853)) * f1( 0.10890232255290658)
w2 ( 0.5950003227123795 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.7109067966159823) - present_state_Q (0.7068552881237853)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5125106179969788 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27944397526271525) - present_state_Q ( 0.27944397526271525)) * f1( 0.15110998421969882)
w2 ( 0.5979103480481929 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.27944397526271525) - present_state_Q (0.27944397526271525)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5035227910464539 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.059810421448745654) - present_state_Q ( 0.05536372328072764)) * f1( 0.35862752786837476)
w2 ( 0.6079350408027587 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.059810421448745654) - present_state_Q (0.05536372328072764)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49532432962698797 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07970488855228042) - present_state_Q ( 0.06982782988033634)) * f1( 0.34426681279015753)
w2 ( 0.6174607471617544 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07970488855228042) - present_state_Q (0.06982782988033634)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49040789428185444 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12009368001743773) - present_state_Q ( 0.12009368001743773)) * f1( 0.2561768345657906)
w2 ( 0.6251373746811266 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12009368001743773) - present_state_Q (0.12009368001743773)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4871369829377858 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15184800589399042) - present_state_Q ( 0.15184800589399042)) * f1( 0.20025563438833488)
w2 ( 0.631670846468943 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15184800589399042) - present_state_Q (0.15184800589399042)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4847909133967586 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17331322549200867) - present_state_Q ( 0.17331322549200867)) * f1( 0.1629010234800902)
w2 ( 0.6374315703512307 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17331322549200867) - present_state_Q (0.17331322549200867)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.48166137967283573 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1585338852611926) - present_state_Q ( 0.1585338852611926)) * f1( 0.19892852818463008)
w2 ( 0.6437243504818277 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1585338852611926) - present_state_Q (0.1585338852611926)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4781724640300796 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1588398879934601) - present_state_Q ( 0.1538063494153716)) * f1( 0.2152619976461172)
w2 ( 0.6502074560571868 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1588398879934601) - present_state_Q (0.1538063494153716)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.47481489771571933 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1637572929362957) - present_state_Q ( 0.1584326844279555)) * f1( 0.21258082729858918)
w2 ( 0.6565251778518137 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1637572929362957) - present_state_Q (0.1584326844279555)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4720752026645902 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17265485618868712) - present_state_Q ( 0.17265485618868712)) * f1( 0.18945322774159512)
w2 ( 0.662309603029021 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17265485618868712) - present_state_Q (0.17265485618868712)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.49081805061065403 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019432958851807042) - present_state_Q ( -0.019432958851807042)) * f1( 0.3217599200302259)
w2 ( 0.6506593962883535 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.019432958851807042) - present_state_Q (-0.019432958851807042)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5085884180016882 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.019649856529017906) - present_state_Q ( -0.019649856529017906)) * f1( 0.30516753733962476)
w2 ( 0.6390130937058758 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.019649856529017906) - present_state_Q (-0.019649856529017906)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.525008864014921 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01447377330594482) - present_state_Q ( -0.01447377330594482)) * f1( 0.2797476053547246)
w2 ( 0.6272736216253828 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.01447377330594482) - present_state_Q (-0.01447377330594482)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5352395519664972 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012810692613428515) - present_state_Q ( -0.012810692613428515)) * f1( 0.2633582524324304)
w2 ( 0.6195042140924245 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.012810692613428515) - present_state_Q (-0.012810692613428515)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5375612643795246 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005965340575196015) - present_state_Q ( 0.005965340575196015)) * f1( 0.2203415308341618)
w2 ( 0.617396837962071 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.005965340575196015) - present_state_Q (0.005965340575196015)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5397408596836648 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03338085561393922) - present_state_Q ( 0.03338085561393922)) * f1( 0.16760603478837044)
w2 ( 0.6147959825610201 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.03338085561393922) - present_state_Q (0.03338085561393922)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5430006718653743 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05233455944529754) - present_state_Q ( 0.05151985430398054)) * f1( 0.1323585956603196)
w2 ( 0.609870254593831 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.05233455944529754) - present_state_Q (0.05151985430398054)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5458930016423532 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06018165927962538) - present_state_Q ( 0.06018165927962538)) * f1( 0.11379800217717038)
w2 ( 0.6047869847267978 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06018165927962538) - present_state_Q (0.06018165927962538)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5589719897895838 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00238635807765053) - present_state_Q ( 0.00238635807765053)) * f1( 0.21720564013640156)
w2 ( 0.5927440302814001 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.00238635807765053) - present_state_Q (0.00238635807765053)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5325094776791479 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3689358895504318) - present_state_Q ( 0.2288292305187672)) * f1( 0.3501598161602526)
w2 ( 0.6284548251840048 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.3689358895504318) - present_state_Q (0.2288292305187672)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5576665458972734 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.35741553117511954) - present_state_Q ( 0.35741553117511954)) * f1( 0.2729497503134786)
w2 ( 0.5547209069393961 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.35741553117511954) - present_state_Q (0.35741553117511954)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5594311564694725 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4443493087612542) - present_state_Q ( 0.3344287961491649)) * f1( 0.19608120696286993)
w2 ( 0.5475213977175529 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.4443493087612542) - present_state_Q (0.3344287961491649)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5611676663961092 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4664111423712931) - present_state_Q ( 0.4664111423712931)) * f1( 0.14498701834581484)
w2 ( 0.5355443949041365 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4664111423712931) - present_state_Q (0.4664111423712931)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5584983851405626 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19818071963520326) - present_state_Q ( 0.19818071963520326)) * f1( 0.2194458531407157)
w2 ( 0.5428426360438355 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19818071963520326) - present_state_Q (0.19818071963520326)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5569493644766398 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23163226032170503) - present_state_Q ( 0.23141359278947715)) * f1( 0.16883126495180978)
w2 ( 0.5483476140383972 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23163226032170503) - present_state_Q (0.23141359278947715)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5560073089499791 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29324728156331975) - present_state_Q ( 0.29324728156331975)) * f1( 0.26112034404430645)
w2 ( 0.5512338097658381 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.29324728156331975) - present_state_Q (0.29324728156331975)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5558488844226029 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3246521550926749) - present_state_Q ( 0.3248787471325776)) * f1( 0.20882513379071163)
w2 ( 0.5518407272359733 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3246521550926749) - present_state_Q (0.3248787471325776)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5562652280811606 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36863743787267933) - present_state_Q ( 0.36863743787267933)) * f1( 0.13103407411127221)
w2 ( 0.5492988317091404 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.36863743787267933) - present_state_Q (0.36863743787267933)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5567031220012689 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39283791695900805) - present_state_Q ( 0.39784188238767687)) * f1( 0.07477940536230379)
w2 ( 0.5446141844537983 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.39283791695900805) - present_state_Q (0.39784188238767687)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5549197155346788 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2562639155855688) - present_state_Q ( 0.15845562722137682)) * f1( 0.10668171995630936)
w2 ( 0.5513010150272855 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2562639155855688) - present_state_Q (0.15845562722137682)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5539192478068197 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2886724485987071) - present_state_Q ( 0.18257134152623855)) * f1( 0.06838658534254959)
w2 ( 0.5571528511606307 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2886724485987071) - present_state_Q (0.18257134152623855)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5539039193423099 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31577808203514723) - present_state_Q ( 0.32362086488333225)) * f1( 0.019264262535191246)
w2 ( 0.5576302677598417 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.31577808203514723) - present_state_Q (0.32362086488333225)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5536328717035371 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10585988870446429) - present_state_Q ( 0.10424985118018312)) * f1( 0.013136217523834792)
w2 ( 0.5617569905136469 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10585988870446429) - present_state_Q (0.10424985118018312)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5736730538526377 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1085142836153672) - present_state_Q ( -0.1085142836153672)) * f1( 0.39893888713379566)
w2 ( 0.5517102476187236 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1085142836153672) - present_state_Q (-0.1085142836153672)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5895788993223037 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.6428578706859839) - present_state_Q ( 0.6366911644556553)) * f1( 0.23655143168955003)
w2 ( 0.4575734947845356 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.6428578706859839) - present_state_Q (0.6366911644556553)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5928597763836342 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.538289396077831) - present_state_Q ( 0.5320937913246881)) * f1( 0.18404508963666857)
w2 ( 0.43261641554416885 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.538289396077831) - present_state_Q (0.5320937913246881)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5951889202560201 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.5260491793920532) - present_state_Q ( 0.5260491793920532)) * f1( 0.13428774482798755)
w2 ( 0.40833421894077016 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.5260491793920532) - present_state_Q (0.5260491793920532)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5967857636848692 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.52403398232045) - present_state_Q ( 0.5121853880271354)) * f1( 0.09993888741133894)
w2 ( 0.3859647403694575 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.52403398232045) - present_state_Q (0.5121853880271354)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5906719477410247 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02684129003583581) - present_state_Q ( -0.033199459720409455)) * f1( 0.18497828619885326)
w2 ( 0.39257504698379403 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02684129003583581) - present_state_Q (-0.033199459720409455)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.585124617104098 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02351066813527526) - present_state_Q ( -0.02351066813527526)) * f1( 0.17272815802785746)
w2 ( 0.39899823901022896 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02351066813527526) - present_state_Q (-0.02351066813527526)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5780735705121243 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04225352147897152) - present_state_Q ( -0.04225352147897152)) * f1( 0.2085934614836127)
w2 ( 0.40575880239685047 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04225352147897152) - present_state_Q (-0.04225352147897152)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5711612957751203 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03831131945250209) - present_state_Q ( -0.03831131945250209)) * f1( 0.20665722500691047)
w2 ( 0.4124484061469955 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03831131945250209) - present_state_Q (-0.03831131945250209)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5649397000447233 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02112641572553267) - present_state_Q ( -0.026916032466557577)) * f1( 0.191549592917501)
w2 ( 0.4189444739648756 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02112641572553267) - present_state_Q (-0.026916032466557577)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5606358319060916 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.002202322461537065) - present_state_Q ( 0.002202322461537065)) * f1( 0.14441642590347128)
w2 ( 0.42490483216056796 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.002202322461537065) - present_state_Q (0.002202322461537065)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5572440857137426 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01798213825334996) - present_state_Q ( 0.01798213825334996)) * f1( 0.11950507685350045)
w2 ( 0.43058115367200767 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.01798213825334996) - present_state_Q (0.01798213825334996)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5567247462384921 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00938253261394581) - present_state_Q ( -0.00938253261394581)) * f1( 0.01683738393011072)
w2 ( 0.43058115367200767 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.00938253261394581) - present_state_Q (-0.00938253261394581)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5410374131255422 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.20575997723967504) - present_state_Q ( -0.21277423201982598)) * f1( 0.5368729606033851)
w2 ( 0.43642511835792486 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.20575997723967504) - present_state_Q (-0.21277423201982598)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5190822050349736 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17325211357802878) - present_state_Q ( -0.17325211357802878)) * f1( 0.48155105530411585)
w2 ( 0.4455436564023294 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.17325211357802878) - present_state_Q (-0.17325211357802878)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.5045780421047021 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.039466419081916476) - present_state_Q ( -0.04350644921922428)) * f1( 0.42714604667524136)
w2 ( 0.4591260486947707 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.039466419081916476) - present_state_Q (-0.04350644921922428)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5067771489625906 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1858430185363855) - present_state_Q ( 0.1768459091448648)) * f1( 0.3774538602919061)
w2 ( 0.4544651201114726 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1858430185363855) - present_state_Q (0.1768459091448648)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5058481386432292 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29956285040020747) - present_state_Q ( 0.29956285040020747)) * f1( 0.3056615122216171)
w2 ( 0.45750446357545393 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.29956285040020747) - present_state_Q (0.29956285040020747)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5058177990033094 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33769017217273467) - present_state_Q ( 0.33254087939763954)) * f1( 0.24703774637381876)
w2 ( 0.4576272773574173 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.33769017217273467) - present_state_Q (0.33254087939763954)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5062179309786656 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3617292637183736) - present_state_Q ( 0.3561097709947199)) * f1( 0.20069975110154878)
w2 ( 0.45563359289512906 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3617292637183736) - present_state_Q (0.3561097709947199)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5080005215390586 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47758142877456067) - present_state_Q ( 0.47686788678587055)) * f1( 0.13806785657150103)
w2 ( 0.4401404236261193 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.47758142877456067) - present_state_Q (0.47686788678587055)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5091600853020471 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4885288482924977) - present_state_Q ( 0.4848564973301651)) * f1( 0.0852597766828239)
w2 ( 0.42381999012600946 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4885288482924977) - present_state_Q (0.4848564973301651)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.4890147546053637 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15069506056068652) - present_state_Q ( -0.15069506056068652)) * f1( 0.4624460270608367)
w2 ( 0.4325325012161018 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15069506056068652) - present_state_Q (-0.15069506056068652)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4718112277929952 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11978629700071128) - present_state_Q ( -0.11978629700071128)) * f1( 0.4218539324246168)
w2 ( 0.44068865456211465 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11978629700071128) - present_state_Q (-0.11978629700071128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.45799847495691454 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08466690511910979) - present_state_Q ( -0.08495996574148329)) * f1( 0.3668791382172278)
w2 ( 0.4482185200667061 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08466690511910979) - present_state_Q (-0.08495996574148329)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4471526845844712 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05316029994482539) - present_state_Q ( -0.05316029994482539)) * f1( 0.31180017350843947)
w2 ( 0.45517540546571295 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05316029994482539) - present_state_Q (-0.05316029994482539)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.43943463087031676 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018083472924006735) - present_state_Q ( -0.018083472924006735)) * f1( 0.2440297414708596)
w2 ( 0.4615009079783451 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.018083472924006735) - present_state_Q (-0.018083472924006735)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4333125762642356 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011430685241262242) - present_state_Q ( -0.07208114366915735)) * f1( 0.16403155009972234)
w2 ( 0.4615009079783451 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.011430685241262242) - present_state_Q (-0.07208114366915735)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.43038449268288825 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03900535293448182) - present_state_Q ( 0.04356417555277586)) * f1( 0.11247309381847659)
w2 ( 0.46670763517315855 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03900535293448182) - present_state_Q (0.04356417555277586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4278354999758456 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05027784861912741) - present_state_Q ( 0.05027784861912741)) * f1( 0.10005862001917917)
w2 ( 0.47180263389801425 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05027784861912741) - present_state_Q (0.05027784861912741)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4268219544841914 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2877338992470897) - present_state_Q ( 0.2829046970920007)) * f1( 0.2209667267717336)
w2 ( 0.4754721293246309 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2877338992470897) - present_state_Q (0.2829046970920007)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.41531982685025914 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008727974909852193) - present_state_Q ( 0.017153198874600706)) * f1( 0.40540476195598446)
w2 ( 0.4868209132692863 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.008727974909852193) - present_state_Q (0.017153198874600706)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4085368500345682 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07607512193075126) - present_state_Q ( 0.07409034836524017)) * f1( 0.29047016092966255)
w2 ( 0.49616159982239966 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07607512193075126) - present_state_Q (0.07409034836524017)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.4013922574202463 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09734961208377883) - present_state_Q ( 0.0038194083301114207)) * f1( 0.23354787120499704)
w2 ( 0.5022799108799649 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09734961208377883) - present_state_Q (0.0038194083301114207)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3960486354528077 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12036924090150046) - present_state_Q ( 0.02557960894624693)) * f1( 0.1865416480900194)
w2 ( 0.508009057182843 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12036924090150046) - present_state_Q (0.02557960894624693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.449316030176989 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15948062700380267) - present_state_Q ( 0.15948062700380267)) * f1( 0.2888983110143597)
w2 ( 0.4271906800398089 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.15948062700380267) - present_state_Q (0.15948062700380267)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4699422797320731 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22645530790176643) - present_state_Q ( 0.22645530790176643)) * f1( 0.25660610435969594)
w2 ( 0.3628858978708817 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.22645530790176643) - present_state_Q (0.22645530790176643)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5041672625527148 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03596820009906343) - present_state_Q ( 0.03596820009906343)) * f1( 0.5412165050198259)
w2 ( 0.31229618746374915 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.03596820009906343) - present_state_Q (0.03596820009906343)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5333005585352214 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.007904625025843715) - present_state_Q ( 0.007904625025843715)) * f1( 0.47986520132266547)
w2 ( 0.2637270544618884 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.007904625025843715) - present_state_Q (0.007904625025843715)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5660648558899352 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13074931262072795) - present_state_Q ( 0.12313811054171969)) * f1( 0.46142791670950767)
w2 ( 0.16431820936273783 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.13074931262072795) - present_state_Q (0.12313811054171969)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5838955374863949 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07806243462741316) - present_state_Q ( 0.08366545824176319)) * f1( 0.3747049766544892)
w2 ( 0.07866355070251388 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07806243462741316) - present_state_Q (0.08366545824176319)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5849282425746108 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06785318099402093) - present_state_Q ( -0.07951399147290938)) * f1( 0.37867798012172055)
w2 ( 0.0737547119097452 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.06785318099402093) - present_state_Q (-0.07951399147290938)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.575318194521437 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.043281482079078254) - present_state_Q ( -0.046346080624178596)) * f1( 0.28098082417144815)
w2 ( 0.12847758109634852 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.043281482079078254) - present_state_Q (-0.046346080624178596)) * f2(1.6)
============================================================================
GUIDE learning . . .
w1 ( -0.570281481555214 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08946262222030268) - present_state_Q ( 0.09550075283285117)) * f1( 0.23597184033698704)
w2 ( 0.16689777278640075 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08946262222030268) - present_state_Q (0.09550075283285117)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.568008054575115 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19572859728272984) - present_state_Q ( 0.19572859728272984)) * f1( 0.18357144168051648)
w2 ( 0.18918974002659852 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19572859728272984) - present_state_Q (0.19572859728272984)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5668414984489236 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2630687922506707) - present_state_Q ( 0.2517125841059119)) * f1( 0.15638677519883382)
w2 ( 0.20261671314804647 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2630687922506707) - present_state_Q (0.2517125841059119)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5664751184565673 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2922671856661955) - present_state_Q ( 0.2980648108762421)) * f1( 0.11757303050783387)
w2 ( 0.2082258565323144 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2922671856661955) - present_state_Q (0.2980648108762421)) * f2(1.8)
============================================================================
GUIDE learning . . .
w1 ( -0.564092293677049 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.033302484685687875) - present_state_Q ( 0.033302484685687875)) * f1( 0.08824369561621012)
w2 ( 0.21902696708362965 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.033302484685687875) - present_state_Q (0.033302484685687875)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5605933044223896 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16901881013567166) - present_state_Q ( -0.1802341665261175)) * f1( 0.5524811281231926)
w2 ( 0.22282690421438267 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.16901881013567166) - present_state_Q (-0.1802341665261175)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5597216216410494 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13934116689354806) - present_state_Q ( -0.13230471642085476)) * f1( 0.47449881554250783)
w2 ( 0.22392914019827267 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13934116689354806) - present_state_Q (-0.13230471642085476)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5478712134051754 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10291054844494177) - present_state_Q ( -0.09699428044511835)) * f1( 0.4133336208913657)
w2 ( 0.24113133373431014 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.10291054844494177) - present_state_Q (-0.09699428044511835)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5690686825588129 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07254669192609245) - present_state_Q ( -0.0724919831729674)) * f1( 0.3963902064935577)
w2 ( 0.20904557257313164 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07254669192609245) - present_state_Q (-0.0724919831729674)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5874864286549858 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07501837962827992) - present_state_Q ( -0.06932041282942739)) * f1( 0.34222188347744703)
w2 ( 0.1767546870651276 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07501837962827992) - present_state_Q (-0.06932041282942739)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6037474136699342 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07263361722203825) - present_state_Q ( -0.07263361722203825)) * f1( 0.30415413998618906)
w2 ( 0.14467690239511766 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07263361722203825) - present_state_Q (-0.07263361722203825)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6203795378265217 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06576337278348743) - present_state_Q ( -0.07215531432530498)) * f1( 0.31121762509797085)
w2 ( 0.10192322055887415 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.06576337278348743) - present_state_Q (-0.07215531432530498)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.6120912353050328 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09053377697393941) - present_state_Q ( -0.09702004873014564)) * f1( 0.28782159031682275)
w2 ( 0.12496055424149428 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09053377697393941) - present_state_Q (-0.09702004873014564)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.6013465493385465 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07123236946043512) - present_state_Q ( -0.07756769954368953)) * f1( 0.29004849717936354)
w2 ( 0.15459611124930597 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07123236946043512) - present_state_Q (-0.07756769954368953)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.5866242418175912 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1441689290766549) - present_state_Q ( -0.1441689290766549)) * f1( 0.3425767950327077)
w2 ( 0.17178619269606554 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1441689290766549) - present_state_Q (-0.1441689290766549)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5874175158951731 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0902785573876882) - present_state_Q ( -0.08417547764998762)) * f1( 0.31919443473297654)
w2 ( 0.17029505001073866 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0902785573876882) - present_state_Q (-0.08417547764998762)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5800218061856702 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06449521720448212) - present_state_Q ( -0.08682837892183735)) * f1( 0.26377558505385734)
w2 ( 0.18151020429879422 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.06449521720448212) - present_state_Q (-0.08682837892183735)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5885662245305033 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02119797366255581) - present_state_Q ( -0.02119797366255581)) * f1( 0.22430897399775493)
w2 ( 0.15865489487657222 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02119797366255581) - present_state_Q (-0.02119797366255581)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.6072123703741499 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.31847859780347426) - present_state_Q ( -0.31847859780347426)) * f1( 0.5950215322976601)
w2 ( 0.15238750963703476 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.31847859780347426) - present_state_Q (-0.31847859780347426)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.6268608200952265 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.315949146015156) - present_state_Q ( -0.30978483009905927)) * f1( 0.6105604102324728)
w2 ( 0.1395151062569365 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.315949146015156) - present_state_Q (-0.30978483009905927)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6455907681318661 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2994835463500396) - present_state_Q ( -0.2994835463500396)) * f1( 0.5667758734687584)
w2 ( 0.12629651392553792 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2994835463500396) - present_state_Q (-0.2994835463500396)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6634229149272571 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.28711946680532374) - present_state_Q ( -0.27440509790458645)) * f1( 0.5032967005011971)
w2 ( 0.11212423997450008 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.28711946680532374) - present_state_Q (-0.27440509790458645)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6803707964056283 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2704751945779691) - present_state_Q ( -0.2704751945779691)) * f1( 0.4752999685010033)
w2 ( 0.09786134697930696 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.2704751945779691) - present_state_Q (-0.2704751945779691)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.6964371138270694 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.25348367647368736) - present_state_Q ( -0.2602815871343239)) * f1( 0.44009256056830043)
w2 ( 0.08325867575878516 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.25348367647368736) - present_state_Q (-0.2602815871343239)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.710984476152183 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22307560097721127) - present_state_Q ( -0.21612331384744468)) * f1( 0.35814688677389084)
w2 ( 0.0670113059087741 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.22307560097721127) - present_state_Q (-0.21612331384744468)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7235528600420462 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19882229775999555) - present_state_Q ( -0.19882229775999555)) * f1( 0.29849394193569523)
w2 ( 0.058590107268454025 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.19882229775999555) - present_state_Q (-0.19882229775999555)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.7350211287654667 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16356442808405347) - present_state_Q ( -0.15733931780487342)) * f1( 0.2498440275693887)
w2 ( 0.04022942226831275 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.16356442808405347) - present_state_Q (-0.15733931780487342)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7451334184171701 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14102215775723834) - present_state_Q ( -0.14102215775723834)) * f1( 0.21375429972802312)
w2 ( 0.021306219947573327 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.14102215775723834) - present_state_Q (-0.14102215775723834)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.7528924479113968 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11020719844318277) - present_state_Q ( -0.10594595445366811)) * f1( 0.1536214047087755)
w2 ( 0.001103229331947314 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.11020719844318277) - present_state_Q (-0.10594595445366811)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.5635128019636073 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10929500609879225) - present_state_Q ( -0.12055563595592614)) * f1( 0.6019983021324019)
w2 ( 0.21998958061823434 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.10929500609879225) - present_state_Q (-0.12055563595592614)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5510101107429749 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03023539580113621) - present_state_Q ( -0.03705658941535972)) * f1( 0.534227590053372)
w2 ( 0.24807354659846387 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.03023539580113621) - present_state_Q (-0.03705658941535972)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5394430301844073 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04879900262530443) - present_state_Q ( 0.04879900262530443)) * f1( 0.45169634538512043)
w2 ( 0.27880325431493097 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04879900262530443) - present_state_Q (0.04879900262530443)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5316456057889974 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11734452855416219) - present_state_Q ( 0.11773993285604847)) * f1( 0.40194044632988957)
w2 ( 0.3020825967148551 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11734452855416219) - present_state_Q (0.11773993285604847)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5238470767866008 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1603811189296314) - present_state_Q ( 0.1053222237087903)) * f1( 0.3700968669045225)
w2 ( 0.32315418553327235 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1603811189296314) - present_state_Q (0.1053222237087903)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.5223779825620616 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2763680415377427) - present_state_Q ( 0.28237754965726136)) * f1( 0.3245953210856387)
w2 ( 0.3294904811627842 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2763680415377427) - present_state_Q (0.28237754965726136)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5215857141412846 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30537952866938245) - present_state_Q ( 0.3041929434933268)) * f1( 0.30072808460281425)
w2 ( 0.33317878247508975 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.30537952866938245) - present_state_Q (0.3041929434933268)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.521290859481211 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3295337561826954) - present_state_Q ( 0.3222855795835772)) * f1( 0.27639697938984936)
w2 ( 0.3346722739199467 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3295337561826954) - present_state_Q (0.3222855795835772)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5218373340197534 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36345333891828957) - present_state_Q ( 0.36345333891828957)) * f1( 0.2015915734149249)
w2 ( 0.3308771532162422 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.36345333891828957) - present_state_Q (0.36345333891828957)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.5213172687789498 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37331759338722065) - present_state_Q ( 0.3071460326047214)) * f1( 0.1722884611612822)
w2 ( 0.3344994404243223 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.37331759338722065) - present_state_Q (0.3071460326047214)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.5220960279239116 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4050962555826799) - present_state_Q ( 0.39963587943411477)) * f1( 0.13171122706286414)
w2 ( 0.32622176488170374 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4050962555826799) - present_state_Q (0.39963587943411477)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.4812776856586101 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.29072127970652417) - present_state_Q ( -0.34564044100980723)) * f1( 0.6620246516416317)
w2 ( 0.32622176488170374 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.29072127970652417) - present_state_Q (-0.34564044100980723)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.4522404417287151 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21637073454875516) - present_state_Q ( -0.21691818515374578)) * f1( 0.5862780397640042)
w2 ( 0.33612738711568113 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.21637073454875516) - present_state_Q (-0.21691818515374578)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.4201197798930787 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2140575938507297) - present_state_Q ( -0.22260475700037125)) * f1( 0.6408764181186776)
w2 ( 0.3461513670679871 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.2140575938507297) - present_state_Q (-0.22260475700037125)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3946868908487293 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1622394474079631) - present_state_Q ( -0.1675260068112031)) * f1( 0.5635447116654576)
w2 ( 0.35517740830939526 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1622394474079631) - present_state_Q (-0.1675260068112031)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.37362617567508194 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12881394527179946) - present_state_Q ( -0.12881394527179946)) * f1( 0.5063492899496231)
w2 ( 0.36349605932428763 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.12881394527179946) - present_state_Q (-0.12881394527179946)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3547636104905449 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10567228605540785) - present_state_Q ( -0.10567228605540785)) * f1( 0.4774063208980928)
w2 ( 0.371398160473285 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.10567228605540785) - present_state_Q (-0.10567228605540785)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.33856092978771857 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08017454088094962) - present_state_Q ( -0.08017454088094962)) * f1( 0.43537208554743556)
w2 ( 0.3788413022091421 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08017454088094962) - present_state_Q (-0.08017454088094962)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32506760450357236 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05495637506500932) - present_state_Q ( -0.05495637506500932)) * f1( 0.3861184915483412)
w2 ( 0.38583051696031223 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05495637506500932) - present_state_Q (-0.05495637506500932)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30381281271427785 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09999574514573997) - present_state_Q ( -0.09999574514573997)) * f1( 0.5450000125615577)
w2 ( 0.39363044037293554 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09999574514573997) - present_state_Q (-0.09999574514573997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2837559246767094 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08364632825257995) - present_state_Q ( -0.08364632825257995)) * f1( 0.5344488761896653)
w2 ( 0.40113607428148196 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08364632825257995) - present_state_Q (-0.08364632825257995)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26647445730982106 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05874079986894805) - present_state_Q ( -0.05874079986894805)) * f1( 0.4897448921412808)
w2 ( 0.408193408679123 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05874079986894805) - present_state_Q (-0.05874079986894805)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2513624461911511 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034909778470329) - present_state_Q ( -0.03854856657983541)) * f1( 0.4510272749178443)
w2 ( 0.41489456045377904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.034909778470329) - present_state_Q (-0.03854856657983541)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23883648512891786 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.016906739587986427) - present_state_Q ( -0.016906739587986427)) * f1( 0.39737698766180524)
w2 ( 0.4211988817663628 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.016906739587986427) - present_state_Q (-0.016906739587986427)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2290731936546646 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.005264750881668945) - present_state_Q ( 0.005264750881668945)) * f1( 0.33066566621500443)
w2 ( 0.4271041162504927 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.005264750881668945) - present_state_Q (0.005264750881668945)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2209442892287842 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01949472048665621) - present_state_Q ( 0.01949472048665621)) * f1( 0.2877949257686961)
w2 ( 0.4327532112817329 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.01949472048665621) - present_state_Q (0.01949472048665621)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21613601930283344 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.045535717467968195) - present_state_Q ( 0.045535717467968195)) * f1( 0.18563469067945948)
w2 ( 0.43793356836730946 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.045535717467968195) - present_state_Q (0.045535717467968195)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21344290653231118 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06360842804711513) - present_state_Q ( 0.06360842804711513)) * f1( 0.11094072012472023)
w2 ( 0.4427886166624614 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06360842804711513) - present_state_Q (0.06360842804711513)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2113065845436732 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06565553006305462) - present_state_Q ( 0.06565553006305462)) * f1( 0.5222001443512814)
w2 ( 0.44442501758019143 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.06565553006305462) - present_state_Q (0.06565553006305462)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2123505556265718 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2419005980070883) - present_state_Q ( 0.24396053843440096)) * f1( 0.528045426841352)
w2 ( 0.4428433792894961 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2419005980070883) - present_state_Q (0.24396053843440096)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.22865301102040805 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24680619729396813) - present_state_Q ( 0.24680619729396813)) * f1( 0.5060900632942871)
w2 ( 0.41707333308433037 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.24680619729396813) - present_state_Q (0.24680619729396813)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.2663621077195236 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22895721367502422) - present_state_Q ( 0.22634227477257868)) * f1( 0.4693416947188477)
w2 ( 0.35279760881192423 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.22895721367502422) - present_state_Q (0.22634227477257868)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.29940356399311 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31510898159271533) - present_state_Q ( 0.24454945983033044)) * f1( 0.40639470046384346)
w2 ( 0.27149375264481834 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.31510898159271533) - present_state_Q (0.24454945983033044)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3266569689797899 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22125934936728725) - present_state_Q ( 0.2240387111677435)) * f1( 0.33985497917579954)
w2 ( 0.17526421949709656 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.22125934936728725) - present_state_Q (0.2240387111677435)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.3472752428851632 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1116015224331381) - present_state_Q ( 0.11456736479617333)) * f1( 0.29312002404046844)
w2 ( 0.0908553539907534 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1116015224331381) - present_state_Q (0.11456736479617333)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.35681458317027015 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0211899444448905) - present_state_Q ( 0.009545569097757556)) * f1( 0.23413642797417414)
w2 ( 0.05011269652542654 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.0211899444448905) - present_state_Q (0.009545569097757556)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.36427491690829616 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.011406206093316433) - present_state_Q ( -0.01966842849725261)) * f1( 0.1955669087364065)
w2 ( 0.011965477314218632 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.011406206093316433) - present_state_Q (-0.01966842849725261)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.3328857912426001 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.22341095441916387) - present_state_Q ( -0.22341095441916387)) * f1( 0.6264421038967624)
w2 ( 0.03200827167330854 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.22341095441916387) - present_state_Q (-0.22341095441916387)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3043455163417704 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1805450195277986) - present_state_Q ( -0.1889526687786916)) * f1( 0.6060816735220145)
w2 ( 0.050844198346345015 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1805450195277986) - present_state_Q (-0.1889526687786916)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2799759242169747 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15012044355471768) - present_state_Q ( -0.15012044355471768)) * f1( 0.5600809400518213)
w2 ( 0.06824853431431485 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15012044355471768) - present_state_Q (-0.15012044355471768)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2599693437476141 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1123464578085594) - present_state_Q ( -0.1123464578085594)) * f1( 0.49877814288796885)
w2 ( 0.084293006795423 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1123464578085594) - present_state_Q (-0.1123464578085594)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2443509534312863 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06319720832223244) - present_state_Q ( -0.06319720832223244)) * f1( 0.4376401107891415)
w2 ( 0.10570565604482354 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06319720832223244) - present_state_Q (-0.06319720832223244)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23140545148321295 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.032619512738738074) - present_state_Q ( -0.032619512738738074)) * f1( 0.3930531271392822)
w2 ( 0.1254671097327154 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.032619512738738074) - present_state_Q (-0.032619512738738074)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.22126804049626855 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0023641235758745244) - present_state_Q ( -0.0023641235758745244)) * f1( 0.33553396827013127)
w2 ( 0.14359477240581264 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0023641235758745244) - present_state_Q (-0.0023641235758745244)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.21306165311150846 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.025893413162978128) - present_state_Q ( 0.02154698476067189)) * f1( 0.2919982413090754)
w2 ( 0.1604573137991502 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.025893413162978128) - present_state_Q (0.02154698476067189)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20742463124947552 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04800104112310832) - present_state_Q ( 0.04927232790219278)) * f1( 0.22060309629109207)
w2 ( 0.1757889803717573 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04800104112310832) - present_state_Q (0.04927232790219278)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19834042237752597 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006296654276643221) - present_state_Q ( 0.006296654276643221)) * f1( 0.3086371058558725)
w2 ( 0.18756230081779815 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.006296654276643221) - present_state_Q (0.006296654276643221)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1906316779918638 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02321943118199668) - present_state_Q ( 0.02072838024168132)) * f1( 0.27375428283644876)
w2 ( 0.1988260433328589 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.02321943118199668) - present_state_Q (0.02072838024168132)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1853762878488537 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04139982304933274) - present_state_Q ( 0.04139982304933274)) * f1( 0.2000223398623089)
w2 ( 0.2093356497030829 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04139982304933274) - present_state_Q (0.04139982304933274)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18177913837385537 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05692870252891599) - present_state_Q ( 0.05692870252891599)) * f1( 0.14460078828513953)
w2 ( 0.21928621641204193 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05692870252891599) - present_state_Q (0.05692870252891599)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17581384492386642 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0072446181755589645) - present_state_Q ( 0.0069462093819685974)) * f1( 0.20305429011621157)
w2 ( 0.2251617814607537 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0072446181755589645) - present_state_Q (0.0069462093819685974)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1721883762584594 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02226469166970306) - present_state_Q ( 0.02226469166970306)) * f1( 0.12949870149479345)
w2 ( 0.23076101701069904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.02226469166970306) - present_state_Q (0.02226469166970306)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17144856540394593 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004193472036083058) - present_state_Q ( -0.004193472036083058)) * f1( 0.02435397863203346)
w2 ( 0.23076101701069904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.004193472036083058) - present_state_Q (-0.004193472036083058)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18130832591017773 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05467257744030649) - present_state_Q ( 0.05467257744030649)) * f1( 0.2194934047730855)
w2 ( 0.212792804222848 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.05467257744030649) - present_state_Q (0.05467257744030649)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1994154437276719 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07873931944641926) - present_state_Q ( 0.07873931944641926)) * f1( 0.26990687185283035)
w2 ( 0.17254088097274134 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07873931944641926) - present_state_Q (0.07873931944641926)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19389000680192442 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09164679925044694) - present_state_Q ( 0.08817301977515499)) * f1( 0.25002920571752735)
w2 ( 0.19022021378473253 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09164679925044694) - present_state_Q (0.08817301977515499)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18711928100258465 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09131780238154977) - present_state_Q ( 0.09177803628950809)) * f1( 0.3115072083110498)
w2 ( 0.20760851330062427 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09131780238154977) - present_state_Q (0.09177803628950809)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.18110131667441137 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11240206695134772) - present_state_Q ( 0.11010166098967308)) * f1( 0.29919498060733263)
w2 ( 0.2236995969570612 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11240206695134772) - present_state_Q (0.11010166098967308)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17686515095903557 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13453876871575107) - present_state_Q ( 0.13578067387890747)) * f1( 0.23842457073003964)
w2 ( 0.2379134531964746 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13453876871575107) - present_state_Q (0.13578067387890747)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17413314239365021 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15945387370704248) - present_state_Q ( 0.15945387370704248)) * f1( 0.17457870407318818)
w2 ( 0.25043277428956756 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15945387370704248) - present_state_Q (0.15945387370704248)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.17324285890638236 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2316517997614356) - present_state_Q ( 0.23320066259004146)) * f1( 0.09895940234381524)
w2 ( 0.25942922602817775 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2316517997614356) - present_state_Q (0.23320066259004146)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16652114037462123 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011662229578956552) - present_state_Q ( 0.011662229578956552)) * f1( 0.23218051168513207)
w2 ( 0.26521930589575654 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.011662229578956552) - present_state_Q (0.011662229578956552)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16311069357330973 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08401880611531756) - present_state_Q ( 0.03246281414531632)) * f1( 0.12359419943638372)
w2 ( 0.27073808722508086 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08401880611531756) - present_state_Q (0.03246281414531632)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16072274019336985 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03942277743984154) - present_state_Q ( 0.03942277743984154)) * f1( 0.09027513575347891)
w2 ( 0.2760284772311637 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03942277743984154) - present_state_Q (0.03942277743984154)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15865896232661902 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04439855636771889) - present_state_Q ( 0.04254066822078475)) * f1( 0.07880046849755273)
w2 ( 0.28126646097948343 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04439855636771889) - present_state_Q (0.04254066822078475)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14816812439057592 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04774811769410285) - present_state_Q ( 0.04774811769410285)) * f1( 0.40816141583213716)
w2 ( 0.29154752874249573 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04774811769410285) - present_state_Q (0.04774811769410285)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1436874152730039 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1372991280819023) - present_state_Q ( 0.1372991280819023)) * f1( 0.253964132422996)
w2 ( 0.302133375826073 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1372991280819023) - present_state_Q (0.1372991280819023)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14004256737666468 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15249696150767283) - present_state_Q ( 0.14965372607290034)) * f1( 0.2201048669617586)
w2 ( 0.31206913403074504 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15249696150767283) - present_state_Q (0.14965372607290034)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13692403834780426 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15935065687192723) - present_state_Q ( 0.15935065687192723)) * f1( 0.19915961317321043)
w2 ( 0.32146419855966096 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15935065687192723) - present_state_Q (0.15935065687192723)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.133577971274122 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16157427026734864) - present_state_Q ( 0.16297021261817426)) * f1( 0.21842991835846576)
w2 ( 0.3306554314241746 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16157427026734864) - present_state_Q (0.16297021261817426)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1308912321720061 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17616292309233064) - present_state_Q ( 0.17349190962305902)) * f1( 0.18641808221764702)
w2 ( 0.339302894385345 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17616292309233064) - present_state_Q (0.17349190962305902)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12867425831839527 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1883445893457202) - present_state_Q ( 0.12104954157311613)) * f1( 0.11209013726558618)
w2 ( 0.34721429107980323 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1883445893457202) - present_state_Q (0.12104954157311613)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11685119681086321 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07635475161568556) - present_state_Q ( 0.07382045067779407)) * f1( 0.5056587588259329)
w2 ( 0.3565668920591542 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07635475161568556) - present_state_Q (0.07382045067779407)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10775896590159498 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09248345773968573) - present_state_Q ( 0.09340427581537883)) * f1( 0.421240709138435)
w2 ( 0.3652006548574978 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09248345773968573) - present_state_Q (0.09340427581537883)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09763049986822925 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10539164594668693) - present_state_Q ( 0.033625878220538606)) * f1( 0.36576309378241334)
w2 ( 0.3707389205849804 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10539164594668693) - present_state_Q (0.033625878220538606)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09204392029288941 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12165901427939568) - present_state_Q ( 0.11992399392083603)) * f1( 0.2906015471747962)
w2 ( 0.3784285968852646 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12165901427939568) - present_state_Q (0.11992399392083603)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08791201389210973 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1305343387682245) - present_state_Q ( 0.1305343387682245)) * f1( 0.22638214365029655)
w2 ( 0.3857293606896085 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1305343387682245) - present_state_Q (0.1305343387682245)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0846195190829109 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13785677317030565) - present_state_Q ( 0.13784059666057394)) * f1( 0.18713196168454516)
w2 ( 0.39276716391586675 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13785677317030565) - present_state_Q (0.13784059666057394)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07956276419182118 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.061089505888418175) - present_state_Q ( 0.061089505888418175)) * f1( 0.2063817790980811)
w2 ( 0.3976675528098752 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.061089505888418175) - present_state_Q (0.061089505888418175)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07674466559037986 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06380718920721035) - present_state_Q ( 0.06380718920721035)) * f1( 0.19765931355589225)
w2 ( 0.4005190234041454 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06380718920721035) - present_state_Q (0.06380718920721035)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07257307942171842 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06679700728750934) - present_state_Q ( 0.0667598885376957)) * f1( 0.17387418448541772)
w2 ( 0.4053174196479665 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06679700728750934) - present_state_Q (0.0667598885376957)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06920232372712073 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07071365769673128) - present_state_Q ( 0.07071365769673128)) * f1( 0.1426124716676238)
w2 ( 0.4100445738094254 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07071365769673128) - present_state_Q (0.07071365769673128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06655739395759212 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07416188749748882) - present_state_Q ( 0.07416188749748882)) * f1( 0.11339254004444606)
w2 ( 0.4147096598344706 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07416188749748882) - present_state_Q (0.07416188749748882)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06445303674451541 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07687380822181653) - present_state_Q ( 0.07687380822181653)) * f1( 0.09117129419075487)
w2 ( 0.41932593128647794 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07687380822181653) - present_state_Q (0.07687380822181653)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0629539487868862 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16222820692182982) - present_state_Q ( 0.1614861654726803)) * f1( 0.09687995100467088)
w2 ( 0.425515397495258 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16222820692182982) - present_state_Q (0.1614861654726803)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08575881382157743 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2348685769721639) - present_state_Q ( 0.15045768701564693)) * f1( 0.31369711293741814)
w2 ( 0.3964365643225208 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2348685769721639) - present_state_Q (0.15045768701564693)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10849334346260349 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2132421359632271) - present_state_Q ( 0.2132421359632271)) * f1( 0.2870818931976753)
w2 ( 0.3489214889805065 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2132421359632271) - present_state_Q (0.2132421359632271)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12618364874758486 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18480762165641845) - present_state_Q ( 0.18429079866822082)) * f1( 0.23100122016906732)
w2 ( 0.3029728867903518 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.18480762165641845) - present_state_Q (0.18429079866822082)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.11679490603572243 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07211115402876118) - present_state_Q ( 0.07102855760858659)) * f1( 0.3975205789768716)
w2 ( 0.3124201891021234 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07211115402876118) - present_state_Q (0.07102855760858659)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10913309603877543 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08493935524750301) - present_state_Q ( 0.08493935524750301)) * f1( 0.34272659443814546)
w2 ( 0.3213623723132133 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08493935524750301) - present_state_Q (0.08493935524750301)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10352089662256776 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09836822780631496) - present_state_Q ( 0.09943508073156482)) * f1( 0.26673730747433066)
w2 ( 0.329778441995176 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09836822780631496) - present_state_Q (0.09943508073156482)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09791089115333014 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10806325274926161) - present_state_Q ( 0.04417460421573009)) * f1( 0.2104027775446912)
w2 ( 0.3351110764163599 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10806325274926161) - present_state_Q (0.04417460421573009)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09485180157233027 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11754002038562832) - present_state_Q ( 0.11854235348017936)) * f1( 0.15832842397571573)
w2 ( 0.34283954235869524 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11754002038562832) - present_state_Q (0.11854235348017936)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09266428563138754 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12704807137783294) - present_state_Q ( 0.1260212935463701)) * f1( 0.11717777852255644)
w2 ( 0.3503068829023518 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12704807137783294) - present_state_Q (0.1260212935463701)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08740033420600146 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0155353060763956) - present_state_Q ( -0.0155353060763956)) * f1( 0.16765149561713594)
w2 ( 0.3503068829023518 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0155353060763956) - present_state_Q (-0.0155353060763956)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.07862697838196195 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04092432642115476) - present_state_Q ( 0.04092432642115476)) * f1( 0.33337458516623003)
w2 ( 0.355570245026771 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04092432642115476) - present_state_Q (0.04092432642115476)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07140556661317418 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.048931151987810693) - present_state_Q ( 0.048931151987810693)) * f1( 0.28212831618406126)
w2 ( 0.3606894842909904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.048931151987810693) - present_state_Q (0.048931151987810693)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06627874090531208 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057396828676883914) - present_state_Q ( 0.057396828676883914)) * f1( 0.20644144260028152)
w2 ( 0.3656563413748065 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.057396828676883914) - present_state_Q (0.057396828676883914)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06172922094264121 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06202508631546162) - present_state_Q ( 0.0608417537984306)) * f1( 0.1854216647550366)
w2 ( 0.3705635564714688 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06202508631546162) - present_state_Q (0.0608417537984306)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05877222150819157 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06651157897631728) - present_state_Q ( 0.06651157897631728)) * f1( 0.12313669607201842)
w2 ( 0.3753663480498951 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06651157897631728) - present_state_Q (0.06651157897631728)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09365074459353201 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19890597281288325) - present_state_Q ( 0.19890597281288325)) * f1( 0.44772573405935107)
w2 ( 0.3286254255179994 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.19890597281288325) - present_state_Q (0.19890597281288325)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12379932940348162 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15997950702458613) - present_state_Q ( 0.15918427904811683)) * f1( 0.40566656920426303)
w2 ( 0.2840342458172599 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.15997950702458613) - present_state_Q (0.15918427904811683)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13456461752566115 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12815731555435983) - present_state_Q ( 0.12815731555435983)) * f1( 0.3413849827752584)
w2 ( 0.26511375077732446 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.12815731555435983) - present_state_Q (0.12815731555435983)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14141796460775583 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17645833074840317) - present_state_Q ( 0.17645833074840317)) * f1( 0.2647996964481496)
w2 ( 0.24440875096343945 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.17645833074840317) - present_state_Q (0.17645833074840317)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13800412834231882 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2117756930072387) - present_state_Q ( 0.16467841405455771)) * f1( 0.21813768004480266)
w2 ( 0.25692868338313274 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2117756930072387) - present_state_Q (0.16467841405455771)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1358649934958393 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18218369049332833) - present_state_Q ( 0.1836116851046396)) * f1( 0.15891743142253087)
w2 ( 0.2676972180987082 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18218369049332833) - present_state_Q (0.1836116851046396)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13455152320029443 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19791899971196306) - present_state_Q ( 0.1993420903583538)) * f1( 0.1090471043305685)
w2 ( 0.2773332028677356 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19791899971196306) - present_state_Q (0.1993420903583538)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13382677101378657 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26060729188756454) - present_state_Q ( 0.26208941546379905)) * f1( 0.11329330981444574)
w2 ( 0.28373033424023136 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26060729188756454) - present_state_Q (0.26208941546379905)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13335760546883949 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27246691232037) - present_state_Q ( 0.27230286175605706)) * f1( 0.08539003368015999)
w2 ( 0.2892247171878294 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.27246691232037) - present_state_Q (0.27230286175605706)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12830496929290952 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09144597662987332) - present_state_Q ( 0.08555410381357384)) * f1( 0.2259772358360131)
w2 ( 0.29816833694180594 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09144597662987332) - present_state_Q (0.08555410381357384)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1251983005120462 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10026476999834374) - present_state_Q ( 0.10026476999834374)) * f1( 0.14810466720893226)
w2 ( 0.30655880522186557 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10026476999834374) - present_state_Q (0.10026476999834374)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12166679258917862 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10347780988873317) - present_state_Q ( 0.1014574495760782)) * f1( 0.16906038201877582)
w2 ( 0.31491441847837737 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10347780988873317) - present_state_Q (0.1014574495760782)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11917259350902659 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1108130324636021) - present_state_Q ( 0.1108130324636021)) * f1( 0.12454289790406281)
w2 ( 0.3229251493096877 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1108130324636021) - present_state_Q (0.1108130324636021)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10910630015950593 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07714186062392728) - present_state_Q ( 0.07714186062392728)) * f1( 0.4365785586096772)
w2 ( 0.3321480423272263 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07714186062392728) - present_state_Q (0.07714186062392728)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10066214310686168 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09067347579877462) - present_state_Q ( 0.09067347579877462)) * f1( 0.38664807687954983)
w2 ( 0.3408837971984704 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09067347579877462) - present_state_Q (0.09067347579877462)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09314005795536658 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1013646712145613) - present_state_Q ( 0.1002733686525901)) * f1( 0.35842819468383286)
w2 ( 0.349278321137225 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1013646712145613) - present_state_Q (0.1002733686525901)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08711274331082128 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11256452713508876) - present_state_Q ( 0.11159457252799174)) * f1( 0.30187608365427504)
w2 ( 0.3572647963446457 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11256452713508876) - present_state_Q (0.11159457252799174)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08098028123472843 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.120709926132573) - present_state_Q ( 0.050991185506417916)) * f1( 0.23488840994827712)
w2 ( 0.3624863924867825 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.120709926132573) - present_state_Q (0.050991185506417916)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07761965656340426 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12917230760337786) - present_state_Q ( 0.13010778109214596)) * f1( 0.18383210919478543)
w2 ( 0.3697987704735102 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12917230760337786) - present_state_Q (0.13010778109214596)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0751090037300633 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13698962677312748) - present_state_Q ( 0.13689721815486638)) * f1( 0.14200384957300152)
w2 ( 0.37687084025440803 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13698962677312748) - present_state_Q (0.13689721815486638)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07326305862952458 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14358113167078645) - present_state_Q ( 0.06970703195026558)) * f1( 0.07545215379215174)
w2 ( 0.3817638618787443 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14358113167078645) - present_state_Q (0.06970703195026558)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10277970851183785 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19839927487843842) - present_state_Q ( 0.12190378762274379)) * f1( 0.420426852290071)
w2 ( 0.3536813074733483 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.19839927487843842) - present_state_Q (0.12190378762274379)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12676504322904633 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31680790008016857) - present_state_Q ( 0.3177489334926465)) * f1( 0.34960571985435485)
w2 ( 0.2850744931248853 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.31680790008016857) - present_state_Q (0.3177489334926465)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.14898705511785637 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36020575477582134) - present_state_Q ( 0.36020575477582134)) * f1( 0.30685538069619095)
w2 ( 0.1836885680231318 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.36020575477582134) - present_state_Q (0.36020575477582134)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15902662495871445 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21865881789996042) - present_state_Q ( 0.21955230251997246)) * f1( 0.2524494002694349)
w2 ( 0.12801246912093509 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.21865881789996042) - present_state_Q (0.21955230251997246)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15630020856290844 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15070225450682956) - present_state_Q ( 0.15254087251931162)) * f1( 0.16774916940432544)
w2 ( 0.15076657853132708 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15070225450682956) - present_state_Q (0.15254087251931162)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1546141621981871 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1904515909730957) - present_state_Q ( 0.19056234273836017)) * f1( 0.13122738219023197)
w2 ( 0.16875417282158 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1904515909730957) - present_state_Q (0.19056234273836017)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15380302785948977 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2232188907654825) - present_state_Q ( 0.22355763836856585)) * f1( 0.08212833417788358)
w2 ( 0.18258116792069753 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2232188907654825) - present_state_Q (0.22355763836856585)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14941233462498468 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04684195925928447) - present_state_Q ( 0.04684195925928447)) * f1( 0.1702860358049744)
w2 ( 0.19289485738736328 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04684195925928447) - present_state_Q (0.04684195925928447)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1461926167215598 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.057761297804647366) - present_state_Q ( 0.057761297804647366)) * f1( 0.1298195707802993)
w2 ( 0.20281545066639597 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.057761297804647366) - present_state_Q (0.057761297804647366)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.14303404914589266 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09985487763178948) - present_state_Q ( 0.0997276940499734)) * f1( 0.15022356697871045)
w2 ( 0.2154309182891883 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09985487763178948) - present_state_Q (0.0997276940499734)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14125078735254346 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11472278762551566) - present_state_Q ( 0.11619664700264234)) * f1( 0.09132024192049333)
w2 ( 0.22714745619478288 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11472278762551566) - present_state_Q (0.11619664700264234)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1403784850225367 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1289123357862195) - present_state_Q ( 0.12956739521329952)) * f1( 0.04758259142864319)
w2 ( 0.23814688649670224 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1289123357862195) - present_state_Q (0.12956739521329952)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13144333614338655 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02470084209459139) - present_state_Q ( 0.02470084209459139)) * f1( 0.502626257098885)
w2 ( 0.24525765618129694 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.02470084209459139) - present_state_Q (0.02470084209459139)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13212367892136104 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12668952881564477) - present_state_Q ( 0.1252767855878072)) * f1( 0.5396191350458134)
w2 ( 0.24424902956479752 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.12668952881564477) - present_state_Q (0.1252767855878072)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1230641174028393 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13147997302594677) - present_state_Q ( 0.13003183249380565)) * f1( 0.4947439527243148)
w2 ( 0.25889832274950064 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13147997302594677) - present_state_Q (0.13003183249380565)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11541505329403776 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15005234509390164) - present_state_Q ( 0.15005234509390164)) * f1( 0.46371204141413075)
w2 ( 0.27209455390273973 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15005234509390164) - present_state_Q (0.15005234509390164)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.10941381151254388 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17043258589398552) - present_state_Q ( 0.17043258589398552)) * f1( 0.40933184952787094)
w2 ( 0.2838234077183728 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17043258589398552) - present_state_Q (0.17043258589398552)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.10466710597569419 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18749119841847053) - present_state_Q ( 0.18749119841847053)) * f1( 0.3616319293628797)
w2 ( 0.2943240414322429 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18749119841847053) - present_state_Q (0.18749119841847053)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.101284663141059 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2038859740857856) - present_state_Q ( 0.20482429657219547)) * f1( 0.29268924833665394)
w2 ( 0.3035691854991535 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2038859740857856) - present_state_Q (0.20482429657219547)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09580405492731514 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14777388943626696) - present_state_Q ( 0.14871435211178682)) * f1( 0.33003179505224156)
w2 ( 0.3135329677090639 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14777388943626696) - present_state_Q (0.14871435211178682)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09141900195392365 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16021445077639637) - present_state_Q ( 0.16101696107616692)) * f1( 0.2828984594632648)
w2 ( 0.32283323674915226 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16021445077639637) - present_state_Q (0.16101696107616692)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0883675110820419 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17421952817548017) - present_state_Q ( 0.17421952817548017)) * f1( 0.2130893299822896)
w2 ( 0.3314253822276763 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17421952817548017) - present_state_Q (0.17421952817548017)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07681742372076707 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02901905321527984) - present_state_Q ( 0.02901905321527984)) * f1( 0.42171633866271296)
w2 ( 0.33690303926980125 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.02901905321527984) - present_state_Q (0.02901905321527984)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06284115396695587 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08644360216249256) - present_state_Q ( 0.08644360216249256)) * f1( 0.628992892563847)
w2 ( 0.3457910695919515 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08644360216249256) - present_state_Q (0.08644360216249256)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.050926500618604695 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1023066164291205) - present_state_Q ( 0.1023066164291205)) * f1( 0.573029123981322)
w2 ( 0.3541080314005032 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1023066164291205) - present_state_Q (0.1023066164291205)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.04087187423641302 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11552235144417308) - present_state_Q ( 0.11552235144417308)) * f1( 0.5129129392111734)
w2 ( 0.36194922674851293 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11552235144417308) - present_state_Q (0.11552235144417308)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.03248910957717514 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12638442061380079) - present_state_Q ( 0.12638442061380079)) * f1( 0.45007160619064424)
w2 ( 0.3693993876064161 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12638442061380079) - present_state_Q (0.12638442061380079)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.025676645677280542 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13533903348615678) - present_state_Q ( 0.13533903348615678)) * f1( 0.3823041541629597)
w2 ( 0.3765271824009145 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13533903348615678) - present_state_Q (0.13533903348615678)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.02013530669688509 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14204525703661766) - present_state_Q ( 0.14233244867875888)) * f1( 0.32241066008602187)
w2 ( 0.3834020654819106 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14204525703661766) - present_state_Q (0.14233244867875888)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.01565624326517533 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1475576443162535) - present_state_Q ( 0.1479539709062497)) * f1( 0.2685260953760883)
w2 ( 0.3900741372229256 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1475576443162535) - present_state_Q (0.1479539709062497)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.010405360308577569 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1523285307901007) - present_state_Q ( 0.07459847842775816)) * f1( 0.2182100111095014)
w2 ( 0.39488682471595066 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1523285307901007) - present_state_Q (0.07459847842775816)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.007759157935432401 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15622728470404812) - present_state_Q ( 0.15622728470404812)) * f1( 0.16601493183355953)
w2 ( 0.4012626424666049 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15622728470404812) - present_state_Q (0.15622728470404812)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.004718028501034775 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1595106941666657) - present_state_Q ( 0.07925561018907526)) * f1( 0.1284827957545847)
w2 ( 0.4059965516511568 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1595106941666657) - present_state_Q (0.07925561018907526)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.00027226359657097667 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08027833156270843) - present_state_Q ( 0.08027833156270843)) * f1( 0.19520415515102327)
w2 ( 0.41055154168302804 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08027833156270843) - present_state_Q (0.08027833156270843)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0033490744777444027 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08206670885424713) - present_state_Q ( 0.08206670885424713)) * f1( 0.16013702495519616)
w2 ( 0.4150743409236516 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08206670885424713) - present_state_Q (0.08206670885424713)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0076813530452878665 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0837237610279758) - present_state_Q ( 0.08366054507297183)) * f1( 0.1927926334670134)
w2 ( 0.4195685775442481 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0837237610279758) - present_state_Q (0.08366054507297183)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01254793803070249 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08556699088221183) - present_state_Q ( 0.08559028900636176)) * f1( 0.2182653873122824)
w2 ( 0.4240279057458853 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08556699088221183) - present_state_Q (0.08559028900636176)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01605641727832753 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17265549982068215) - present_state_Q ( 0.17265549982068215)) * f1( 0.2426165569896108)
w2 ( 0.4298123077523407 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17265549982068215) - present_state_Q (0.17265549982068215)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.020615230297990183 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17713174187740088) - present_state_Q ( 0.17713174187740088)) * f1( 0.32428272672587993)
w2 ( 0.4354355650447543 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17713174187740088) - present_state_Q (0.17713174187740088)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.026046903634747275 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18241839919939926) - present_state_Q ( 0.18241839919939926)) * f1( 0.39990691650440996)
w2 ( 0.4408685026735759 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18241839919939926) - present_state_Q (0.18241839919939926)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.03620680558149391 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1879390641539371) - present_state_Q ( 0.10028457881392762)) * f1( 0.46496422181468783)
w2 ( 0.4452386892256052 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1879390641539371) - present_state_Q (0.10028457881392762)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04244107869333295 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1967346672261924) - present_state_Q ( 0.19640732626534174)) * f1( 0.5057571437470095)
w2 ( 0.4501693348438963 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1967346672261924) - present_state_Q (0.19640732626534174)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.04903228858293089 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2037381393192495) - present_state_Q ( 0.2041331568911217)) * f1( 0.5670313690057924)
w2 ( 0.45481896112552844 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2037381393192495) - present_state_Q (0.2041331568911217)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.05578126475322986 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21230870538753407) - present_state_Q ( 0.21230870538753407)) * f1( 0.6196145808274374)
w2 ( 0.4591758477315772 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21230870538753407) - present_state_Q (0.21230870538753407)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.062335152293420924 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21928822881392146) - present_state_Q ( 0.21928822881392146)) * f1( 0.6385278261233442)
w2 ( 0.46328147149427606 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21928822881392146) - present_state_Q (0.21928822881392146)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.06856703164885532 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22524575761672688) - present_state_Q ( 0.22524575761672688)) * f1( 0.6406203811140949)
w2 ( 0.4671726242200739 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22524575761672688) - present_state_Q (0.22524575761672688)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.07441400495937057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23060413375261904) - present_state_Q ( 0.22990628294786714)) * f1( 0.6276665654747805)
w2 ( 0.4708987894371697 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23060413375261904) - present_state_Q (0.22990628294786714)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.08001450021110407 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23587168319057147) - present_state_Q ( 0.23587168319057147)) * f1( 0.6384842133096431)
w2 ( 0.4744074088423091 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23587168319057147) - present_state_Q (0.23587168319057147)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.08530966501077905 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24084915896188647) - present_state_Q ( 0.2403801471771497)) * f1( 0.6326001350590404)
w2 ( 0.47775559959107067 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.24084915896188647) - present_state_Q (0.2403801471771497)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09035235803150929 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2455586427196137) - present_state_Q ( 0.2455586427196137)) * f1( 0.6383380227352289)
w2 ( 0.48091548845316456 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2455586427196137) - present_state_Q (0.2455586427196137)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09513467879961907 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24993119187761387) - present_state_Q ( 0.24993119187761387)) * f1( 0.637116703432056)
w2 ( 0.4839179655455705 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.24993119187761387) - present_state_Q (0.24993119187761387)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09967496337947891 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2542672790670655) - present_state_Q ( 0.2542672790670655)) * f1( 0.638043809205359)
w2 ( 0.48676434349915615 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2542672790670655) - present_state_Q (0.2542672790670655)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.10396591490652553 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2578521594693858) - present_state_Q ( 0.25594977407168223)) * f1( 0.6144375136497786)
w2 ( 0.4895577611741664 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2578521594693858) - present_state_Q (0.25594977407168223)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.10805405929894422 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2624019672302066) - present_state_Q ( 0.2624019672302066)) * f1( 0.6403912553494118)
w2 ( 0.49211129035387896 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2624019672302066) - present_state_Q (0.2624019672302066)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.11193249657915418 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26600813666572753) - present_state_Q ( 0.26600813666572753)) * f1( 0.6400835005450993)
w2 ( 0.49453499743391277 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26600813666572753) - present_state_Q (0.26600813666572753)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.10279535124602378 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26954150286738854) - present_state_Q ( 0.26954150286738854)) * f1( 0.6408103641564057)
w2 ( 0.4888315033306868 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.26954150286738854) - present_state_Q (0.26954150286738854)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.06097232538808275 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26505569861809936) - present_state_Q ( 0.263070313358583)) * f1( 0.6570113454320305)
w2 ( 0.4633689135908159 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.26505569861809936) - present_state_Q (0.263070313358583)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.0014279129843993987 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23031988926995736) - present_state_Q ( 0.23031988926995736)) * f1( 0.7375858399263383)
w2 ( 0.4310773975770974 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.23031988926995736) - present_state_Q (0.23031988926995736)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06009268934872639 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17359258552555015) - present_state_Q ( 0.17359258552555015)) * f1( 0.8135135035555326)
w2 ( 0.4008280644981776 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.17359258552555015) - present_state_Q (0.17359258552555015)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11083208076816314 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11710631595179037) - present_state_Q ( 0.11710631595179037)) * f1( 0.7193039671870968)
w2 ( 0.3726122371239131 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.11710631595179037) - present_state_Q (0.11710631595179037)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1552873049386405 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07525672892037656) - present_state_Q ( 0.07525672892037656)) * f1( 0.6657654121241092)
w2 ( 0.34590299488277954 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07525672892037656) - present_state_Q (0.07525672892037656)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.19441820763517642 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.043355742633349986) - present_state_Q ( 0.04325476680326895)) * f1( 0.6124546445533509)
w2 ( 0.32034622718118216 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.043355742633349986) - present_state_Q (0.04325476680326895)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.227712436307587 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02400462850965232) - present_state_Q ( 0.02400462850965232)) * f1( 0.5356178499404056)
w2 ( 0.29548206055483467 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.02400462850965232) - present_state_Q (0.02400462850965232)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.25684970430611276 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.00909996457029763) - present_state_Q ( 0.00909996457029763)) * f1( 0.4790816936510088)
w2 ( 0.27115446183030395 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.00909996457029763) - present_state_Q (0.00909996457029763)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.28276790041446975 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015846725722467037) - present_state_Q ( -0.0030943519512443424)) * f1( 0.43432456729797764)
w2 ( 0.2472845745986427 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.0015846725722467037) - present_state_Q (-0.0030943519512443424)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13703756153081237 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20735974731905088) - present_state_Q ( 0.16113256238204018)) * f1( 0.20932657031171173)
w2 ( 0.25091515948469145 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20735974731905088) - present_state_Q (0.16113256238204018)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13534092342143092 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22676267247644125) - present_state_Q ( 0.22669208918309253)) * f1( 0.1767622689064885)
w2 ( 0.2605135772911466 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22676267247644125) - present_state_Q (0.22669208918309253)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13432048688189724 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2438514081093501) - present_state_Q ( 0.2434497428061132)) * f1( 0.1260803757921707)
w2 ( 0.26860711709162877 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2438514081093501) - present_state_Q (0.2434497428061132)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13345821164599117 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2528077579380477) - present_state_Q ( 0.25265864196364896)) * f1( 0.11873449462703846)
w2 ( 0.27586933047464435 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2528077579380477) - present_state_Q (0.25265864196364896)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12750275539293274 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.026401480659819326) - present_state_Q ( 0.026401480659819326)) * f1( 0.21559097098821206)
w2 ( 0.2813941038227676 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.026401480659819326) - present_state_Q (0.026401480659819326)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11952429611635378 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06968577943289385) - present_state_Q ( 0.06968577943289385)) * f1( 0.3362426322795335)
w2 ( 0.2908854157631834 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06968577943289385) - present_state_Q (0.06968577943289385)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.11286224385177675 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08125575189127231) - present_state_Q ( 0.08125575189127231)) * f1( 0.293650877306432)
w2 ( 0.2999602086950976 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08125575189127231) - present_state_Q (0.08125575189127231)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1077795459883499 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09341717792655707) - present_state_Q ( 0.09341717792655707)) * f1( 0.23539232115899256)
w2 ( 0.30859719028974153 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09341717792655707) - present_state_Q (0.09341717792655707)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10441952774545697 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10575775418390707) - present_state_Q ( 0.10575775418390707)) * f1( 0.16404895539178405)
w2 ( 0.31678991113912086 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10575775418390707) - present_state_Q (0.10575775418390707)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1021294778284335 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11457059280421432) - present_state_Q ( 0.11457059280421432)) * f1( 0.11631322142196181)
w2 ( 0.3246653697981691 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11457059280421432) - present_state_Q (0.11457059280421432)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10001873012194452 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1187062455500157) - present_state_Q ( 0.1187062455500157)) * f1( 0.109272098580582)
w2 ( 0.33239194495836855 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1187062455500157) - present_state_Q (0.1187062455500157)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09591519737193069 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05036247227631303) - present_state_Q ( 0.05036247227631303)) * f1( 0.16112898749776047)
w2 ( 0.33748542045739494 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05036247227631303) - present_state_Q (0.05036247227631303)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09241463454486593 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05413512420107677) - present_state_Q ( 0.05413512420107677)) * f1( 0.13931014329865263)
w2 ( 0.34251098822177556 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05413512420107677) - present_state_Q (0.05413512420107677)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08961839210181083 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05807118274451811) - present_state_Q ( 0.05807118274451811)) * f1( 0.11287189470811465)
w2 ( 0.3474657069323742 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05807118274451811) - present_state_Q (0.05807118274451811)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08681563414969254 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05954422702370509) - present_state_Q ( 0.059309331916787736)) * f1( 0.11363526203547379)
w2 ( 0.3523986087480859 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05954422702370509) - present_state_Q (0.059309331916787736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08482580919488943 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06336979392276691) - present_state_Q ( 0.06336979392276691)) * f1( 0.08189685989727291)
w2 ( 0.35725795245747605 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06336979392276691) - present_state_Q (0.06336979392276691)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08284218978505772 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13354536459206728) - present_state_Q ( 0.13354536459206728)) * f1( 0.11031803268063531)
w2 ( 0.3644503193321616 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13354536459206728) - present_state_Q (0.13354536459206728)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08114558493050147 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06702589498523119) - present_state_Q ( 0.06702589498523119)) * f1( 0.07078722685163555)
w2 ( 0.3692438532224274 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06702589498523119) - present_state_Q (0.06702589498523119)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.09946935182648146 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12790043106661445) - present_state_Q ( 0.1268751956974137)) * f1( 0.2566047876713309)
w2 ( 0.3406804471187973 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.12790043106661445) - present_state_Q (0.1268751956974137)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13894614097429367 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.29428152142882) - present_state_Q ( 0.2953321079357522)) * f1( 0.4559026308138878)
w2 ( 0.25409005153951025 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.29428152142882) - present_state_Q (0.2953321079357522)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16973610456154284 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24691611458445775) - present_state_Q ( 0.19882646814391733)) * f1( 0.397733848583943)
w2 ( 0.1766765658709631 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.24691611458445775) - present_state_Q (0.19882646814391733)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.19601821659447655 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15386984844502855) - present_state_Q ( 0.15140133059783978)) * f1( 0.35708695332606616)
w2 ( 0.08835484438056264 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.15386984844502855) - present_state_Q (0.15140133059783978)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.21379862378095263 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05215219263397134) - present_state_Q ( 0.05215219263397134)) * f1( 0.274839867226003)
w2 ( 0.010722407576093732 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.05215219263397134) - present_state_Q (0.05215219263397134)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.22113984845536172 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03124089732585785) - present_state_Q ( -0.029096415810639105)) * f1( 0.19627490654451119)
w2 ( -0.03416091329453988 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03124089732585785) - present_state_Q (-0.029096415810639105)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.21609474820778155 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06472053513562824) - present_state_Q ( -0.07155271779453622)) * f1( 0.13819138456747646)
w2 ( 0.009648766419176925 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06472053513562824) - present_state_Q (-0.07155271779453622)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2134120477547175 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009270662089933525) - present_state_Q ( -0.007340908806098138)) * f1( 0.08755154239527772)
w2 ( 0.046418427530829504 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.009270662089933525) - present_state_Q (-0.007340908806098138)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.20753525257324318 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02915935485707414) - present_state_Q ( -0.02915935485707414)) * f1( 0.18013528649246677)
w2 ( 0.05294329591825684 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02915935485707414) - present_state_Q (-0.02915935485707414)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20364871688036726 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005289163839305697) - present_state_Q ( -0.005289163839305697)) * f1( 0.1275276459225543)
w2 ( 0.06513370581647185 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.005289163839305697) - present_state_Q (-0.005289163839305697)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1974327187055206 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026104714207677432) - present_state_Q ( -0.026104714207677432)) * f1( 0.1921517403616122)
w2 ( 0.07160359067221005 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.026104714207677432) - present_state_Q (-0.026104714207677432)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1914453959842701 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02254744871939389) - present_state_Q ( -0.02258196534296049)) * f1( 0.18691270484120942)
w2 ( 0.07801013508163046 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02254744871939389) - present_state_Q (-0.02258196534296049)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1873818483778851 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009603383952839486) - present_state_Q ( -0.009603383952839486)) * f1( 0.13165848590705495)
w2 ( 0.08418299599278158 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.009603383952839486) - present_state_Q (-0.009603383952839486)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.18580569890554488 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00956997353454241) - present_state_Q ( -0.00956997353454241)) * f1( 0.05107204148847463)
w2 ( 0.08418299599278158 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.00956997353454241) - present_state_Q (-0.00956997353454241)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18256532327477967 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.000963446695749666) - present_state_Q ( -0.0030378717773748842)) * f1( 0.10696373196838527)
w2 ( 0.09024182653493758 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.000963446695749666) - present_state_Q (-0.0030378717773748842)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21394098088614888 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04708748528979751) - present_state_Q ( -0.04891701822017024)) * f1( 0.5645218505488782)
w2 ( 0.056894322716409006 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.04708748528979751) - present_state_Q (-0.04891701822017024)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2399576479219055 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055710830490574716) - present_state_Q ( -0.055710830490574716)) * f1( 0.4731505307885385)
w2 ( 0.01290550251173038 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.055710830490574716) - present_state_Q (-0.055710830490574716)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.24084576638779276 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08897732051715844) - present_state_Q ( -0.08700100090312939)) * f1( 0.405594086103846)
w2 ( 0.011153764019843462 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.08897732051715844) - present_state_Q (-0.08700100090312939)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.2268923707591418 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07642442174514018) - present_state_Q ( -0.08107063061258252)) * f1( 0.3736567313521132)
w2 ( 0.04102801909488894 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07642442174514018) - present_state_Q (-0.08107063061258252)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.21567114070683993 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.031184343619787752) - present_state_Q ( -0.03555773588958999)) * f1( 0.3375422220158242)
w2 ( 0.07427194924765007 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.031184343619787752) - present_state_Q (-0.03555773588958999)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.20847504190448318 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019350634475042705) - present_state_Q ( 0.019350634475042705)) * f1( 0.25465305461179655)
w2 ( 0.10253039214489623 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.019350634475042705) - present_state_Q (0.019350634475042705)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2035107960656715 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06061966151454324) - present_state_Q ( 0.0604021591296179)) * f1( 0.2020780647430218)
w2 ( 0.12709637284707986 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06061966151454324) - present_state_Q (0.0604021591296179)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.20065745187898718 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09950337123586657) - present_state_Q ( 0.09950337123586657)) * f1( 0.13558495246762842)
w2 ( 0.14814106943585187 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09950337123586657) - present_state_Q (0.09950337123586657)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.19876163610092726 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1299416907611595) - present_state_Q ( 0.1276199055959637)) * f1( 0.10226963238955168)
w2 ( 0.1666784957838671 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1299416907611595) - present_state_Q (0.1276199055959637)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.196764185664463 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0802063370872084) - present_state_Q ( 0.08240970043707778)) * f1( 0.08853517901365471)
w2 ( 0.18021515178016567 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0802063370872084) - present_state_Q (0.08240970043707778)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19509460991001223 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.058790707217061935) - present_state_Q ( 0.058790707217061935)) * f1( 0.06756998714022365)
w2 ( 0.19009868632035143 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.058790707217061935) - present_state_Q (0.058790707217061935)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18832129441129106 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005334373218937066) - present_state_Q ( -0.005334373218937066)) * f1( 0.22222095476140794)
w2 ( 0.19619470503829228 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.005334373218937066) - present_state_Q (-0.005334373218937066)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17394743273443083 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04108930486357075) - present_state_Q ( -0.04108930486357075)) * f1( 0.4265489259849364)
w2 ( 0.20293431252583655 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04108930486357075) - present_state_Q (-0.04108930486357075)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1616072029838901 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.025820760883686858) - present_state_Q ( -0.025820760883686858)) * f1( 0.38176834429192225)
w2 ( 0.2093990862217429 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.025820760883686858) - present_state_Q (-0.025820760883686858)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15201536422116663 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.008504961989492937) - present_state_Q ( -0.008504961989492937)) * f1( 0.31177310357177673)
w2 ( 0.2155521755375538 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.008504961989492937) - present_state_Q (-0.008504961989492937)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14489081696358888 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006312242853554584) - present_state_Q ( 0.006312242853554584)) * f1( 0.24206890166982473)
w2 ( 0.22143855516618982 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.006312242853554584) - present_state_Q (0.006312242853554584)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14043459191338256 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06212334455337941) - present_state_Q ( 0.06212334455337941)) * f1( 0.18256559019709265)
w2 ( 0.23120211476226815 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06212334455337941) - present_state_Q (0.06212334455337941)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13807504053261066 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07805796194848923) - present_state_Q ( 0.07805796194848923)) * f1( 0.10270178992162986)
w2 ( 0.24039202813212254 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07805796194848923) - present_state_Q (0.07805796194848923)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13735676040102135 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08996019606902443) - present_state_Q ( 0.09159489563480044)) * f1( 0.033039393654721694)
w2 ( 0.2490880730910066 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08996019606902443) - present_state_Q (0.09159489563480044)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1415844935200982 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.035359033765127494) - present_state_Q ( -0.035359033765127494)) * f1( 0.6201125312991546)
w2 ( 0.24772453569877892 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.035359033765127494) - present_state_Q (-0.035359033765127494)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17477093037651542 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07229833208996914) - present_state_Q ( 0.022753424950213347)) * f1( 0.5391578373549935)
w2 ( 0.22310359202913027 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07229833208996914) - present_state_Q (0.022753424950213347)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21119837244046252 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1846293334150632) - present_state_Q ( 0.1846293334150632)) * f1( 0.47545079059131057)
w2 ( 0.13116362402030343 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1846293334150632) - present_state_Q (0.1846293334150632)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.20964139741864643 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06952928299641775) - present_state_Q ( 0.06952928299641775)) * f1( 0.4160404496143378)
w2 ( 0.13565446145669033 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.06952928299641775) - present_state_Q (0.06952928299641775)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.20147218514168475 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1137253238991392) - present_state_Q ( 0.08659443160780114)) * f1( 0.36343452714196856)
w2 ( 0.16262783355054386 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1137253238991392) - present_state_Q (0.08659443160780114)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1958261804139097 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13228710204467864) - present_state_Q ( 0.13228710204467864)) * f1( 0.3120346273693487)
w2 ( 0.18434082652971856 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13228710204467864) - present_state_Q (0.13228710204467864)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.19200731815057365 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17191834229956368) - present_state_Q ( 0.1702989033674805)) * f1( 0.25997590496109996)
w2 ( 0.20196797823321566 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17191834229956368) - present_state_Q (0.1702989033674805)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1884129302486581 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19588608976053604) - present_state_Q ( 0.15899351572266218)) * f1( 0.22381679471640026)
w2 ( 0.2180274875585548 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19588608976053604) - present_state_Q (0.15899351572266218)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.18652910443573203 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22927419956712083) - present_state_Q ( 0.22528301337896756)) * f1( 0.19292716080220892)
w2 ( 0.22974481634788416 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22927419956712083) - present_state_Q (0.22528301337896756)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.17353726216387672 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.044592930964087465) - present_state_Q ( 0.044592930964087465)) * f1( 0.4999432079339307)
w2 ( 0.24533679807582343 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.044592930964087465) - present_state_Q (0.044592930964087465)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1621358985965902 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06332747677169134) - present_state_Q ( 0.06516216388890934)) * f1( 0.4727510042143678)
w2 ( 0.25980703310311903 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06332747677169134) - present_state_Q (0.06516216388890934)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15040987328945876 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0854173825154173) - present_state_Q ( 0.03453684010679128)) * f1( 0.4279494777840369)
w2 ( 0.27076722902890904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0854173825154173) - present_state_Q (0.03453684010679128)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.14296369401803835 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10880826181302812) - present_state_Q ( 0.1074152175845946)) * f1( 0.3659674636306511)
w2 ( 0.2829751655447115 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10880826181302812) - present_state_Q (0.1074152175845946)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13618503136653515 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11944255020121403) - present_state_Q ( 0.11944255020121403)) * f1( 0.3521352009780958)
w2 ( 0.29452526783384597 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11944255020121403) - present_state_Q (0.11944255020121403)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.130826456172004 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1369292675341402) - present_state_Q ( 0.13571292807039992)) * f1( 0.30107738140142765)
w2 ( 0.30520406775482684 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1369292675341402) - present_state_Q (0.13571292807039992)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12677474240097417 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14986167123148478) - present_state_Q ( 0.15083147491976873)) * f1( 0.24682290324116765)
w2 ( 0.3150533492870296 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14986167123148478) - present_state_Q (0.15083147491976873)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13156501297053488 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09295561237018002) - present_state_Q ( 0.09295561237018002)) * f1( 0.26082267428356254)
w2 ( 0.3077069472417031 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.09295561237018002) - present_state_Q (0.09295561237018002)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.147036260063999 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07995457685900917) - present_state_Q ( 0.07995457685900917)) * f1( 0.32780905093158014)
w2 ( 0.28882858247477877 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07995457685900917) - present_state_Q (0.07995457685900917)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.15948614500246974 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07648626306184367) - present_state_Q ( 0.07648626306184367)) * f1( 0.265547898940527)
w2 ( 0.2700750770045524 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.07648626306184367) - present_state_Q (0.07648626306184367)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17022331879638541 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06999589684237922) - present_state_Q ( 0.07113496651459578)) * f1( 0.23133711261660916)
w2 ( 0.2515096619313381 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06999589684237922) - present_state_Q (0.07113496651459578)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.15905861828654666 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07221954308143952) - present_state_Q ( 0.07059113679074905)) * f1( 0.4718193778381395)
w2 ( 0.2657075109823818 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07221954308143952) - present_state_Q (0.07059113679074905)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14989048928344556 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09402787812080757) - present_state_Q ( 0.09226558873815772)) * f1( 0.42222746918550186)
w2 ( 0.2787357429268172 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09402787812080757) - present_state_Q (0.09226558873815772)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14291479499785478 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11252494972425667) - present_state_Q ( 0.1141841946486727)) * f1( 0.3539734332782479)
w2 ( 0.2905598409462424 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11252494972425667) - present_state_Q (0.1141841946486727)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.13573682639639553 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12866359401350155) - present_state_Q ( 0.07338764810737378)) * f1( 0.29973305613156553)
w2 ( 0.30013898939800143 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12866359401350155) - present_state_Q (0.07338764810737378)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13153606715166882 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14511058338103694) - present_state_Q ( 0.14620481013804215)) * f1( 0.2495902136522796)
w2 ( 0.31023736429000515 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14511058338103694) - present_state_Q (0.14620481013804215)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12831340709134395 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1596354056309736) - present_state_Q ( 0.15911636428581452)) * f1( 0.20546497149733062)
w2 ( 0.31964819486664214 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1596354056309736) - present_state_Q (0.15911636428581452)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1261969778092616 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17297257532800925) - present_state_Q ( 0.17297257532800925)) * f1( 0.14664361284227334)
w2 ( 0.32830767579892967 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17297257532800925) - present_state_Q (0.17297257532800925)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12467314927238302 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18278922443924925) - present_state_Q ( 0.18279124423795778)) * f1( 0.11246989815280987)
w2 ( 0.3364369364912877 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18278922443924925) - present_state_Q (0.18279124423795778)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12281153152921775 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05792380631889998) - present_state_Q ( 0.05792380631889998)) * f1( 0.07510503291210069)
w2 ( 0.3413943079775475 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05792380631889998) - present_state_Q (0.05792380631889998)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15695857883083303 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12507195999846732) - present_state_Q ( 0.12262752701438272)) * f1( 0.669392008620849)
w2 ( 0.3107870881166753 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.12507195999846732) - present_state_Q (0.12262752701438272)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14746637818226163 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15552698679260296) - present_state_Q ( 0.15552698679260296)) * f1( 0.5931672189838162)
w2 ( 0.3235891450676079 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15552698679260296) - present_state_Q (0.15552698679260296)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13928791737195262 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3078386375242919) - present_state_Q ( 0.17927096492167155)) * f1( 0.539786438872408)
w2 ( 0.3357101769740685 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3078386375242919) - present_state_Q (0.17927096492167155)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13916550862417712 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.33431803415571976) - present_state_Q ( 0.33105698544018075)) * f1( 0.5154447584780847)
w2 ( 0.33599515513111544 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.33431803415571976) - present_state_Q (0.33105698544018075)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.13962820814333057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3443322525268475) - present_state_Q ( 0.3456167382975828)) * f1( 0.41373360704803885)
w2 ( 0.3346531335657277 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3443322525268475) - present_state_Q (0.3456167382975828)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.14017085743460403 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.35178869007675917) - present_state_Q ( 0.34981492257919045)) * f1( 0.3707620285905358)
w2 ( 0.3328968071371459 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.35178869007675917) - present_state_Q (0.34981492257919045)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.1428551983199348 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4216296540957664) - present_state_Q ( 0.41275090577473594)) * f1( 0.380283214306065)
w2 ( 0.3230144954860236 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4216296540957664) - present_state_Q (0.41275090577473594)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1448313561176719 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4126969446018533) - present_state_Q ( 0.4126969446018533)) * f1( 0.27666720947784024)
w2 ( 0.3130146804661901 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4126969446018533) - present_state_Q (0.4126969446018533)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1463272772511793 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.40440370438237816) - present_state_Q ( 0.4042863347851555)) * f1( 0.2343015958501411)
w2 ( 0.3040762454576216 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.40440370438237816) - present_state_Q (0.4042863347851555)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1474356492773308 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.400023295566988) - present_state_Q ( 0.39749902500518125)) * f1( 0.19277143103721384)
w2 ( 0.296026708094834 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.400023295566988) - present_state_Q (0.39749902500518125)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.14743231290205422 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.39689236802196964) - present_state_Q ( 0.339378950065812)) * f1( 0.10752555250846213)
w2 ( 0.29606394250320023 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.39689236802196964) - present_state_Q (0.339378950065812)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.14643745137408526 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22056816729452464) - present_state_Q ( 0.2221674371977009)) * f1( 0.0995963267198713)
w2 ( 0.30405509286574034 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22056816729452464) - present_state_Q (0.2221674371977009)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1400005415875581 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02891237856012037) - present_state_Q ( -0.02891237856012037)) * f1( 0.19743841680405627)
w2 ( 0.30405509286574034 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02891237856012037) - present_state_Q (-0.02891237856012037)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1322820066070133 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.020776539577832376) - present_state_Q ( 0.022201163373051135)) * f1( 0.2757836131365953)
w2 ( 0.309652622677435 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.020776539577832376) - present_state_Q (0.022201163373051135)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12498085661782984 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.030207848349647383) - present_state_Q ( -0.029081707967831064)) * f1( 0.21984628683648358)
w2 ( 0.309652622677435 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.030207848349647383) - present_state_Q (-0.029081707967831064)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12062964707394958 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.041243655001176896) - present_state_Q ( 0.041243655001176896)) * f1( 0.16552030522215913)
w2 ( 0.3149102368874138 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.041243655001176896) - present_state_Q (0.041243655001176896)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11723233360998124 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.047075018907596106) - present_state_Q ( 0.047075018907596106)) * f1( 0.13186665845200707)
w2 ( 0.3200628865470771 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.047075018907596106) - present_state_Q (0.047075018907596106)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11439018285508372 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05090438506379833) - present_state_Q ( 0.05090438506379833)) * f1( 0.11181379609167018)
w2 ( 0.32514660761592873 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05090438506379833) - present_state_Q (0.05090438506379833)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14929746980733244 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07093657818806687) - present_state_Q ( 0.06980543700985262)) * f1( 0.526734065220013)
w2 ( 0.2986381364482869 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07093657818806687) - present_state_Q (0.06980543700985262)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17567180970619806 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18770148671512116) - present_state_Q ( 0.18770148671512116)) * f1( 0.3429999350263159)
w2 ( 0.2371236294047982 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.18770148671512116) - present_state_Q (0.18770148671512116)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.19847248199248801 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2352009375973878) - present_state_Q ( 0.2352009375973878)) * f1( 0.2809068670203891)
w2 ( 0.1397219281442803 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2352009375973878) - present_state_Q (0.2352009375973878)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.20303352303263125 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15847634674487826) - present_state_Q ( 0.15826907019483177)) * f1( 0.18814512134016642)
w2 ( 0.10578292717143215 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.15847634674487826) - present_state_Q (0.15826907019483177)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20061236626211126 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11992443212798898) - present_state_Q ( 0.12219593786423211)) * f1( 0.1275659299455207)
w2 ( 0.1323544379202315 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11992443212798898) - present_state_Q (0.12219593786423211)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1945873533108066 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011245364342833908) - present_state_Q ( 0.011245364342833908)) * f1( 0.2078456657591088)
w2 ( 0.14394960480388946 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.011245364342833908) - present_state_Q (0.011245364342833908)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1859900146640741 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0015541750768537504) - present_state_Q ( 0.0015541750768537504)) * f1( 0.2879203909784131)
w2 ( 0.15589365450112272 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0015541750768537504) - present_state_Q (0.0015541750768537504)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18045299287797245 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05275423775220856) - present_state_Q ( 0.05275423775220856)) * f1( 0.21926959370439011)
w2 ( 0.17104492566250346 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05275423775220856) - present_state_Q (0.05275423775220856)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1768351137069792 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07273269903222665) - present_state_Q ( 0.0745714343591645)) * f1( 0.1554727388606381)
w2 ( 0.18500703579514693 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07273269903222665) - present_state_Q (0.0745714343591645)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17434836681876895 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09270742773426499) - present_state_Q ( 0.09086952727522545)) * f1( 0.11386140331397356)
w2 ( 0.198111108725039 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09270742773426499) - present_state_Q (0.09086952727522545)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17321081972895086 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10957342976942597) - present_state_Q ( 0.10904415302866233)) * f1( 0.05633842395880523)
w2 ( 0.21022590012193582 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10957342976942597) - present_state_Q (0.10904415302866233)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.1679381472492296 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.010602283126224736) - present_state_Q ( 0.010602283126224736)) * f1( 0.18152963508495534)
w2 ( 0.21603505902566378 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.010602283126224736) - present_state_Q (0.010602283126224736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16305973454709433 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014643163479437563) - present_state_Q ( 0.014643163479437563)) * f1( 0.17008552728228474)
w2 ( 0.2217714820830339 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.014643163479437563) - present_state_Q (0.014643163479437563)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1578118569341463 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01625791033982566) - present_state_Q ( 0.014546581283027719)) * f1( 0.1828024264627397)
w2 ( 0.227513066278053 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.01625791033982566) - present_state_Q (0.014546581283027719)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15363378584383552 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.021973258234871403) - present_state_Q ( 0.021973258234871403)) * f1( 0.149097510655095)
w2 ( 0.23311754762982534 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.021973258234871403) - present_state_Q (0.021973258234871403)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15039841381190172 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.028505368194358758) - present_state_Q ( 0.028505368194358758)) * f1( 0.11793070926484164)
w2 ( 0.23860445100232688 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.028505368194358758) - present_state_Q (0.028505368194358758)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1477484699111553 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032977370697606395) - present_state_Q ( 0.032977370697606395)) * f1( 0.09802975396601067)
w2 ( 0.24401085832976996 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.032977370697606395) - present_state_Q (0.032977370697606395)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14476161572792323 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.032502738510796006) - present_state_Q ( 0.032502738510796006)) * f1( 0.11031879494223683)
w2 ( 0.24942580903657563 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.032502738510796006) - present_state_Q (0.032502738510796006)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13412366246324076 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039621554891502195) - present_state_Q ( 0.041170100589549966)) * f1( 0.404804980452956)
w2 ( 0.25993749123255966 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.039621554891502195) - present_state_Q (0.041170100589549966)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.12365063964631787 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.054509257419984726) - present_state_Q ( 0.005203422965971684)) * f1( 0.3488129866224191)
w2 ( 0.2659424412880802 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.054509257419984726) - present_state_Q (0.005203422965971684)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.11546138620876671 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06693517766467388) - present_state_Q ( 0.06455727901725478)) * f1( 0.3382085011253934)
w2 ( 0.27562789083804873 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06693517766467388) - present_state_Q (0.06455727901725478)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10928204952938166 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07907254764303466) - present_state_Q ( 0.07907254764303466)) * f1( 0.27003494168873504)
w2 ( 0.28478127912289947 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07907254764303466) - present_state_Q (0.07907254764303466)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10453135024873586 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09018796803655588) - present_state_Q ( 0.09018796803655588)) * f1( 0.21709460716350593)
w2 ( 0.29353451227358346 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09018796803655588) - present_state_Q (0.09018796803655588)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10131288203586834 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1013033027394898) - present_state_Q ( 0.1013033027394898)) * f1( 0.15412124813855468)
w2 ( 0.30188759337496185 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1013033027394898) - present_state_Q (0.1013033027394898)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09964176909250864 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11224630804949262) - present_state_Q ( 0.11224630804949262)) * f1( 0.08398467331607176)
w2 ( 0.3098467262851801 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11224630804949262) - present_state_Q (0.11224630804949262)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09877541851426921 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12040420403645749) - present_state_Q ( 0.11945625022878209)) * f1( 0.044985555014869345)
w2 ( 0.3175500930921747 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12040420403645749) - present_state_Q (0.11945625022878209)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09787884220526392 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05991119998475532) - present_state_Q ( 0.05991119998475532)) * f1( 0.0364343546988843)
w2 ( 0.3224716914924491 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05991119998475532) - present_state_Q (0.05991119998475532)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0931774277874065 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10752235619156242) - present_state_Q ( 0.10646327888117338)) * f1( 0.23013551456368603)
w2 ( 0.3306432497619684 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10752235619156242) - present_state_Q (0.10646327888117338)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0921593799617934 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2506908887370011) - present_state_Q ( 0.2516026985523742)) * f1( 0.13857327427690225)
w2 ( 0.33652056098767447 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2506908887370011) - present_state_Q (0.2516026985523742)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.07925367767078367 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.024511337086970585) - present_state_Q ( 0.024511337086970585)) * f1( 0.4643344511248334)
w2 ( 0.342079356920109 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.024511337086970585) - present_state_Q (0.024511337086970585)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06859771782421119 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.036774280943171006) - present_state_Q ( 0.036774280943171006)) * f1( 0.39924444354858313)
w2 ( 0.3474174198631319 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.036774280943171006) - present_state_Q (0.036774280943171006)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.059095349502795075 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04535532768954939) - present_state_Q ( 0.04442348444635351)) * f1( 0.3653182689035186)
w2 ( 0.35261966082958396 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04535532768954939) - present_state_Q (0.04442348444635351)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05106214716415165 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.051823752920515304) - present_state_Q ( 0.05178921570393877)) * f1( 0.31702522482064205)
w2 ( 0.35768752402134624 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.051823752920515304) - present_state_Q (0.05178921570393877)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04500851529527673 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05901702579395279) - present_state_Q ( 0.05901702579395279)) * f1( 0.245200793653787)
w2 ( 0.36262521755705507 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05901702579395279) - present_state_Q (0.05901702579395279)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04022128398248058 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0636477840378066) - present_state_Q ( 0.0636477840378066)) * f1( 0.19723511018671655)
w2 ( 0.36747955744437455 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0636477840378066) - present_state_Q (0.0636477840378066)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03723258487955699 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06845340388068853) - present_state_Q ( 0.06845340388068853)) * f1( 0.1253691356641615)
w2 ( 0.3722473961745222 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06845340388068853) - present_state_Q (0.06845340388068853)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.034649134760736285 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0708570629988384) - present_state_Q ( 0.070385746984618)) * f1( 0.10914451047200004)
w2 ( 0.3769813953608275 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0708570629988384) - present_state_Q (0.070385746984618)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.029922559412400185 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2076802249866493) - present_state_Q ( 0.13228394591448378)) * f1( 0.534172421841276)
w2 ( 0.38052075842419475 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2076802249866493) - present_state_Q (0.13228394591448378)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.024728904819416444 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21385950471037923) - present_state_Q ( 0.21385950471037923)) * f1( 0.4830118354831713)
w2 ( 0.38697234516983425 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21385950471037923) - present_state_Q (0.21385950471037923)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.015238286902432492 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2186598291967742) - present_state_Q ( 0.14175825939654402)) * f1( 0.5269411955986973)
w2 ( 0.3941766541107596 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2186598291967742) - present_state_Q (0.14175825939654402)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.006568105154074511 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22869880781494079) - present_state_Q ( 0.1500268079002497)) * f1( 0.5016215925711421)
w2 ( 0.40109037702600936 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22869880781494079) - present_state_Q (0.1500268079002497)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.005939898758622214 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3178325480018342) - present_state_Q ( 0.31790023518428895)) * f1( 0.45249982556609936)
w2 ( 0.4022010185952809 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3178325480018342) - present_state_Q (0.31790023518428895)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.005432569695689681 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3193639241417768) - present_state_Q ( 0.3193639241417768)) * f1( 0.40352383632275546)
w2 ( 0.403206816057073 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3193639241417768) - present_state_Q (0.3193639241417768)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.005015197703696652 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3206404373587524) - present_state_Q ( 0.32058943554114416)) * f1( 0.3637352883078647)
w2 ( 0.40412478471265145 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3206404373587524) - present_state_Q (0.32058943554114416)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.004661589035363026 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3216669714783046) - present_state_Q ( 0.32161856565907804)) * f1( 0.33523346643022434)
w2 ( 0.40496863523175164 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3216669714783046) - present_state_Q (0.32161856565907804)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.0022455027753766517 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.32263912818610113) - present_state_Q ( 0.2417370421192186)) * f1( 0.26689161365241065)
w2 ( 0.4104002474737151 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.32263912818610113) - present_state_Q (0.2417370421192186)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0021388658298531885 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3277936148551174) - present_state_Q ( 0.327835821301127)) * f1( 0.2157096767622098)
w2 ( 0.4107957306884659 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3277936148551174) - present_state_Q (0.327835821301127)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.002068995174933513 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3283062744355938) - present_state_Q ( 0.3283062744355938)) * f1( 0.15443236811243832)
w2 ( 0.41115767892910315 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3283062744355938) - present_state_Q (0.3283062744355938)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.006226203604058832 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1637131053126758) - present_state_Q ( 0.08150080434883389)) * f1( 0.35318179850769)
w2 ( 0.4158550890527518 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1637131053126758) - present_state_Q (0.08150080434883389)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.012677689834277611 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16905891683282337) - present_state_Q ( 0.16905891683282337)) * f1( 0.4363624103059389)
w2 ( 0.42176896804677017 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16905891683282337) - present_state_Q (0.16905891683282337)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.019913989405771694 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1751516220770081) - present_state_Q ( 0.1751516220770081)) * f1( 0.5082972483580421)
w2 ( 0.42746350965199786 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1751516220770081) - present_state_Q (0.1751516220770081)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.028273184118006292 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26801586082385065) - present_state_Q ( 0.18252315889345108)) * f1( 0.5793793899130983)
w2 ( 0.43323464673955525 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26801586082385065) - present_state_Q (0.18252315889345108)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.031441274270652436 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27789214073857577) - present_state_Q ( 0.27789214073857577)) * f1( 0.6349250448735263)
w2 ( 0.43622847113967217 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.27789214073857577) - present_state_Q (0.27789214073857577)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.034404370917502716 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28183949537917247) - present_state_Q ( 0.28183949537917247)) * f1( 0.6393638032073301)
w2 ( 0.43900913838919686 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28183949537917247) - present_state_Q (0.28183949537917247)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03716115719990647 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28538336962492905) - present_state_Q ( 0.28538336962492905)) * f1( 0.6388108837714558)
w2 ( 0.4415984364294507 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28538336962492905) - present_state_Q (0.28538336962492905)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0397257741410553 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28866643445751533) - present_state_Q ( 0.28866643445751533)) * f1( 0.6379610966448748)
w2 ( 0.44401044896874486 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28866643445751533) - present_state_Q (0.28866643445751533)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03573088842038394 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2917630607247783) - present_state_Q ( 0.2917630607247783)) * f1( 0.6382957133446003)
w2 ( 0.44025524368960683 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2917630607247783) - present_state_Q (0.2917630607247783)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0320067962885973 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2869832540257499) - present_state_Q ( 0.2869832540257499)) * f1( 0.6389459882268577)
w2 ( 0.43675814797221635 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.2869832540257499) - present_state_Q (0.2869832540257499)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03492937054513253 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28250214098427) - present_state_Q ( 0.28250214098427)) * f1( 0.6388409516707758)
w2 ( 0.4395030323590658 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28250214098427) - present_state_Q (0.28250214098427)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03764779403348467 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.285983399712618) - present_state_Q ( 0.285983399712618)) * f1( 0.6379038599733217)
w2 ( 0.44205992877458444 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.285983399712618) - present_state_Q (0.285983399712618)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.01732085684665175 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2892892337998343) - present_state_Q ( 0.2892892337998343)) * f1( 0.63890268082348)
w2 ( 0.3904383101493934 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2892892337998343) - present_state_Q (0.2892892337998343)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06050739434524307 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22494115175519794) - present_state_Q ( 0.22494115175519794)) * f1( 0.538185519167322)
w2 ( 0.3422914879546127 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.22494115175519794) - present_state_Q (0.22494115175519794)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09704684985812168 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17623061065926376) - present_state_Q ( 0.17623061065926376)) * f1( 0.4816648019449069)
w2 ( 0.2967750349790125 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.17623061065926376) - present_state_Q (0.17623061065926376)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.12696872009577437 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13694922496126838) - present_state_Q ( 0.1379723267902478)) * f1( 0.4131272087221123)
w2 ( 0.2533183907213652 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.13694922496126838) - present_state_Q (0.1379723267902478)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.15009172737840507 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10397658672159715) - present_state_Q ( 0.05584079897435232)) * f1( 0.3582501050643228)
w2 ( 0.2275006651092775 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.10397658672159715) - present_state_Q (0.05584079897435232)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17464650572308119 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.132082541610365) - present_state_Q ( 0.130629468273755)) * f1( 0.3422644586143806)
w2 ( 0.17010696798026 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.132082541610365) - present_state_Q (0.130629468273755)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.19524598490647854 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08274559585856092) - present_state_Q ( 0.08274559585856092)) * f1( 0.305416809255965)
w2 ( 0.11614928507844363 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.08274559585856092) - present_state_Q (0.08274559585856092)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.21112417255221066 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.044482112821069654) - present_state_Q ( 0.044482112821069654)) * f1( 0.2480835406929694)
w2 ( 0.06494657295532662 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.044482112821069654) - present_state_Q (0.044482112821069654)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.21955869230664013 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008250325189954562) - present_state_Q ( 0.008250325189954562)) * f1( 0.20702003302581604)
w2 ( 0.032352549541649886 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.008250325189954562) - present_state_Q (0.008250325189954562)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.21802358446874776 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006075250443873945) - present_state_Q ( -0.006075250443873945)) * f1( 0.1455523793727176)
w2 ( 0.04078996757360881 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.006075250443873945) - present_state_Q (-0.006075250443873945)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.21649746475649234 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014807754555201374) - present_state_Q ( 0.014807754555201374)) * f1( 0.08175363021903101)
w2 ( 0.05572380924563431 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.014807754555201374) - present_state_Q (0.014807754555201374)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.21265187043164957 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.015383049623673081) - present_state_Q ( -0.015383049623673081)) * f1( 0.12253174189654997)
w2 ( 0.06200070413886043 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.015383049623673081) - present_state_Q (-0.015383049623673081)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20874136089343814 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002574844685382282) - present_state_Q ( -0.002695506174042985)) * f1( 0.12929953436936656)
w2 ( 0.07409822500708062 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.002574844685382282) - present_state_Q (-0.002695506174042985)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.206262212919525 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009594541807018957) - present_state_Q ( 0.011745912047113838)) * f1( 0.08572032815697185)
w2 ( 0.08566676669242414 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.009594541807018957) - present_state_Q (0.011745912047113838)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.19753862185467938 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0738341686864369) - present_state_Q ( -0.0738341686864369)) * f1( 0.5240944224989142)
w2 ( 0.09232479676513587 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0738341686864369) - present_state_Q (-0.0738341686864369)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18115392025285804 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.055546779014560586) - present_state_Q ( -0.055546779014560586)) * f1( 0.46814489669086606)
w2 ( 0.10632448080966006 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.055546779014560586) - present_state_Q (-0.055546779014560586)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1673901835493781 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03527446432531568) - present_state_Q ( -0.03311985288492707)) * f1( 0.4175987199349476)
w2 ( 0.11950817706775588 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03527446432531568) - present_state_Q (-0.03311985288492707)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.15751857463177463 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03281284719198374) - present_state_Q ( 0.03419772105740918)) * f1( 0.3668603456586847)
w2 ( 0.14103486216069902 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03281284719198374) - present_state_Q (0.03419772105740918)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.14966444930334374 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.061705758320510146) - present_state_Q ( 0.06213215764285841)) * f1( 0.3218397081373442)
w2 ( 0.16055793561583442 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.061705758320510146) - present_state_Q (0.06213215764285841)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1430884494551246 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08753995971446046) - present_state_Q ( 0.08454926438783975)) * f1( 0.29330334831791655)
w2 ( 0.17849431414252293 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08753995971446046) - present_state_Q (0.08454926438783975)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13913371824107731 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11411405467324812) - present_state_Q ( 0.11411405467324812)) * f1( 0.20044522636165185)
w2 ( 0.19427810220604907 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11411405467324812) - present_state_Q (0.11411405467324812)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.13670401812374716 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13471857587249847) - present_state_Q ( 0.13633787646274761)) * f1( 0.13716736347851866)
w2 ( 0.20844882069600926 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13471857587249847) - present_state_Q (0.13633787646274761)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.155710553460386 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0017272175736721754) - present_state_Q ( -0.0017272175736721754)) * f1( 0.3175984313319315)
w2 ( 0.19647991061233536 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.0017272175736721754) - present_state_Q (-0.0017272175736721754)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17023742562383923 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.001691611701699) - present_state_Q ( 0.001691611701699)) * f1( 0.2415017452900835)
w2 ( 0.18444946160170478 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.001691611701699) - present_state_Q (0.001691611701699)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.184262517784091 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07316265379802338) - present_state_Q ( 0.0749052209499233)) * f1( 0.2100857427797669)
w2 ( 0.14439412426749754 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07316265379802338) - present_state_Q (0.0749052209499233)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19456948128440155 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05557964629006225) - present_state_Q ( 0.05750549456487254)) * f1( 0.15809498505691827)
w2 ( 0.10527727247134555 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.05557964629006225) - present_state_Q (0.05750549456487254)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2011252114868342 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03931099874145216) - present_state_Q ( 0.02145511263789827)) * f1( 0.10616154298344173)
w2 ( 0.08057631196079543 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.03931099874145216) - present_state_Q (0.02145511263789827)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23779180068062453 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.026042163122540152) - present_state_Q ( -0.02739382121562821)) * f1( 0.5481400897805936)
w2 ( 0.02814572718276155 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.026042163122540152) - present_state_Q (-0.02739382121562821)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.26370482787987415 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07541362855697298) - present_state_Q ( -0.07666682216046791)) * f1( 0.48811960666477106)
w2 ( -0.046176708514570564 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07541362855697298) - present_state_Q (-0.07666682216046791)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2821226729363463 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17695109937206943) - present_state_Q ( -0.17414820462815622)) * f1( 0.41524007576242966)
w2 ( -0.10827327525783767 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.17695109937206943) - present_state_Q (-0.17414820462815622)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.28883921294838366 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2068922849460437) - present_state_Q ( -0.22854693999761125)) * f1( 0.3495607377520376)
w2 ( -0.13133034987747685 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.2068922849460437) - present_state_Q (-0.22854693999761125)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2849879032001348 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.2168731176670201) - present_state_Q ( -0.24662999027201402)) * f1( 0.3082461328924626)
w2 ( -0.11633722845683941 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.2168731176670201) - present_state_Q (-0.24662999027201402)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2740757671825889 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.15966607996486035) - present_state_Q ( -0.18293352565622822)) * f1( 0.23368113681871294)
w2 ( -0.0696405366908652 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.15966607996486035) - present_state_Q (-0.18293352565622822)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.26526220915586984 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1249464335834152) - present_state_Q ( -0.12780399208158433)) * f1( 0.21221670193107847)
w2 ( -0.028109601818540914 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1249464335834152) - present_state_Q (-0.12780399208158433)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.25978600168389887 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06773573318016272) - present_state_Q ( -0.07335765354387092)) * f1( 0.1493847596599681)
w2 ( 0.015880487808561654 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06773573318016272) - present_state_Q (-0.07335765354387092)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2600369098616055 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1043772505653898) - present_state_Q ( -0.1043772505653898)) * f1( 0.4140074808879439)
w2 ( 0.01575927831873867 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1043772505653898) - present_state_Q (-0.1043772505653898)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2716318910991645 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09620100030894879) - present_state_Q ( -0.09620100030894879)) * f1( 0.3699513286792557)
w2 ( 0.01575927831873867 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09620100030894879) - present_state_Q (-0.09620100030894879)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2822711247385048 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08920224322678046) - present_state_Q ( -0.08647414076325763)) * f1( 0.32995388009976206)
w2 ( 0.00931035664755026 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.08920224322678046) - present_state_Q (-0.08647414076325763)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2914193981998093 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07507631000106935) - present_state_Q ( -0.07604362577998278)) * f1( 0.2759959850008195)
w2 ( 0.002681076543147776 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.07507631000106935) - present_state_Q (-0.07604362577998278)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.308939379599345 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09948367930248768) - present_state_Q ( -0.09948367930248768)) * f1( 0.3432163240641223)
w2 ( -0.007528217229407446 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09948367930248768) - present_state_Q (-0.09948367930248768)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32252243555540294 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08035336259719407) - present_state_Q ( -0.08114992222778329)) * f1( 0.25779905069140197)
w2 ( -0.01806592551004617 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08035336259719407) - present_state_Q (-0.08114992222778329)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3349776152005894 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0838038494163317) - present_state_Q ( -0.0838038494163317)) * f1( 0.23743303029583737)
w2 ( -0.03904898693105823 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.0838038494163317) - present_state_Q (-0.0838038494163317)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21478689758331174 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00879551608232567) - present_state_Q ( -0.010810441953142035)) * f1( 0.21854293131092767)
w2 ( 0.07006400230622051 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.00879551608232567) - present_state_Q (-0.010810441953142035)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.22550176499731195 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02212957677499594) - present_state_Q ( -0.01783535609897184)) * f1( 0.2787588915170222)
w2 ( 0.04700134621150884 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.02212957677499594) - present_state_Q (-0.01783535609897184)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.21814073731897352 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02679773203743111) - present_state_Q ( -0.0316566475147431)) * f1( 0.2237551710513135)
w2 ( 0.060160421183948834 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02679773203743111) - present_state_Q (-0.0316566475147431)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21291027538354512 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004655631609586854) - present_state_Q ( -0.001771469115363286)) * f1( 0.17359307707097826)
w2 ( 0.07823877554121311 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.004655631609586854) - present_state_Q (-0.001771469115363286)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.20728749351089604 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03308715928135023) - present_state_Q ( 0.020291537313674224)) * f1( 0.1986728120242026)
w2 ( 0.10088014983036998 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03308715928135023) - present_state_Q (0.020291537313674224)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.2037539910168124 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06675218379501875) - present_state_Q ( 0.06994042728563162)) * f1( 0.14925995785226678)
w2 ( 0.12455362893975701 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06675218379501875) - present_state_Q (0.06994042728563162)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2009481918089206 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13875944484109606) - present_state_Q ( 0.1412564178796653)) * f1( 0.16254239963948383)
w2 ( 0.1487203626643792 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13875944484109606) - present_state_Q (0.1412564178796653)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.19970251890418655 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1867951809905356) - present_state_Q ( 0.1889180562894046)) * f1( 0.09599713870065263)
w2 ( 0.16688696731773006 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1867951809905356) - present_state_Q (0.1889180562894046)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.24026312848722342 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08184229983894864) - present_state_Q ( -0.08184229983894864)) * f1( 0.645985311455903)
w2 ( 0.05408625088372736 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08184229983894864) - present_state_Q (-0.08184229983894864)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.24172849672406038 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11237361924057984) - present_state_Q ( -0.11237361924057984)) * f1( 0.7109413005995553)
w2 ( 0.065712216985085 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.11237361924057984) - present_state_Q (-0.11237361924057984)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24010953353459993 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12429951401082583) - present_state_Q ( -0.1265540087874943)) * f1( 0.6966247481405621)
w2 ( 0.07594924784015236 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12429951401082583) - present_state_Q (-0.1265540087874943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.27151461442939345 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12145705802685328) - present_state_Q ( -0.12414029508112961)) * f1( 0.6435396043736187)
w2 ( 0.05642903141129013 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12145705802685328) - present_state_Q (-0.12414029508112961)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2878037312439787 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1346492829131181) - present_state_Q ( -0.137960715866206)) * f1( 0.591247468457975)
w2 ( 0.0454088629142859 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1346492829131181) - present_state_Q (-0.137960715866206)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.28662249659856287 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13196275662521761) - present_state_Q ( -0.1353428306005962)) * f1( 0.5333717360188748)
w2 ( 0.046294725111808877 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.13196275662521761) - present_state_Q (-0.1353428306005962)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3104422412880193 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12305001465716943) - present_state_Q ( -0.12022541143690492)) * f1( 0.48406284617619977)
w2 ( 0.026611541510656395 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12305001465716943) - present_state_Q (-0.12022541143690492)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.29271249654045295 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12705254978464234) - present_state_Q ( -0.12338016838352187)) * f1( 0.4317221278641656)
w2 ( 0.0430385380468587 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.12705254978464234) - present_state_Q (-0.12338016838352187)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.276896492569906 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09115936286729373) - present_state_Q ( -0.09435126554855527)) * f1( 0.4105543487107745)
w2 ( 0.06615265780256827 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09115936286729373) - present_state_Q (-0.09435126554855527)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.26691495756161804 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07520498163874373) - present_state_Q ( -0.08601284366654895)) * f1( 0.35841326232042237)
w2 ( 0.07172250471262176 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.07520498163874373) - present_state_Q (-0.08601284366654895)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26810108326560633 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06504543603933349) - present_state_Q ( -0.06810673176417753)) * f1( 0.3089045044906028)
w2 ( 0.07095454847582665 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.06504543603933349) - present_state_Q (-0.06810673176417753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23817869044576523 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09893036101180167) - present_state_Q ( -0.11621633111400835)) * f1( 0.646505642332659)
w2 ( 0.0757932325926807 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09893036101180167) - present_state_Q (-0.11621633111400835)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23946512350219665 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12277381566257974) - present_state_Q ( -0.12277381566257974)) * f1( 0.6782976242752256)
w2 ( 0.07587669537435057 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12277381566257974) - present_state_Q (-0.12277381566257974)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.22563235242239013 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.004970625331396306) - present_state_Q ( 0.002124772751781953)) * f1( 0.3219622230469682)
w2 ( 0.037536549874932754 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.004970625331396306) - present_state_Q (0.002124772751781953)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.23388261412988492 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014495105825916307) - present_state_Q ( -0.016826876579433624)) * f1( 0.14112114768806977)
w2 ( 0.014151644514806428 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.014495105825916307) - present_state_Q (-0.016826876579433624)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2394034819303488 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11847852392068699) - present_state_Q ( -0.12258189531680248)) * f1( 0.6773671564834729)
w2 ( 0.07588144755091882 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.11847852392068699) - present_state_Q (-0.12258189531680248)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2695468735157326 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13156668835366783) - present_state_Q ( -0.13609068810255212)) * f1( 0.6318495303119482)
w2 ( 0.06634012793626252 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.13156668835366783) - present_state_Q (-0.13609068810255212)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24433867223017194 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13985222288575538) - present_state_Q ( -0.14454496127959227)) * f1( 0.5854751153610904)
w2 ( 0.07495132271608286 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13985222288575538) - present_state_Q (-0.14454496127959227)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2219443709606061 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11578247282258634) - present_state_Q ( -0.1192351898953553)) * f1( 0.5493418344850822)
w2 ( 0.08310446156834479 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11578247282258634) - present_state_Q (-0.1192351898953553)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20161988152144758 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08338786085735589) - present_state_Q ( -0.0861530272264861)) * f1( 0.537949267814573)
w2 ( 0.09821703121397482 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08338786085735589) - present_state_Q (-0.0861530272264861)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18212154022383611 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06693896464596776) - present_state_Q ( -0.06916900162053759)) * f1( 0.5379222192162155)
w2 ( 0.11271603542021245 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06693896464596776) - present_state_Q (-0.06916900162053759)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.16232858105999506 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07351179535567284) - present_state_Q ( -0.07539862710825447)) * f1( 0.5377828129057208)
w2 ( 0.1200769843716662 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07351179535567284) - present_state_Q (-0.07539862710825447)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1431587257371998 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06181544614882063) - present_state_Q ( -0.06315643276816428)) * f1( 0.5370085112139289)
w2 ( 0.12721648213473183 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06181544614882063) - present_state_Q (-0.06315643276816428)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1244896062790697 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05049465695799686) - present_state_Q ( -0.05166391475569124)) * f1( 0.5386134221687984)
w2 ( 0.13414877111592965 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05049465695799686) - present_state_Q (-0.05166391475569124)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10641448474005474 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03908752636900944) - present_state_Q ( -0.04010111648645496)) * f1( 0.5376422394621543)
w2 ( 0.14087261839292073 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03908752636900944) - present_state_Q (-0.04010111648645496)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08886092057355058 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.028256815486137656) - present_state_Q ( -0.029079998071191986)) * f1( 0.5380331623992288)
w2 ( 0.1473977047233723 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.028256815486137656) - present_state_Q (-0.029079998071191986)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07171025047749391 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.011238596874768518) - present_state_Q ( -0.018240944069905944)) * f1( 0.5370244276850806)
w2 ( 0.15378500079851995 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.011238596874768518) - present_state_Q (-0.018240944069905944)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.056662099249731625 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.023485311539587286) - present_state_Q ( 0.02289861264859911)) * f1( 0.5384918810585974)
w2 ( 0.16496299753873434 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.023485311539587286) - present_state_Q (0.02289861264859911)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.042251428109292305 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03599317662179166) - present_state_Q ( 0.0355256976857269)) * f1( 0.5375639401484249)
w2 ( 0.17568594233779242 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03599317662179166) - present_state_Q (0.0355256976857269)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.028417276174990132 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04788514737418413) - present_state_Q ( 0.047551636698159636)) * f1( 0.5377981586369137)
w2 ( 0.18597541745936277 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04788514737418413) - present_state_Q (0.047551636698159636)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.013292854123759992 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02210107279275446) - present_state_Q ( 0.02186420560663801)) * f1( 0.5394914625465461)
w2 ( 0.19158233549281553 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.02210107279275446) - present_state_Q (0.02186420560663801)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.001288817907378467 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0312883536359243) - present_state_Q ( 0.0311887199152945)) * f1( 0.5362089372912239)
w2 ( 0.1970211378017815 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0312883536359243) - present_state_Q (0.0311887199152945)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01551266333447447 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04010604722053482) - present_state_Q ( 0.040098851671876615)) * f1( 0.538962181968139)
w2 ( 0.20229937286278502 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04010604722053482) - present_state_Q (0.040098851671876615)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.029265235393366625 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04896161028775672) - present_state_Q ( 0.04878997481280617)) * f1( 0.5369871092178488)
w2 ( 0.2074214965871044 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04896161028775672) - present_state_Q (0.04878997481280617)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.04267710583913928 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.05742325520940755) - present_state_Q ( 0.05728162865832265)) * f1( 0.5397984717554143)
w2 ( 0.21239071052435676 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.05742325520940755) - present_state_Q (0.05728162865832265)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05561185444993889 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0658444428152155) - present_state_Q ( 0.06536234908500024)) * f1( 0.5362174058003183)
w2 ( 0.21721515242828718 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0658444428152155) - present_state_Q (0.06536234908500024)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06845294884495867 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11685673127112854) - present_state_Q ( 0.0734137007854711)) * f1( 0.5389259285858359)
w2 ( 0.22198059187512 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11685673127112854) - present_state_Q (0.0734137007854711)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07851445555970854 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12638575726385262) - present_state_Q ( 0.12561948208918233)) * f1( 0.5379935555814485)
w2 ( 0.22946135562060813 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12638575726385262) - present_state_Q (0.12561948208918233)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.08818747093674478 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13454427578783174) - present_state_Q ( 0.1341383257810424)) * f1( 0.5394393074609046)
w2 ( 0.23663399969251775 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13454427578783174) - present_state_Q (0.1341383257810424)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.09975016236603179 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09577004378024104) - present_state_Q ( 0.09480414493528369)) * f1( 0.5383683701603682)
w2 ( 0.24092945688137254 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09577004378024104) - present_state_Q (0.09480414493528369)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11095599212346452 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10291916329529968) - present_state_Q ( 0.10179822525503035)) * f1( 0.537466131453763)
w2 ( 0.24509933070286255 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10291916329529968) - present_state_Q (0.10179822525503035)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12186849662612931 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10950932542654784) - present_state_Q ( 0.10896502726300823)) * f1( 0.5402606923268524)
w2 ( 0.24913904880845547 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10950932542654784) - present_state_Q (0.10896502726300823)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13244039377609626 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1161465336835299) - present_state_Q ( 0.11553476249467448)) * f1( 0.5391627414143011)
w2 ( 0.25306064662592903 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1161465336835299) - present_state_Q (0.11553476249467448)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1426891707820167 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1233436800258543) - present_state_Q ( 0.1218820418402076)) * f1( 0.5381282136287717)
w2 ( 0.25686969314917657 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1233436800258543) - present_state_Q (0.1218820418402076)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1526226902155812 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12958546477613897) - present_state_Q ( 0.12801303160095318)) * f1( 0.5371051815011092)
w2 ( 0.2605686034467098 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12958546477613897) - present_state_Q (0.12801303160095318)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16228643260637277 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13524107645640557) - present_state_Q ( 0.13449931162534656)) * f1( 0.5397991007735093)
w2 ( 0.26414909936711567 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13524107645640557) - present_state_Q (0.13449931162534656)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17163433667823216 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14170362110004092) - present_state_Q ( 0.13986135003323633)) * f1( 0.536283463516072)
w2 ( 0.267635279608651 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14170362110004092) - present_state_Q (0.13986135003323633)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18072458612888612 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14696316328337783) - present_state_Q ( 0.1460281424571051)) * f1( 0.5389427798983449)
w2 ( 0.2710086430860757 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14696316328337783) - present_state_Q (0.1460281424571051)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1895352653678959 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1532410184927252) - present_state_Q ( 0.15124933797355128)) * f1( 0.5369917366258367)
w2 ( 0.2742901383635901 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1532410184927252) - present_state_Q (0.15124933797355128)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19809813966659875 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15806723803074965) - present_state_Q ( 0.15715634021358027)) * f1( 0.5397323413260163)
w2 ( 0.27746314603538 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15806723803074965) - present_state_Q (0.15715634021358027)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.20639198466439138 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16393223045936028) - present_state_Q ( 0.16170808924013497)) * f1( 0.53617595910704)
w2 ( 0.28055684871149605 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16393223045936028) - present_state_Q (0.16170808924013497)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.21444901122368554 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16842250321982669) - present_state_Q ( 0.16734330535317604)) * f1( 0.5389353457293808)
w2 ( 0.2835468276108722 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16842250321982669) - present_state_Q (0.16734330535317604)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.22226991157208595 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17449227905348325) - present_state_Q ( 0.172096790482672)) * f1( 0.5380646163956443)
w2 ( 0.28645387635932573 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17449227905348325) - present_state_Q (0.172096790482672)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2298570282408247 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1783348532870751) - present_state_Q ( 0.17723021709583486)) * f1( 0.5396116864205945)
w2 ( 0.2892659417239832 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1783348532870751) - present_state_Q (0.17723021709583486)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.23722302748181495 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18417878172814878) - present_state_Q ( 0.18163765354993583)) * f1( 0.538528084838234)
w2 ( 0.29200154621644075 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18417878172814878) - present_state_Q (0.18163765354993583)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2443639624277549 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1874944229860001) - present_state_Q ( 0.18623485410492488)) * f1( 0.5388791561200199)
w2 ( 0.29465183798031425 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1874944229860001) - present_state_Q (0.18623485410492488)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.25129953568558955 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19312119647578396) - present_state_Q ( 0.19039504518792028)) * f1( 0.5379871740732816)
w2 ( 0.2972301794695074 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19312119647578396) - present_state_Q (0.19039504518792028)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.25802182411645674 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19626143672581112) - present_state_Q ( 0.19504449306513225)) * f1( 0.5395889682059866)
w2 ( 0.2997218124816564 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19626143672581112) - present_state_Q (0.19504449306513225)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.26455414673845923 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20110893123180612) - present_state_Q ( 0.19820307447279117)) * f1( 0.5358411539407518)
w2 ( 0.30215996885466423 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20110893123180612) - present_state_Q (0.19820307447279117)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2708926743920027 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20553349009616717) - present_state_Q ( 0.20261463959635423)) * f1( 0.5374425144278102)
w2 ( 0.3045187430429295 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20553349009616717) - present_state_Q (0.20261463959635423)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23589802605581883 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0019011576304739514) - present_state_Q ( -0.019706700904205426)) * f1( 0.5108714102555563)
w2 ( 0.02765644835914942 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.0019011576304739514) - present_state_Q (-0.019706700904205426)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2610221864533099 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08473827475633865) - present_state_Q ( -0.09280873880285562)) * f1( 0.4872185639356693)
w2 ( -0.01359675873467285 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08473827475633865) - present_state_Q (-0.09280873880285562)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.23091985028007184 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.018675189949900167) - present_state_Q ( -0.018675189949900167)) * f1( 0.4228045557615765)
w2 ( 0.03901138036881694 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.018675189949900167) - present_state_Q (-0.018675189949900167)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.25582335896626246 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07918359837961417) - present_state_Q ( -0.08698587445337755)) * f1( 0.47805635825927306)
w2 ( 0.007755431245741906 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07918359837961417) - present_state_Q (-0.08698587445337755)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.28133399462767145 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1285498952942922) - present_state_Q ( -0.1285498952942922)) * f1( 0.526747208837395)
w2 ( -0.030988976293069063 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1285498952942922) - present_state_Q (-0.1285498952942922)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3066054005265202 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.16626837570028832) - present_state_Q ( -0.17246617095890215)) * f1( 0.5689699200891747)
w2 ( -0.04875540295751413 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.16626837570028832) - present_state_Q (-0.17246617095890215)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3320534256462789 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.19989798177796117) - present_state_Q ( -0.209649062369464)) * f1( 0.6201681407435332)
w2 ( -0.06516903238984742 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.19989798177796117) - present_state_Q (-0.209649062369464)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2310359353312343 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.037298408049891686) - present_state_Q ( -0.03909956465454186)) * f1( 0.438760078198645)
w2 ( 0.05178895012339729 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.037298408049891686) - present_state_Q (-0.03909956465454186)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2586998303328479 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1059305124419006) - present_state_Q ( -0.1059305124419006)) * f1( 0.5481662076061374)
w2 ( 0.031602448571305716 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1059305124419006) - present_state_Q (-0.1059305124419006)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23608847427612636 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08418380153434361) - present_state_Q ( -0.10509478680492726)) * f1( 0.5925861960527906)
w2 ( 0.075600294825454 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08418380153434361) - present_state_Q (-0.10509478680492726)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2640578898926677 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08197733669116566) - present_state_Q ( -0.09958990641101136)) * f1( 0.5499210613278191)
w2 ( 0.055255981735129786 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08197733669116566) - present_state_Q (-0.09958990641101136)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2383536773888035 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0939842652761009) - present_state_Q ( -0.09421737621468848)) * f1( 0.6229162437124987)
w2 ( 0.06505952467990728 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.0939842652761009) - present_state_Q (-0.09421737621468848)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2647604025644945 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1030378298119818) - present_state_Q ( -0.11374269089888753)) * f1( 0.5317920714439256)
w2 ( 0.05512830283826106 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.1030378298119818) - present_state_Q (-0.11374269089888753)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.22479413689328484 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04801697403460804) - present_state_Q ( -0.05201513773099707)) * f1( 0.33524555996333066)
w2 ( 0.07461103549897485 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.04801697403460804) - present_state_Q (-0.05201513773099707)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2406934743792032 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.045306939492237894) - present_state_Q ( -0.049468360029539776)) * f1( 0.28644237798738714)
w2 ( 0.06350978882058117 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.045306939492237894) - present_state_Q (-0.049468360029539776)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24030990856878046 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07858267883953032) - present_state_Q ( -0.08219886332856727)) * f1( 0.6477140017699051)
w2 ( 0.05412720241910099 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.07858267883953032) - present_state_Q (-0.08219886332856727)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2719583767325433 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10794512622998909) - present_state_Q ( -0.10794512622998909)) * f1( 0.6293826545316198)
w2 ( 0.0138992515076602 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.10794512622998909) - present_state_Q (-0.10794512622998909)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.22503370856082322 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03182263016940809) - present_state_Q ( -0.03372526146518501)) * f1( 0.3296385080900995)
w2 ( 0.0628884866303539 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.03182263016940809) - present_state_Q (-0.03372526146518501)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2494131756644237 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09141873382418585) - present_state_Q ( -0.09390061332978296)) * f1( 0.4731660484859061)
w2 ( 0.05258366142930119 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09141873382418585) - present_state_Q (-0.09390061332978296)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.22525357160362702 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.034593059438440224) - present_state_Q ( -0.034593059438440224)) * f1( 0.33384576428585155)
w2 ( 0.06291211683220799 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.034593059438440224) - present_state_Q (-0.034593059438440224)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23062672205583867 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08650895684304746) - present_state_Q ( -0.08650895684304746)) * f1( 0.4399103619269471)
w2 ( 0.06046927805538284 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.08650895684304746) - present_state_Q (-0.08650895684304746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.25524178751659743 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08408263798256388) - present_state_Q ( -0.08408263798256388)) * f1( 0.46946142337531427)
w2 ( 0.03949625302275514 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08408263798256388) - present_state_Q (-0.08408263798256388)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.27934487513349804 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0884439999163269) - present_state_Q ( -0.09634325052087792)) * f1( 0.4703030937938608)
w2 ( 0.008746184054509849 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.0884439999163269) - present_state_Q (-0.09634325052087792)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.30287383554059655 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13045701546898106) - present_state_Q ( -0.1287077786580791)) * f1( 0.48579636850983615)
w2 ( -0.030000849776595676 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.13045701546898106) - present_state_Q (-0.1287077786580791)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.3048413524007507 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.17029618047566786) - present_state_Q ( -0.176296350430987)) * f1( 0.48302455837187097)
w2 ( -0.034074176538253655 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.17029618047566786) - present_state_Q (-0.176296350430987)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.29329285334558053 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1578769751513479) - present_state_Q ( -0.15861036953185395)) * f1( 0.4755939368946175)
w2 ( -0.024361269657584887 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1578769751513479) - present_state_Q (-0.15861036953185395)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2821847644942713 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14578301109596845) - present_state_Q ( -0.14578301109596845)) * f1( 0.48044388247817077)
w2 ( -0.019737175457857456 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.14578301109596845) - present_state_Q (-0.14578301109596845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2615824214473793 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14032489093253353) - present_state_Q ( -0.14032489093253353)) * f1( 0.48329135020941444)
w2 ( -0.011211327421071851 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.14032489093253353) - present_state_Q (-0.14032489093253353)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24189719979585975 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12663125907493725) - present_state_Q ( -0.12663125907493725)) * f1( 0.47552504829054554)
w2 ( -0.0029319647577229786 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.12663125907493725) - present_state_Q (-0.12663125907493725)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2225664225927854 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11617179671041912) - present_state_Q ( -0.11617179671041912)) * f1( 0.4778286142064421)
w2 ( 0.005159127583064565 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11617179671041912) - present_state_Q (-0.11617179671041912)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20330638262489295 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1071081172786917) - present_state_Q ( -0.1071081172786917)) * f1( 0.4858771666252681)
w2 ( 0.013087073694081014 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1071081172786917) - present_state_Q (-0.1071081172786917)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.18463374624540635 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09569864776713671) - present_state_Q ( -0.09569864776713671)) * f1( 0.4835857154930021)
w2 ( 0.020809649353889476 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09569864776713671) - present_state_Q (-0.09569864776713671)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16674154310342287 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08382722603605752) - present_state_Q ( -0.08382722603605752)) * f1( 0.47656052967632706)
w2 ( 0.028318539422538515 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08382722603605752) - present_state_Q (-0.08382722603605752)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14804730872473257 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07844572973063081) - present_state_Q ( -0.07844572973063081)) * f1( 0.5044300061621051)
w2 ( 0.035730562557689874 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07844572973063081) - present_state_Q (-0.07844572973063081)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23796128655027252 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11315958300735497) - present_state_Q ( -0.11446073664072985)) * f1( 0.6379941731283434)
w2 ( 0.07572966225922402 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.11315958300735497) - present_state_Q (-0.11446073664072985)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2643546794082493 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09562444686052542) - present_state_Q ( -0.11077037931237022)) * f1( 0.5291462052068436)
w2 ( 0.06575382095175038 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09562444686052542) - present_state_Q (-0.11077037931237022)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2255225011959082 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01649041442958877) - present_state_Q ( -0.01649041442958877)) * f1( 0.3291464465744451)
w2 ( 0.05055724907162192 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.01649041442958877) - present_state_Q (-0.01649041442958877)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2395232492322979 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.004289382716093228) - present_state_Q ( -0.0022255278349030475)) * f1( 0.23404661010154956)
w2 ( -0.0092630919720487 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.004289382716093228) - present_state_Q (-0.0022255278349030475)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.23585019774577703 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06482500405366595) - present_state_Q ( -0.06482500405366595)) * f1( 0.23196876403313732)
w2 ( 0.006571158392781239 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.06482500405366595) - present_state_Q (-0.06482500405366595)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2290644792649379 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04094049207683018) - present_state_Q ( -0.04094049207683018)) * f1( 0.20144842329462126)
w2 ( 0.04025580267969596 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04094049207683018) - present_state_Q (-0.04094049207683018)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2243287847669855 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013370257169286838) - present_state_Q ( 0.010950559879087628)) * f1( 0.16308247990444996)
w2 ( 0.07510217858023689 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.013370257169286838) - present_state_Q (0.010950559879087628)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.22210648868156943 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06687632452336364) - present_state_Q ( 0.06913659830122468)) * f1( 0.09355025935195149)
w2 ( 0.1036083026783703 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06687632452336364) - present_state_Q (0.06913659830122468)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2286367845393 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.033895624571952034) - present_state_Q ( -0.04897472869571057)) * f1( 0.4035709410582034)
w2 ( 0.06349017334196475 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.033895624571952034) - present_state_Q (-0.04897472869571057)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24133593759967267 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03418022478457437) - present_state_Q ( -0.04231131179579042)) * f1( 0.3516731393987415)
w2 ( 0.041823770701004716 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.03418022478457437) - present_state_Q (-0.04231131179579042)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23387265580673816 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04222041327450288) - present_state_Q ( -0.04222041327450288)) * f1( 0.31358541371009346)
w2 ( 0.060863640456768925 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.04222041327450288) - present_state_Q (-0.04222041327450288)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.23867254378069883 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03530400305257085) - present_state_Q ( -0.05286583808804343)) * f1( 0.5885675797018027)
w2 ( 0.04161360171504704 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.03530400305257085) - present_state_Q (-0.05286583808804343)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.2659642179514336 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08222243834369253) - present_state_Q ( -0.08222243834369253)) * f1( 0.5188533129831838)
w2 ( -0.010986378834020628 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.08222243834369253) - present_state_Q (-0.08222243834369253)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2880178395933434 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12745763689026027) - present_state_Q ( -0.12745763689026027)) * f1( 0.4544438741452002)
w2 ( -0.04010366644194658 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12745763689026027) - present_state_Q (-0.12745763689026027)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2683604519479803 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1578692097260405) - present_state_Q ( -0.1658899430144298)) * f1( 0.4367308523322121)
w2 ( 0.004906635762235996 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1578692097260405) - present_state_Q (-0.1658899430144298)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2283495336185349 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02783841030497356) - present_state_Q ( -0.02783841030497356)) * f1( 0.3841637748322987)
w2 ( 0.051170040848892706 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.02783841030497356) - present_state_Q (-0.02783841030497356)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.24766426212962964 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.048434539429227985) - present_state_Q ( -0.04858712760751986)) * f1( 0.3472271252776479)
w2 ( 0.017794661268768518 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.048434539429227985) - present_state_Q (-0.04858712760751986)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2643797234370902 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06465424926028762) - present_state_Q ( -0.0659074469140137)) * f1( 0.3092260587649499)
w2 ( -0.014638817411952387 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.06465424926028762) - present_state_Q (-0.0659074469140137)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.27661487450707106 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06891047787761763) - present_state_Q ( -0.06891047787761763)) * f1( 0.22742737850224587)
w2 ( -0.04691765160656104 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.06891047787761763) - present_state_Q (-0.06891047787761763)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2921806758188203 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09274197289469868) - present_state_Q ( -0.09274197289469868)) * f1( 0.30135198883260195)
w2 ( -0.05724829609445647 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.09274197289469868) - present_state_Q (-0.09274197289469868)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.30016835373775885 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09768121731012543) - present_state_Q ( -0.09768121731012543)) * f1( 0.25594402731382104)
w2 ( -0.06973177227129196 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.09768121731012543) - present_state_Q (-0.09768121731012543)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.29185189135321943 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08926904121101897) - present_state_Q ( -0.09290520598841273)) * f1( 0.2165867796199926)
w2 ( -0.05437264019659953 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08926904121101897) - present_state_Q (-0.09290520598841273)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.28687174654719005 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051556448962510396) - present_state_Q ( -0.0624309770018303)) * f1( 0.1393923497790679)
w2 ( -0.04008162691237636 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.051556448962510396) - present_state_Q (-0.0624309770018303)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2259111443224156 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01615106082509684) - present_state_Q ( -0.01804939524557706)) * f1( 0.33670469388461305)
w2 ( 0.05065282404220817 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.01615106082509684) - present_state_Q (-0.01804939524557706)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23844598405071554 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002852017749725988) - present_state_Q ( -0.012180409711686893)) * f1( 0.3229756494801527)
w2 ( 0.004080248994613872 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.002852017749725988) - present_state_Q (-0.012180409711686893)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.23239024545530987 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05035364478731849) - present_state_Q ( -0.05261173681612639)) * f1( 0.2446008291596169)
w2 ( 0.038740941121849115 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.05035364478731849) - present_state_Q (-0.05261173681612639)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.22696090456377677 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.013280602774623534) - present_state_Q ( 0.010807488947209051)) * f1( 0.18688318237407064)
w2 ( 0.07941382110808458 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.013280602774623534) - present_state_Q (0.010807488947209051)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2239085952407031 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07837837817856105) - present_state_Q ( 0.08068239642039564)) * f1( 0.1343709534007124)
w2 ( 0.11121558290372904 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07837837817856105) - present_state_Q (0.08068239642039564)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.22250667451722617 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13334005624808887) - present_state_Q ( 0.1378173534802146)) * f1( 0.07987394394476095)
w2 ( 0.13578791420397224 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13334005624808887) - present_state_Q (0.1378173534802146)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2219719636600952 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18094929790809042) - present_state_Q ( 0.18139930912337096)) * f1( 0.039116897419256356)
w2 ( 0.15492530109741356 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18094929790809042) - present_state_Q (0.18139930912337096)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.22176549588650105 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.060106384597983266) - present_state_Q ( 0.060106384597983266)) * f1( 0.00839626685393519)
w2 ( 0.16476147125188617 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.060106384597983266) - present_state_Q (0.060106384597983266)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21254244743910125 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0823148176756286) - present_state_Q ( -0.08364773112122537)) * f1( 0.5257807347599203)
w2 ( 0.16826979623895943 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0823148176756286) - present_state_Q (-0.08364773112122537)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19671322090998028 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.03435874821373948) - present_state_Q ( -0.03435874821373948)) * f1( 0.4783358238991452)
w2 ( 0.18150671117465406 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.03435874821373948) - present_state_Q (-0.03435874821373948)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.18434101694394645 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.006882114736705816) - present_state_Q ( -0.006882114736705816)) * f1( 0.40406434727100116)
w2 ( 0.19375446730517545 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.006882114736705816) - present_state_Q (-0.006882114736705816)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.17796027381178572 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12960429849651717) - present_state_Q ( 0.12960429849651717)) * f1( 0.3479972600355392)
w2 ( 0.2120900804404889 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12960429849651717) - present_state_Q (0.12960429849651717)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.17304175249280618 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.15710865479165892) - present_state_Q ( 0.15695501592796007)) * f1( 0.30981669858993793)
w2 ( 0.2279656653956095 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.15710865479165892) - present_state_Q (0.15695501592796007)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.1699276715767809 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18703867427447507) - present_state_Q ( 0.18703867427447507)) * f1( 0.2365151215330871)
w2 ( 0.24113218471090675 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18703867427447507) - present_state_Q (0.18703867427447507)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16784874949590095 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2094481522717682) - present_state_Q ( 0.2094481522717682)) * f1( 0.18645599121754758)
w2 ( 0.2522818510064476 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2094481522717682) - present_state_Q (0.2094481522717682)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16666006490799257 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23286896670828933) - present_state_Q ( 0.23072632143842983)) * f1( 0.12842234233353167)
w2 ( 0.26153790852968756 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23286896670828933) - present_state_Q (0.23072632143842983)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.15897569126679034 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008501098372732677) - present_state_Q ( 0.008501098372732677)) * f1( 0.2628493116055663)
w2 ( 0.26738488875897837 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.008501098372732677) - present_state_Q (0.008501098372732677)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1520351255545525 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014968370082725911) - present_state_Q ( 0.014968370082725911)) * f1( 0.24222953435343308)
w2 ( 0.2731154580974893 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.014968370082725911) - present_state_Q (0.014968370082725911)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.1462814285030851 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.02327539965584717) - present_state_Q ( 0.02327539965584717)) * f1( 0.20618716792786593)
w2 ( 0.2786965009036841 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.02327539965584717) - present_state_Q (0.02327539965584717)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14185981186190325 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03189781299593643) - present_state_Q ( 0.03189781299593643)) * f1( 0.16298369129132187)
w2 ( 0.28412234026975725 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03189781299593643) - present_state_Q (0.03189781299593643)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13839969023128768 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.038336431920268685) - present_state_Q ( 0.038336431920268685)) * f1( 0.13032610075417536)
w2 ( 0.2894322844951924 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.038336431920268685) - present_state_Q (0.038336431920268685)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13478949768383397 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039023155953842675) - present_state_Q ( 0.039023155953842675)) * f1( 0.1362958321197993)
w2 ( 0.2947298676880233 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.039023155953842675) - present_state_Q (0.039023155953842675)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13262440344421647 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0410483719571223) - present_state_Q ( 0.0410483719571223)) * f1( 0.13278187016071144)
w2 ( 0.2979909969927951 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.0410483719571223) - present_state_Q (0.0410483719571223)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13910822025389857 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.039870151566559126) - present_state_Q ( 0.039870151566559126)) * f1( 0.14875126537551409)
w2 ( 0.289273334264597 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.039870151566559126) - present_state_Q (0.039870151566559126)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14719458798368398 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03159937559478085) - present_state_Q ( 0.03159937559478085)) * f1( 0.18874004145993475)
w2 ( 0.28070454550389096 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.03159937559478085) - present_state_Q (0.03159937559478085)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.23718268310608873 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.009741952983963055) - present_state_Q ( 0.009741952983963055)) * f1( 0.2532523614292788)
w2 ( 0.1404107609444635 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.009741952983963055) - present_state_Q (0.009741952983963055)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.23899133035571438 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03503776203914538) - present_state_Q ( 0.0020854677887681278)) * f1( 0.2877775032444578)
w2 ( 0.140818203588492 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.03503776203914538) - present_state_Q (0.0020854677887681278)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24688514011062435 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06667852255628545) - present_state_Q ( -0.0710528222116863)) * f1( 0.4689869181240578)
w2 ( 0.15404917065100732 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.06667852255628545) - present_state_Q (-0.0710528222116863)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.25441341519750654 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0890881705190929) - present_state_Q ( 0.08641670287823426)) * f1( 0.22762959907075553)
w2 ( 0.07306741500585952 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.0890881705190929) - present_state_Q (0.08641670287823426)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.26874040323210946 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0028884611545921396) - present_state_Q ( 0.0028884611545921396)) * f1( 0.362682991758397)
w2 ( 0.11789319374865934 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.0028884611545921396) - present_state_Q (0.0028884611545921396)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.26336945448816285 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.037093403748910236) - present_state_Q ( 0.037093403748910236)) * f1( 0.14139982055986275)
w2 ( 0.009729008668457592 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.037093403748910236) - present_state_Q (0.037093403748910236)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.2714071752723439 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002749361289803076) - present_state_Q ( -0.002749361289803076)) * f1( 0.2734433552052458)
w2 ( 0.1120416592583087 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.002749361289803076) - present_state_Q (-0.002749361289803076)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.26703388501700986 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.006180173620761807) - present_state_Q ( 0.006180173620761807)) * f1( 0.2249197055050881)
w2 ( 0.12370792988278756 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.006180173620761807) - present_state_Q (0.006180173620761807)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2653441437561043 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.01949904752133827) - present_state_Q ( 0.01949904752133827)) * f1( 0.20493919865206728)
w2 ( 0.1286549813166353 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.01949904752133827) - present_state_Q (0.01949904752133827)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2613814828749783 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.037677492395320054) - present_state_Q ( 0.037677492395320054)) * f1( 0.14892168274489034)
w2 ( 0.14462039672728802 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.037677492395320054) - present_state_Q (0.037677492395320054)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.284601931620588 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10993191161087076) - present_state_Q ( -0.10707950213141619)) * f1( 0.4213514654327039)
w2 ( -0.010427538892729248 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.10993191161087076) - present_state_Q (-0.10707950213141619)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3045624588817098 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11953676529143209) - present_state_Q ( -0.11953676529143209)) * f1( 0.405358280871186)
w2 ( -0.030124215342237696 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.11953676529143209) - present_state_Q (-0.11953676529143209)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.32081133773152154 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10512760208589267) - present_state_Q ( -0.11115244515434021)) * f1( 0.3253938761242272)
w2 ( -0.050098627944407656 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.10512760208589267) - present_state_Q (-0.11115244515434021)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3373275937945738 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.12967495720423722) - present_state_Q ( -0.12967495720423722)) * f1( 0.34174448696768067)
w2 ( -0.06943032948505512 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.12967495720423722) - present_state_Q (-0.12967495720423722)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.35231577930789537 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13200759894534156) - present_state_Q ( -0.13307677665840817)) * f1( 0.3121732310120905)
w2 ( -0.08863528881450017 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.13200759894534156) - present_state_Q (-0.13307677665840817)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.21050089422485735 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.061815721763056095) - present_state_Q ( -0.09757168692491078)) * f1( 0.5885675797018027)
w2 ( 0.16858927354685826 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.061815721763056095) - present_state_Q (-0.09757168692491078)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.19084831767153151 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02250366117041544) - present_state_Q ( -0.02644960294730446)) * f1( 0.6061882423127075)
w2 ( 0.18804122775667403 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.02250366117041544) - present_state_Q (-0.02644960294730446)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.17452323711362167 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.035606424926203584) - present_state_Q ( 0.03459576846003025)) * f1( 0.6069595748005286)
w2 ( 0.20955841767928124 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.035606424926203584) - present_state_Q (0.03459576846003025)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1608061469107255 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06947997781131758) - present_state_Q ( 0.06761908104445137)) * f1( 0.5731480503874198)
w2 ( 0.22870473101821567 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06947997781131758) - present_state_Q (0.06761908104445137)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.1499832480234779 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10006503003869446) - present_state_Q ( 0.10006503003869446)) * f1( 0.51551981294534)
w2 ( 0.24550004885542967 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10006503003869446) - present_state_Q (0.10006503003869446)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.14551470812338876 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22764796268368226) - present_state_Q ( 0.2256139352678714)) * f1( 0.45995885719080704)
w2 ( 0.2571581521754893 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.22764796268368226) - present_state_Q (0.2256139352678714)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.14255056999426693 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2506368216095459) - present_state_Q ( 0.2506368216095459)) * f1( 0.3982618784617995)
w2 ( 0.26608937544165834 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2506368216095459) - present_state_Q (0.2506368216095459)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.138740631768617 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26853796817405734) - present_state_Q ( 0.21675855467685307)) * f1( 0.34605839013333467)
w2 ( 0.2770988996557136 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26853796817405734) - present_state_Q (0.21675855467685307)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.13750570930623257 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.28929257699969363) - present_state_Q ( 0.28929257699969363)) * f1( 0.3115605142929761)
w2 ( 0.2818553013397467 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.28929257699969363) - present_state_Q (0.28929257699969363)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.13665124496876144 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2995656063139758) - present_state_Q ( 0.2995656063139758)) * f1( 0.2811574551251595)
w2 ( 0.2855022158578373 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2995656063139758) - present_state_Q (0.2995656063139758)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.280136251850726 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06100835401717134) - present_state_Q ( -0.06100835401717134)) * f1( 0.34406581665023545)
w2 ( 0.1337185470995971 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.06100835401717134) - present_state_Q (-0.06100835401717134)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.29551940720533765 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.051033062192246514) - present_state_Q ( -0.051033062192246514)) * f1( 0.2776390813339297)
w2 ( 0.12263714221905754 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.051033062192246514) - present_state_Q (-0.051033062192246514)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.3081808373791893 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.01718123641576754) - present_state_Q ( -0.014684189947406105)) * f1( 0.21568480878394872)
w2 ( 0.0991557848712907 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.01718123641576754) - present_state_Q (-0.014684189947406105)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13590234375421656 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36328471335030005) - present_state_Q ( 0.30732176031926106)) * f1( 0.25818205109078224)
w2 ( 0.28898302117972957 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.36328471335030005) - present_state_Q (0.30732176031926106)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.31798708657960967 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.014432421799116836) - present_state_Q ( -0.011570760745632923)) * f1( 0.1662435443093804)
w2 ( 0.07556088561391955 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.014432421799116836) - present_state_Q (-0.011570760745632923)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.3258833869676311 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.012418694862490417) - present_state_Q ( -0.012418694862490417)) * f1( 0.13410308439485116)
w2 ( 0.0520079586289692 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.012418694862490417) - present_state_Q (-0.012418694862490417)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.327316916519965 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.002859471992326973) - present_state_Q ( -0.002859471992326973)) * f1( 0.07261080616627134)
w2 ( 0.04411089962069297 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.002859471992326973) - present_state_Q (-0.002859471992326973)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.1367409976149406 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37053541404866785) - present_state_Q ( 0.37053541404866785)) * f1( 0.2504799745361083)
w2 ( 0.2842955590095974 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.37053541404866785) - present_state_Q (0.37053541404866785)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.32549103419159 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.013260091814998248) - present_state_Q ( -0.010507629312234649)) * f1( 0.05905533218963403)
w2 ( 0.050294532023307664 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.013260091814998248) - present_state_Q (-0.010507629312234649)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.32567764872778043 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21661122813210398) - present_state_Q ( -0.21912219299773492)) * f1( 0.7350125830692987)
w2 ( 0.050192974830688644 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.21661122813210398) - present_state_Q (-0.21912219299773492)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13622447650372646 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36663318805001843) - present_state_Q ( 0.3122380521725445)) * f1( 0.21146999907373043)
w2 ( 0.2872265910054923 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.36663318805001843) - present_state_Q (0.3122380521725445)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2953988595445471 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.18949196245303834) - present_state_Q ( -0.18949196245303834)) * f1( 0.6434864449678074)
w2 ( 0.06901468547899803 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.18949196245303834) - present_state_Q (-0.18949196245303834)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.13685426488701213 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38147618449737086) - present_state_Q ( 0.3829816170414242)) * f1( 0.14047116096453915)
w2 ( 0.2809498312026561 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.38147618449737086) - present_state_Q (0.3829816170414242)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2696952307356627 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13675189284848535) - present_state_Q ( -0.1376695683245565)) * f1( 0.6062256973099442)
w2 ( 0.09445434822138052 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13675189284848535) - present_state_Q (-0.1376695683245565)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2502014426353232 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08347618626003053) - present_state_Q ( -0.08347618626003053)) * f1( 0.5196561867652134)
w2 ( 0.11696206227942217 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08347618626003053) - present_state_Q (-0.08347618626003053)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.23214601829804715 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.052950770629064906) - present_state_Q ( -0.05792968047084847)) * f1( 0.5120151046659703)
w2 ( 0.1381201384838987 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.052950770629064906) - present_state_Q (-0.05792968047084847)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.2193788796902435 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.008969501736537475) - present_state_Q ( 0.008969501736537475)) * f1( 0.4373394374579956)
w2 ( 0.16147433435886802 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.008969501736537475) - present_state_Q (0.008969501736537475)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.20810109355784095 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03897202999698188) - present_state_Q ( 0.03661431614980949)) * f1( 0.42194194567856386)
w2 ( 0.18285696530685913 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03897202999698188) - present_state_Q (0.03661431614980949)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.11759190917144428 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03025654095385781) - present_state_Q ( -0.02433659282990889)) * f1( 0.5884110307919407)
w2 ( 0.287497076141162 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03025654095385781) - present_state_Q (-0.02433659282990889)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.10316772446243225 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04876997751112663) - present_state_Q ( 0.04876997751112663)) * f1( 0.5632092667938503)
w2 ( 0.2977413569507614 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04876997751112663) - present_state_Q (0.04876997751112663)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.2004171093815164 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07506216174676958) - present_state_Q ( 0.07693430210404165)) * f1( 0.3332575958913436)
w2 ( 0.20130271843250996 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07506216174676958) - present_state_Q (0.07693430210404165)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.09163031546876339 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06905331612772125) - present_state_Q ( 0.06905331612772125)) * f1( 0.4850666903175342)
w2 ( 0.30725543757016344 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06905331612772125) - present_state_Q (0.06905331612772125)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08134523385265532 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08134681012219794) - present_state_Q ( 0.08134681012219794)) * f1( 0.45351109720923743)
w2 ( 0.3163269524057643 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08134681012219794) - present_state_Q (0.08134681012219794)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07207675421363013 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09276172332847984) - present_state_Q ( 0.09185413728499255)) * f1( 0.426289802548539)
w2 ( 0.3250238338076785 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09276172332847984) - present_state_Q (0.09185413728499255)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06412097171201514 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10386672523481964) - present_state_Q ( 0.10243457782148344)) * f1( 0.3825776563114629)
w2 ( 0.3333419175957585 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10386672523481964) - present_state_Q (0.10243457782148344)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.19442481947990475 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10308698314487391) - present_state_Q ( 0.10308698314487391)) * f1( 0.2891728744116845)
w2 ( 0.21788045564607905 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10308698314487391) - present_state_Q (0.10308698314487391)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.058383922343100796 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11410164095437017) - present_state_Q ( 0.11464145784487145)) * f1( 0.29156309853502704)
w2 ( 0.3412126658457811 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11410164095437017) - present_state_Q (0.11464145784487145)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.053724235823582156 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12217013627038885) - present_state_Q ( 0.12217013627038885)) * f1( 0.2451861658728585)
w2 ( 0.3488145409400471 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12217013627038885) - present_state_Q (0.12217013627038885)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.19065011868563003 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1323968151448336) - present_state_Q ( 0.13347827531217718)) * f1( 0.20998393782181576)
w2 ( 0.23226136814226356 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1323968151448336) - present_state_Q (0.13347827531217718)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.04953467255903583 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1282563903517508) - present_state_Q ( 0.12738798564390805)) * f1( 0.22592840169879036)
w2 ( 0.3562320470756978 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1282563903517508) - present_state_Q (0.12738798564390805)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.046755526116025035 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1347887496326424) - present_state_Q ( 0.1347887496326424)) * f1( 0.15552882051365036)
w2 ( 0.36337965208892264 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1347887496326424) - present_state_Q (0.1347887496326424)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.045135581782115804 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14191407359100816) - present_state_Q ( 0.14097911406510497)) * f1( 0.09352363525143555)
w2 ( 0.3703081438206825 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14191407359100816) - present_state_Q (0.14097911406510497)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.04145755550817408 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06713244837345002) - present_state_Q ( 0.06713244837345002)) * f1( 0.15351924395559832)
w2 ( 0.3750997597499604 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06713244837345002) - present_state_Q (0.06713244837345002)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03605658208714089 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0657232119254185) - present_state_Q ( 0.0657232119254185)) * f1( 0.224247182705709)
w2 ( 0.3799167419353029 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0657232119254185) - present_state_Q (0.0657232119254185)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.031382889394217545 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06928604208713691) - present_state_Q ( 0.06890353185487812)) * f1( 0.19635295755632334)
w2 ( 0.38467724338237963 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06928604208713691) - present_state_Q (0.06890353185487812)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.024569851044877897 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06798240628124975) - present_state_Q ( 0.06798240628124975)) * f1( 0.2852841968361212)
w2 ( 0.38945356006931714 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06798240628124975) - present_state_Q (0.06798240628124975)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.014241082409067272 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0672918508275625) - present_state_Q ( 0.0672918508275625)) * f1( 0.4313766968689252)
w2 ( 0.394242306754421 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0672918508275625) - present_state_Q (0.0672918508275625)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0054729319615947365 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07367713031991514) - present_state_Q ( 0.0735090001205121)) * f1( 0.37493366564415626)
w2 ( 0.3989194810126506 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07367713031991514) - present_state_Q (0.0735090001205121)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.17593628744988787 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.00047019534810380237) - present_state_Q ( -0.00047019534810380237)) * f1( 0.4897701782130976)
w2 ( 0.2442782951747953 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.00047019534810380237) - present_state_Q (-0.00047019534810380237)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.004072497414990969 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07726758284852879) - present_state_Q ( 0.07726758284852879)) * f1( 0.45977428034170587)
w2 ( 0.3995286645213771 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.07726758284852879) - present_state_Q (0.07726758284852879)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.16165294149669385 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.019683136017616204) - present_state_Q ( -0.027433010093029453)) * f1( 0.43361531741834614)
w2 ( 0.2508663216486911 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.019683136017616204) - present_state_Q (-0.027433010093029453)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.010917227201471546 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07827028276899024) - present_state_Q ( 0.07827028276899024)) * f1( 0.40158408186216493)
w2 ( 0.39611979943153525 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.07827028276899024) - present_state_Q (0.07827028276899024)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03738318939846655 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07489473235231389) - present_state_Q ( 0.07489473235231389)) * f1( 0.39655009959027154)
w2 ( 0.3827716942491936 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07489473235231389) - present_state_Q (0.07489473235231389)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05346913710341428 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06339748846570592) - present_state_Q ( 0.06339748846570592)) * f1( 0.35194563641679266)
w2 ( 0.3736305394568109 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.06339748846570592) - present_state_Q (0.06339748846570592)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04910820040125413 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.058866045593461445) - present_state_Q ( 0.058866045593461445)) * f1( 0.2966208762117462)
w2 ( 0.3765709506361286 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.058866045593461445) - present_state_Q (0.058866045593461445)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.04381317626023572 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06456439044778943) - present_state_Q ( 0.06456439044778943)) * f1( 0.21890029753893758)
w2 ( 0.3814087916080684 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06456439044778943) - present_state_Q (0.06456439044778943)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0396531950635732 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06863114820419591) - present_state_Q ( 0.06863114820419591)) * f1( 0.17461893362799547)
w2 ( 0.38617343094039286 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06863114820419591) - present_state_Q (0.06863114820419591)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03589482310482753 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07092428983491726) - present_state_Q ( 0.07092428983491726)) * f1( 0.1591396694022838)
w2 ( 0.39089679372336433 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07092428983491726) - present_state_Q (0.07092428983491726)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.03333506083868609 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07423904233707142) - present_state_Q ( 0.07423904233707142)) * f1( 0.10977394695870553)
w2 ( 0.39556049096129703 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07423904233707142) - present_state_Q (0.07423904233707142)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.14298991252239437 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0001674340055064366) - present_state_Q ( -0.0001674340055064366)) * f1( 0.6217886401222004)
w2 ( 0.2628723492728893 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.0001674340055064366) - present_state_Q (-0.0001674340055064366)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.030353364127185138 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07483969385081857) - present_state_Q ( 0.07483969385081857)) * f1( 0.1281654880462261)
w2 ( 0.4002133764719823 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07483969385081857) - present_state_Q (0.07483969385081857)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.12489409760541928 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.014870785453318366) - present_state_Q ( 0.014870785453318366)) * f1( 0.6313603013198462)
w2 ( 0.2743370009965699 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.014870785453318366) - present_state_Q (0.014870785453318366)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.10881608437100272 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.034957725843573054) - present_state_Q ( 0.034957725843573054)) * f1( 0.598723846752949)
w2 ( 0.2850785228662013 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.034957725843573054) - present_state_Q (0.034957725843573054)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.041956918940365226 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06678543320104686) - present_state_Q ( 0.06648622775594075)) * f1( 0.4466209241800075)
w2 ( 0.39501722278326556 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06678543320104686) - present_state_Q (0.06648622775594075)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.093804254740874 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0499767283482461) - present_state_Q ( 0.0499767283482461)) * f1( 0.5886508522016226)
w2 ( 0.2952793606456644 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0499767283482461) - present_state_Q (0.0499767283482461)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08363509388687779 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12717686618552027) - present_state_Q ( 0.126062203415792)) * f1( 0.5448091146055248)
w2 ( 0.30647868963783 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12717686618552027) - present_state_Q (0.126062203415792)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06889112024637979 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13638898546887154) - present_state_Q ( 0.13638898546887154)) * f1( 0.5152404940687124)
w2 ( 0.3741072193063862 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.13638898546887154) - present_state_Q (0.13638898546887154)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.08305727102104817 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11867997316371912) - present_state_Q ( 0.11773627761243127)) * f1( 0.46314546774698306)
w2 ( 0.3618724880945438 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.11867997316371912) - present_state_Q (0.11773627761243127)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.07501793144459513 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1420286188407461) - present_state_Q ( 0.1420286188407461)) * f1( 0.5004907987377706)
w2 ( 0.3168091442204297 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1420286188407461) - present_state_Q (0.1420286188407461)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09077316706755803 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11297203737175018) - present_state_Q ( 0.11297203737175018)) * f1( 0.38259092160654445)
w2 ( 0.3538054947491608 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.11297203737175018) - present_state_Q (0.11297203737175018)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.0907301255392825 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10974499686371006) - present_state_Q ( 0.10974499686371006)) * f1( 0.35007262677421025)
w2 ( 0.35385467486206723 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.10974499686371006) - present_state_Q (0.10974499686371006)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.09863497182331021 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1057151457764055) - present_state_Q ( 0.10457260980299993)) * f1( 0.40746400296581486)
w2 ( 0.3460946310530529 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.1057151457764055) - present_state_Q (0.10457260980299993)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.06801119026953177 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1569700505996461) - present_state_Q ( 0.1569700505996461)) * f1( 0.4414336052050342)
w2 ( 0.32633276148804885 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1569700505996461) - present_state_Q (0.1569700505996461)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06336403888931597 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.173104896991319) - present_state_Q ( 0.17377945134342715)) * f1( 0.32377327116515664)
w2 ( 0.33494462378939116 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.173104896991319) - present_state_Q (0.17377945134342715)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09708804116232678 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17200588866746558) - present_state_Q ( 0.17307661010586894)) * f1( 0.3505873006980528)
w2 ( 0.34874206977870553 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.17200588866746558) - present_state_Q (0.17307661010586894)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09290821653104614 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18089523454218587) - present_state_Q ( 0.17988266288988153)) * f1( 0.3024324996757214)
w2 ( 0.3570344814125658 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18089523454218587) - present_state_Q (0.17988266288988153)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06019127503138217 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18642130463897313) - present_state_Q ( 0.18582970202492485)) * f1( 0.23889058390282267)
w2 ( 0.3429133694957295 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18642130463897313) - present_state_Q (0.18582970202492485)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0895627363992977 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19010778209914286) - present_state_Q ( 0.19010778209914286)) * f1( 0.2595347069259379)
w2 ( 0.3647686611792121 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19010778209914286) - present_state_Q (0.19010778209914286)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05722044860046393 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19171800907820194) - present_state_Q ( 0.19171800907820194)) * f1( 0.23309047053615145)
w2 ( 0.35056059700550657 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19171800907820194) - present_state_Q (0.19171800907820194)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08742696387155009 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20260329638321967) - present_state_Q ( 0.20260329638321967)) * f1( 0.18152527466138377)
w2 ( 0.3718280831745182 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20260329638321967) - present_state_Q (0.20260329638321967)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.05509707531828135 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20019639168554187) - present_state_Q ( 0.20019639168554187)) * f1( 0.17720879101391546)
w2 ( 0.3577499918544873 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20019639168554187) - present_state_Q (0.20019639168554187)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08419677783797087 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19962846504065698) - present_state_Q ( 0.19962846504065698)) * f1( 0.2684341743644938)
w2 ( 0.37904814606232273 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19962846504065698) - present_state_Q (0.19962846504065698)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.08160062696390336 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2080437872631144) - present_state_Q ( 0.2080437872631144)) * f1( 0.2302356559485464)
w2 ( 0.38581378155011453 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2080437872631144) - present_state_Q (0.2080437872631144)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.07995451787010886 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21849219665616698) - present_state_Q ( 0.21849219665616698)) * f1( 0.15926436790308823)
w2 ( 0.39201520293068154 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21849219665616698) - present_state_Q (0.21849219665616698)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.03739720502731386 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.03520438724652057) - present_state_Q ( 0.03520438724652057)) * f1( 0.6596649806621828)
w2 ( 0.36311631288404994 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.03520438724652057) - present_state_Q (0.03520438724652057)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0763347785660619 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.009383026787279546) - present_state_Q ( -0.009383026787279546)) * f1( 0.11735455402936533)
w2 ( 0.39201520293068154 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.009383026787279546) - present_state_Q (-0.009383026787279546)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.021146688044463305 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.04927476110855984) - present_state_Q ( 0.048887732075584764)) * f1( 0.6346872843542579)
w2 ( 0.3682371077647554 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.04927476110855984) - present_state_Q (0.048887732075584764)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0711187177003789 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06209252954811653) - present_state_Q ( 0.06209252954811653)) * f1( 0.2136707716248143)
w2 ( 0.39689753739881545 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06209252954811653) - present_state_Q (0.06209252954811653)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06723046190150117 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06878918621262353) - present_state_Q ( 0.06781252149466013)) * f1( 0.16264334283745582)
w2 ( 0.4016788653413475 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06878918621262353) - present_state_Q (0.06781252149466013)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.06417041289830606 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.0717969690857946) - present_state_Q ( 0.07160280661843428)) * f1( 0.12989597576512008)
w2 ( 0.4063904031471504 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.0717969690857946) - present_state_Q (0.07160280661843428)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.08707709435577646 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1430721862531537) - present_state_Q ( 0.14236644094217818)) * f1( 0.3146266231554038)
w2 ( 0.3772680342544759 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1430721862531537) - present_state_Q (0.14236644094217818)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.014833821012174887 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20901122913052833) - present_state_Q ( 0.20901122913052833)) * f1( 0.5642035056855503)
w2 ( 0.3749505013917068 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20901122913052833) - present_state_Q (0.20901122913052833)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09515217644371446 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20135723219009277) - present_state_Q ( 0.20135723219009277)) * f1( 0.2871431178035644)
w2 ( 0.3603947437162109 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.20135723219009277) - present_state_Q (0.20135723219009277)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.00932769090548109 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21715860493356326) - present_state_Q ( 0.21715860493356326)) * f1( 0.5266138707652871)
w2 ( 0.3812239367252944 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21715860493356326) - present_state_Q (0.21715860493356326)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.00478557533246951 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.224500191240664) - present_state_Q ( 0.2244127943627763)) * f1( 0.46330519698728057)
w2 ( 0.3871061702109718 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.224500191240664) - present_state_Q (0.2244127943627763)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.0017185552937772613 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23033741290854878) - present_state_Q ( 0.15301176317034146)) * f1( 0.3825464624130283)
w2 ( 0.39390704933579235 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23033741290854878) - present_state_Q (0.15301176317034146)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.005827598612761541 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.23716007409389878) - present_state_Q ( 0.23716007409389878)) * f1( 0.4747269380144128)
w2 ( 0.3991004053347218 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.23716007409389878) - present_state_Q (0.23716007409389878)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.01036288601722536 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24264477139133134) - present_state_Q ( 0.2427006291357426)) * f1( 0.556041373167594)
w2 ( 0.40399423621492525 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.24264477139133134) - present_state_Q (0.2427006291357426)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09436292301807525 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18920260228325936) - present_state_Q ( 0.19014149817888926)) * f1( 0.2742485671494179)
w2 ( 0.3621214694391771 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.18920260228325936) - present_state_Q (0.19014149817888926)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.02010113497675612 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.24862425796597967) - present_state_Q ( 0.16803245723578533)) * f1( 0.620943117498276)
w2 ( 0.41026743495735774 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.24862425796597967) - present_state_Q (0.16803245723578533)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( 0.02449987101946772 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.25929801302200556) - present_state_Q ( 0.2594633508204686)) * f1( 0.6617979463068477)
w2 ( 0.41425542198626164 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.25929801302200556) - present_state_Q (0.2594633508204686)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.02993282079987624 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2733538501386784) - present_state_Q ( 0.2730920335932834)) * f1( 1.0015881464040235)
w2 ( 0.4175100230714967 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2733538501386784) - present_state_Q (0.2730920335932834)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09154290738810193 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1954120964602547) - present_state_Q ( 0.19577267427940087)) * f1( 0.227845923975745)
w2 ( 0.36954758156117457 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1954120964602547) - present_state_Q (0.19577267427940087)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.033752670478900945 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27083272836663697) - present_state_Q ( 0.27083272836663697)) * f1( 0.6790778142707796)
w2 ( 0.4208850557396983 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.27083272836663697) - present_state_Q (0.27083272836663697)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.03729209807971143 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2754718699692165) - present_state_Q ( 0.2754718699692165)) * f1( 0.6796747101755398)
w2 ( 0.42400957476136064 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2754718699692165) - present_state_Q (0.2754718699692165)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.0918591472363703 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.347525392123793) - present_state_Q ( 0.3485198273930108)) * f1( 0.22970380522234515)
w2 ( 0.3681708527431114 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.347525392123793) - present_state_Q (0.3485198273930108)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04056979758237731 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.27975378600305967) - present_state_Q ( 0.27975378600305967)) * f1( 0.6797161450144763)
w2 ( 0.4269028703171954 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.27975378600305967) - present_state_Q (0.27975378600305967)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04360372187473126 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2836913799802222) - present_state_Q ( 0.2836913799802222)) * f1( 0.6790681598537707)
w2 ( 0.4295835357982634 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2836913799802222) - present_state_Q (0.2836913799802222)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09215450258599887 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3513956835855633) - present_state_Q ( 0.3504487564333782)) * f1( 0.19292685424273553)
w2 ( 0.3666399339356292 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3513956835855633) - present_state_Q (0.3504487564333782)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( 0.04641412209444578 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2873804478680463) - present_state_Q ( 0.2873804478680463)) * f1( 0.6795366339188402)
w2 ( 0.4320649916133889 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2873804478680463) - present_state_Q (0.2873804478680463)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.04901711948301896 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2908003152894457) - present_state_Q ( 0.2908003152894457)) * f1( 0.6799939091208017)
w2 ( 0.4343617745877588 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2908003152894457) - present_state_Q (0.2908003152894457)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.09147044471756378 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3549613808082914) - present_state_Q ( 0.2816127843708831)) * f1( 0.12695161331593752)
w2 ( 0.3709506022324249 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3549613808082914) - present_state_Q (0.2816127843708831)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.05110442602752243 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31058014068077805) - present_state_Q ( 0.31058014068077805)) * f1( 1.0192984911206673)
w2 ( 0.4355904469909968 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.31058014068077805) - present_state_Q (0.31058014068077805)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.053499143169433536 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.31322018599139645) - present_state_Q ( 0.29608304770561866)) * f1( 0.6795650046498378)
w2 ( 0.43770478524460804 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.31322018599139645) - present_state_Q (0.29608304770561866)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.055600173549114555 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2990108322976775) - present_state_Q ( 0.2990108322976775)) * f1( 0.6801596996735211)
w2 ( 0.43955820030053344 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2990108322976775) - present_state_Q (0.2990108322976775)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05754584772352285 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.30153191906444027) - present_state_Q ( 0.30153191906444027)) * f1( 0.6798000162846988)
w2 ( 0.44127547667105366 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.30153191906444027) - present_state_Q (0.30153191906444027)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.05934781822082205 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3038677525260804) - present_state_Q ( 0.3038677525260804)) * f1( 0.6795010946978262)
w2 ( 0.44286661803464533 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3038677525260804) - present_state_Q (0.3038677525260804)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( 0.055681752693407346 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3941302826574099) - present_state_Q ( 0.39408752897591476)) * f1( 0.6705256526892296)
w2 ( 0.43849265797783143 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3941302826574099) - present_state_Q (0.39408752897591476)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.052586525946781586 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38784966986210406) - present_state_Q ( 0.3867363162822213)) * f1( 0.6454931492163982)
w2 ( 0.43465655003415055 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.38784966986210406) - present_state_Q (0.3867363162822213)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04951686321384317 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38348705094620195) - present_state_Q ( 0.38348705094620195)) * f1( 0.6800565406255018)
w2 ( 0.431045482366024 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.38348705094620195) - present_state_Q (0.38348705094620195)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04672824075175337 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37816759323585714) - present_state_Q ( 0.3786515931627364)) * f1( 0.6829028552128407)
w2 ( 0.427778695658892 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.37816759323585714) - present_state_Q (0.3786515931627364)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04431300867565673 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.37393489799622986) - present_state_Q ( 0.37348947011846173)) * f1( 0.6691138610899864)
w2 ( 0.4248910172333849 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.37393489799622986) - present_state_Q (0.37348947011846173)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.04206392939372518 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.370062485586482) - present_state_Q ( 0.370062485586482)) * f1( 0.6803797056627469)
w2 ( 0.4222465182711582 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.370062485586482) - present_state_Q (0.370062485586482)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.040091906093419706 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3660063626509816) - present_state_Q ( 0.3660063626509816)) * f1( 0.6706256034716306)
w2 ( 0.4198940601602876 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3660063626509816) - present_state_Q (0.3660063626509816)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( 0.038320856620901185 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.36314500601958916) - present_state_Q ( 0.3627623384475278)) * f1( 0.6696386611487184)
w2 ( 0.41777823313264206 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.36314500601958916) - present_state_Q (0.3627623384475278)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.022077216014074454 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3726119521795667) - present_state_Q ( 0.3593239779194252)) * f1( 0.6550321059268955)
w2 ( 0.34401321051652456 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.3726119521795667) - present_state_Q (0.3593239779194252)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.018185534349053952 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.26192952763818717) - present_state_Q ( 0.26185622228249544)) * f1( 0.6048926695381645)
w2 ( 0.34916014895503045 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.26192952763818717) - present_state_Q (0.26185622228249544)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.014759350995945402 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2688217242034432) - present_state_Q ( 0.26863172493179754)) * f1( 0.5881814648346194)
w2 ( 0.3538201847541142 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2688217242034432) - present_state_Q (0.26863172493179754)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.012148154982819032 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2756345476540179) - present_state_Q ( 0.2756345476540179)) * f1( 0.5028405484300943)
w2 ( 0.3579744973230249 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.2756345476540179) - present_state_Q (0.2756345476540179)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.010788271769937855 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13975522398531673) - present_state_Q ( 0.0680015465464726)) * f1( 0.29579412867340044)
w2 ( 0.3588939768400661 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.13975522398531673) - present_state_Q (0.0680015465464726)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.07984819793632311 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.034643615980960794) - present_state_Q ( 0.034643615980960794)) * f1( 0.43234188472170637)
w2 ( 0.3763270171447676 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.034643615980960794) - present_state_Q (0.034643615980960794)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.009447692761751178 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.3560753722806345) - present_state_Q ( 0.28429657691262133)) * f1( 0.2612656243315771)
w2 ( 0.36299885366530144 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.3560753722806345) - present_state_Q (0.28429657691262133)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.011572005122867551 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.433369446023779) - present_state_Q ( 0.433369446023779)) * f1( 0.23594949907849774)
w2 ( 0.3521949534947333 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.433369446023779) - present_state_Q (0.433369446023779)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.012890458934440454 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.4205831874068095) - present_state_Q ( 0.42069370390231176)) * f1( 0.16766673284080605)
w2 ( 0.3427587072753376 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.4205831874068095) - present_state_Q (0.42069370390231176)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.014946037516771616 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.47771745553208134) - present_state_Q ( 0.47782476331463586)) * f1( 0.15805696920481788)
w2 ( 0.3245512847887377 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.47771745553208134) - present_state_Q (0.47782476331463586)) * f2(1.4000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.06952189948367894 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.044365686920880325) - present_state_Q ( 0.04364832201167891)) * f1( 0.39596487127346847)
w2 ( 0.3815427820783758 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.044365686920880325) - present_state_Q (0.04364832201167891)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.015444822113479808 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.38809231228022656) - present_state_Q ( 0.3879443261373687)) * f1( 0.10151289981803176)
w2 ( 0.3186550733996162 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.38809231228022656) - present_state_Q (0.3879443261373687)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.060714736279019806 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.052221196323534225) - present_state_Q ( 0.052117359291377445)) * f1( 0.347965134784283)
w2 ( 0.38660487728519527 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.052221196323534225) - present_state_Q (0.052117359291377445)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05572805943012127 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.062320392988180665) - present_state_Q ( -0.013762383857111326)) * f1( 0.22667287549212278)
w2 ( 0.38660487728519527 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.062320392988180665) - present_state_Q (-0.013762383857111326)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.053251529365095876 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06684943513155772) - present_state_Q ( 0.06741153099463232)) * f1( 0.17781786345588477)
w2 ( 0.38939034553556573 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.06684943513155772) - present_state_Q (0.06741153099463232)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.0038848347760294857 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.005683127906847959) - present_state_Q ( -0.005848462413349909)) * f1( 0.3786681627265577)
w2 ( 0.3186550733996162 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.005683127906847959) - present_state_Q (-0.005848462413349909)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.050121705172417624 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.070394230143995) - present_state_Q ( 0.07082235849439085)) * f1( 0.13249780235132586)
w2 ( 0.3941146868259659 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.070394230143995) - present_state_Q (0.07082235849439085)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.006011922998684716 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.062155693515742544) - present_state_Q ( 0.062155693515742544)) * f1( 0.4055053187592094)
w2 ( 0.3235362709163328 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.062155693515742544) - present_state_Q (0.062155693515742544)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.016768163957298014 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.06740918136056956) - present_state_Q ( 0.06740918136056956)) * f1( 0.44942810776088266)
w2 ( 0.3283229056518425 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.06740918136056956) - present_state_Q (0.06740918136056956)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.02916987159614705 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07459482718075586) - present_state_Q ( 0.07459482718075586)) * f1( 0.5325714892297811)
w2 ( 0.33298019876258894 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.07459482718075586) - present_state_Q (0.07459482718075586)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.041250496169817144 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.08218629918447297) - present_state_Q ( 0.08218629918447297)) * f1( 0.5344644518083664)
w2 ( 0.33750084537726843 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.08218629918447297) - present_state_Q (0.08218629918447297)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.05352612741329041 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.09048882050103202) - present_state_Q ( 0.09069027279104488)) * f1( 0.5621775704254275)
w2 ( 0.3418680175624496 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.09048882050103202) - present_state_Q (0.09069027279104488)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.06608054455358332 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.10043252200508906) - present_state_Q ( 0.10043252200508906)) * f1( 0.5989396214873373)
w2 ( 0.346060232166358 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.10043252200508906) - present_state_Q (0.10043252200508906)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.07946309953041196 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11401351660098899) - present_state_Q ( 0.11401351660098899)) * f1( 0.6779827628597827)
w2 ( 0.3500079888675402 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.11401351660098899) - present_state_Q (0.11401351660098899)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.0924057536236047 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.12490479088884669) - present_state_Q ( 0.12479620557628986)) * f1( 0.6895604139102443)
w2 ( 0.3537618743377921 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.12490479088884669) - present_state_Q (0.12479620557628986)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.10477040332366046 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.13479055149189073) - present_state_Q ( 0.13464039341832143)) * f1( 0.6913857205363786)
w2 ( 0.35733864757240946 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.13479055149189073) - present_state_Q (0.13464039341832143)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11621440433599017 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.14402821242430902) - present_state_Q ( 0.14035734577643494)) * f1( 0.6575293601680312)
w2 ( 0.36081955708172936 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.14402821242430902) - present_state_Q (0.14035734577643494)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.12750216933810976 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1529811497039567) - present_state_Q ( 0.1529811497039567)) * f1( 0.6954149853400121)
w2 ( 0.36406589638705816 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1529811497039567) - present_state_Q (0.1529811497039567)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.13826001350015454 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.16151427251958006) - present_state_Q ( 0.16151427251958006)) * f1( 0.6956830123176272)
w2 ( 0.36715863948170574 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.16151427251958006) - present_state_Q (0.16151427251958006)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.14851639331428157 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1697502396031692) - present_state_Q ( 0.1697502396031692)) * f1( 0.6966476370749116)
w2 ( 0.3701031351688487 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1697502396031692) - present_state_Q (0.1697502396031692)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.1582874454166586 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.17644032076180605) - present_state_Q ( 0.17776366331186233)) * f1( 0.698529192387252)
w2 ( 0.3729007425441351 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.17644032076180605) - present_state_Q (0.17776366331186233)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16758504613728473 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.18430596394843668) - present_state_Q ( 0.18430596394843668)) * f1( 0.6932060540289813)
w2 ( 0.3755832351930632 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.18430596394843668) - present_state_Q (0.18430596394843668)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.17644556247684853 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1904284794829328) - present_state_Q ( 0.19197117852574652)) * f1( 0.6972849557913855)
w2 ( 0.3781246685815141 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.1904284794829328) - present_state_Q (0.19197117852574652)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.18487695540578625 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19704644293587367) - present_state_Q ( 0.19355290369825884)) * f1( 0.6683532775012654)
w2 ( 0.3806477033934207 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.19704644293587367) - present_state_Q (0.19355290369825884)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.19293013261265357 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.20440319135721913) - present_state_Q ( 0.20407028072473377)) * f1( 0.6920318422879292)
w2 ( 0.38297510416164043 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.20440319135721913) - present_state_Q (0.20407028072473377)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.052572613481268876 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07134194430901565) - present_state_Q ( 0.07134194430901565)) * f1( 0.14925655522778164)
w2 ( 0.3908305318284036 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( 0.07134194430901565) - present_state_Q (0.07134194430901565)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.20060087586310346 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21088974534908356) - present_state_Q ( 0.21088974534908356)) * f1( 0.6960795739791429)
w2 ( 0.38517908874535695 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21088974534908356) - present_state_Q (0.21088974534908356)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2079171943762129 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.21589773946164953) - present_state_Q ( 0.21589773946164953)) * f1( 0.6922298874075804)
w2 ( 0.38729292943504723 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( 0.21589773946164953) - present_state_Q (0.21589773946164953)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.20800326414419107 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22115392619386454) - present_state_Q ( 0.2208675357248997)) * f1( 0.6897406934916643)
w2 ( 0.387317886572937 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.22115392619386454) - present_state_Q (0.2208675357248997)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.2080500873171693 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22147076491424567) - present_state_Q ( 0.22147076491424567)) * f1( 0.6923313833182456)
w2 ( 0.3873314128044806 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( 0.22147076491424567) - present_state_Q (0.22147076491424567)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.16647557223855602 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.22173204729080281) - present_state_Q ( 0.22173204729080281)) * f1( 0.6934184291399775)
w2 ( 0.3753402359532461 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.22173204729080281) - present_state_Q (0.22173204729080281)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.11319388621418836 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.190312281603495) - present_state_Q ( 0.19010388795221814)) * f1( 0.6910073304732357)
w2 ( 0.35991878275740874 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.190312281603495) - present_state_Q (0.19010388795221814)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.062198020279444835 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.1504744833183736) - present_state_Q ( 0.1504744833183736)) * f1( 0.6934184291399775)
w2 ( 0.34521024205767803 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.1504744833183736) - present_state_Q (0.1504744833183736)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( 0.01358981791255736 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.11225571388787159) - present_state_Q ( 0.11217408853445829)) * f1( 0.6934632312915744)
w2 ( 0.3311912717147646 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.11225571388787159) - present_state_Q (0.11217408853445829)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.05916486950169856 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2295320759127484) - present_state_Q ( 0.2289897446028337)) * f1( 0.14921813747194013)
w2 ( 0.35775249460527236 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( 0.2295320759127484) - present_state_Q (0.2289897446028337)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.10218906834630985 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.362673700221152) - present_state_Q ( 0.362673700221152)) * f1( 0.53557983411427)
w2 ( 0.29818989880849994 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.362673700221152) - present_state_Q (0.362673700221152)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.14189051995427251 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.2489574912801729) - present_state_Q ( 0.2489574912801729)) * f1( 0.48177763360639236)
w2 ( 0.21578372459328438 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.2489574912801729) - present_state_Q (0.2489574912801729)) * f2(1.0)
============================================================================
GUIDE learning . . .
w1 ( -0.17633799349313262 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.19642544645493426) - present_state_Q ( 0.19598122147469252)) * f1( 0.44371708594442266)
w2 ( 0.12262308337378047 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.19642544645493426) - present_state_Q (0.19598122147469252)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.20274564403180162 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(0.07763146893846862) - present_state_Q ( 0.07763146893846862)) * f1( 0.39422151592518395)
w2 ( 0.04223888472842585 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( 0.07763146893846862) - present_state_Q (0.07763146893846862)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.22306838690086767 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.02012317648979279) - present_state_Q ( -0.02012317648979279)) * f1( 0.34925454749990564)
w2 ( -0.027587812210676535 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.02012317648979279) - present_state_Q (-0.02012317648979279)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.24014829545621197 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10464934636812259) - present_state_Q ( -0.1090972638807165)) * f1( 0.34066633234621313)
w2 ( -0.08775193270140805 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.10464934636812259) - present_state_Q (-0.1090972638807165)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2510714399925152 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.14589711342896505) - present_state_Q ( -0.16344749996924668)) * f1( 0.24212197974213415)
w2 ( -0.14188899806624605 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.14589711342896505) - present_state_Q (-0.16344749996924668)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2589221349576955 ) += alpha ( 0.1 ) * (reward ( -0.6 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.21921247295229962) - present_state_Q ( -0.21921247295229962)) * f1( 0.1949472041673217)
w2 ( -0.1902140509873977 ) += alpha ( 0.1) * (reward ( -0.6) + discount_factor ( 0.1) * next_state_max_Q( -0.21921247295229962) - present_state_Q (-0.21921247295229962)) * f2(1.2000000000000002)
============================================================================
GUIDE learning . . .
w1 ( -0.2580738684969748 ) += alpha ( 0.1 ) * (reward ( -0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1851376435006376) - present_state_Q ( -0.1851376435006376)) * f1( 0.12732168578829964)
w2 ( -0.18488414065535178 ) += alpha ( 0.1) * (reward ( -0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.1851376435006376) - present_state_Q (-0.1851376435006376)) * f2(0.8)
============================================================================
GUIDE learning . . .
w1 ( -0.25378390226113995 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09628927449343508) - present_state_Q ( -0.09628927449343508)) * f1( 0.22982740061130988)
w2 ( -0.18115093371446994 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.09628927449343508) - present_state_Q (-0.09628927449343508)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26486355352246843 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13827319242151156) - present_state_Q ( -0.13827319242151156)) * f1( 0.40208620314150895)
w2 ( -0.18666201625088275 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.13827319242151156) - present_state_Q (-0.13827319242151156)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.25337940328198766 ) += alpha ( 0.1 ) * (reward ( 0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13258660492355892) - present_state_Q ( -0.13258660492355892)) * f1( 0.3596349909475255)
w2 ( -0.1802754573622587 ) += alpha ( 0.1) * (reward ( 0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.13258660492355892) - present_state_Q (-0.13258660492355892)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24010090048379118 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11874688393988926) - present_state_Q ( -0.11874688393988926)) * f1( 0.3263556208450348)
w2 ( -0.1721380134513407 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11874688393988926) - present_state_Q (-0.11874688393988926)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.25032460199042034 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1179708571511087) - present_state_Q ( -0.1179708571511087)) * f1( 0.3479506086503845)
w2 ( -0.17801453802262074 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.1179708571511087) - present_state_Q (-0.1179708571511087)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.25905342130874426 ) += alpha ( 0.1 ) * (reward ( -0.4 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.10570537116620612) - present_state_Q ( -0.10776196969045845)) * f1( 0.2882619667111096)
w2 ( -0.18407070937114398 ) += alpha ( 0.1) * (reward ( -0.4) + discount_factor ( 0.1) * next_state_max_Q( -0.10570537116620612) - present_state_Q (-0.10776196969045845)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.26161603736633887 ) += alpha ( 0.1 ) * (reward ( -0.2 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09231441033612142) - present_state_Q ( -0.09485567072517079)) * f1( 0.22405235398056034)
w2 ( -0.18635822477731281 ) += alpha ( 0.1) * (reward ( -0.2) + discount_factor ( 0.1) * next_state_max_Q( -0.09231441033612142) - present_state_Q (-0.09485567072517079)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2589652867975565 ) += alpha ( 0.1 ) * (reward ( 0.1 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.0780136443174902) - present_state_Q ( -0.0780136443174902)) * f1( 0.1557320406354788)
w2 ( -0.182953979179598 ) += alpha ( 0.1) * (reward ( 0.1) + discount_factor ( 0.1) * next_state_max_Q( -0.0780136443174902) - present_state_Q (-0.0780136443174902)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2565041263538494 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05483475698068514) - present_state_Q ( -0.05483475698068514)) * f1( 0.0704494466048941)
w2 ( -0.17596695355394565 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05483475698068514) - present_state_Q (-0.05483475698068514)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.24850531308398266 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.11631829973624987) - present_state_Q ( -0.1205600950529276)) * f1( 0.195604313835228)
w2 ( -0.15960982295077356 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.11631829973624987) - present_state_Q (-0.1205600950529276)) * f2(0.4)
============================================================================
GUIDE learning . . .
w1 ( -0.24211582975078422 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.13355333120253063) - present_state_Q ( -0.13355333120253063)) * f1( 0.15205887135015167)
w2 ( -0.1343979430658369 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.13355333120253063) - present_state_Q (-0.13355333120253063)) * f2(0.6000000000000001)
============================================================================
GUIDE learning . . .
w1 ( -0.230590470116701 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.09433032299627853) - present_state_Q ( -0.09858590738060746)) * f1( 0.2961653471449973)
w2 ( -0.12661488556421732 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.09433032299627853) - present_state_Q (-0.09858590738060746)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.221686342191737 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08045590364513219) - present_state_Q ( -0.08045590364513219)) * f1( 0.23909455800313925)
w2 ( -0.11916667929860494 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08045590364513219) - present_state_Q (-0.08045590364513219)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.21331411618366342 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07420767664276656) - present_state_Q ( -0.07440758228468132)) * f1( 0.22813424555139514)
w2 ( -0.11182694300619685 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07420767664276656) - present_state_Q (-0.07440758228468132)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.20726840104706057 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.06180539412100402) - present_state_Q ( -0.05892541974462349)) * f1( 0.17139058491518647)
w2 ( -0.10477204539954639 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.06180539412100402) - present_state_Q (-0.05892541974462349)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.2024480734670596 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.04929510375564425) - present_state_Q ( -0.04991508627423103)) * f1( 0.13972548178121078)
w2 ( -0.09787233388157306 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.04929510375564425) - present_state_Q (-0.04991508627423103)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.18658323027343665 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08525499091772248) - present_state_Q ( -0.08525499091772248)) * f1( 0.4211202875763318)
w2 ( -0.09787233388157306 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08525499091772248) - present_state_Q (-0.08525499091772248)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.16696304169389953 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.1093381985888854) - present_state_Q ( -0.11106452525832045)) * f1( 0.49034448780808265)
w2 ( -0.08986971977358442 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.1093381985888854) - present_state_Q (-0.11106452525832045)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.15124019427374027 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.08731491973128554) - present_state_Q ( -0.08731491973128554)) * f1( 0.41530733432428973)
w2 ( -0.08229805121842128 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.08731491973128554) - present_state_Q (-0.08731491973128554)) * f2(0.2)
============================================================================
GUIDE learning . . .
w1 ( -0.13953411196460352 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.05116176129692924) - present_state_Q ( -0.05116176129692924)) * f1( 0.33828151003514295)
w2 ( -0.08229805121842128 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.05116176129692924) - present_state_Q (-0.05116176129692924)) * f2(0.0)
============================================================================
GUIDE learning . . .
w1 ( -0.12560032745487174 ) += alpha ( 0.1 ) * (reward ( 0.3 ) + discount_factor ( 0.1 ) * next_state_max_Q(-0.07001756069874183) - present_state_Q ( -0.07001756069874183)) * f1( 0.3838341012170841)
w2 ( -0.07503773512584393 ) += alpha ( 0.1) * (reward ( 0.3) + discount_factor ( 0.1) * next_state_max_Q( -0.07001756069874183) - present_state_Q (-0.07001756069874183)) * f2(0.2)
============================================================================
